{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a948bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "data_train = torch.from_numpy(np.loadtxt('two_spiral_train_data.txt')).float()\n",
    "data_test = torch.from_numpy(np.loadtxt('two_spiral_test_data.txt')).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2b929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiralNet:\n",
    "    def __init__(self, hidden, batch, r, epoch):\n",
    "        # 训练参数\n",
    "        self.batch_size = batch\n",
    "        self.input_num = 2\n",
    "        self.hidden_num = hidden\n",
    "        self.output_num = 2\n",
    "        self.lr = r\n",
    "        self.epoch_num = epoch\n",
    "        # 参数\n",
    "        self.u1 = torch.normal(0, 0.01, size = (self.input_num, self.hidden_num), requires_grad = True)\n",
    "        self.v1 = torch.normal(0, 0.01, size = (self.input_num, self.hidden_num), requires_grad = True)\n",
    "        self.b1 = torch.zeros(self.hidden_num, requires_grad=True)\n",
    "        self.u2 = torch.normal(0, 0.01, size = (self.hidden_num, self.output_num), requires_grad = True)\n",
    "        self.v2 = torch.normal(0, 0.01, size = (self.hidden_num, self.output_num), requires_grad = True)\n",
    "        self.b2 = torch.zeros(self.output_num, requires_grad=True)\n",
    "        \n",
    "    def load_data(self, data_ori, is_train = True):\n",
    "        data_in = data_ori[:, 0:2]\n",
    "        data_out = data_ori[:, 2]\n",
    "        return data.DataLoader(data.TensorDataset(data_in, data_out), self.batch_size, shuffle = is_train)\n",
    "        \n",
    "    def quadratic_layer(self, u, v, b, x):\n",
    "        return torch.matmul(x**2, u) + torch.matmul(x, v) + b    \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        x_exp = torch.exp(-x)\n",
    "        return 1 / (1 + x_exp)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_max = torch.max(x)\n",
    "        x_exp = torch.exp(x - x_max)\n",
    "        partition = x_exp.sum(1, keepdim=True)\n",
    "        return x_exp / partition\n",
    "    \n",
    "    def mlqp(self, x, u1, v1, b1, u2, v2, b2):\n",
    "        out1 = self.sigmoid(self.quadratic_layer(u1, v1, b1, x))\n",
    "        out2 = self.quadratic_layer(u2, v2, b2, out1)\n",
    "        return self.softmax(out2)\n",
    "    \n",
    "    def square_error(self, y_hat, y):\n",
    "        return (y_hat - y)**2\n",
    "    \n",
    "    def cross_entropy(self, y_hat, y):\n",
    "        return -1 * torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "    def sgd(self):\n",
    "        with torch.no_grad():\n",
    "            for param in [self.u1, self.v1, self.b1, self.u2, self.v2, self.b2]:\n",
    "                #print(param.grad)\n",
    "                param -= self.lr * param.grad / self.batch_size\n",
    "                param.grad.zero_()\n",
    "                \n",
    "    def accuracy(self, y_hat, y):\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        cmp = y_hat.type(y.dtype) == y\n",
    "        return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "    def train_spiral(self, train_iter):\n",
    "        accuracy_max = 0\n",
    "        for i in range(self.epoch_num):\n",
    "            accuracy_rate = [0., 0., 0.] # loss, predict right, num\n",
    "            for x, y in train_iter:\n",
    "                y_hat = self.mlqp(x, self.u1, self.v1, self.b1, self.u2, self.v2, self.b2)\n",
    "                #print(y.long())\n",
    "                l = self.cross_entropy(y_hat, y.long())\n",
    "                #print(l)\n",
    "                #updater.zero_grad()\n",
    "                l.sum().backward()\n",
    "                #updater.step()\n",
    "                self.sgd()\n",
    "                accuracy_rate[0] = accuracy_rate[0] + l.sum()\n",
    "                accuracy_rate[1] = accuracy_rate[1] + self.accuracy(y_hat, y)\n",
    "                accuracy_rate[2] = accuracy_rate[2] + y.numel()\n",
    "            accuracy_now = accuracy_rate[1] / accuracy_rate[2]\n",
    "            if accuracy_now > accuracy_max:\n",
    "                accuracy_max = accuracy_now\n",
    "                print('max accuracy:', accuracy_max)\n",
    "                self.u1_max, self.v1_max, self.b1_max, self.u2_max, self.v2_max, self.b2_max = self.u1 * 1, self.v1 * 1, self.b1 * 1, self.u2 * 1, self.v2 * 1, self.b2 * 1\n",
    "            print('loss: {0}, accuracy: {1}'.format(accuracy_rate[0] / accuracy_rate[2], accuracy_rate[1] / accuracy_rate[2]))\n",
    "            \n",
    "            \n",
    "    def plot_decision_boundary(self, x, y_hat, color = ['b', 'r']):\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        col = []\n",
    "        for i in range(0, len(y_hat)):\n",
    "            col.append(color[y_hat[i]])\n",
    "        plt.scatter(x[:, 0], x[:, 1], c = col)\n",
    "        \n",
    "    def evaluate_spiral(self, test_iter, u1, v1, b1, u2, v2, b2):\n",
    "        accuracy_rate = [0., 0.]\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_iter:\n",
    "                y_hat = self.mlqp(x, u1, v1, b1, u2, v2, b2)\n",
    "                self.plot_decision_boundary(x, y_hat)\n",
    "                accuracy_rate[0] = accuracy_rate[0] + self.accuracy(y_hat, y)\n",
    "                accuracy_rate[1] = accuracy_rate[1] + y.numel()\n",
    "        print('accuracy: {0}'.format(accuracy_rate[0] / accuracy_rate[1]))\n",
    "        \n",
    "    def train_evaluate(self, data_train_iter, data_test_iter, u1, v1, b1, u2, v2, b2):\n",
    "        time_begin = time.time()\n",
    "        self.train_spiral(data_train_iter)\n",
    "        time_end = time.time()\n",
    "        print('the training time is:', time_end - time_begin)\n",
    "        self.evaluate_spiral(data_test_iter, u1, v1, b1, u2, v2, b2)\n",
    "        \n",
    "    def create_points(self, low, high, num):\n",
    "        result = []\n",
    "        for x in np.linspace(low, high, num):\n",
    "            for y in np.linspace(low, high, num):\n",
    "                result.append([x, y])\n",
    "        return np.array(result)\n",
    "\n",
    "    def decision_boundary(self, u1, v1, b1, u2, v2, b2):\n",
    "        space = torch.from_numpy(self.create_points(-6, 6, 121)).float()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.mlqp(space, u1, v1, b1, u2, v2, b2)\n",
    "            self.plot_decision_boundary(space, y_hat)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f70667",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = SpiralNet(hidden = 512, batch = 30, r = 0.1, epoch = 1000)\n",
    "data_train_iter = q2.load_data(data_train, True)\n",
    "data_test_iter = q2.load_data(data_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247f28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.5033333333333333\n",
      "loss: 3.425926446914673, accuracy: 0.5033333333333333\n",
      "loss: 2.3668150901794434, accuracy: 0.49\n",
      "loss: 1.9264609813690186, accuracy: 0.4633333333333333\n",
      "loss: 1.6864231824874878, accuracy: 0.48333333333333334\n",
      "max accuracy: 0.55\n",
      "loss: 1.1823276281356812, accuracy: 0.55\n",
      "loss: 1.0993402004241943, accuracy: 0.5133333333333333\n",
      "loss: 1.2686957120895386, accuracy: 0.49666666666666665\n",
      "loss: 1.1709284782409668, accuracy: 0.4766666666666667\n",
      "loss: 0.8960952758789062, accuracy: 0.51\n",
      "loss: 1.1102445125579834, accuracy: 0.49\n",
      "loss: 1.0876898765563965, accuracy: 0.47333333333333333\n",
      "loss: 1.059773325920105, accuracy: 0.49\n",
      "loss: 1.0869866609573364, accuracy: 0.52\n",
      "loss: 0.9436618089675903, accuracy: 0.49\n",
      "loss: 1.0695291757583618, accuracy: 0.5066666666666667\n",
      "loss: 0.9393100738525391, accuracy: 0.49666666666666665\n",
      "loss: 0.8195401430130005, accuracy: 0.49333333333333335\n",
      "loss: 0.7617937326431274, accuracy: 0.5266666666666666\n",
      "loss: 0.879518449306488, accuracy: 0.51\n",
      "loss: 1.0443298816680908, accuracy: 0.49666666666666665\n",
      "loss: 0.8001301288604736, accuracy: 0.5366666666666666\n",
      "loss: 0.8268910646438599, accuracy: 0.5333333333333333\n",
      "loss: 0.8938674926757812, accuracy: 0.4666666666666667\n",
      "loss: 0.8542349338531494, accuracy: 0.5033333333333333\n",
      "loss: 0.9061027765274048, accuracy: 0.5266666666666666\n",
      "loss: 0.8313187956809998, accuracy: 0.4866666666666667\n",
      "loss: 0.8531787991523743, accuracy: 0.5266666666666666\n",
      "loss: 0.8863529562950134, accuracy: 0.5\n",
      "loss: 0.9567636251449585, accuracy: 0.51\n",
      "loss: 1.0891082286834717, accuracy: 0.46\n",
      "loss: 0.8655706644058228, accuracy: 0.53\n",
      "loss: 0.9252536296844482, accuracy: 0.5166666666666667\n",
      "loss: 0.7742261290550232, accuracy: 0.5366666666666666\n",
      "loss: 0.9212368726730347, accuracy: 0.49\n",
      "loss: 0.9389306902885437, accuracy: 0.49666666666666665\n",
      "loss: 0.8701593279838562, accuracy: 0.5\n",
      "loss: 0.8379406332969666, accuracy: 0.49666666666666665\n",
      "loss: 0.7942951917648315, accuracy: 0.51\n",
      "loss: 0.7996387481689453, accuracy: 0.5266666666666666\n",
      "loss: 0.8675492405891418, accuracy: 0.4866666666666667\n",
      "loss: 0.8155653476715088, accuracy: 0.5033333333333333\n",
      "loss: 1.0239295959472656, accuracy: 0.5166666666666667\n",
      "max accuracy: 0.56\n",
      "loss: 0.8002100586891174, accuracy: 0.56\n",
      "loss: 0.7984490990638733, accuracy: 0.5133333333333333\n",
      "loss: 0.7817481160163879, accuracy: 0.53\n",
      "loss: 0.7603521347045898, accuracy: 0.49666666666666665\n",
      "loss: 0.7644547820091248, accuracy: 0.52\n",
      "loss: 0.8970113396644592, accuracy: 0.48333333333333334\n",
      "loss: 0.7859312891960144, accuracy: 0.5033333333333333\n",
      "loss: 0.8836092948913574, accuracy: 0.5066666666666667\n",
      "loss: 0.8607686161994934, accuracy: 0.48333333333333334\n",
      "loss: 0.7542527318000793, accuracy: 0.49333333333333335\n",
      "loss: 0.7586876153945923, accuracy: 0.5266666666666666\n",
      "loss: 0.719443678855896, accuracy: 0.5066666666666667\n",
      "loss: 0.8278442025184631, accuracy: 0.51\n",
      "loss: 0.7682257294654846, accuracy: 0.51\n",
      "loss: 0.7563443183898926, accuracy: 0.49\n",
      "loss: 0.8040598034858704, accuracy: 0.5466666666666666\n",
      "loss: 0.8931773900985718, accuracy: 0.4533333333333333\n",
      "loss: 0.7211499810218811, accuracy: 0.5066666666666667\n",
      "loss: 0.8994991779327393, accuracy: 0.52\n",
      "loss: 0.717031717300415, accuracy: 0.53\n",
      "loss: 0.8062045574188232, accuracy: 0.5333333333333333\n",
      "loss: 0.7822179198265076, accuracy: 0.54\n",
      "loss: 0.8274397850036621, accuracy: 0.5033333333333333\n",
      "loss: 0.7205503582954407, accuracy: 0.52\n",
      "loss: 0.7829648852348328, accuracy: 0.5233333333333333\n",
      "loss: 0.8052207827568054, accuracy: 0.5133333333333333\n",
      "loss: 0.752807080745697, accuracy: 0.5133333333333333\n",
      "loss: 0.7260869145393372, accuracy: 0.5333333333333333\n",
      "loss: 0.7708341479301453, accuracy: 0.52\n",
      "loss: 0.7973177433013916, accuracy: 0.5033333333333333\n",
      "loss: 0.7535165548324585, accuracy: 0.54\n",
      "loss: 0.7477573752403259, accuracy: 0.56\n",
      "loss: 0.7774701714515686, accuracy: 0.5366666666666666\n",
      "loss: 0.7039254307746887, accuracy: 0.53\n",
      "loss: 0.7023546099662781, accuracy: 0.53\n",
      "loss: 0.9235923290252686, accuracy: 0.46\n",
      "loss: 0.7769938707351685, accuracy: 0.49666666666666665\n",
      "loss: 0.7827538847923279, accuracy: 0.5\n",
      "loss: 0.7508993744850159, accuracy: 0.48333333333333334\n",
      "loss: 0.8666590452194214, accuracy: 0.4766666666666667\n",
      "loss: 0.721795380115509, accuracy: 0.5433333333333333\n",
      "loss: 0.7587687969207764, accuracy: 0.5333333333333333\n",
      "loss: 0.9826998114585876, accuracy: 0.42333333333333334\n",
      "loss: 0.8081067800521851, accuracy: 0.5\n",
      "loss: 0.786153256893158, accuracy: 0.5333333333333333\n",
      "loss: 0.8407516479492188, accuracy: 0.46\n",
      "loss: 0.7229290008544922, accuracy: 0.53\n",
      "loss: 0.7432871460914612, accuracy: 0.5433333333333333\n",
      "loss: 0.725408673286438, accuracy: 0.52\n",
      "loss: 0.7691176533699036, accuracy: 0.5333333333333333\n",
      "loss: 0.7800615429878235, accuracy: 0.5\n",
      "loss: 0.8212490081787109, accuracy: 0.4766666666666667\n",
      "loss: 0.82561194896698, accuracy: 0.49\n",
      "loss: 0.7149947881698608, accuracy: 0.5433333333333333\n",
      "loss: 0.8298864364624023, accuracy: 0.4766666666666667\n",
      "loss: 0.7701733112335205, accuracy: 0.5166666666666667\n",
      "loss: 0.7464749217033386, accuracy: 0.5266666666666666\n",
      "loss: 0.7946454882621765, accuracy: 0.49\n",
      "loss: 0.7171488404273987, accuracy: 0.4666666666666667\n",
      "loss: 0.76967453956604, accuracy: 0.5133333333333333\n",
      "loss: 0.8108969330787659, accuracy: 0.49333333333333335\n",
      "loss: 0.7610677480697632, accuracy: 0.5266666666666666\n",
      "loss: 0.7679759860038757, accuracy: 0.5033333333333333\n",
      "loss: 0.7666180729866028, accuracy: 0.5066666666666667\n",
      "loss: 0.7307855486869812, accuracy: 0.51\n",
      "loss: 0.7608888745307922, accuracy: 0.52\n",
      "loss: 0.797210156917572, accuracy: 0.48333333333333334\n",
      "loss: 0.7268888354301453, accuracy: 0.52\n",
      "loss: 0.8383446931838989, accuracy: 0.4666666666666667\n",
      "loss: 0.7439081072807312, accuracy: 0.4766666666666667\n",
      "loss: 0.7301477193832397, accuracy: 0.55\n",
      "loss: 0.7764329314231873, accuracy: 0.5133333333333333\n",
      "loss: 0.8598073124885559, accuracy: 0.4866666666666667\n",
      "loss: 0.7557271122932434, accuracy: 0.49333333333333335\n",
      "loss: 0.7020412087440491, accuracy: 0.56\n",
      "loss: 0.6976464986801147, accuracy: 0.49333333333333335\n",
      "max accuracy: 0.5633333333333334\n",
      "loss: 0.7000039219856262, accuracy: 0.5633333333333334\n",
      "loss: 0.825244128704071, accuracy: 0.4666666666666667\n",
      "loss: 0.7459853291511536, accuracy: 0.5366666666666666\n",
      "loss: 0.8060196042060852, accuracy: 0.49\n",
      "loss: 0.7515156269073486, accuracy: 0.5166666666666667\n",
      "loss: 0.7177175283432007, accuracy: 0.54\n",
      "loss: 0.8260215520858765, accuracy: 0.49333333333333335\n",
      "loss: 0.8447041511535645, accuracy: 0.4633333333333333\n",
      "loss: 0.7294694781303406, accuracy: 0.47\n",
      "loss: 0.7519121766090393, accuracy: 0.48\n",
      "loss: 0.7630729675292969, accuracy: 0.5\n",
      "loss: 0.7177909016609192, accuracy: 0.5633333333333334\n",
      "loss: 0.7278317809104919, accuracy: 0.5433333333333333\n",
      "loss: 0.7735721468925476, accuracy: 0.47\n",
      "loss: 0.723114550113678, accuracy: 0.5266666666666666\n",
      "loss: 0.7760462164878845, accuracy: 0.49333333333333335\n",
      "loss: 0.8187739849090576, accuracy: 0.47333333333333333\n",
      "max accuracy: 0.5833333333333334\n",
      "loss: 0.7332413196563721, accuracy: 0.5833333333333334\n",
      "loss: 0.7052261829376221, accuracy: 0.5133333333333333\n",
      "loss: 0.755192220211029, accuracy: 0.49666666666666665\n",
      "loss: 0.8288344144821167, accuracy: 0.45666666666666667\n",
      "loss: 0.7896955609321594, accuracy: 0.48333333333333334\n",
      "loss: 0.7948051691055298, accuracy: 0.43\n",
      "loss: 0.7207915782928467, accuracy: 0.5333333333333333\n",
      "loss: 0.7785390615463257, accuracy: 0.53\n",
      "loss: 0.7350062131881714, accuracy: 0.4766666666666667\n",
      "loss: 0.8761343359947205, accuracy: 0.49666666666666665\n",
      "loss: 0.8354656100273132, accuracy: 0.49\n",
      "loss: 0.747866690158844, accuracy: 0.49\n",
      "loss: 0.7904790043830872, accuracy: 0.5333333333333333\n",
      "loss: 0.6996263265609741, accuracy: 0.5466666666666666\n",
      "loss: 0.7202782034873962, accuracy: 0.5533333333333333\n",
      "loss: 0.7834835648536682, accuracy: 0.54\n",
      "loss: 0.7516823410987854, accuracy: 0.47\n",
      "loss: 0.7312357425689697, accuracy: 0.5333333333333333\n",
      "loss: 0.7978715896606445, accuracy: 0.5133333333333333\n",
      "loss: 0.7741639018058777, accuracy: 0.5\n",
      "loss: 0.6892481446266174, accuracy: 0.52\n",
      "loss: 0.7418901324272156, accuracy: 0.5133333333333333\n",
      "loss: 0.7224757075309753, accuracy: 0.5166666666666667\n",
      "loss: 0.6888468861579895, accuracy: 0.49666666666666665\n",
      "loss: 0.8014217615127563, accuracy: 0.48333333333333334\n",
      "loss: 0.7265167832374573, accuracy: 0.49333333333333335\n",
      "loss: 0.7744581699371338, accuracy: 0.5233333333333333\n",
      "loss: 0.7086994051933289, accuracy: 0.5533333333333333\n",
      "loss: 0.7559053301811218, accuracy: 0.49\n",
      "loss: 0.71208256483078, accuracy: 0.5333333333333333\n",
      "loss: 0.7139273881912231, accuracy: 0.5033333333333333\n",
      "loss: 0.7462281584739685, accuracy: 0.54\n",
      "loss: 0.7172821164131165, accuracy: 0.48\n",
      "loss: 0.7827256917953491, accuracy: 0.47333333333333333\n",
      "loss: 0.732366681098938, accuracy: 0.5166666666666667\n",
      "loss: 0.7034921646118164, accuracy: 0.5233333333333333\n",
      "loss: 0.7008183598518372, accuracy: 0.52\n",
      "loss: 0.8470944762229919, accuracy: 0.48333333333333334\n",
      "loss: 0.7048050761222839, accuracy: 0.5133333333333333\n",
      "loss: 0.7749025225639343, accuracy: 0.5066666666666667\n",
      "loss: 0.7073262929916382, accuracy: 0.52\n",
      "loss: 0.7111026644706726, accuracy: 0.5366666666666666\n",
      "loss: 0.761426568031311, accuracy: 0.48\n",
      "loss: 0.7968013882637024, accuracy: 0.4866666666666667\n",
      "loss: 0.7241941094398499, accuracy: 0.51\n",
      "loss: 0.7577366232872009, accuracy: 0.4633333333333333\n",
      "loss: 0.7330303192138672, accuracy: 0.4866666666666667\n",
      "loss: 0.7094766497612, accuracy: 0.49666666666666665\n",
      "loss: 0.7261936664581299, accuracy: 0.5433333333333333\n",
      "loss: 0.7304090261459351, accuracy: 0.5566666666666666\n",
      "loss: 0.7121451497077942, accuracy: 0.49\n",
      "loss: 0.680824875831604, accuracy: 0.55\n",
      "loss: 0.8929995894432068, accuracy: 0.52\n",
      "loss: 0.7775880694389343, accuracy: 0.5166666666666667\n",
      "loss: 0.7266057133674622, accuracy: 0.52\n",
      "loss: 0.7394454479217529, accuracy: 0.5333333333333333\n",
      "loss: 0.6982031464576721, accuracy: 0.56\n",
      "loss: 0.7254660725593567, accuracy: 0.5166666666666667\n",
      "loss: 0.7160260081291199, accuracy: 0.5366666666666666\n",
      "loss: 0.704837441444397, accuracy: 0.52\n",
      "loss: 0.7095931172370911, accuracy: 0.5066666666666667\n",
      "loss: 0.7574244141578674, accuracy: 0.5533333333333333\n",
      "loss: 0.7165934443473816, accuracy: 0.49333333333333335\n",
      "loss: 0.6880191564559937, accuracy: 0.5766666666666667\n",
      "loss: 0.7049949169158936, accuracy: 0.5466666666666666\n",
      "loss: 0.6934742331504822, accuracy: 0.5333333333333333\n",
      "loss: 0.7982358932495117, accuracy: 0.5233333333333333\n",
      "loss: 0.6967000961303711, accuracy: 0.55\n",
      "loss: 0.737887978553772, accuracy: 0.52\n",
      "loss: 0.7398380041122437, accuracy: 0.56\n",
      "loss: 0.7151011824607849, accuracy: 0.53\n",
      "loss: 0.7184297442436218, accuracy: 0.5066666666666667\n",
      "loss: 0.7245593070983887, accuracy: 0.54\n",
      "loss: 0.7148868441581726, accuracy: 0.52\n",
      "loss: 0.7773097157478333, accuracy: 0.5233333333333333\n",
      "loss: 0.6900390386581421, accuracy: 0.5366666666666666\n",
      "loss: 0.7506275177001953, accuracy: 0.5033333333333333\n",
      "loss: 0.7026294469833374, accuracy: 0.5233333333333333\n",
      "loss: 0.6999225616455078, accuracy: 0.5566666666666666\n",
      "loss: 0.7332965731620789, accuracy: 0.57\n",
      "loss: 0.7490889430046082, accuracy: 0.5133333333333333\n",
      "loss: 0.7506768703460693, accuracy: 0.5366666666666666\n",
      "loss: 0.6974721550941467, accuracy: 0.5633333333333334\n",
      "loss: 0.7018401026725769, accuracy: 0.55\n",
      "loss: 0.7388656139373779, accuracy: 0.5366666666666666\n",
      "loss: 0.6891953349113464, accuracy: 0.5366666666666666\n",
      "loss: 0.7185367941856384, accuracy: 0.5366666666666666\n",
      "loss: 0.7079450488090515, accuracy: 0.5466666666666666\n",
      "loss: 0.7260724902153015, accuracy: 0.5266666666666666\n",
      "loss: 0.7012513875961304, accuracy: 0.5433333333333333\n",
      "loss: 0.8042437434196472, accuracy: 0.47333333333333333\n",
      "loss: 0.7243097424507141, accuracy: 0.5433333333333333\n",
      "loss: 0.7849770188331604, accuracy: 0.5133333333333333\n",
      "loss: 0.7080606818199158, accuracy: 0.5433333333333333\n",
      "loss: 0.7430292367935181, accuracy: 0.51\n",
      "loss: 0.7103595733642578, accuracy: 0.5433333333333333\n",
      "loss: 0.7355996370315552, accuracy: 0.5166666666666667\n",
      "loss: 0.6995102167129517, accuracy: 0.55\n",
      "loss: 0.7557023167610168, accuracy: 0.52\n",
      "loss: 0.7106785178184509, accuracy: 0.5233333333333333\n",
      "loss: 0.6975026726722717, accuracy: 0.5566666666666666\n",
      "loss: 0.7072640657424927, accuracy: 0.5766666666666667\n",
      "loss: 0.7140705585479736, accuracy: 0.58\n",
      "loss: 0.7100042104721069, accuracy: 0.5466666666666666\n",
      "loss: 0.7354305982589722, accuracy: 0.5133333333333333\n",
      "loss: 0.7733449935913086, accuracy: 0.5066666666666667\n",
      "loss: 0.7653855681419373, accuracy: 0.54\n",
      "loss: 0.6830443739891052, accuracy: 0.56\n",
      "loss: 0.7134369611740112, accuracy: 0.5266666666666666\n",
      "loss: 0.8648759126663208, accuracy: 0.4666666666666667\n",
      "loss: 0.7062138915061951, accuracy: 0.5433333333333333\n",
      "loss: 0.7023212909698486, accuracy: 0.5766666666666667\n",
      "loss: 0.733418345451355, accuracy: 0.52\n",
      "loss: 0.7245867848396301, accuracy: 0.5566666666666666\n",
      "loss: 0.8066537380218506, accuracy: 0.5166666666666667\n",
      "loss: 0.7424459457397461, accuracy: 0.5266666666666666\n",
      "loss: 0.6967836022377014, accuracy: 0.5133333333333333\n",
      "loss: 0.7639288902282715, accuracy: 0.5233333333333333\n",
      "loss: 0.7325296401977539, accuracy: 0.52\n",
      "loss: 0.7108288407325745, accuracy: 0.56\n",
      "loss: 0.7469664812088013, accuracy: 0.5133333333333333\n",
      "loss: 0.7362110614776611, accuracy: 0.49\n",
      "loss: 0.756529688835144, accuracy: 0.52\n",
      "loss: 0.7077340483665466, accuracy: 0.5433333333333333\n",
      "loss: 0.7487714886665344, accuracy: 0.5333333333333333\n",
      "loss: 0.7167834043502808, accuracy: 0.5366666666666666\n",
      "loss: 0.7586805820465088, accuracy: 0.5466666666666666\n",
      "loss: 0.6994189023971558, accuracy: 0.57\n",
      "loss: 0.7235293388366699, accuracy: 0.54\n",
      "loss: 0.6946731805801392, accuracy: 0.54\n",
      "loss: 0.7887513041496277, accuracy: 0.51\n",
      "loss: 0.7892748117446899, accuracy: 0.5\n",
      "loss: 0.7223005294799805, accuracy: 0.55\n",
      "loss: 0.7228764295578003, accuracy: 0.5233333333333333\n",
      "loss: 0.7121587991714478, accuracy: 0.56\n",
      "loss: 0.7456715703010559, accuracy: 0.5433333333333333\n",
      "loss: 0.7282227873802185, accuracy: 0.5766666666666667\n",
      "loss: 0.7070910930633545, accuracy: 0.5466666666666666\n",
      "loss: 0.6789454817771912, accuracy: 0.5666666666666667\n",
      "loss: 0.7263150811195374, accuracy: 0.5766666666666667\n",
      "loss: 0.703617513179779, accuracy: 0.57\n",
      "loss: 0.7418209910392761, accuracy: 0.55\n",
      "loss: 0.6929705739021301, accuracy: 0.5666666666666667\n",
      "loss: 0.7421631217002869, accuracy: 0.5266666666666666\n",
      "loss: 0.720108687877655, accuracy: 0.57\n",
      "loss: 0.702976405620575, accuracy: 0.5433333333333333\n",
      "loss: 0.7663745284080505, accuracy: 0.51\n",
      "loss: 0.7038228511810303, accuracy: 0.5333333333333333\n",
      "loss: 0.770176887512207, accuracy: 0.51\n",
      "loss: 0.7557963728904724, accuracy: 0.5666666666666667\n",
      "loss: 0.7171934247016907, accuracy: 0.5733333333333334\n",
      "loss: 0.7207233905792236, accuracy: 0.5566666666666666\n",
      "loss: 0.6890066266059875, accuracy: 0.5733333333333334\n",
      "loss: 0.7628411650657654, accuracy: 0.5266666666666666\n",
      "max accuracy: 0.5866666666666667\n",
      "loss: 0.6835458874702454, accuracy: 0.5866666666666667\n",
      "loss: 0.7056336402893066, accuracy: 0.5766666666666667\n",
      "loss: 0.6919082403182983, accuracy: 0.57\n",
      "loss: 0.7122308611869812, accuracy: 0.5733333333333334\n",
      "loss: 0.6886199712753296, accuracy: 0.57\n",
      "loss: 0.7001568078994751, accuracy: 0.58\n",
      "loss: 0.6835628747940063, accuracy: 0.5733333333333334\n",
      "loss: 0.6967137455940247, accuracy: 0.56\n",
      "max accuracy: 0.59\n",
      "loss: 0.7038482427597046, accuracy: 0.59\n",
      "loss: 0.7384617328643799, accuracy: 0.54\n",
      "loss: 0.7078821063041687, accuracy: 0.56\n",
      "loss: 0.7586290240287781, accuracy: 0.5033333333333333\n",
      "loss: 0.7290315628051758, accuracy: 0.54\n",
      "max accuracy: 0.5966666666666667\n",
      "loss: 0.6777140498161316, accuracy: 0.5966666666666667\n",
      "max accuracy: 0.6033333333333334\n",
      "loss: 0.7126264572143555, accuracy: 0.6033333333333334\n",
      "loss: 0.7756336331367493, accuracy: 0.5566666666666666\n",
      "loss: 0.6898354887962341, accuracy: 0.5566666666666666\n",
      "loss: 0.6928258538246155, accuracy: 0.58\n",
      "loss: 0.7171781063079834, accuracy: 0.5866666666666667\n",
      "loss: 0.7134519815444946, accuracy: 0.57\n",
      "loss: 0.6965473294258118, accuracy: 0.5766666666666667\n",
      "loss: 0.7148276567459106, accuracy: 0.6033333333333334\n",
      "loss: 0.6878855228424072, accuracy: 0.5566666666666666\n",
      "loss: 0.6990365982055664, accuracy: 0.59\n",
      "loss: 0.6931478977203369, accuracy: 0.5533333333333333\n",
      "loss: 0.7218614816665649, accuracy: 0.55\n",
      "loss: 0.6902628540992737, accuracy: 0.56\n",
      "loss: 0.6909400224685669, accuracy: 0.58\n",
      "loss: 0.7098117470741272, accuracy: 0.5566666666666666\n",
      "loss: 0.6924377679824829, accuracy: 0.5966666666666667\n",
      "loss: 0.6828975081443787, accuracy: 0.58\n",
      "loss: 0.7333754897117615, accuracy: 0.53\n",
      "loss: 0.6986067295074463, accuracy: 0.5733333333333334\n",
      "loss: 0.6947880983352661, accuracy: 0.56\n",
      "loss: 0.7055714726448059, accuracy: 0.5633333333333334\n",
      "loss: 0.6896701455116272, accuracy: 0.5366666666666666\n",
      "loss: 0.6964289546012878, accuracy: 0.5533333333333333\n",
      "loss: 0.6846662163734436, accuracy: 0.5533333333333333\n",
      "loss: 0.7142062187194824, accuracy: 0.57\n",
      "loss: 0.7365978360176086, accuracy: 0.5533333333333333\n",
      "loss: 0.690942645072937, accuracy: 0.56\n",
      "loss: 0.7320683598518372, accuracy: 0.5733333333333334\n",
      "loss: 0.7562373876571655, accuracy: 0.54\n",
      "loss: 0.6820164918899536, accuracy: 0.58\n",
      "max accuracy: 0.6066666666666667\n",
      "loss: 0.7006415128707886, accuracy: 0.6066666666666667\n",
      "loss: 0.7149307727813721, accuracy: 0.5633333333333334\n",
      "loss: 0.6702253818511963, accuracy: 0.6033333333333334\n",
      "loss: 0.7189900875091553, accuracy: 0.5566666666666666\n",
      "loss: 0.7550379037857056, accuracy: 0.5666666666666667\n",
      "max accuracy: 0.6133333333333333\n",
      "loss: 0.7195193767547607, accuracy: 0.6133333333333333\n",
      "loss: 0.6967766284942627, accuracy: 0.59\n",
      "loss: 0.7345341444015503, accuracy: 0.57\n",
      "loss: 0.7015730738639832, accuracy: 0.5766666666666667\n",
      "loss: 0.7009400725364685, accuracy: 0.5833333333333334\n",
      "loss: 0.6814941167831421, accuracy: 0.5966666666666667\n",
      "loss: 0.7136976718902588, accuracy: 0.5566666666666666\n",
      "loss: 0.7180364727973938, accuracy: 0.5933333333333334\n",
      "loss: 0.7303646206855774, accuracy: 0.5166666666666667\n",
      "loss: 0.7106128931045532, accuracy: 0.57\n",
      "loss: 0.6717362403869629, accuracy: 0.56\n",
      "loss: 0.690426766872406, accuracy: 0.5733333333333334\n",
      "loss: 0.7092621326446533, accuracy: 0.5533333333333333\n",
      "loss: 0.7399608492851257, accuracy: 0.5666666666666667\n",
      "loss: 0.7207167148590088, accuracy: 0.55\n",
      "loss: 0.6924579739570618, accuracy: 0.5766666666666667\n",
      "loss: 0.6878278851509094, accuracy: 0.6133333333333333\n",
      "loss: 0.7362560629844666, accuracy: 0.5766666666666667\n",
      "loss: 0.718944251537323, accuracy: 0.58\n",
      "loss: 0.6719799041748047, accuracy: 0.6\n",
      "loss: 0.7104563117027283, accuracy: 0.55\n",
      "loss: 0.7105820178985596, accuracy: 0.5433333333333333\n",
      "loss: 0.6930980086326599, accuracy: 0.5933333333333334\n",
      "loss: 0.6879051923751831, accuracy: 0.56\n",
      "loss: 0.6923236846923828, accuracy: 0.6033333333333334\n",
      "loss: 0.719204843044281, accuracy: 0.5966666666666667\n",
      "loss: 0.6779263019561768, accuracy: 0.5833333333333334\n",
      "loss: 0.6741510629653931, accuracy: 0.5833333333333334\n",
      "loss: 0.71917325258255, accuracy: 0.5733333333333334\n",
      "loss: 0.6806125640869141, accuracy: 0.5866666666666667\n",
      "loss: 0.7212036848068237, accuracy: 0.56\n",
      "loss: 0.6977086663246155, accuracy: 0.5466666666666666\n",
      "loss: 0.6795831918716431, accuracy: 0.6133333333333333\n",
      "loss: 0.69951993227005, accuracy: 0.59\n",
      "loss: 0.7546622157096863, accuracy: 0.57\n",
      "loss: 0.6941871047019958, accuracy: 0.5566666666666666\n",
      "loss: 0.6895673274993896, accuracy: 0.5866666666666667\n",
      "loss: 0.7149778008460999, accuracy: 0.5833333333333334\n",
      "loss: 0.7574235796928406, accuracy: 0.5666666666666667\n",
      "loss: 0.7215452790260315, accuracy: 0.58\n",
      "loss: 0.7107951641082764, accuracy: 0.5533333333333333\n",
      "loss: 0.7076586484909058, accuracy: 0.5733333333333334\n",
      "loss: 0.715244710445404, accuracy: 0.5966666666666667\n",
      "loss: 0.7125124335289001, accuracy: 0.56\n",
      "loss: 0.6953206658363342, accuracy: 0.5666666666666667\n",
      "loss: 0.7211808562278748, accuracy: 0.5533333333333333\n",
      "loss: 0.6936072111129761, accuracy: 0.5833333333333334\n",
      "loss: 0.7266566753387451, accuracy: 0.5433333333333333\n",
      "loss: 0.6712034344673157, accuracy: 0.6033333333333334\n",
      "loss: 0.6955958604812622, accuracy: 0.6066666666666667\n",
      "loss: 0.666871190071106, accuracy: 0.5833333333333334\n",
      "loss: 0.7087337970733643, accuracy: 0.58\n",
      "loss: 0.7012183666229248, accuracy: 0.5366666666666666\n",
      "max accuracy: 0.6266666666666667\n",
      "loss: 0.6699618697166443, accuracy: 0.6266666666666667\n",
      "loss: 0.7645647525787354, accuracy: 0.5533333333333333\n",
      "loss: 0.6792677640914917, accuracy: 0.5866666666666667\n",
      "loss: 0.7319360375404358, accuracy: 0.5766666666666667\n",
      "loss: 0.6898460388183594, accuracy: 0.5766666666666667\n",
      "loss: 0.6817025542259216, accuracy: 0.6066666666666667\n",
      "loss: 0.6810439229011536, accuracy: 0.6166666666666667\n",
      "loss: 0.6925407648086548, accuracy: 0.61\n",
      "loss: 0.693128764629364, accuracy: 0.5666666666666667\n",
      "loss: 0.681359052658081, accuracy: 0.5833333333333334\n",
      "loss: 0.7173612713813782, accuracy: 0.5833333333333334\n",
      "loss: 0.7099458575248718, accuracy: 0.5966666666666667\n",
      "loss: 0.6688246130943298, accuracy: 0.5933333333333334\n",
      "loss: 0.6684808135032654, accuracy: 0.5733333333333334\n",
      "loss: 0.6868802905082703, accuracy: 0.61\n",
      "loss: 0.7179691791534424, accuracy: 0.5633333333333334\n",
      "loss: 0.6967254877090454, accuracy: 0.5833333333333334\n",
      "loss: 0.6936228275299072, accuracy: 0.5733333333333334\n",
      "loss: 0.6873663067817688, accuracy: 0.57\n",
      "loss: 0.6938175559043884, accuracy: 0.59\n",
      "loss: 0.6981825232505798, accuracy: 0.5633333333333334\n",
      "loss: 0.6871668696403503, accuracy: 0.5766666666666667\n",
      "loss: 0.7975569367408752, accuracy: 0.52\n",
      "loss: 0.7336432337760925, accuracy: 0.5766666666666667\n",
      "loss: 0.6860647201538086, accuracy: 0.5966666666666667\n",
      "loss: 0.6881824731826782, accuracy: 0.6166666666666667\n",
      "loss: 0.7060115933418274, accuracy: 0.5733333333333334\n",
      "loss: 0.6731685400009155, accuracy: 0.59\n",
      "loss: 0.7271414399147034, accuracy: 0.58\n",
      "loss: 0.7030879855155945, accuracy: 0.58\n",
      "loss: 0.6966621279716492, accuracy: 0.5833333333333334\n",
      "loss: 0.6802162528038025, accuracy: 0.6233333333333333\n",
      "loss: 0.7219201326370239, accuracy: 0.5633333333333334\n",
      "loss: 0.7128552198410034, accuracy: 0.5466666666666666\n",
      "loss: 0.7003481388092041, accuracy: 0.5733333333333334\n",
      "loss: 0.6990899443626404, accuracy: 0.5966666666666667\n",
      "loss: 0.6840425729751587, accuracy: 0.5933333333333334\n",
      "loss: 0.6906452178955078, accuracy: 0.58\n",
      "loss: 0.6710208058357239, accuracy: 0.5833333333333334\n",
      "loss: 0.7389512062072754, accuracy: 0.57\n",
      "loss: 0.6790026426315308, accuracy: 0.5566666666666666\n",
      "loss: 0.7219862937927246, accuracy: 0.55\n",
      "loss: 0.681050181388855, accuracy: 0.58\n",
      "loss: 0.6689122319221497, accuracy: 0.6033333333333334\n",
      "loss: 0.6870207190513611, accuracy: 0.57\n",
      "loss: 0.7275549173355103, accuracy: 0.58\n",
      "loss: 0.6745473742485046, accuracy: 0.5966666666666667\n",
      "loss: 0.7128644585609436, accuracy: 0.5766666666666667\n",
      "loss: 0.69097501039505, accuracy: 0.57\n",
      "max accuracy: 0.6366666666666667\n",
      "loss: 0.6714262366294861, accuracy: 0.6366666666666667\n",
      "loss: 0.691910445690155, accuracy: 0.59\n",
      "loss: 0.7161020040512085, accuracy: 0.5766666666666667\n",
      "loss: 0.6789844036102295, accuracy: 0.5866666666666667\n",
      "loss: 0.6902212500572205, accuracy: 0.5866666666666667\n",
      "loss: 0.7199820876121521, accuracy: 0.55\n",
      "loss: 0.7599080801010132, accuracy: 0.5433333333333333\n",
      "loss: 0.6848099231719971, accuracy: 0.5933333333333334\n",
      "loss: 0.7194896340370178, accuracy: 0.5333333333333333\n",
      "loss: 0.727320671081543, accuracy: 0.5466666666666666\n",
      "loss: 0.7051183581352234, accuracy: 0.52\n",
      "loss: 0.7478445768356323, accuracy: 0.5566666666666666\n",
      "loss: 0.6730433106422424, accuracy: 0.5833333333333334\n",
      "loss: 0.682032585144043, accuracy: 0.6033333333333334\n",
      "loss: 0.6894345879554749, accuracy: 0.5933333333333334\n",
      "loss: 0.6789451837539673, accuracy: 0.5633333333333334\n",
      "loss: 0.6675940752029419, accuracy: 0.5866666666666667\n",
      "loss: 0.6719949841499329, accuracy: 0.5933333333333334\n",
      "loss: 0.7394192218780518, accuracy: 0.5433333333333333\n",
      "loss: 0.7108286619186401, accuracy: 0.5733333333333334\n",
      "loss: 0.7162195444107056, accuracy: 0.54\n",
      "loss: 0.6860252022743225, accuracy: 0.59\n",
      "loss: 0.7194738388061523, accuracy: 0.56\n",
      "loss: 0.6841427087783813, accuracy: 0.5733333333333334\n",
      "loss: 0.6911035776138306, accuracy: 0.5633333333333334\n",
      "loss: 0.6819799542427063, accuracy: 0.57\n",
      "loss: 0.6747772097587585, accuracy: 0.58\n",
      "loss: 0.7193682789802551, accuracy: 0.5533333333333333\n",
      "loss: 0.7048724293708801, accuracy: 0.5633333333333334\n",
      "loss: 0.6956093311309814, accuracy: 0.5833333333333334\n",
      "loss: 0.7120360136032104, accuracy: 0.5633333333333334\n",
      "loss: 0.7019109725952148, accuracy: 0.5666666666666667\n",
      "loss: 0.6649034023284912, accuracy: 0.6\n",
      "loss: 0.6691092252731323, accuracy: 0.5766666666666667\n",
      "loss: 0.6945464015007019, accuracy: 0.5666666666666667\n",
      "loss: 0.7046642899513245, accuracy: 0.58\n",
      "loss: 0.7125632166862488, accuracy: 0.5533333333333333\n",
      "loss: 0.6847407817840576, accuracy: 0.6\n",
      "loss: 0.7052669525146484, accuracy: 0.5633333333333334\n",
      "loss: 0.6575866937637329, accuracy: 0.5966666666666667\n",
      "loss: 0.7138848900794983, accuracy: 0.5866666666666667\n",
      "loss: 0.6936389207839966, accuracy: 0.5733333333333334\n",
      "loss: 0.6863793730735779, accuracy: 0.5466666666666666\n",
      "loss: 0.7249345183372498, accuracy: 0.5633333333333334\n",
      "loss: 0.7000541090965271, accuracy: 0.58\n",
      "loss: 0.6956334710121155, accuracy: 0.57\n",
      "loss: 0.714970052242279, accuracy: 0.55\n",
      "loss: 0.6471309661865234, accuracy: 0.62\n",
      "loss: 0.6890329718589783, accuracy: 0.5466666666666666\n",
      "loss: 0.7273497581481934, accuracy: 0.5433333333333333\n",
      "loss: 0.7018441557884216, accuracy: 0.6\n",
      "loss: 0.6818985342979431, accuracy: 0.5833333333333334\n",
      "loss: 0.658600389957428, accuracy: 0.5966666666666667\n",
      "loss: 0.7040886878967285, accuracy: 0.5533333333333333\n",
      "loss: 0.694866955280304, accuracy: 0.5233333333333333\n",
      "loss: 0.7337661981582642, accuracy: 0.5466666666666666\n",
      "loss: 0.7458820939064026, accuracy: 0.5666666666666667\n",
      "loss: 0.695171058177948, accuracy: 0.58\n",
      "loss: 0.6690290570259094, accuracy: 0.5733333333333334\n",
      "loss: 0.6571851968765259, accuracy: 0.59\n",
      "loss: 0.6788820624351501, accuracy: 0.58\n",
      "loss: 0.6737855076789856, accuracy: 0.58\n",
      "loss: 0.6805508136749268, accuracy: 0.5433333333333333\n",
      "loss: 0.6881241202354431, accuracy: 0.5766666666666667\n",
      "loss: 0.6884629726409912, accuracy: 0.5966666666666667\n",
      "loss: 0.668789267539978, accuracy: 0.5666666666666667\n",
      "loss: 0.6978391408920288, accuracy: 0.5766666666666667\n",
      "loss: 0.669797956943512, accuracy: 0.5533333333333333\n",
      "loss: 0.6752611994743347, accuracy: 0.55\n",
      "loss: 0.6978927850723267, accuracy: 0.5633333333333334\n",
      "loss: 0.6749033331871033, accuracy: 0.6033333333333334\n",
      "loss: 0.6972152590751648, accuracy: 0.5666666666666667\n",
      "loss: 0.6707519292831421, accuracy: 0.5966666666666667\n",
      "loss: 0.6690998077392578, accuracy: 0.5966666666666667\n",
      "loss: 0.6897541284561157, accuracy: 0.56\n",
      "loss: 0.6805437803268433, accuracy: 0.5966666666666667\n",
      "loss: 0.6870941519737244, accuracy: 0.5833333333333334\n",
      "loss: 0.687237024307251, accuracy: 0.59\n",
      "loss: 0.713096022605896, accuracy: 0.55\n",
      "loss: 0.6816744208335876, accuracy: 0.58\n",
      "loss: 0.7041610479354858, accuracy: 0.5633333333333334\n",
      "loss: 0.6636137366294861, accuracy: 0.5733333333333334\n",
      "loss: 0.6737700700759888, accuracy: 0.6\n",
      "loss: 0.698708176612854, accuracy: 0.59\n",
      "loss: 0.675842821598053, accuracy: 0.5766666666666667\n",
      "loss: 0.6558512449264526, accuracy: 0.5733333333333334\n",
      "loss: 0.6750282645225525, accuracy: 0.5666666666666667\n",
      "loss: 0.678053081035614, accuracy: 0.6033333333333334\n",
      "loss: 0.678740918636322, accuracy: 0.5866666666666667\n",
      "loss: 0.7014837265014648, accuracy: 0.5666666666666667\n",
      "loss: 0.680858850479126, accuracy: 0.5666666666666667\n",
      "loss: 0.6692249774932861, accuracy: 0.6066666666666667\n",
      "loss: 0.6867878437042236, accuracy: 0.5466666666666666\n",
      "loss: 0.6815729737281799, accuracy: 0.57\n",
      "loss: 0.6905533075332642, accuracy: 0.59\n",
      "loss: 0.6673861145973206, accuracy: 0.59\n",
      "loss: 0.7174742221832275, accuracy: 0.5666666666666667\n",
      "loss: 0.6673257946968079, accuracy: 0.6033333333333334\n",
      "loss: 0.6856337189674377, accuracy: 0.5666666666666667\n",
      "loss: 0.6809215545654297, accuracy: 0.58\n",
      "loss: 0.6991112232208252, accuracy: 0.59\n",
      "loss: 0.6623379588127136, accuracy: 0.6166666666666667\n",
      "loss: 0.6591790914535522, accuracy: 0.59\n",
      "loss: 0.669740617275238, accuracy: 0.5833333333333334\n",
      "loss: 0.684048593044281, accuracy: 0.5433333333333333\n",
      "loss: 0.6577110290527344, accuracy: 0.6066666666666667\n",
      "loss: 0.7156474590301514, accuracy: 0.5433333333333333\n",
      "loss: 0.665576159954071, accuracy: 0.5566666666666666\n",
      "loss: 0.6735098958015442, accuracy: 0.5733333333333334\n",
      "loss: 0.6890754699707031, accuracy: 0.5733333333333334\n",
      "loss: 0.6622143387794495, accuracy: 0.5866666666666667\n",
      "loss: 0.6848337650299072, accuracy: 0.55\n",
      "loss: 0.6624430418014526, accuracy: 0.5966666666666667\n",
      "loss: 0.7183467745780945, accuracy: 0.5666666666666667\n",
      "loss: 0.7147772908210754, accuracy: 0.56\n",
      "loss: 0.6814004778862, accuracy: 0.58\n",
      "loss: 0.6628325581550598, accuracy: 0.5866666666666667\n",
      "loss: 0.6906169652938843, accuracy: 0.56\n",
      "loss: 0.7494794130325317, accuracy: 0.5033333333333333\n",
      "loss: 0.6578249335289001, accuracy: 0.6166666666666667\n",
      "loss: 0.672202467918396, accuracy: 0.5633333333333334\n",
      "loss: 0.663991391658783, accuracy: 0.5833333333333334\n",
      "loss: 0.7043806910514832, accuracy: 0.5533333333333333\n",
      "max accuracy: 0.6433333333333333\n",
      "loss: 0.6492511630058289, accuracy: 0.6433333333333333\n",
      "loss: 0.6564613580703735, accuracy: 0.6166666666666667\n",
      "loss: 0.6892156600952148, accuracy: 0.57\n",
      "loss: 0.6706201434135437, accuracy: 0.6\n",
      "loss: 0.6809328198432922, accuracy: 0.5566666666666666\n",
      "loss: 0.7004324793815613, accuracy: 0.53\n",
      "loss: 0.6629428267478943, accuracy: 0.5766666666666667\n",
      "loss: 0.6753537654876709, accuracy: 0.54\n",
      "loss: 0.6729165315628052, accuracy: 0.5733333333333334\n",
      "loss: 0.6607943773269653, accuracy: 0.57\n",
      "loss: 0.6612597107887268, accuracy: 0.58\n",
      "loss: 0.6658402681350708, accuracy: 0.6\n",
      "loss: 0.7362008690834045, accuracy: 0.54\n",
      "loss: 0.6696959137916565, accuracy: 0.5833333333333334\n",
      "loss: 0.7078121304512024, accuracy: 0.5466666666666666\n",
      "loss: 0.6686862111091614, accuracy: 0.5866666666666667\n",
      "loss: 0.6803171038627625, accuracy: 0.5566666666666666\n",
      "loss: 0.6595320105552673, accuracy: 0.5733333333333334\n",
      "loss: 0.6989077925682068, accuracy: 0.54\n",
      "loss: 0.649309515953064, accuracy: 0.6233333333333333\n",
      "loss: 0.6942285299301147, accuracy: 0.59\n",
      "max accuracy: 0.6566666666666666\n",
      "loss: 0.6508873701095581, accuracy: 0.6566666666666666\n",
      "loss: 0.6827157139778137, accuracy: 0.59\n",
      "loss: 0.6723635792732239, accuracy: 0.5566666666666666\n",
      "loss: 0.6846751570701599, accuracy: 0.5633333333333334\n",
      "loss: 0.6569585800170898, accuracy: 0.57\n",
      "loss: 0.6585050821304321, accuracy: 0.5666666666666667\n",
      "loss: 0.6533896923065186, accuracy: 0.6\n",
      "loss: 0.7115088701248169, accuracy: 0.5433333333333333\n",
      "loss: 0.7014895677566528, accuracy: 0.5766666666666667\n",
      "loss: 0.6548293232917786, accuracy: 0.5866666666666667\n",
      "loss: 0.663149356842041, accuracy: 0.5933333333333334\n",
      "loss: 0.6901064515113831, accuracy: 0.5866666666666667\n",
      "loss: 0.6525545120239258, accuracy: 0.61\n",
      "loss: 0.6459887027740479, accuracy: 0.59\n",
      "loss: 0.6638684868812561, accuracy: 0.6\n",
      "loss: 0.6680991649627686, accuracy: 0.56\n",
      "loss: 0.7224622368812561, accuracy: 0.54\n",
      "loss: 0.6999065279960632, accuracy: 0.5566666666666666\n",
      "loss: 0.6616376042366028, accuracy: 0.5933333333333334\n",
      "loss: 0.7063947916030884, accuracy: 0.55\n",
      "loss: 0.667128324508667, accuracy: 0.5666666666666667\n",
      "loss: 0.6683257818222046, accuracy: 0.5766666666666667\n",
      "loss: 0.6803820133209229, accuracy: 0.5966666666666667\n",
      "loss: 0.7000234723091125, accuracy: 0.5633333333333334\n",
      "loss: 0.6535419225692749, accuracy: 0.58\n",
      "loss: 0.6692095398902893, accuracy: 0.58\n",
      "loss: 0.6640540957450867, accuracy: 0.55\n",
      "loss: 0.6707574725151062, accuracy: 0.5866666666666667\n",
      "loss: 0.680913507938385, accuracy: 0.5533333333333333\n",
      "loss: 0.6789202094078064, accuracy: 0.5433333333333333\n",
      "loss: 0.6545120477676392, accuracy: 0.6\n",
      "loss: 0.6524722576141357, accuracy: 0.6166666666666667\n",
      "loss: 0.6576400399208069, accuracy: 0.5766666666666667\n",
      "loss: 0.6613931655883789, accuracy: 0.59\n",
      "loss: 0.7177681922912598, accuracy: 0.5633333333333334\n",
      "loss: 0.665301501750946, accuracy: 0.5733333333333334\n",
      "loss: 0.6547352075576782, accuracy: 0.6033333333333334\n",
      "loss: 0.7159959673881531, accuracy: 0.5766666666666667\n",
      "loss: 0.6486292481422424, accuracy: 0.59\n",
      "loss: 0.6327820420265198, accuracy: 0.59\n",
      "loss: 0.6584117412567139, accuracy: 0.5966666666666667\n",
      "loss: 0.6624905467033386, accuracy: 0.5766666666666667\n",
      "loss: 0.6968698501586914, accuracy: 0.58\n",
      "loss: 0.6836228370666504, accuracy: 0.59\n",
      "loss: 0.6482673287391663, accuracy: 0.6033333333333334\n",
      "loss: 0.6593037247657776, accuracy: 0.6\n",
      "loss: 0.6604088544845581, accuracy: 0.5666666666666667\n",
      "loss: 0.6734398007392883, accuracy: 0.5633333333333334\n",
      "loss: 0.6398380994796753, accuracy: 0.65\n",
      "loss: 0.6668336391448975, accuracy: 0.58\n",
      "loss: 0.6802653074264526, accuracy: 0.58\n",
      "loss: 0.6491742730140686, accuracy: 0.6033333333333334\n",
      "loss: 0.6775122284889221, accuracy: 0.5766666666666667\n",
      "loss: 0.6648477911949158, accuracy: 0.5966666666666667\n",
      "loss: 0.6991411447525024, accuracy: 0.56\n",
      "loss: 0.7206329107284546, accuracy: 0.5466666666666666\n",
      "loss: 0.6895779967308044, accuracy: 0.59\n",
      "loss: 0.6493825316429138, accuracy: 0.6033333333333334\n",
      "loss: 0.6726546287536621, accuracy: 0.5633333333333334\n",
      "loss: 0.657654881477356, accuracy: 0.6\n",
      "loss: 0.640877902507782, accuracy: 0.6166666666666667\n",
      "loss: 0.655986487865448, accuracy: 0.5966666666666667\n",
      "loss: 0.673520565032959, accuracy: 0.61\n",
      "loss: 0.6731468439102173, accuracy: 0.59\n",
      "loss: 0.6754850745201111, accuracy: 0.5966666666666667\n",
      "loss: 0.6684943437576294, accuracy: 0.5866666666666667\n",
      "loss: 0.6522918939590454, accuracy: 0.59\n",
      "loss: 0.6773110032081604, accuracy: 0.61\n",
      "loss: 0.6748495101928711, accuracy: 0.5633333333333334\n",
      "loss: 0.6391515731811523, accuracy: 0.6266666666666667\n",
      "loss: 0.6719262599945068, accuracy: 0.5866666666666667\n",
      "loss: 0.6900520324707031, accuracy: 0.5866666666666667\n",
      "loss: 0.6453925967216492, accuracy: 0.6133333333333333\n",
      "loss: 0.6615400314331055, accuracy: 0.59\n",
      "loss: 0.6645421981811523, accuracy: 0.59\n",
      "loss: 0.657082200050354, accuracy: 0.5633333333333334\n",
      "loss: 0.6858788728713989, accuracy: 0.58\n",
      "loss: 0.724077582359314, accuracy: 0.5566666666666666\n",
      "loss: 0.6777454614639282, accuracy: 0.5833333333333334\n",
      "loss: 0.66700679063797, accuracy: 0.5933333333333334\n",
      "loss: 0.6555872559547424, accuracy: 0.5966666666666667\n",
      "loss: 0.6812835335731506, accuracy: 0.58\n",
      "loss: 0.7020682096481323, accuracy: 0.57\n",
      "loss: 0.6557639241218567, accuracy: 0.5966666666666667\n",
      "loss: 0.6493825316429138, accuracy: 0.6066666666666667\n",
      "loss: 0.6544520258903503, accuracy: 0.6\n",
      "loss: 0.6474698185920715, accuracy: 0.58\n",
      "loss: 0.6511189937591553, accuracy: 0.6166666666666667\n",
      "loss: 0.642930805683136, accuracy: 0.61\n",
      "loss: 0.6457695960998535, accuracy: 0.5733333333333334\n",
      "loss: 0.6662699580192566, accuracy: 0.5966666666666667\n",
      "loss: 0.6817887425422668, accuracy: 0.58\n",
      "loss: 0.6565502882003784, accuracy: 0.5766666666666667\n",
      "loss: 0.6600005030632019, accuracy: 0.6266666666666667\n",
      "loss: 0.6489551663398743, accuracy: 0.6033333333333334\n",
      "loss: 0.6593950390815735, accuracy: 0.57\n",
      "loss: 0.6441878080368042, accuracy: 0.6133333333333333\n",
      "loss: 0.6862801313400269, accuracy: 0.5833333333333334\n",
      "loss: 0.6502621173858643, accuracy: 0.56\n",
      "loss: 0.6754088401794434, accuracy: 0.5666666666666667\n",
      "loss: 0.6432254910469055, accuracy: 0.6166666666666667\n",
      "loss: 0.7095363140106201, accuracy: 0.56\n",
      "loss: 0.6467142105102539, accuracy: 0.59\n",
      "loss: 0.6607041954994202, accuracy: 0.5966666666666667\n",
      "loss: 0.6362923383712769, accuracy: 0.6\n",
      "loss: 0.6801931858062744, accuracy: 0.5333333333333333\n",
      "loss: 0.6407597064971924, accuracy: 0.6066666666666667\n",
      "loss: 0.6644909977912903, accuracy: 0.5866666666666667\n",
      "loss: 0.7084507942199707, accuracy: 0.5333333333333333\n",
      "loss: 0.6479865312576294, accuracy: 0.6033333333333334\n",
      "loss: 0.6492491960525513, accuracy: 0.5966666666666667\n",
      "loss: 0.6351252198219299, accuracy: 0.62\n",
      "loss: 0.6643739938735962, accuracy: 0.5933333333333334\n",
      "loss: 0.6523745656013489, accuracy: 0.6033333333333334\n",
      "loss: 0.6762707233428955, accuracy: 0.6\n",
      "loss: 0.638874888420105, accuracy: 0.6366666666666667\n",
      "loss: 0.658989429473877, accuracy: 0.5633333333333334\n",
      "loss: 0.6395002603530884, accuracy: 0.59\n",
      "loss: 0.648556649684906, accuracy: 0.6033333333333334\n",
      "loss: 0.6407363414764404, accuracy: 0.6233333333333333\n",
      "loss: 0.6719933152198792, accuracy: 0.6133333333333333\n",
      "loss: 0.6631069183349609, accuracy: 0.58\n",
      "loss: 0.6322116851806641, accuracy: 0.5866666666666667\n",
      "loss: 0.6893983483314514, accuracy: 0.5833333333333334\n",
      "loss: 0.6289672255516052, accuracy: 0.63\n",
      "loss: 0.6567414402961731, accuracy: 0.5733333333333334\n",
      "loss: 0.645612359046936, accuracy: 0.5866666666666667\n",
      "loss: 0.6493078470230103, accuracy: 0.6\n",
      "loss: 0.6585140228271484, accuracy: 0.6333333333333333\n",
      "loss: 0.6456509232521057, accuracy: 0.5933333333333334\n",
      "loss: 0.6583718657493591, accuracy: 0.5766666666666667\n",
      "loss: 0.6346533894538879, accuracy: 0.5966666666666667\n",
      "loss: 0.6452579498291016, accuracy: 0.6133333333333333\n",
      "loss: 0.6394079327583313, accuracy: 0.6266666666666667\n",
      "loss: 0.6469748020172119, accuracy: 0.6133333333333333\n",
      "loss: 0.6543894410133362, accuracy: 0.5833333333333334\n",
      "loss: 0.6432031989097595, accuracy: 0.6033333333333334\n",
      "loss: 0.6532555818557739, accuracy: 0.5733333333333334\n",
      "loss: 0.6357001662254333, accuracy: 0.61\n",
      "loss: 0.6360010504722595, accuracy: 0.6166666666666667\n",
      "loss: 0.6759223341941833, accuracy: 0.5466666666666666\n",
      "loss: 0.6383050680160522, accuracy: 0.6\n",
      "loss: 0.6499906778335571, accuracy: 0.6266666666666667\n",
      "loss: 0.6940997242927551, accuracy: 0.5733333333333334\n",
      "loss: 0.6628798842430115, accuracy: 0.5733333333333334\n",
      "loss: 0.6360207200050354, accuracy: 0.6133333333333333\n",
      "loss: 0.6363171339035034, accuracy: 0.5966666666666667\n",
      "loss: 0.6479076147079468, accuracy: 0.6233333333333333\n",
      "loss: 0.6488211154937744, accuracy: 0.5766666666666667\n",
      "loss: 0.6517782807350159, accuracy: 0.5966666666666667\n",
      "loss: 0.6312402486801147, accuracy: 0.6066666666666667\n",
      "loss: 0.6384210586547852, accuracy: 0.57\n",
      "loss: 0.6492648124694824, accuracy: 0.6\n",
      "loss: 0.629710853099823, accuracy: 0.6366666666666667\n",
      "loss: 0.6541422009468079, accuracy: 0.5633333333333334\n",
      "loss: 0.6481422185897827, accuracy: 0.6\n",
      "loss: 0.6364456415176392, accuracy: 0.6333333333333333\n",
      "loss: 0.6599048376083374, accuracy: 0.5933333333333334\n",
      "loss: 0.667256772518158, accuracy: 0.58\n",
      "loss: 0.6400173902511597, accuracy: 0.6033333333333334\n",
      "loss: 0.6242750287055969, accuracy: 0.64\n",
      "loss: 0.629225492477417, accuracy: 0.62\n",
      "loss: 0.6334531903266907, accuracy: 0.6433333333333333\n",
      "loss: 0.6238754391670227, accuracy: 0.6266666666666667\n",
      "loss: 0.6652109026908875, accuracy: 0.5566666666666666\n",
      "loss: 0.6301168203353882, accuracy: 0.6033333333333334\n",
      "loss: 0.633431077003479, accuracy: 0.6233333333333333\n",
      "loss: 0.6401546001434326, accuracy: 0.6066666666666667\n",
      "loss: 0.6525816917419434, accuracy: 0.5966666666666667\n",
      "loss: 0.6435577273368835, accuracy: 0.59\n",
      "loss: 0.6451724171638489, accuracy: 0.5666666666666667\n",
      "loss: 0.6386350989341736, accuracy: 0.6133333333333333\n",
      "loss: 0.648114800453186, accuracy: 0.6066666666666667\n",
      "loss: 0.6463159918785095, accuracy: 0.63\n",
      "loss: 0.6333617568016052, accuracy: 0.5933333333333334\n",
      "loss: 0.6543058753013611, accuracy: 0.5766666666666667\n",
      "loss: 0.6579384207725525, accuracy: 0.5833333333333334\n",
      "loss: 0.6543781161308289, accuracy: 0.5833333333333334\n",
      "loss: 0.6393535137176514, accuracy: 0.62\n",
      "loss: 0.6595231294631958, accuracy: 0.5966666666666667\n",
      "loss: 0.6218290328979492, accuracy: 0.63\n",
      "loss: 0.6571275591850281, accuracy: 0.5966666666666667\n",
      "loss: 0.6357006430625916, accuracy: 0.5866666666666667\n",
      "loss: 0.6439526081085205, accuracy: 0.6033333333333334\n",
      "loss: 0.6259846091270447, accuracy: 0.64\n",
      "loss: 0.6517228484153748, accuracy: 0.5733333333333334\n",
      "loss: 0.6188092827796936, accuracy: 0.64\n",
      "loss: 0.6278280019760132, accuracy: 0.64\n",
      "loss: 0.6358806490898132, accuracy: 0.61\n",
      "loss: 0.6327607035636902, accuracy: 0.59\n",
      "loss: 0.6410500407218933, accuracy: 0.6066666666666667\n",
      "loss: 0.6368911266326904, accuracy: 0.6166666666666667\n",
      "loss: 0.6449475288391113, accuracy: 0.58\n",
      "loss: 0.6571425199508667, accuracy: 0.5833333333333334\n",
      "loss: 0.6305946111679077, accuracy: 0.59\n",
      "loss: 0.623060405254364, accuracy: 0.6166666666666667\n",
      "loss: 0.6282398700714111, accuracy: 0.6133333333333333\n",
      "loss: 0.6336377859115601, accuracy: 0.5866666666666667\n",
      "loss: 0.6291617155075073, accuracy: 0.6133333333333333\n",
      "loss: 0.6463526487350464, accuracy: 0.6133333333333333\n",
      "loss: 0.6521472930908203, accuracy: 0.5733333333333334\n",
      "loss: 0.6387197375297546, accuracy: 0.6066666666666667\n",
      "loss: 0.6358933448791504, accuracy: 0.5766666666666667\n",
      "loss: 0.6400582194328308, accuracy: 0.6233333333333333\n",
      "loss: 0.6578252911567688, accuracy: 0.6233333333333333\n",
      "loss: 0.6208640336990356, accuracy: 0.6233333333333333\n",
      "loss: 0.6267271041870117, accuracy: 0.6233333333333333\n",
      "loss: 0.6243990659713745, accuracy: 0.64\n",
      "loss: 0.6221189498901367, accuracy: 0.6566666666666666\n",
      "loss: 0.6370169520378113, accuracy: 0.6166666666666667\n",
      "loss: 0.614034116268158, accuracy: 0.64\n",
      "loss: 0.6280648708343506, accuracy: 0.63\n",
      "loss: 0.64899080991745, accuracy: 0.57\n",
      "loss: 0.6474538445472717, accuracy: 0.5833333333333334\n",
      "loss: 0.6283901333808899, accuracy: 0.6333333333333333\n",
      "loss: 0.6317462921142578, accuracy: 0.6166666666666667\n",
      "loss: 0.6191214919090271, accuracy: 0.6433333333333333\n",
      "loss: 0.6198025941848755, accuracy: 0.6333333333333333\n",
      "loss: 0.614871621131897, accuracy: 0.6433333333333333\n",
      "loss: 0.6147321462631226, accuracy: 0.64\n",
      "loss: 0.6410864591598511, accuracy: 0.58\n",
      "loss: 0.6364646553993225, accuracy: 0.5933333333333334\n",
      "loss: 0.6087980270385742, accuracy: 0.6266666666666667\n",
      "loss: 0.6347565650939941, accuracy: 0.6033333333333334\n",
      "loss: 0.6279759407043457, accuracy: 0.6333333333333333\n",
      "loss: 0.6368439793586731, accuracy: 0.6033333333333334\n",
      "loss: 0.6295906901359558, accuracy: 0.59\n",
      "loss: 0.6269675493240356, accuracy: 0.63\n",
      "loss: 0.6321189999580383, accuracy: 0.5966666666666667\n",
      "loss: 0.6137596368789673, accuracy: 0.6433333333333333\n",
      "loss: 0.6352762579917908, accuracy: 0.5933333333333334\n",
      "loss: 0.6294121742248535, accuracy: 0.61\n",
      "loss: 0.6244106292724609, accuracy: 0.6033333333333334\n",
      "loss: 0.6125566363334656, accuracy: 0.6433333333333333\n",
      "loss: 0.6248021721839905, accuracy: 0.6166666666666667\n",
      "loss: 0.6135526299476624, accuracy: 0.6266666666666667\n",
      "loss: 0.6173418164253235, accuracy: 0.6166666666666667\n",
      "loss: 0.6111693978309631, accuracy: 0.6466666666666666\n",
      "loss: 0.6248641014099121, accuracy: 0.61\n",
      "loss: 0.6140235662460327, accuracy: 0.65\n",
      "loss: 0.6176725029945374, accuracy: 0.6166666666666667\n",
      "loss: 0.6354920268058777, accuracy: 0.59\n",
      "loss: 0.6121293902397156, accuracy: 0.6033333333333334\n",
      "loss: 0.6174039840698242, accuracy: 0.6266666666666667\n",
      "loss: 0.6194832921028137, accuracy: 0.6366666666666667\n",
      "loss: 0.6549076437950134, accuracy: 0.5966666666666667\n",
      "loss: 0.6267486810684204, accuracy: 0.6233333333333333\n",
      "loss: 0.6151397824287415, accuracy: 0.63\n",
      "loss: 0.6147662401199341, accuracy: 0.63\n",
      "loss: 0.6071216464042664, accuracy: 0.6533333333333333\n",
      "loss: 0.6332331299781799, accuracy: 0.6133333333333333\n",
      "loss: 0.6202650666236877, accuracy: 0.6366666666666667\n",
      "loss: 0.6175950765609741, accuracy: 0.6433333333333333\n",
      "loss: 0.6192002296447754, accuracy: 0.62\n",
      "loss: 0.6423903107643127, accuracy: 0.5833333333333334\n",
      "loss: 0.6145046949386597, accuracy: 0.6366666666666667\n",
      "loss: 0.6209831237792969, accuracy: 0.6333333333333333\n",
      "loss: 0.6385657787322998, accuracy: 0.5966666666666667\n",
      "loss: 0.6204301118850708, accuracy: 0.62\n",
      "loss: 0.6156142354011536, accuracy: 0.6433333333333333\n",
      "loss: 0.6022235155105591, accuracy: 0.6433333333333333\n",
      "loss: 0.6224982142448425, accuracy: 0.6366666666666667\n",
      "loss: 0.6298589706420898, accuracy: 0.6033333333333334\n",
      "loss: 0.6053411960601807, accuracy: 0.6466666666666666\n",
      "loss: 0.6165992021560669, accuracy: 0.6233333333333333\n",
      "loss: 0.6147383451461792, accuracy: 0.6166666666666667\n",
      "loss: 0.6299350261688232, accuracy: 0.5866666666666667\n",
      "loss: 0.6090688109397888, accuracy: 0.63\n",
      "loss: 0.6080006957054138, accuracy: 0.63\n",
      "loss: 0.6154309511184692, accuracy: 0.6266666666666667\n",
      "loss: 0.6162170171737671, accuracy: 0.6366666666666667\n",
      "loss: 0.6127414107322693, accuracy: 0.6333333333333333\n",
      "loss: 0.6005487442016602, accuracy: 0.65\n",
      "loss: 0.6065189838409424, accuracy: 0.6266666666666667\n",
      "loss: 0.6079804301261902, accuracy: 0.6166666666666667\n",
      "loss: 0.6055931448936462, accuracy: 0.6566666666666666\n",
      "loss: 0.6049693822860718, accuracy: 0.6433333333333333\n",
      "loss: 0.648158848285675, accuracy: 0.6\n",
      "loss: 0.5994368195533752, accuracy: 0.6333333333333333\n",
      "loss: 0.6001335382461548, accuracy: 0.63\n",
      "loss: 0.619648814201355, accuracy: 0.6233333333333333\n",
      "loss: 0.6175137162208557, accuracy: 0.6033333333333334\n",
      "loss: 0.6071258783340454, accuracy: 0.65\n",
      "loss: 0.6094344258308411, accuracy: 0.6166666666666667\n",
      "loss: 0.6143254041671753, accuracy: 0.6266666666666667\n",
      "loss: 0.6112804412841797, accuracy: 0.62\n",
      "loss: 0.6052138805389404, accuracy: 0.65\n",
      "loss: 0.618502676486969, accuracy: 0.6033333333333334\n",
      "loss: 0.6069167256355286, accuracy: 0.6333333333333333\n",
      "loss: 0.5953332781791687, accuracy: 0.6333333333333333\n",
      "loss: 0.6100309491157532, accuracy: 0.6366666666666667\n",
      "loss: 0.6080199480056763, accuracy: 0.63\n",
      "loss: 0.6103206872940063, accuracy: 0.6433333333333333\n",
      "max accuracy: 0.66\n",
      "loss: 0.6003443598747253, accuracy: 0.66\n",
      "loss: 0.6045742034912109, accuracy: 0.6433333333333333\n",
      "loss: 0.6088177561759949, accuracy: 0.63\n",
      "loss: 0.5968725681304932, accuracy: 0.65\n",
      "loss: 0.6021636128425598, accuracy: 0.6166666666666667\n",
      "loss: 0.5974581241607666, accuracy: 0.6333333333333333\n",
      "loss: 0.5957360863685608, accuracy: 0.6433333333333333\n",
      "loss: 0.5998858213424683, accuracy: 0.6433333333333333\n",
      "loss: 0.6051734685897827, accuracy: 0.66\n",
      "loss: 0.5959952473640442, accuracy: 0.64\n",
      "loss: 0.600693941116333, accuracy: 0.6466666666666666\n",
      "loss: 0.6159786581993103, accuracy: 0.6166666666666667\n",
      "loss: 0.5957349538803101, accuracy: 0.6166666666666667\n",
      "loss: 0.6351256966590881, accuracy: 0.5866666666666667\n",
      "loss: 0.6044642925262451, accuracy: 0.62\n",
      "loss: 0.602258563041687, accuracy: 0.61\n",
      "loss: 0.5932599902153015, accuracy: 0.63\n",
      "loss: 0.6024085283279419, accuracy: 0.6433333333333333\n",
      "loss: 0.5950863361358643, accuracy: 0.6233333333333333\n",
      "max accuracy: 0.6633333333333333\n",
      "loss: 0.5884522795677185, accuracy: 0.6633333333333333\n",
      "loss: 0.6053716540336609, accuracy: 0.6333333333333333\n",
      "loss: 0.6007382869720459, accuracy: 0.6333333333333333\n",
      "loss: 0.5845414400100708, accuracy: 0.6633333333333333\n",
      "loss: 0.5967652797698975, accuracy: 0.6433333333333333\n",
      "loss: 0.6166513562202454, accuracy: 0.6266666666666667\n",
      "loss: 0.5969827771186829, accuracy: 0.6466666666666666\n",
      "max accuracy: 0.68\n",
      "loss: 0.5916168093681335, accuracy: 0.68\n",
      "loss: 0.5865129232406616, accuracy: 0.6333333333333333\n",
      "loss: 0.6099866628646851, accuracy: 0.6266666666666667\n",
      "loss: 0.592974841594696, accuracy: 0.6233333333333333\n",
      "loss: 0.5907115936279297, accuracy: 0.6633333333333333\n",
      "loss: 0.5943394303321838, accuracy: 0.6533333333333333\n",
      "loss: 0.603749692440033, accuracy: 0.6333333333333333\n",
      "loss: 0.5794856548309326, accuracy: 0.6566666666666666\n",
      "loss: 0.6000614166259766, accuracy: 0.65\n",
      "loss: 0.5977053046226501, accuracy: 0.6166666666666667\n",
      "loss: 0.5929672122001648, accuracy: 0.6333333333333333\n",
      "loss: 0.5804084539413452, accuracy: 0.6533333333333333\n",
      "loss: 0.6026905179023743, accuracy: 0.6533333333333333\n",
      "loss: 0.590897798538208, accuracy: 0.6533333333333333\n",
      "loss: 0.58498615026474, accuracy: 0.6566666666666666\n",
      "loss: 0.601514458656311, accuracy: 0.63\n",
      "loss: 0.5939022302627563, accuracy: 0.6566666666666666\n",
      "loss: 0.5934900641441345, accuracy: 0.6433333333333333\n",
      "loss: 0.5836177468299866, accuracy: 0.6733333333333333\n",
      "loss: 0.5831266641616821, accuracy: 0.6633333333333333\n",
      "loss: 0.6119352579116821, accuracy: 0.6133333333333333\n",
      "loss: 0.5866794586181641, accuracy: 0.6333333333333333\n",
      "max accuracy: 0.6866666666666666\n",
      "loss: 0.5802865624427795, accuracy: 0.6866666666666666\n",
      "loss: 0.5783057808876038, accuracy: 0.68\n",
      "loss: 0.575404942035675, accuracy: 0.6566666666666666\n",
      "loss: 0.5887957215309143, accuracy: 0.6533333333333333\n",
      "loss: 0.5916382670402527, accuracy: 0.64\n",
      "loss: 0.588260293006897, accuracy: 0.65\n",
      "loss: 0.5820598602294922, accuracy: 0.6733333333333333\n",
      "loss: 0.5829804539680481, accuracy: 0.6566666666666666\n",
      "loss: 0.5753612518310547, accuracy: 0.65\n",
      "loss: 0.6041486859321594, accuracy: 0.6133333333333333\n",
      "loss: 0.5994946956634521, accuracy: 0.6166666666666667\n",
      "loss: 0.5724050402641296, accuracy: 0.68\n",
      "loss: 0.5934988260269165, accuracy: 0.6533333333333333\n",
      "loss: 0.5827661156654358, accuracy: 0.6233333333333333\n",
      "loss: 0.5762128233909607, accuracy: 0.6666666666666666\n",
      "loss: 0.5853161215782166, accuracy: 0.6433333333333333\n",
      "loss: 0.5799127817153931, accuracy: 0.6533333333333333\n",
      "loss: 0.5760107636451721, accuracy: 0.6733333333333333\n",
      "loss: 0.5821216106414795, accuracy: 0.6533333333333333\n",
      "loss: 0.5763957500457764, accuracy: 0.65\n",
      "loss: 0.5817859768867493, accuracy: 0.6733333333333333\n",
      "loss: 0.5756692886352539, accuracy: 0.65\n",
      "loss: 0.5761000514030457, accuracy: 0.6666666666666666\n",
      "loss: 0.5707215070724487, accuracy: 0.68\n",
      "loss: 0.5681187510490417, accuracy: 0.67\n",
      "loss: 0.5816991329193115, accuracy: 0.6366666666666667\n",
      "loss: 0.5822950601577759, accuracy: 0.66\n",
      "loss: 0.5766602158546448, accuracy: 0.6566666666666666\n",
      "loss: 0.5802268385887146, accuracy: 0.6633333333333333\n",
      "loss: 0.5783998966217041, accuracy: 0.6633333333333333\n",
      "loss: 0.5747597813606262, accuracy: 0.67\n",
      "loss: 0.5913109183311462, accuracy: 0.6266666666666667\n",
      "loss: 0.5861402153968811, accuracy: 0.6533333333333333\n",
      "loss: 0.575031578540802, accuracy: 0.67\n",
      "max accuracy: 0.7033333333333334\n",
      "loss: 0.5719398260116577, accuracy: 0.7033333333333334\n",
      "loss: 0.5715462565422058, accuracy: 0.6666666666666666\n",
      "loss: 0.5774534344673157, accuracy: 0.67\n",
      "loss: 0.5620923638343811, accuracy: 0.68\n",
      "loss: 0.5771517157554626, accuracy: 0.6833333333333333\n",
      "loss: 0.5767321586608887, accuracy: 0.6733333333333333\n",
      "loss: 0.564470648765564, accuracy: 0.6766666666666666\n",
      "loss: 0.5699188113212585, accuracy: 0.6633333333333333\n",
      "loss: 0.5709900856018066, accuracy: 0.6433333333333333\n",
      "loss: 0.5629558563232422, accuracy: 0.6933333333333334\n",
      "loss: 0.5756857991218567, accuracy: 0.6733333333333333\n",
      "loss: 0.5685790777206421, accuracy: 0.68\n",
      "loss: 0.5677766799926758, accuracy: 0.6633333333333333\n",
      "loss: 0.566290020942688, accuracy: 0.6866666666666666\n",
      "loss: 0.5716217160224915, accuracy: 0.6633333333333333\n",
      "loss: 0.5621728897094727, accuracy: 0.6933333333333334\n",
      "loss: 0.5774438977241516, accuracy: 0.68\n",
      "loss: 0.575066328048706, accuracy: 0.6366666666666667\n",
      "loss: 0.5621306896209717, accuracy: 0.68\n",
      "loss: 0.5707371830940247, accuracy: 0.6366666666666667\n",
      "loss: 0.5762256979942322, accuracy: 0.6566666666666666\n",
      "loss: 0.5665987730026245, accuracy: 0.6666666666666666\n",
      "loss: 0.5619571208953857, accuracy: 0.6866666666666666\n",
      "loss: 0.5736274123191833, accuracy: 0.6533333333333333\n",
      "loss: 0.5820418000221252, accuracy: 0.6666666666666666\n",
      "loss: 0.5613629221916199, accuracy: 0.67\n",
      "loss: 0.5670244693756104, accuracy: 0.6766666666666666\n",
      "loss: 0.5707534551620483, accuracy: 0.66\n",
      "loss: 0.5677696466445923, accuracy: 0.67\n",
      "loss: 0.5606316328048706, accuracy: 0.6666666666666666\n",
      "loss: 0.5803077816963196, accuracy: 0.6433333333333333\n",
      "loss: 0.5538463592529297, accuracy: 0.68\n",
      "loss: 0.5585783123970032, accuracy: 0.68\n",
      "loss: 0.5601502060890198, accuracy: 0.66\n",
      "loss: 0.5821042060852051, accuracy: 0.6366666666666667\n",
      "loss: 0.5623682737350464, accuracy: 0.6766666666666666\n",
      "max accuracy: 0.7133333333333334\n",
      "loss: 0.558370053768158, accuracy: 0.7133333333333334\n",
      "loss: 0.5729647874832153, accuracy: 0.6366666666666667\n",
      "loss: 0.555182158946991, accuracy: 0.6866666666666666\n",
      "loss: 0.5578576922416687, accuracy: 0.66\n",
      "loss: 0.5576211214065552, accuracy: 0.6666666666666666\n",
      "the training time is: 12.376624345779419\n",
      "accuracy: 0.62\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q2.train_evaluate(data_train_iter, data_test_iter, q2.u1, q2.v1, q2.b1, q2.u2, q2.v2, q2.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfa5d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#q2.evaluate_spiral(data_test_iter, q2.u1_max, q2.v1_max, q2.b1_max, q2.u2_max, q2.v2_max, q2.b2_max)\n",
    "#q2.decision_boundary(q2.u1_max, q2.v1_max, q2.b1_max, q2.u2_max, q2.v2_max, q2.b2_max)\n",
    "q2.decision_boundary(q2.u1, q2.v1, q2.b1, q2.u2, q2.v2, q2.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7645bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q2.decision_boundary(q2.u1_max, q2.v1_max, q2.b1_max, q2.u2_max, q2.v2_max, q2.b2_max) # highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310ac7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1ef4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 3]) torch.Size([64, 3]) torch.Size([85, 3]) torch.Size([80, 3])\n"
     ]
    }
   ],
   "source": [
    "data_train_in_0 = []\n",
    "data_train_in_1 = []\n",
    "data_train_out_0 = []\n",
    "data_train_out_1 = []\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    if data_train[i][0] ** 2 + data_train[i][1] ** 2 <= 9.5:\n",
    "        if data_train[i][2]:\n",
    "            data_train_in_1.append(list(data_train[i]))\n",
    "        else:\n",
    "            data_train_in_0.append(list(data_train[i]))\n",
    "    if data_train[i][0] ** 2 + data_train[i][1] ** 2 >= 8.5:\n",
    "        if data_train[i][2]:\n",
    "            data_train_out_1.append(list(data_train[i]))\n",
    "        else:\n",
    "            data_train_out_0.append(list(data_train[i]))\n",
    "            \n",
    "data_train_in_0 = torch.from_numpy(np.array(data_train_in_0))\n",
    "data_train_in_1 = torch.from_numpy(np.array(data_train_in_1))\n",
    "data_train_out_0 = torch.from_numpy(np.array(data_train_out_0))\n",
    "data_train_out_1 = torch.from_numpy(np.array(data_train_out_1))\n",
    "\n",
    "print(data_train_in_0.shape, data_train_in_1.shape, data_train_out_0.shape, data_train_out_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce587a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_in_0 = SpiralNet(hidden = 512, batch = 30, r = 0.1, epoch = 8000)\n",
    "q3_in_1 = SpiralNet(hidden = 512, batch = 30, r = 0.1, epoch = 8000)\n",
    "q3_out_0 = SpiralNet(hidden = 512, batch = 30, r = 0.1, epoch = 8000)\n",
    "q3_out_1 = SpiralNet(hidden = 512, batch = 30, r = 0.1, epoch = 8000)\n",
    "q3_in_0_data = torch.cat((data_train_in_0, data_train_in_1, data_train_out_1), axis = 0)\n",
    "q3_in_1_data = torch.cat((data_train_in_0, data_train_in_1, data_train_out_0), axis = 0)\n",
    "q3_out_0_data = torch.cat((data_train_out_0, data_train_in_1, data_train_out_1), axis = 0)\n",
    "q3_out_1_data = torch.cat((data_train_in_0, data_train_out_1, data_train_out_0), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2530896",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_in_0_iter = q3_in_0.load_data(q3_in_0_data, True)\n",
    "q3_in_1_iter = q3_in_1.load_data(q3_in_1_data, True)\n",
    "q3_out_0_iter = q3_out_0.load_data(q3_out_0_data, True)\n",
    "q3_out_1_iter = q3_out_1.load_data(q3_out_1_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_in_0.train_evaluate(q3_in_0_iter, data_test_iter, q3_in_0.u1, q3_in_0.v1, q3_in_0.b1, q3_in_0.u2, q3_in_0.v2, q3_in_0.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1cf4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_in_0.decision_boundary(q3_in_0.u1, q3_in_0.v1, q3_in_0.b1, q3_in_0.u2, q3_in_0.v2, q3_in_0.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c858ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "loss: nan, accuracy: 0.7205240174672489\n",
      "the training time is: 81.67635369300842\n",
      "accuracy: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_in_1.train_evaluate(q3_in_1_iter, data_test_iter, q3_in_1.u1, q3_in_1.v1, q3_in_1.b1, q3_in_1.u2, q3_in_1.v2, q3_in_1.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02f3c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_in_1.decision_boundary(q3_in_1.u1, q3_in_1.v1, q3_in_1.b1, q3_in_1.u2, q3_in_1.v2, q3_in_1.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2abeb552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.4978165938864629\n",
      "loss: 3.2445266246795654, accuracy: 0.4978165938864629\n",
      "max accuracy: 0.5327510917030568\n",
      "loss: 1.7386746406555176, accuracy: 0.5327510917030568\n",
      "max accuracy: 0.5764192139737991\n",
      "loss: 1.0012255907058716, accuracy: 0.5764192139737991\n",
      "loss: 1.1153762340545654, accuracy: 0.5545851528384279\n",
      "max accuracy: 0.6069868995633187\n",
      "loss: 0.5892412066459656, accuracy: 0.6069868995633187\n",
      "loss: 0.7585726380348206, accuracy: 0.5676855895196506\n",
      "max accuracy: 0.611353711790393\n",
      "loss: 0.6072366237640381, accuracy: 0.611353711790393\n",
      "max accuracy: 0.6157205240174672\n",
      "loss: 0.7263003587722778, accuracy: 0.6157205240174672\n",
      "max accuracy: 0.6244541484716157\n",
      "loss: 0.5856679081916809, accuracy: 0.6244541484716157\n",
      "loss: 0.6332281231880188, accuracy: 0.6157205240174672\n",
      "max accuracy: 0.6593886462882096\n",
      "loss: 0.620800793170929, accuracy: 0.6593886462882096\n",
      "loss: 0.652094841003418, accuracy: 0.6506550218340611\n",
      "loss: 0.6419848799705505, accuracy: 0.6200873362445415\n",
      "loss: 0.5820738077163696, accuracy: 0.6244541484716157\n",
      "loss: 0.6297964453697205, accuracy: 0.6506550218340611\n",
      "loss: 0.5764425992965698, accuracy: 0.6026200873362445\n",
      "loss: 0.592307984828949, accuracy: 0.6506550218340611\n",
      "loss: 0.5742119550704956, accuracy: 0.6157205240174672\n",
      "loss: 0.5753834247589111, accuracy: 0.6375545851528385\n",
      "loss: 0.5885075330734253, accuracy: 0.5720524017467249\n",
      "loss: 0.5626238584518433, accuracy: 0.6244541484716157\n",
      "loss: 0.5695236921310425, accuracy: 0.6331877729257642\n",
      "loss: 0.5552211999893188, accuracy: 0.6200873362445415\n",
      "loss: 0.5811101794242859, accuracy: 0.6200873362445415\n",
      "loss: 0.5506771206855774, accuracy: 0.6069868995633187\n",
      "loss: 0.5689212083816528, accuracy: 0.5938864628820961\n",
      "max accuracy: 0.6637554585152838\n",
      "loss: 0.5449979305267334, accuracy: 0.6637554585152838\n",
      "loss: 0.5548464059829712, accuracy: 0.6244541484716157\n",
      "loss: 0.5523274540901184, accuracy: 0.6462882096069869\n",
      "loss: 0.5773845911026001, accuracy: 0.611353711790393\n",
      "loss: 0.5469679236412048, accuracy: 0.6157205240174672\n",
      "max accuracy: 0.6812227074235808\n",
      "loss: 0.5491122603416443, accuracy: 0.6812227074235808\n",
      "loss: 0.6244320273399353, accuracy: 0.6550218340611353\n",
      "loss: 0.5536167621612549, accuracy: 0.611353711790393\n",
      "loss: 0.5383588075637817, accuracy: 0.6419213973799127\n",
      "loss: 0.5313704609870911, accuracy: 0.6157205240174672\n",
      "loss: 0.5275906920433044, accuracy: 0.6550218340611353\n",
      "max accuracy: 0.6899563318777293\n",
      "loss: 0.544093906879425, accuracy: 0.6899563318777293\n",
      "loss: 0.5267934799194336, accuracy: 0.6550218340611353\n",
      "loss: 0.5345951318740845, accuracy: 0.6550218340611353\n",
      "loss: 0.525783360004425, accuracy: 0.6637554585152838\n",
      "loss: 0.5472195148468018, accuracy: 0.6681222707423581\n",
      "loss: 0.55550217628479, accuracy: 0.6506550218340611\n",
      "loss: 0.5874832272529602, accuracy: 0.62882096069869\n",
      "loss: 0.5532337427139282, accuracy: 0.6681222707423581\n",
      "loss: 0.5293149352073669, accuracy: 0.6768558951965066\n",
      "loss: 0.5203726291656494, accuracy: 0.6681222707423581\n",
      "loss: 0.5241140723228455, accuracy: 0.6637554585152838\n",
      "loss: 0.5474058985710144, accuracy: 0.6812227074235808\n",
      "max accuracy: 0.7117903930131004\n",
      "loss: 0.5276212096214294, accuracy: 0.7117903930131004\n",
      "loss: 0.5531826615333557, accuracy: 0.6375545851528385\n",
      "max accuracy: 0.7292576419213974\n",
      "loss: 0.5180102586746216, accuracy: 0.7292576419213974\n",
      "loss: 0.5234258770942688, accuracy: 0.6855895196506551\n",
      "max accuracy: 0.7336244541484717\n",
      "loss: 0.5185610055923462, accuracy: 0.7336244541484717\n",
      "loss: 0.5753368139266968, accuracy: 0.611353711790393\n",
      "loss: 0.5310727953910828, accuracy: 0.6943231441048034\n",
      "loss: 0.5142686367034912, accuracy: 0.7205240174672489\n",
      "loss: 0.5327398180961609, accuracy: 0.6637554585152838\n",
      "loss: 0.5207251906394958, accuracy: 0.6855895196506551\n",
      "loss: 0.5235861539840698, accuracy: 0.7292576419213974\n",
      "loss: 0.5278147459030151, accuracy: 0.7074235807860262\n",
      "loss: 0.5140954256057739, accuracy: 0.6637554585152838\n",
      "loss: 0.5317156314849854, accuracy: 0.6986899563318777\n",
      "loss: 0.5222616791725159, accuracy: 0.7074235807860262\n",
      "loss: 0.5188888907432556, accuracy: 0.6855895196506551\n",
      "loss: 0.5115720629692078, accuracy: 0.6855895196506551\n",
      "loss: 0.5126909017562866, accuracy: 0.7161572052401747\n",
      "loss: 0.5349183678627014, accuracy: 0.6506550218340611\n",
      "loss: 0.5069087743759155, accuracy: 0.7030567685589519\n",
      "loss: 0.5103810429573059, accuracy: 0.7030567685589519\n",
      "loss: 0.5080488324165344, accuracy: 0.7292576419213974\n",
      "loss: 0.5044369101524353, accuracy: 0.7074235807860262\n",
      "loss: 0.5034680366516113, accuracy: 0.7074235807860262\n",
      "loss: 0.4994596242904663, accuracy: 0.7292576419213974\n",
      "loss: 0.5049228668212891, accuracy: 0.6986899563318777\n",
      "loss: 0.5079860687255859, accuracy: 0.6943231441048034\n",
      "loss: 0.5034735798835754, accuracy: 0.7030567685589519\n",
      "loss: 0.5061543583869934, accuracy: 0.7074235807860262\n",
      "loss: 0.501204252243042, accuracy: 0.7336244541484717\n",
      "loss: 0.5061920285224915, accuracy: 0.7205240174672489\n",
      "loss: 0.511335015296936, accuracy: 0.6855895196506551\n",
      "max accuracy: 0.74235807860262\n",
      "loss: 0.5092741847038269, accuracy: 0.74235807860262\n",
      "loss: 0.5045462250709534, accuracy: 0.7205240174672489\n",
      "loss: 0.5227909088134766, accuracy: 0.6986899563318777\n",
      "max accuracy: 0.7554585152838428\n",
      "loss: 0.4997839331626892, accuracy: 0.7554585152838428\n",
      "loss: 0.5181370973587036, accuracy: 0.7117903930131004\n",
      "loss: 0.5043377876281738, accuracy: 0.6855895196506551\n",
      "loss: 0.5286591649055481, accuracy: 0.6943231441048034\n",
      "loss: 0.4952198565006256, accuracy: 0.7467248908296943\n",
      "loss: 0.50116366147995, accuracy: 0.7379912663755459\n",
      "loss: 0.494186133146286, accuracy: 0.7467248908296943\n",
      "loss: 0.5076276063919067, accuracy: 0.7205240174672489\n",
      "loss: 0.5162866115570068, accuracy: 0.7074235807860262\n",
      "loss: 0.4942668080329895, accuracy: 0.7117903930131004\n",
      "loss: 0.5058502554893494, accuracy: 0.7336244541484717\n",
      "loss: 0.5043085217475891, accuracy: 0.7161572052401747\n",
      "loss: 0.5041965842247009, accuracy: 0.7379912663755459\n",
      "loss: 0.4932483732700348, accuracy: 0.7467248908296943\n",
      "loss: 0.494087815284729, accuracy: 0.7248908296943232\n",
      "loss: 0.5039117932319641, accuracy: 0.7510917030567685\n",
      "loss: 0.4921354055404663, accuracy: 0.74235807860262\n",
      "loss: 0.5010346174240112, accuracy: 0.7117903930131004\n",
      "loss: 0.49901139736175537, accuracy: 0.7117903930131004\n",
      "loss: 0.5008456110954285, accuracy: 0.7205240174672489\n",
      "loss: 0.4891436696052551, accuracy: 0.7074235807860262\n",
      "loss: 0.48954305052757263, accuracy: 0.6986899563318777\n",
      "loss: 0.507691502571106, accuracy: 0.7248908296943232\n",
      "loss: 0.49636292457580566, accuracy: 0.7161572052401747\n",
      "loss: 0.494059294462204, accuracy: 0.74235807860262\n",
      "loss: 0.4933297634124756, accuracy: 0.7379912663755459\n",
      "loss: 0.49836012721061707, accuracy: 0.7030567685589519\n",
      "loss: 0.48909637331962585, accuracy: 0.7554585152838428\n",
      "loss: 0.4907727539539337, accuracy: 0.7336244541484717\n",
      "loss: 0.4992537200450897, accuracy: 0.7205240174672489\n",
      "loss: 0.48693785071372986, accuracy: 0.7336244541484717\n",
      "loss: 0.49436095356941223, accuracy: 0.7248908296943232\n",
      "loss: 0.4948856830596924, accuracy: 0.7205240174672489\n",
      "loss: 0.49285486340522766, accuracy: 0.7379912663755459\n",
      "loss: 0.4926994740962982, accuracy: 0.7248908296943232\n",
      "loss: 0.48602527379989624, accuracy: 0.7161572052401747\n",
      "loss: 0.49932926893234253, accuracy: 0.7248908296943232\n",
      "loss: 0.48844072222709656, accuracy: 0.7379912663755459\n",
      "loss: 0.49202024936676025, accuracy: 0.7248908296943232\n",
      "loss: 0.4852762818336487, accuracy: 0.7510917030567685\n",
      "loss: 0.4884389638900757, accuracy: 0.7292576419213974\n",
      "loss: 0.48366665840148926, accuracy: 0.7467248908296943\n",
      "loss: 0.4845868945121765, accuracy: 0.7292576419213974\n",
      "loss: 0.4957656264305115, accuracy: 0.7074235807860262\n",
      "loss: 0.49663442373275757, accuracy: 0.74235807860262\n",
      "loss: 0.4812077581882477, accuracy: 0.74235807860262\n",
      "loss: 0.48652198910713196, accuracy: 0.7554585152838428\n",
      "loss: 0.48755112290382385, accuracy: 0.7248908296943232\n",
      "max accuracy: 0.7641921397379913\n",
      "loss: 0.48442545533180237, accuracy: 0.7641921397379913\n",
      "loss: 0.4860285520553589, accuracy: 0.7467248908296943\n",
      "loss: 0.4854970872402191, accuracy: 0.7379912663755459\n",
      "loss: 0.48058024048805237, accuracy: 0.7510917030567685\n",
      "loss: 0.47862064838409424, accuracy: 0.7379912663755459\n",
      "loss: 0.480406254529953, accuracy: 0.7510917030567685\n",
      "loss: 0.48335179686546326, accuracy: 0.7161572052401747\n",
      "loss: 0.49777939915657043, accuracy: 0.7379912663755459\n",
      "loss: 0.48055750131607056, accuracy: 0.7205240174672489\n",
      "loss: 0.48298379778862, accuracy: 0.759825327510917\n",
      "loss: 0.4810434579849243, accuracy: 0.7554585152838428\n",
      "loss: 0.4951929450035095, accuracy: 0.7117903930131004\n",
      "max accuracy: 0.7685589519650655\n",
      "loss: 0.48578840494155884, accuracy: 0.7685589519650655\n",
      "loss: 0.48732760548591614, accuracy: 0.7336244541484717\n",
      "loss: 0.47850754857063293, accuracy: 0.759825327510917\n",
      "loss: 0.480579137802124, accuracy: 0.7554585152838428\n",
      "loss: 0.4780426323413849, accuracy: 0.74235807860262\n",
      "loss: 0.4811936616897583, accuracy: 0.7467248908296943\n",
      "loss: 0.48587295413017273, accuracy: 0.74235807860262\n",
      "loss: 0.4776371717453003, accuracy: 0.7379912663755459\n",
      "max accuracy: 0.7729257641921398\n",
      "loss: 0.4785120487213135, accuracy: 0.7729257641921398\n",
      "loss: 0.49424633383750916, accuracy: 0.7379912663755459\n",
      "loss: 0.4742870330810547, accuracy: 0.7554585152838428\n",
      "loss: 0.47532835602760315, accuracy: 0.7554585152838428\n",
      "loss: 0.47753018140792847, accuracy: 0.7510917030567685\n",
      "loss: 0.4717898368835449, accuracy: 0.7641921397379913\n",
      "loss: 0.4808485805988312, accuracy: 0.7379912663755459\n",
      "loss: 0.478470116853714, accuracy: 0.7379912663755459\n",
      "loss: 0.48027998208999634, accuracy: 0.7554585152838428\n",
      "loss: 0.47615373134613037, accuracy: 0.7685589519650655\n",
      "loss: 0.47592493891716003, accuracy: 0.7554585152838428\n",
      "loss: 0.4753589928150177, accuracy: 0.7467248908296943\n",
      "loss: 0.47439706325531006, accuracy: 0.7467248908296943\n",
      "loss: 0.47892555594444275, accuracy: 0.759825327510917\n",
      "loss: 0.48805665969848633, accuracy: 0.7467248908296943\n",
      "loss: 0.4762715697288513, accuracy: 0.7379912663755459\n",
      "loss: 0.4743334650993347, accuracy: 0.7685589519650655\n",
      "loss: 0.48218590021133423, accuracy: 0.7379912663755459\n",
      "loss: 0.4781079590320587, accuracy: 0.7641921397379913\n",
      "loss: 0.4767051339149475, accuracy: 0.7292576419213974\n",
      "loss: 0.4702155292034149, accuracy: 0.7729257641921398\n",
      "loss: 0.475032776594162, accuracy: 0.7510917030567685\n",
      "loss: 0.4724818170070648, accuracy: 0.759825327510917\n",
      "loss: 0.4719993472099304, accuracy: 0.7554585152838428\n",
      "loss: 0.4746614694595337, accuracy: 0.7379912663755459\n",
      "loss: 0.47519946098327637, accuracy: 0.7554585152838428\n",
      "loss: 0.4685528576374054, accuracy: 0.74235807860262\n",
      "loss: 0.47174039483070374, accuracy: 0.759825327510917\n",
      "loss: 0.4682140350341797, accuracy: 0.7554585152838428\n",
      "loss: 0.4665834605693817, accuracy: 0.759825327510917\n",
      "loss: 0.47391897439956665, accuracy: 0.7554585152838428\n",
      "loss: 0.4731611907482147, accuracy: 0.759825327510917\n",
      "loss: 0.4667064845561981, accuracy: 0.7510917030567685\n",
      "loss: 0.4706208407878876, accuracy: 0.7554585152838428\n",
      "loss: 0.4753202795982361, accuracy: 0.7336244541484717\n",
      "loss: 0.4698832631111145, accuracy: 0.7554585152838428\n",
      "loss: 0.48176947236061096, accuracy: 0.7554585152838428\n",
      "loss: 0.4774562120437622, accuracy: 0.7510917030567685\n",
      "loss: 0.4701244831085205, accuracy: 0.7641921397379913\n",
      "loss: 0.47594696283340454, accuracy: 0.74235807860262\n",
      "loss: 0.46395769715309143, accuracy: 0.7641921397379913\n",
      "loss: 0.47036218643188477, accuracy: 0.74235807860262\n",
      "loss: 0.4649708867073059, accuracy: 0.7641921397379913\n",
      "loss: 0.4722558259963989, accuracy: 0.759825327510917\n",
      "loss: 0.465634286403656, accuracy: 0.7641921397379913\n",
      "loss: 0.46641701459884644, accuracy: 0.759825327510917\n",
      "loss: 0.470327228307724, accuracy: 0.7510917030567685\n",
      "loss: 0.4640081226825714, accuracy: 0.7729257641921398\n",
      "loss: 0.4694158136844635, accuracy: 0.7554585152838428\n",
      "loss: 0.46369126439094543, accuracy: 0.7729257641921398\n",
      "loss: 0.4661705195903778, accuracy: 0.759825327510917\n",
      "loss: 0.46532148122787476, accuracy: 0.74235807860262\n",
      "loss: 0.4700324237346649, accuracy: 0.7510917030567685\n",
      "loss: 0.46640923619270325, accuracy: 0.759825327510917\n",
      "loss: 0.4622056782245636, accuracy: 0.7729257641921398\n",
      "loss: 0.4707353115081787, accuracy: 0.7641921397379913\n",
      "loss: 0.46223926544189453, accuracy: 0.7554585152838428\n",
      "loss: 0.46294525265693665, accuracy: 0.7554585152838428\n",
      "loss: 0.46323367953300476, accuracy: 0.7379912663755459\n",
      "loss: 0.46407273411750793, accuracy: 0.759825327510917\n",
      "loss: 0.46529683470726013, accuracy: 0.7510917030567685\n",
      "loss: 0.4676514267921448, accuracy: 0.7641921397379913\n",
      "loss: 0.4629060626029968, accuracy: 0.7467248908296943\n",
      "loss: 0.4679386615753174, accuracy: 0.7685589519650655\n",
      "loss: 0.4592444598674774, accuracy: 0.759825327510917\n",
      "loss: 0.4642220735549927, accuracy: 0.7467248908296943\n",
      "loss: 0.4674645662307739, accuracy: 0.7729257641921398\n",
      "loss: 0.4649313688278198, accuracy: 0.759825327510917\n",
      "loss: 0.46704986691474915, accuracy: 0.7554585152838428\n",
      "loss: 0.46259722113609314, accuracy: 0.759825327510917\n",
      "loss: 0.45998668670654297, accuracy: 0.7641921397379913\n",
      "loss: 0.4586735963821411, accuracy: 0.7510917030567685\n",
      "loss: 0.46077191829681396, accuracy: 0.7641921397379913\n",
      "loss: 0.45970427989959717, accuracy: 0.7641921397379913\n",
      "loss: 0.46264779567718506, accuracy: 0.7510917030567685\n",
      "loss: 0.46288445591926575, accuracy: 0.7641921397379913\n",
      "loss: 0.46361392736434937, accuracy: 0.7467248908296943\n",
      "loss: 0.46272191405296326, accuracy: 0.7467248908296943\n",
      "loss: 0.45843589305877686, accuracy: 0.7685589519650655\n",
      "loss: 0.4597961902618408, accuracy: 0.759825327510917\n",
      "loss: 0.45703446865081787, accuracy: 0.7685589519650655\n",
      "max accuracy: 0.777292576419214\n",
      "loss: 0.4550993740558624, accuracy: 0.777292576419214\n",
      "loss: 0.457675963640213, accuracy: 0.759825327510917\n",
      "loss: 0.45513951778411865, accuracy: 0.7685589519650655\n",
      "loss: 0.4559982419013977, accuracy: 0.759825327510917\n",
      "loss: 0.4560850262641907, accuracy: 0.759825327510917\n",
      "loss: 0.4588255286216736, accuracy: 0.7554585152838428\n",
      "loss: 0.45748287439346313, accuracy: 0.777292576419214\n",
      "loss: 0.4583157002925873, accuracy: 0.7685589519650655\n",
      "loss: 0.4562855064868927, accuracy: 0.7510917030567685\n",
      "loss: 0.46410372853279114, accuracy: 0.759825327510917\n",
      "loss: 0.45319122076034546, accuracy: 0.7641921397379913\n",
      "loss: 0.4573059380054474, accuracy: 0.7510917030567685\n",
      "loss: 0.45407527685165405, accuracy: 0.7641921397379913\n",
      "loss: 0.4570562243461609, accuracy: 0.7641921397379913\n",
      "loss: 0.4697146713733673, accuracy: 0.74235807860262\n",
      "loss: 0.4519137442111969, accuracy: 0.7685589519650655\n",
      "loss: 0.45347025990486145, accuracy: 0.759825327510917\n",
      "loss: 0.4537867307662964, accuracy: 0.759825327510917\n",
      "loss: 0.45298096537590027, accuracy: 0.7641921397379913\n",
      "loss: 0.466386616230011, accuracy: 0.7467248908296943\n",
      "loss: 0.4508214592933655, accuracy: 0.7641921397379913\n",
      "loss: 0.44773417711257935, accuracy: 0.7729257641921398\n",
      "loss: 0.46598732471466064, accuracy: 0.7379912663755459\n",
      "loss: 0.45428159832954407, accuracy: 0.7554585152838428\n",
      "loss: 0.451017826795578, accuracy: 0.7685589519650655\n",
      "loss: 0.45798754692077637, accuracy: 0.7685589519650655\n",
      "loss: 0.4547547698020935, accuracy: 0.7685589519650655\n",
      "loss: 0.45688629150390625, accuracy: 0.7510917030567685\n",
      "loss: 0.45384255051612854, accuracy: 0.7685589519650655\n",
      "loss: 0.4516623020172119, accuracy: 0.7685589519650655\n",
      "loss: 0.46131280064582825, accuracy: 0.7554585152838428\n",
      "loss: 0.4513803720474243, accuracy: 0.7641921397379913\n",
      "loss: 0.4609837830066681, accuracy: 0.7510917030567685\n",
      "loss: 0.45007628202438354, accuracy: 0.7554585152838428\n",
      "loss: 0.4494001865386963, accuracy: 0.7729257641921398\n",
      "loss: 0.4506472051143646, accuracy: 0.7685589519650655\n",
      "loss: 0.4476545453071594, accuracy: 0.7685589519650655\n",
      "loss: 0.44818419218063354, accuracy: 0.7641921397379913\n",
      "loss: 0.4468975365161896, accuracy: 0.7641921397379913\n",
      "loss: 0.4486921429634094, accuracy: 0.7685589519650655\n",
      "loss: 0.4479738175868988, accuracy: 0.7554585152838428\n",
      "loss: 0.45325905084609985, accuracy: 0.7554585152838428\n",
      "loss: 0.4558216333389282, accuracy: 0.7510917030567685\n",
      "loss: 0.4526262581348419, accuracy: 0.759825327510917\n",
      "loss: 0.4556155800819397, accuracy: 0.7641921397379913\n",
      "loss: 0.4450223743915558, accuracy: 0.7641921397379913\n",
      "loss: 0.4494808614253998, accuracy: 0.759825327510917\n",
      "loss: 0.44555723667144775, accuracy: 0.759825327510917\n",
      "loss: 0.44720906019210815, accuracy: 0.759825327510917\n",
      "loss: 0.4498662054538727, accuracy: 0.7685589519650655\n",
      "loss: 0.44713258743286133, accuracy: 0.777292576419214\n",
      "loss: 0.4481638967990875, accuracy: 0.7641921397379913\n",
      "loss: 0.44465121626853943, accuracy: 0.7641921397379913\n",
      "loss: 0.4469880759716034, accuracy: 0.7685589519650655\n",
      "loss: 0.4429406523704529, accuracy: 0.7685589519650655\n",
      "loss: 0.4427916407585144, accuracy: 0.7467248908296943\n",
      "loss: 0.44576823711395264, accuracy: 0.7729257641921398\n",
      "loss: 0.4407985508441925, accuracy: 0.7729257641921398\n",
      "loss: 0.44626256823539734, accuracy: 0.7641921397379913\n",
      "loss: 0.442038893699646, accuracy: 0.7729257641921398\n",
      "loss: 0.4451143145561218, accuracy: 0.7729257641921398\n",
      "loss: 0.44525423645973206, accuracy: 0.759825327510917\n",
      "loss: 0.4440557062625885, accuracy: 0.7641921397379913\n",
      "loss: 0.44350141286849976, accuracy: 0.7641921397379913\n",
      "loss: 0.4459361135959625, accuracy: 0.777292576419214\n",
      "loss: 0.4484325349330902, accuracy: 0.7554585152838428\n",
      "loss: 0.4445263147354126, accuracy: 0.759825327510917\n",
      "loss: 0.44115281105041504, accuracy: 0.7729257641921398\n",
      "loss: 0.4490935504436493, accuracy: 0.7641921397379913\n",
      "loss: 0.44310638308525085, accuracy: 0.759825327510917\n",
      "loss: 0.44724828004837036, accuracy: 0.759825327510917\n",
      "loss: 0.4422813355922699, accuracy: 0.7685589519650655\n",
      "loss: 0.45131534337997437, accuracy: 0.7641921397379913\n",
      "max accuracy: 0.7860262008733624\n",
      "loss: 0.4469497501850128, accuracy: 0.7860262008733624\n",
      "loss: 0.44296953082084656, accuracy: 0.759825327510917\n",
      "loss: 0.44523924589157104, accuracy: 0.7641921397379913\n",
      "loss: 0.4427397847175598, accuracy: 0.7685589519650655\n",
      "loss: 0.4459501802921295, accuracy: 0.759825327510917\n",
      "loss: 0.4371011257171631, accuracy: 0.7641921397379913\n",
      "loss: 0.4338972270488739, accuracy: 0.7641921397379913\n",
      "loss: 0.4385974407196045, accuracy: 0.7729257641921398\n",
      "loss: 0.44013649225234985, accuracy: 0.7729257641921398\n",
      "loss: 0.4404245615005493, accuracy: 0.7729257641921398\n",
      "loss: 0.4390721023082733, accuracy: 0.7641921397379913\n",
      "loss: 0.43758219480514526, accuracy: 0.7729257641921398\n",
      "loss: 0.44500598311424255, accuracy: 0.7641921397379913\n",
      "loss: 0.4405847489833832, accuracy: 0.7860262008733624\n",
      "loss: 0.44428640604019165, accuracy: 0.7554585152838428\n",
      "loss: 0.45286086201667786, accuracy: 0.74235807860262\n",
      "loss: 0.4409313201904297, accuracy: 0.7729257641921398\n",
      "loss: 0.448562890291214, accuracy: 0.7816593886462883\n",
      "loss: 0.44476455450057983, accuracy: 0.7510917030567685\n",
      "loss: 0.44142255187034607, accuracy: 0.7641921397379913\n",
      "loss: 0.43437325954437256, accuracy: 0.7554585152838428\n",
      "loss: 0.4354321360588074, accuracy: 0.7816593886462883\n",
      "loss: 0.43370509147644043, accuracy: 0.7685589519650655\n",
      "loss: 0.4350963830947876, accuracy: 0.7729257641921398\n",
      "loss: 0.43297645449638367, accuracy: 0.7860262008733624\n",
      "loss: 0.4375411570072174, accuracy: 0.7685589519650655\n",
      "loss: 0.43606942892074585, accuracy: 0.7816593886462883\n",
      "loss: 0.4381409287452698, accuracy: 0.7554585152838428\n",
      "loss: 0.4390038549900055, accuracy: 0.7729257641921398\n",
      "loss: 0.44001296162605286, accuracy: 0.777292576419214\n",
      "loss: 0.43285033106803894, accuracy: 0.777292576419214\n",
      "loss: 0.4317440688610077, accuracy: 0.7685589519650655\n",
      "loss: 0.4374140501022339, accuracy: 0.7685589519650655\n",
      "loss: 0.4381561875343323, accuracy: 0.759825327510917\n",
      "loss: 0.4337662160396576, accuracy: 0.777292576419214\n",
      "loss: 0.4344416558742523, accuracy: 0.7641921397379913\n",
      "loss: 0.4363240599632263, accuracy: 0.7860262008733624\n",
      "loss: 0.437774658203125, accuracy: 0.7729257641921398\n",
      "loss: 0.43420183658599854, accuracy: 0.7816593886462883\n",
      "loss: 0.4327027201652527, accuracy: 0.7860262008733624\n",
      "loss: 0.43327322602272034, accuracy: 0.7685589519650655\n",
      "loss: 0.42986392974853516, accuracy: 0.7641921397379913\n",
      "loss: 0.43617498874664307, accuracy: 0.7685589519650655\n",
      "loss: 0.42810964584350586, accuracy: 0.7860262008733624\n",
      "max accuracy: 0.7903930131004366\n",
      "loss: 0.43263566493988037, accuracy: 0.7903930131004366\n",
      "loss: 0.4318554401397705, accuracy: 0.7554585152838428\n",
      "loss: 0.43072396516799927, accuracy: 0.7860262008733624\n",
      "loss: 0.44092875719070435, accuracy: 0.759825327510917\n",
      "loss: 0.438204288482666, accuracy: 0.7685589519650655\n",
      "loss: 0.42914310097694397, accuracy: 0.7903930131004366\n",
      "loss: 0.4361530840396881, accuracy: 0.759825327510917\n",
      "loss: 0.4286983013153076, accuracy: 0.7685589519650655\n",
      "loss: 0.4237059950828552, accuracy: 0.777292576419214\n",
      "loss: 0.43165645003318787, accuracy: 0.7816593886462883\n",
      "loss: 0.4299338161945343, accuracy: 0.7860262008733624\n",
      "loss: 0.43863222002983093, accuracy: 0.7510917030567685\n",
      "loss: 0.4300563633441925, accuracy: 0.7816593886462883\n",
      "loss: 0.43063342571258545, accuracy: 0.7816593886462883\n",
      "loss: 0.42217615246772766, accuracy: 0.7816593886462883\n",
      "loss: 0.4393773376941681, accuracy: 0.7685589519650655\n",
      "loss: 0.42972368001937866, accuracy: 0.7816593886462883\n",
      "loss: 0.43785181641578674, accuracy: 0.7641921397379913\n",
      "max accuracy: 0.7947598253275109\n",
      "loss: 0.4282282590866089, accuracy: 0.7947598253275109\n",
      "loss: 0.42393961548805237, accuracy: 0.7903930131004366\n",
      "loss: 0.43496233224868774, accuracy: 0.7729257641921398\n",
      "loss: 0.4272870421409607, accuracy: 0.7816593886462883\n",
      "loss: 0.4289466440677643, accuracy: 0.7685589519650655\n",
      "loss: 0.4325229823589325, accuracy: 0.7816593886462883\n",
      "loss: 0.43476665019989014, accuracy: 0.7816593886462883\n",
      "loss: 0.4346327483654022, accuracy: 0.7685589519650655\n",
      "loss: 0.42756596207618713, accuracy: 0.7554585152838428\n",
      "max accuracy: 0.7991266375545851\n",
      "loss: 0.42540428042411804, accuracy: 0.7991266375545851\n",
      "loss: 0.4242531359195709, accuracy: 0.7860262008733624\n",
      "loss: 0.42405131459236145, accuracy: 0.7860262008733624\n",
      "max accuracy: 0.8078602620087336\n",
      "loss: 0.4199298322200775, accuracy: 0.8078602620087336\n",
      "loss: 0.43480291962623596, accuracy: 0.7554585152838428\n",
      "loss: 0.42355793714523315, accuracy: 0.7860262008733624\n",
      "loss: 0.427748441696167, accuracy: 0.7729257641921398\n",
      "loss: 0.4209589958190918, accuracy: 0.7991266375545851\n",
      "loss: 0.42058223485946655, accuracy: 0.7903930131004366\n",
      "loss: 0.4297996461391449, accuracy: 0.7816593886462883\n",
      "loss: 0.4215726852416992, accuracy: 0.7860262008733624\n",
      "loss: 0.42207852005958557, accuracy: 0.777292576419214\n",
      "loss: 0.4193135201931, accuracy: 0.7860262008733624\n",
      "loss: 0.4242689311504364, accuracy: 0.7947598253275109\n",
      "loss: 0.43101224303245544, accuracy: 0.7554585152838428\n",
      "loss: 0.42870816588401794, accuracy: 0.7991266375545851\n",
      "loss: 0.4314034581184387, accuracy: 0.7947598253275109\n",
      "loss: 0.4288843870162964, accuracy: 0.777292576419214\n",
      "loss: 0.4193035066127777, accuracy: 0.7947598253275109\n",
      "loss: 0.41835975646972656, accuracy: 0.7729257641921398\n",
      "loss: 0.4204578697681427, accuracy: 0.7903930131004366\n",
      "loss: 0.4261406660079956, accuracy: 0.7729257641921398\n",
      "loss: 0.4179953634738922, accuracy: 0.8078602620087336\n",
      "loss: 0.4279569387435913, accuracy: 0.7729257641921398\n",
      "loss: 0.4169614017009735, accuracy: 0.7860262008733624\n",
      "loss: 0.4158082902431488, accuracy: 0.7903930131004366\n",
      "loss: 0.41584062576293945, accuracy: 0.7860262008733624\n",
      "loss: 0.42319899797439575, accuracy: 0.8034934497816594\n",
      "loss: 0.41686248779296875, accuracy: 0.8034934497816594\n",
      "loss: 0.4223417043685913, accuracy: 0.7947598253275109\n",
      "loss: 0.4142569601535797, accuracy: 0.8078602620087336\n",
      "loss: 0.41432836651802063, accuracy: 0.7991266375545851\n",
      "loss: 0.42041128873825073, accuracy: 0.7903930131004366\n",
      "loss: 0.4231521487236023, accuracy: 0.7860262008733624\n",
      "loss: 0.4273383915424347, accuracy: 0.7860262008733624\n",
      "loss: 0.41739749908447266, accuracy: 0.7991266375545851\n",
      "loss: 0.4201701879501343, accuracy: 0.8034934497816594\n",
      "loss: 0.4124673008918762, accuracy: 0.7860262008733624\n",
      "loss: 0.4161120355129242, accuracy: 0.7816593886462883\n",
      "max accuracy: 0.8122270742358079\n",
      "loss: 0.41076481342315674, accuracy: 0.8122270742358079\n",
      "loss: 0.4182076156139374, accuracy: 0.8034934497816594\n",
      "loss: 0.4185043275356293, accuracy: 0.7860262008733624\n",
      "loss: 0.41187164187431335, accuracy: 0.7903930131004366\n",
      "loss: 0.4203253388404846, accuracy: 0.8034934497816594\n",
      "loss: 0.41511625051498413, accuracy: 0.8034934497816594\n",
      "loss: 0.41805529594421387, accuracy: 0.7991266375545851\n",
      "max accuracy: 0.8165938864628821\n",
      "loss: 0.41317591071128845, accuracy: 0.8165938864628821\n",
      "loss: 0.42879101634025574, accuracy: 0.7903930131004366\n",
      "loss: 0.4094032645225525, accuracy: 0.8165938864628821\n",
      "loss: 0.41354304552078247, accuracy: 0.8034934497816594\n",
      "loss: 0.4291369616985321, accuracy: 0.777292576419214\n",
      "loss: 0.41542020440101624, accuracy: 0.7947598253275109\n",
      "loss: 0.4146748185157776, accuracy: 0.7991266375545851\n",
      "loss: 0.4109843969345093, accuracy: 0.8078602620087336\n",
      "loss: 0.40983009338378906, accuracy: 0.8034934497816594\n",
      "loss: 0.41257473826408386, accuracy: 0.8034934497816594\n",
      "loss: 0.4181860089302063, accuracy: 0.7816593886462883\n",
      "loss: 0.41081953048706055, accuracy: 0.7947598253275109\n",
      "loss: 0.4104992747306824, accuracy: 0.8034934497816594\n",
      "loss: 0.4121306538581848, accuracy: 0.7860262008733624\n",
      "loss: 0.4090818762779236, accuracy: 0.8034934497816594\n",
      "loss: 0.4074461758136749, accuracy: 0.8078602620087336\n",
      "loss: 0.4131881892681122, accuracy: 0.7903930131004366\n",
      "loss: 0.413827508687973, accuracy: 0.7991266375545851\n",
      "loss: 0.41303449869155884, accuracy: 0.8165938864628821\n",
      "loss: 0.4002673923969269, accuracy: 0.8034934497816594\n",
      "loss: 0.41209232807159424, accuracy: 0.8078602620087336\n",
      "loss: 0.4132234454154968, accuracy: 0.777292576419214\n",
      "loss: 0.41735413670539856, accuracy: 0.7685589519650655\n",
      "loss: 0.4042343497276306, accuracy: 0.8122270742358079\n",
      "loss: 0.4078483283519745, accuracy: 0.8122270742358079\n",
      "loss: 0.4173515737056732, accuracy: 0.7991266375545851\n",
      "loss: 0.40741896629333496, accuracy: 0.7947598253275109\n",
      "loss: 0.409536749124527, accuracy: 0.7947598253275109\n",
      "loss: 0.41339337825775146, accuracy: 0.7991266375545851\n",
      "loss: 0.4156339764595032, accuracy: 0.7641921397379913\n",
      "loss: 0.4078351557254791, accuracy: 0.8078602620087336\n",
      "loss: 0.4132179617881775, accuracy: 0.7991266375545851\n",
      "loss: 0.40946149826049805, accuracy: 0.8034934497816594\n",
      "loss: 0.40214672684669495, accuracy: 0.8034934497816594\n",
      "loss: 0.4037609100341797, accuracy: 0.7991266375545851\n",
      "max accuracy: 0.8209606986899564\n",
      "loss: 0.400265634059906, accuracy: 0.8209606986899564\n",
      "loss: 0.40206536650657654, accuracy: 0.8122270742358079\n",
      "loss: 0.4089256823062897, accuracy: 0.8078602620087336\n",
      "loss: 0.4040718972682953, accuracy: 0.8034934497816594\n",
      "loss: 0.40376198291778564, accuracy: 0.8165938864628821\n",
      "loss: 0.4107912480831146, accuracy: 0.7947598253275109\n",
      "loss: 0.4011611342430115, accuracy: 0.8034934497816594\n",
      "loss: 0.409779816865921, accuracy: 0.8078602620087336\n",
      "loss: 0.40180590748786926, accuracy: 0.8165938864628821\n",
      "max accuracy: 0.834061135371179\n",
      "loss: 0.39721187949180603, accuracy: 0.834061135371179\n",
      "loss: 0.4094277322292328, accuracy: 0.7685589519650655\n",
      "loss: 0.400704950094223, accuracy: 0.8122270742358079\n",
      "loss: 0.40958285331726074, accuracy: 0.8165938864628821\n",
      "loss: 0.40155428647994995, accuracy: 0.8034934497816594\n",
      "loss: 0.41381847858428955, accuracy: 0.7903930131004366\n",
      "loss: 0.4009895920753479, accuracy: 0.8034934497816594\n",
      "loss: 0.40860041975975037, accuracy: 0.7903930131004366\n",
      "loss: 0.403992623090744, accuracy: 0.8165938864628821\n",
      "loss: 0.4046227037906647, accuracy: 0.8122270742358079\n",
      "loss: 0.39946502447128296, accuracy: 0.7947598253275109\n",
      "loss: 0.4003284275531769, accuracy: 0.8034934497816594\n",
      "loss: 0.39384573698043823, accuracy: 0.8253275109170306\n",
      "loss: 0.40438058972358704, accuracy: 0.8034934497816594\n",
      "loss: 0.40986379981040955, accuracy: 0.8034934497816594\n",
      "loss: 0.40089020133018494, accuracy: 0.8078602620087336\n",
      "loss: 0.405108243227005, accuracy: 0.8165938864628821\n",
      "loss: 0.39963382482528687, accuracy: 0.8209606986899564\n",
      "loss: 0.40254297852516174, accuracy: 0.8122270742358079\n",
      "loss: 0.40565210580825806, accuracy: 0.8078602620087336\n",
      "loss: 0.39355725049972534, accuracy: 0.8253275109170306\n",
      "loss: 0.4017898440361023, accuracy: 0.8209606986899564\n",
      "loss: 0.39492595195770264, accuracy: 0.7991266375545851\n",
      "loss: 0.40206456184387207, accuracy: 0.7991266375545851\n",
      "loss: 0.4035876989364624, accuracy: 0.7991266375545851\n",
      "loss: 0.39655160903930664, accuracy: 0.8078602620087336\n",
      "loss: 0.39678215980529785, accuracy: 0.8165938864628821\n",
      "loss: 0.3904978632926941, accuracy: 0.8209606986899564\n",
      "loss: 0.3977756202220917, accuracy: 0.7991266375545851\n",
      "loss: 0.3897606432437897, accuracy: 0.8253275109170306\n",
      "loss: 0.39600202441215515, accuracy: 0.8122270742358079\n",
      "loss: 0.3899465799331665, accuracy: 0.8209606986899564\n",
      "loss: 0.41473937034606934, accuracy: 0.7903930131004366\n",
      "loss: 0.39401713013648987, accuracy: 0.8034934497816594\n",
      "loss: 0.40020549297332764, accuracy: 0.8165938864628821\n",
      "loss: 0.3984105885028839, accuracy: 0.7947598253275109\n",
      "loss: 0.414306640625, accuracy: 0.8209606986899564\n",
      "loss: 0.39227354526519775, accuracy: 0.8165938864628821\n",
      "loss: 0.3904479444026947, accuracy: 0.8296943231441049\n",
      "loss: 0.3982280194759369, accuracy: 0.7991266375545851\n",
      "loss: 0.3921327590942383, accuracy: 0.8296943231441049\n",
      "loss: 0.395273894071579, accuracy: 0.8122270742358079\n",
      "loss: 0.39126312732696533, accuracy: 0.7991266375545851\n",
      "loss: 0.39600643515586853, accuracy: 0.8165938864628821\n",
      "loss: 0.39200887084007263, accuracy: 0.8122270742358079\n",
      "loss: 0.39804816246032715, accuracy: 0.8209606986899564\n",
      "loss: 0.3913220763206482, accuracy: 0.8253275109170306\n",
      "loss: 0.396334707736969, accuracy: 0.7860262008733624\n",
      "loss: 0.3914705514907837, accuracy: 0.8078602620087336\n",
      "loss: 0.3905506134033203, accuracy: 0.7991266375545851\n",
      "loss: 0.3852742314338684, accuracy: 0.7991266375545851\n",
      "loss: 0.39884060621261597, accuracy: 0.7947598253275109\n",
      "loss: 0.3957284986972809, accuracy: 0.8209606986899564\n",
      "loss: 0.3921588659286499, accuracy: 0.7991266375545851\n",
      "loss: 0.3849019408226013, accuracy: 0.8209606986899564\n",
      "loss: 0.40840011835098267, accuracy: 0.8078602620087336\n",
      "loss: 0.38831961154937744, accuracy: 0.8165938864628821\n",
      "loss: 0.391306072473526, accuracy: 0.8122270742358079\n",
      "loss: 0.4046330451965332, accuracy: 0.7991266375545851\n",
      "loss: 0.38419121503829956, accuracy: 0.8253275109170306\n",
      "loss: 0.4007464051246643, accuracy: 0.7991266375545851\n",
      "loss: 0.38314089179039, accuracy: 0.8253275109170306\n",
      "loss: 0.3948083519935608, accuracy: 0.7903930131004366\n",
      "loss: 0.3851993680000305, accuracy: 0.8122270742358079\n",
      "loss: 0.40787550806999207, accuracy: 0.7991266375545851\n",
      "loss: 0.3950996696949005, accuracy: 0.8165938864628821\n",
      "loss: 0.38648301362991333, accuracy: 0.8209606986899564\n",
      "loss: 0.3885062336921692, accuracy: 0.8122270742358079\n",
      "loss: 0.38355594873428345, accuracy: 0.8165938864628821\n",
      "loss: 0.39778295159339905, accuracy: 0.8034934497816594\n",
      "loss: 0.3853290379047394, accuracy: 0.8209606986899564\n",
      "max accuracy: 0.8384279475982532\n",
      "loss: 0.38502711057662964, accuracy: 0.8384279475982532\n",
      "loss: 0.3924545645713806, accuracy: 0.7947598253275109\n",
      "loss: 0.38646024465560913, accuracy: 0.7991266375545851\n",
      "loss: 0.398033082485199, accuracy: 0.8122270742358079\n",
      "loss: 0.3973621428012848, accuracy: 0.8078602620087336\n",
      "loss: 0.37879592180252075, accuracy: 0.8384279475982532\n",
      "loss: 0.38541656732559204, accuracy: 0.8165938864628821\n",
      "loss: 0.37959176301956177, accuracy: 0.8078602620087336\n",
      "loss: 0.3703466057777405, accuracy: 0.834061135371179\n",
      "loss: 0.3925376534461975, accuracy: 0.8078602620087336\n",
      "loss: 0.38130152225494385, accuracy: 0.8078602620087336\n",
      "loss: 0.3776516020298004, accuracy: 0.8122270742358079\n",
      "loss: 0.3878785967826843, accuracy: 0.8165938864628821\n",
      "loss: 0.38702481985092163, accuracy: 0.8384279475982532\n",
      "loss: 0.39004403352737427, accuracy: 0.8034934497816594\n",
      "loss: 0.3860151469707489, accuracy: 0.8165938864628821\n",
      "loss: 0.3991680443286896, accuracy: 0.7947598253275109\n",
      "loss: 0.386892169713974, accuracy: 0.8122270742358079\n",
      "loss: 0.38937386870384216, accuracy: 0.8165938864628821\n",
      "loss: 0.39147481322288513, accuracy: 0.8034934497816594\n",
      "loss: 0.3772943615913391, accuracy: 0.8296943231441049\n",
      "loss: 0.38655009865760803, accuracy: 0.8253275109170306\n",
      "loss: 0.39134687185287476, accuracy: 0.8165938864628821\n",
      "loss: 0.3808957636356354, accuracy: 0.8165938864628821\n",
      "loss: 0.384944885969162, accuracy: 0.8209606986899564\n",
      "loss: 0.39008674025535583, accuracy: 0.8122270742358079\n",
      "loss: 0.37800636887550354, accuracy: 0.8253275109170306\n",
      "loss: 0.3832934498786926, accuracy: 0.8165938864628821\n",
      "loss: 0.386529803276062, accuracy: 0.8209606986899564\n",
      "loss: 0.39962708950042725, accuracy: 0.8165938864628821\n",
      "loss: 0.3799997568130493, accuracy: 0.8122270742358079\n",
      "loss: 0.3763517439365387, accuracy: 0.8209606986899564\n",
      "loss: 0.3977968096733093, accuracy: 0.8253275109170306\n",
      "loss: 0.3829680383205414, accuracy: 0.8253275109170306\n",
      "loss: 0.37996748089790344, accuracy: 0.8165938864628821\n",
      "loss: 0.4065890610218048, accuracy: 0.7947598253275109\n",
      "loss: 0.37620168924331665, accuracy: 0.8209606986899564\n",
      "loss: 0.382107138633728, accuracy: 0.7991266375545851\n",
      "loss: 0.37394478917121887, accuracy: 0.834061135371179\n",
      "loss: 0.3771544098854065, accuracy: 0.8209606986899564\n",
      "loss: 0.366872102022171, accuracy: 0.8296943231441049\n",
      "loss: 0.4007986783981323, accuracy: 0.7816593886462883\n",
      "loss: 0.37722402811050415, accuracy: 0.834061135371179\n",
      "loss: 0.39476877450942993, accuracy: 0.8165938864628821\n",
      "loss: 0.3859519064426422, accuracy: 0.8122270742358079\n",
      "loss: 0.3918047547340393, accuracy: 0.8209606986899564\n",
      "loss: 0.38882502913475037, accuracy: 0.8209606986899564\n",
      "loss: 0.38224631547927856, accuracy: 0.8165938864628821\n",
      "max accuracy: 0.8427947598253275\n",
      "loss: 0.37181347608566284, accuracy: 0.8427947598253275\n",
      "loss: 0.38295212388038635, accuracy: 0.8296943231441049\n",
      "loss: 0.3795999586582184, accuracy: 0.8253275109170306\n",
      "loss: 0.3715919256210327, accuracy: 0.7947598253275109\n",
      "loss: 0.3727304935455322, accuracy: 0.8209606986899564\n",
      "loss: 0.3789239823818207, accuracy: 0.7991266375545851\n",
      "loss: 0.3706020414829254, accuracy: 0.8209606986899564\n",
      "loss: 0.3784812092781067, accuracy: 0.8165938864628821\n",
      "loss: 0.38351890444755554, accuracy: 0.7991266375545851\n",
      "loss: 0.3650083541870117, accuracy: 0.8296943231441049\n",
      "loss: 0.38738682866096497, accuracy: 0.8165938864628821\n",
      "loss: 0.37467750906944275, accuracy: 0.834061135371179\n",
      "loss: 0.3925890624523163, accuracy: 0.8034934497816594\n",
      "loss: 0.3766503930091858, accuracy: 0.8078602620087336\n",
      "loss: 0.36165910959243774, accuracy: 0.8253275109170306\n",
      "max accuracy: 0.8602620087336245\n",
      "loss: 0.3651377558708191, accuracy: 0.8602620087336245\n",
      "loss: 0.37529975175857544, accuracy: 0.7991266375545851\n",
      "loss: 0.3650413453578949, accuracy: 0.834061135371179\n",
      "loss: 0.38372355699539185, accuracy: 0.8165938864628821\n",
      "loss: 0.3818991184234619, accuracy: 0.8384279475982532\n",
      "loss: 0.38388606905937195, accuracy: 0.8034934497816594\n",
      "loss: 0.3674306869506836, accuracy: 0.8296943231441049\n",
      "loss: 0.37049761414527893, accuracy: 0.8253275109170306\n",
      "loss: 0.3663540184497833, accuracy: 0.8034934497816594\n",
      "loss: 0.3563331365585327, accuracy: 0.8471615720524017\n",
      "loss: 0.3848223090171814, accuracy: 0.8034934497816594\n",
      "loss: 0.37150338292121887, accuracy: 0.8209606986899564\n",
      "loss: 0.3684464395046234, accuracy: 0.8165938864628821\n",
      "loss: 0.36635828018188477, accuracy: 0.8427947598253275\n",
      "loss: 0.3770689368247986, accuracy: 0.8209606986899564\n",
      "loss: 0.3634679615497589, accuracy: 0.8253275109170306\n",
      "loss: 0.3672296702861786, accuracy: 0.834061135371179\n",
      "loss: 0.3811790347099304, accuracy: 0.8165938864628821\n",
      "loss: 0.37491074204444885, accuracy: 0.8253275109170306\n",
      "loss: 0.3637389838695526, accuracy: 0.8253275109170306\n",
      "loss: 0.3714374601840973, accuracy: 0.8209606986899564\n",
      "loss: 0.3696449398994446, accuracy: 0.8253275109170306\n",
      "loss: 0.3760957717895508, accuracy: 0.8209606986899564\n",
      "loss: 0.3613310754299164, accuracy: 0.8165938864628821\n",
      "loss: 0.3680144250392914, accuracy: 0.8122270742358079\n",
      "loss: 0.37150782346725464, accuracy: 0.8209606986899564\n",
      "loss: 0.3619062602519989, accuracy: 0.834061135371179\n",
      "loss: 0.3703266382217407, accuracy: 0.8078602620087336\n",
      "loss: 0.37448742985725403, accuracy: 0.8122270742358079\n",
      "loss: 0.36561253666877747, accuracy: 0.8384279475982532\n",
      "loss: 0.35745134949684143, accuracy: 0.8296943231441049\n",
      "loss: 0.37832269072532654, accuracy: 0.8165938864628821\n",
      "loss: 0.3632644712924957, accuracy: 0.8427947598253275\n",
      "loss: 0.3816271126270294, accuracy: 0.8034934497816594\n",
      "loss: 0.3552021086215973, accuracy: 0.834061135371179\n",
      "loss: 0.3612726926803589, accuracy: 0.834061135371179\n",
      "loss: 0.35865846276283264, accuracy: 0.8296943231441049\n",
      "loss: 0.38201189041137695, accuracy: 0.8296943231441049\n",
      "loss: 0.3556740880012512, accuracy: 0.8427947598253275\n",
      "loss: 0.3670436143875122, accuracy: 0.8209606986899564\n",
      "loss: 0.36251378059387207, accuracy: 0.8034934497816594\n",
      "loss: 0.362692654132843, accuracy: 0.834061135371179\n",
      "loss: 0.3779909908771515, accuracy: 0.8122270742358079\n",
      "loss: 0.36026331782341003, accuracy: 0.8209606986899564\n",
      "loss: 0.3641514778137207, accuracy: 0.8209606986899564\n",
      "loss: 0.37057095766067505, accuracy: 0.8209606986899564\n",
      "loss: 0.3793119192123413, accuracy: 0.8165938864628821\n",
      "loss: 0.3522408604621887, accuracy: 0.8427947598253275\n",
      "loss: 0.3788958489894867, accuracy: 0.8471615720524017\n",
      "loss: 0.35395491123199463, accuracy: 0.8471615720524017\n",
      "loss: 0.3689732253551483, accuracy: 0.8078602620087336\n",
      "loss: 0.3613986670970917, accuracy: 0.8296943231441049\n",
      "loss: 0.36220666766166687, accuracy: 0.8384279475982532\n",
      "loss: 0.3558178246021271, accuracy: 0.8427947598253275\n",
      "loss: 0.36435893177986145, accuracy: 0.8384279475982532\n",
      "loss: 0.35903167724609375, accuracy: 0.8384279475982532\n",
      "loss: 0.36099037528038025, accuracy: 0.8253275109170306\n",
      "loss: 0.36058664321899414, accuracy: 0.8078602620087336\n",
      "loss: 0.3757550120353699, accuracy: 0.7991266375545851\n",
      "loss: 0.3701286017894745, accuracy: 0.8209606986899564\n",
      "loss: 0.35032621026039124, accuracy: 0.8384279475982532\n",
      "loss: 0.35317084193229675, accuracy: 0.8296943231441049\n",
      "loss: 0.3636125326156616, accuracy: 0.8165938864628821\n",
      "loss: 0.36379125714302063, accuracy: 0.8209606986899564\n",
      "loss: 0.36295008659362793, accuracy: 0.8296943231441049\n",
      "loss: 0.3513372242450714, accuracy: 0.8602620087336245\n",
      "loss: 0.37028831243515015, accuracy: 0.8122270742358079\n",
      "loss: 0.3727043867111206, accuracy: 0.8253275109170306\n",
      "loss: 0.36921223998069763, accuracy: 0.8209606986899564\n",
      "loss: 0.3551558554172516, accuracy: 0.8384279475982532\n",
      "loss: 0.3638025224208832, accuracy: 0.8384279475982532\n",
      "loss: 0.36621764302253723, accuracy: 0.8471615720524017\n",
      "loss: 0.368755966424942, accuracy: 0.8209606986899564\n",
      "loss: 0.36720678210258484, accuracy: 0.8209606986899564\n",
      "loss: 0.3566513955593109, accuracy: 0.851528384279476\n",
      "loss: 0.3532783091068268, accuracy: 0.834061135371179\n",
      "loss: 0.38929492235183716, accuracy: 0.8034934497816594\n",
      "loss: 0.34931913018226624, accuracy: 0.8471615720524017\n",
      "loss: 0.3487303853034973, accuracy: 0.834061135371179\n",
      "max accuracy: 0.8646288209606987\n",
      "loss: 0.35606011748313904, accuracy: 0.8646288209606987\n",
      "loss: 0.3646012246608734, accuracy: 0.8384279475982532\n",
      "loss: 0.3534280061721802, accuracy: 0.834061135371179\n",
      "loss: 0.35638606548309326, accuracy: 0.8296943231441049\n",
      "loss: 0.3611820638179779, accuracy: 0.8209606986899564\n",
      "loss: 0.3586214482784271, accuracy: 0.8209606986899564\n",
      "loss: 0.3566804528236389, accuracy: 0.834061135371179\n",
      "loss: 0.3485771119594574, accuracy: 0.851528384279476\n",
      "loss: 0.36196812987327576, accuracy: 0.8296943231441049\n",
      "loss: 0.3502640724182129, accuracy: 0.834061135371179\n",
      "loss: 0.36675286293029785, accuracy: 0.8296943231441049\n",
      "loss: 0.3373523950576782, accuracy: 0.8646288209606987\n",
      "loss: 0.37759488821029663, accuracy: 0.8122270742358079\n",
      "loss: 0.3358088731765747, accuracy: 0.8558951965065502\n",
      "loss: 0.36481326818466187, accuracy: 0.8209606986899564\n",
      "loss: 0.3500085473060608, accuracy: 0.8209606986899564\n",
      "loss: 0.3621012568473816, accuracy: 0.834061135371179\n",
      "loss: 0.3403240442276001, accuracy: 0.8384279475982532\n",
      "loss: 0.353381872177124, accuracy: 0.834061135371179\n",
      "max accuracy: 0.868995633187773\n",
      "loss: 0.33538439869880676, accuracy: 0.868995633187773\n",
      "loss: 0.3330383896827698, accuracy: 0.868995633187773\n",
      "loss: 0.38399627804756165, accuracy: 0.8034934497816594\n",
      "loss: 0.3452640473842621, accuracy: 0.8296943231441049\n",
      "loss: 0.3491210341453552, accuracy: 0.851528384279476\n",
      "loss: 0.3661135733127594, accuracy: 0.8384279475982532\n",
      "loss: 0.37463608384132385, accuracy: 0.8296943231441049\n",
      "loss: 0.35224297642707825, accuracy: 0.834061135371179\n",
      "loss: 0.35118377208709717, accuracy: 0.851528384279476\n",
      "loss: 0.3447999358177185, accuracy: 0.8253275109170306\n",
      "loss: 0.3400154709815979, accuracy: 0.8427947598253275\n",
      "loss: 0.3666403293609619, accuracy: 0.8558951965065502\n",
      "loss: 0.343231201171875, accuracy: 0.8427947598253275\n",
      "loss: 0.3532150387763977, accuracy: 0.8427947598253275\n",
      "loss: 0.3638208508491516, accuracy: 0.8471615720524017\n",
      "loss: 0.3383418917655945, accuracy: 0.8646288209606987\n",
      "loss: 0.3457331657409668, accuracy: 0.868995633187773\n",
      "loss: 0.3491270840167999, accuracy: 0.834061135371179\n",
      "loss: 0.3439735472202301, accuracy: 0.8253275109170306\n",
      "loss: 0.34768345952033997, accuracy: 0.8471615720524017\n",
      "loss: 0.34695670008659363, accuracy: 0.851528384279476\n",
      "loss: 0.3475414514541626, accuracy: 0.8296943231441049\n",
      "loss: 0.3411944508552551, accuracy: 0.8296943231441049\n",
      "loss: 0.37897560000419617, accuracy: 0.7991266375545851\n",
      "loss: 0.3349657952785492, accuracy: 0.8602620087336245\n",
      "loss: 0.3379824757575989, accuracy: 0.8602620087336245\n",
      "loss: 0.3480904996395111, accuracy: 0.8209606986899564\n",
      "loss: 0.34894201159477234, accuracy: 0.8471615720524017\n",
      "loss: 0.3435978889465332, accuracy: 0.8427947598253275\n",
      "loss: 0.35883209109306335, accuracy: 0.8253275109170306\n",
      "max accuracy: 0.8820960698689956\n",
      "loss: 0.32735487818717957, accuracy: 0.8820960698689956\n",
      "loss: 0.3361387252807617, accuracy: 0.8471615720524017\n",
      "loss: 0.3388291299343109, accuracy: 0.8427947598253275\n",
      "loss: 0.3365432918071747, accuracy: 0.851528384279476\n",
      "loss: 0.33534592390060425, accuracy: 0.851528384279476\n",
      "loss: 0.3331466615200043, accuracy: 0.8646288209606987\n",
      "loss: 0.3319736123085022, accuracy: 0.8558951965065502\n",
      "loss: 0.3723926842212677, accuracy: 0.8122270742358079\n",
      "loss: 0.336843878030777, accuracy: 0.851528384279476\n",
      "loss: 0.3375301659107208, accuracy: 0.8777292576419214\n",
      "loss: 0.3352210223674774, accuracy: 0.8471615720524017\n",
      "loss: 0.3482464849948883, accuracy: 0.851528384279476\n",
      "loss: 0.33481571078300476, accuracy: 0.8471615720524017\n",
      "loss: 0.34814566373825073, accuracy: 0.834061135371179\n",
      "loss: 0.34513503313064575, accuracy: 0.8384279475982532\n",
      "loss: 0.35193493962287903, accuracy: 0.851528384279476\n",
      "loss: 0.3549060821533203, accuracy: 0.8165938864628821\n",
      "loss: 0.33200886845588684, accuracy: 0.8646288209606987\n",
      "loss: 0.37097957730293274, accuracy: 0.8122270742358079\n",
      "loss: 0.3618342876434326, accuracy: 0.8384279475982532\n",
      "loss: 0.3367447257041931, accuracy: 0.8558951965065502\n",
      "loss: 0.3454032838344574, accuracy: 0.8733624454148472\n",
      "loss: 0.38375118374824524, accuracy: 0.8471615720524017\n",
      "loss: 0.3548992872238159, accuracy: 0.8209606986899564\n",
      "loss: 0.3322756588459015, accuracy: 0.851528384279476\n",
      "loss: 0.3653092086315155, accuracy: 0.834061135371179\n",
      "loss: 0.3399922847747803, accuracy: 0.834061135371179\n",
      "loss: 0.33883997797966003, accuracy: 0.8558951965065502\n",
      "loss: 0.34656062722206116, accuracy: 0.851528384279476\n",
      "loss: 0.3549247682094574, accuracy: 0.8078602620087336\n",
      "loss: 0.33079370856285095, accuracy: 0.851528384279476\n",
      "loss: 0.33750078082084656, accuracy: 0.851528384279476\n",
      "loss: 0.3269423842430115, accuracy: 0.8733624454148472\n",
      "loss: 0.32376059889793396, accuracy: 0.8602620087336245\n",
      "loss: 0.3377474248409271, accuracy: 0.851528384279476\n",
      "loss: 0.3413751721382141, accuracy: 0.851528384279476\n",
      "loss: 0.34076738357543945, accuracy: 0.8165938864628821\n",
      "loss: 0.3250797688961029, accuracy: 0.8558951965065502\n",
      "loss: 0.3277500569820404, accuracy: 0.8646288209606987\n",
      "loss: 0.3300236165523529, accuracy: 0.8820960698689956\n",
      "loss: 0.33994951844215393, accuracy: 0.8558951965065502\n",
      "loss: 0.32879576086997986, accuracy: 0.8820960698689956\n",
      "loss: 0.3305998742580414, accuracy: 0.851528384279476\n",
      "loss: 0.40454035997390747, accuracy: 0.8253275109170306\n",
      "loss: 0.3489873707294464, accuracy: 0.8427947598253275\n",
      "loss: 0.3238773047924042, accuracy: 0.851528384279476\n",
      "loss: 0.35042011737823486, accuracy: 0.851528384279476\n",
      "loss: 0.3349655270576477, accuracy: 0.8602620087336245\n",
      "loss: 0.3343958258628845, accuracy: 0.8253275109170306\n",
      "loss: 0.3295186460018158, accuracy: 0.8646288209606987\n",
      "loss: 0.32290151715278625, accuracy: 0.8602620087336245\n",
      "loss: 0.32634079456329346, accuracy: 0.851528384279476\n",
      "loss: 0.3304688334465027, accuracy: 0.8427947598253275\n",
      "loss: 0.35599285364151, accuracy: 0.8384279475982532\n",
      "loss: 0.3339414596557617, accuracy: 0.8427947598253275\n",
      "loss: 0.3384977877140045, accuracy: 0.8558951965065502\n",
      "loss: 0.3364330530166626, accuracy: 0.8646288209606987\n",
      "loss: 0.32485824823379517, accuracy: 0.851528384279476\n",
      "loss: 0.3422013521194458, accuracy: 0.8296943231441049\n",
      "loss: 0.3322691321372986, accuracy: 0.8253275109170306\n",
      "loss: 0.33361056447029114, accuracy: 0.8820960698689956\n",
      "loss: 0.37952032685279846, accuracy: 0.7860262008733624\n",
      "loss: 0.3377354145050049, accuracy: 0.8602620087336245\n",
      "loss: 0.368378609418869, accuracy: 0.8165938864628821\n",
      "loss: 0.33055591583251953, accuracy: 0.8602620087336245\n",
      "loss: 0.33029207587242126, accuracy: 0.851528384279476\n",
      "loss: 0.32464563846588135, accuracy: 0.8777292576419214\n",
      "loss: 0.33444762229919434, accuracy: 0.8427947598253275\n",
      "loss: 0.34172576665878296, accuracy: 0.851528384279476\n",
      "loss: 0.3381997346878052, accuracy: 0.8296943231441049\n",
      "loss: 0.34348684549331665, accuracy: 0.851528384279476\n",
      "loss: 0.3432967960834503, accuracy: 0.8602620087336245\n",
      "loss: 0.3394171893596649, accuracy: 0.868995633187773\n",
      "loss: 0.3422624468803406, accuracy: 0.851528384279476\n",
      "loss: 0.3368172347545624, accuracy: 0.8646288209606987\n",
      "loss: 0.33865103125572205, accuracy: 0.8427947598253275\n",
      "loss: 0.32781049609184265, accuracy: 0.8558951965065502\n",
      "loss: 0.31628331542015076, accuracy: 0.868995633187773\n",
      "loss: 0.3295087516307831, accuracy: 0.8558951965065502\n",
      "loss: 0.3339930474758148, accuracy: 0.8646288209606987\n",
      "loss: 0.3283655047416687, accuracy: 0.8602620087336245\n",
      "loss: 0.33801770210266113, accuracy: 0.8471615720524017\n",
      "loss: 0.3732050061225891, accuracy: 0.834061135371179\n",
      "loss: 0.3307493329048157, accuracy: 0.8646288209606987\n",
      "loss: 0.31351253390312195, accuracy: 0.8646288209606987\n",
      "loss: 0.3266126215457916, accuracy: 0.851528384279476\n",
      "loss: 0.3405832350254059, accuracy: 0.8471615720524017\n",
      "loss: 0.32386183738708496, accuracy: 0.868995633187773\n",
      "loss: 0.32049787044525146, accuracy: 0.868995633187773\n",
      "loss: 0.31486693024635315, accuracy: 0.8558951965065502\n",
      "loss: 0.3532586693763733, accuracy: 0.8384279475982532\n",
      "loss: 0.31330612301826477, accuracy: 0.8777292576419214\n",
      "loss: 0.3242632746696472, accuracy: 0.8733624454148472\n",
      "loss: 0.31391575932502747, accuracy: 0.8646288209606987\n",
      "loss: 0.3277617394924164, accuracy: 0.868995633187773\n",
      "loss: 0.32268357276916504, accuracy: 0.851528384279476\n",
      "loss: 0.3175411522388458, accuracy: 0.8471615720524017\n",
      "loss: 0.32350215315818787, accuracy: 0.8646288209606987\n",
      "loss: 0.32671332359313965, accuracy: 0.8296943231441049\n",
      "loss: 0.33140045404434204, accuracy: 0.8427947598253275\n",
      "loss: 0.320012629032135, accuracy: 0.868995633187773\n",
      "loss: 0.3609049916267395, accuracy: 0.8209606986899564\n",
      "loss: 0.3275573253631592, accuracy: 0.8471615720524017\n",
      "loss: 0.3296365439891815, accuracy: 0.868995633187773\n",
      "loss: 0.34609052538871765, accuracy: 0.8471615720524017\n",
      "loss: 0.31759223341941833, accuracy: 0.8602620087336245\n",
      "max accuracy: 0.8908296943231441\n",
      "loss: 0.31887587904930115, accuracy: 0.8908296943231441\n",
      "loss: 0.33418235182762146, accuracy: 0.8558951965065502\n",
      "loss: 0.3352288603782654, accuracy: 0.8777292576419214\n",
      "loss: 0.3181593716144562, accuracy: 0.8558951965065502\n",
      "loss: 0.3216702342033386, accuracy: 0.868995633187773\n",
      "loss: 0.31281253695487976, accuracy: 0.8646288209606987\n",
      "loss: 0.33114010095596313, accuracy: 0.8427947598253275\n",
      "loss: 0.3388313055038452, accuracy: 0.8646288209606987\n",
      "loss: 0.3326311707496643, accuracy: 0.8558951965065502\n",
      "loss: 0.3128509819507599, accuracy: 0.8777292576419214\n",
      "loss: 0.31048282980918884, accuracy: 0.868995633187773\n",
      "loss: 0.34738051891326904, accuracy: 0.8558951965065502\n",
      "loss: 0.3213896155357361, accuracy: 0.8646288209606987\n",
      "loss: 0.3576176166534424, accuracy: 0.8471615720524017\n",
      "loss: 0.32320573925971985, accuracy: 0.8558951965065502\n",
      "loss: 0.31967970728874207, accuracy: 0.8602620087336245\n",
      "loss: 0.3338509500026703, accuracy: 0.8296943231441049\n",
      "loss: 0.332030713558197, accuracy: 0.8471615720524017\n",
      "loss: 0.33934929966926575, accuracy: 0.8165938864628821\n",
      "loss: 0.3271172344684601, accuracy: 0.8602620087336245\n",
      "loss: 0.32063785195350647, accuracy: 0.8733624454148472\n",
      "loss: 0.30200445652008057, accuracy: 0.8602620087336245\n",
      "loss: 0.33285948634147644, accuracy: 0.8777292576419214\n",
      "loss: 0.3208284080028534, accuracy: 0.8602620087336245\n",
      "loss: 0.31093037128448486, accuracy: 0.8733624454148472\n",
      "loss: 0.32093241810798645, accuracy: 0.8602620087336245\n",
      "loss: 0.31156203150749207, accuracy: 0.851528384279476\n",
      "loss: 0.3395174741744995, accuracy: 0.8296943231441049\n",
      "loss: 0.309462308883667, accuracy: 0.8777292576419214\n",
      "loss: 0.3244105577468872, accuracy: 0.868995633187773\n",
      "loss: 0.3199458122253418, accuracy: 0.8820960698689956\n",
      "loss: 0.31477823853492737, accuracy: 0.8646288209606987\n",
      "loss: 0.3087970018386841, accuracy: 0.8777292576419214\n",
      "loss: 0.31449219584465027, accuracy: 0.8602620087336245\n",
      "loss: 0.31347182393074036, accuracy: 0.8777292576419214\n",
      "loss: 0.3272533118724823, accuracy: 0.868995633187773\n",
      "loss: 0.3434247374534607, accuracy: 0.8602620087336245\n",
      "loss: 0.33388766646385193, accuracy: 0.8253275109170306\n",
      "loss: 0.3255707323551178, accuracy: 0.8427947598253275\n",
      "loss: 0.3095039129257202, accuracy: 0.8558951965065502\n",
      "loss: 0.3143291175365448, accuracy: 0.8820960698689956\n",
      "loss: 0.31919509172439575, accuracy: 0.8733624454148472\n",
      "loss: 0.3196304738521576, accuracy: 0.8384279475982532\n",
      "loss: 0.32879602909088135, accuracy: 0.8384279475982532\n",
      "loss: 0.32926464080810547, accuracy: 0.8427947598253275\n",
      "loss: 0.3244200050830841, accuracy: 0.868995633187773\n",
      "loss: 0.3260784149169922, accuracy: 0.8471615720524017\n",
      "loss: 0.3156625032424927, accuracy: 0.8646288209606987\n",
      "loss: 0.30248257517814636, accuracy: 0.8908296943231441\n",
      "loss: 0.3098016679286957, accuracy: 0.8602620087336245\n",
      "loss: 0.2978808283805847, accuracy: 0.868995633187773\n",
      "loss: 0.32809799909591675, accuracy: 0.8427947598253275\n",
      "loss: 0.3128892779350281, accuracy: 0.8733624454148472\n",
      "loss: 0.3003423810005188, accuracy: 0.851528384279476\n",
      "loss: 0.3056146800518036, accuracy: 0.8777292576419214\n",
      "loss: 0.37161970138549805, accuracy: 0.7991266375545851\n",
      "loss: 0.3272617757320404, accuracy: 0.834061135371179\n",
      "loss: 0.3098739981651306, accuracy: 0.8820960698689956\n",
      "loss: 0.30950412154197693, accuracy: 0.8733624454148472\n",
      "loss: 0.3246220052242279, accuracy: 0.8471615720524017\n",
      "loss: 0.3071317970752716, accuracy: 0.8733624454148472\n",
      "loss: 0.31536102294921875, accuracy: 0.8908296943231441\n",
      "loss: 0.32442378997802734, accuracy: 0.8864628820960698\n",
      "loss: 0.322642982006073, accuracy: 0.8602620087336245\n",
      "loss: 0.30972689390182495, accuracy: 0.8733624454148472\n",
      "loss: 0.3194780945777893, accuracy: 0.851528384279476\n",
      "loss: 0.3029841482639313, accuracy: 0.8864628820960698\n",
      "loss: 0.3053401708602905, accuracy: 0.8733624454148472\n",
      "loss: 0.30588245391845703, accuracy: 0.8777292576419214\n",
      "loss: 0.31259962916374207, accuracy: 0.8777292576419214\n",
      "loss: 0.3090973496437073, accuracy: 0.8864628820960698\n",
      "loss: 0.32332199811935425, accuracy: 0.8646288209606987\n",
      "loss: 0.31674930453300476, accuracy: 0.8820960698689956\n",
      "loss: 0.31923168897628784, accuracy: 0.8646288209606987\n",
      "loss: 0.3216264545917511, accuracy: 0.851528384279476\n",
      "loss: 0.30600687861442566, accuracy: 0.8777292576419214\n",
      "loss: 0.30028727650642395, accuracy: 0.8602620087336245\n",
      "loss: 0.3127034604549408, accuracy: 0.868995633187773\n",
      "loss: 0.3117673695087433, accuracy: 0.8733624454148472\n",
      "loss: 0.328573614358902, accuracy: 0.8646288209606987\n",
      "loss: 0.3021242618560791, accuracy: 0.8777292576419214\n",
      "loss: 0.32191306352615356, accuracy: 0.8209606986899564\n",
      "loss: 0.2991233170032501, accuracy: 0.868995633187773\n",
      "max accuracy: 0.9039301310043668\n",
      "loss: 0.29719093441963196, accuracy: 0.9039301310043668\n",
      "loss: 0.31699347496032715, accuracy: 0.8384279475982532\n",
      "max accuracy: 0.9170305676855895\n",
      "loss: 0.29814961552619934, accuracy: 0.9170305676855895\n",
      "loss: 0.3249446451663971, accuracy: 0.8558951965065502\n",
      "loss: 0.3428616225719452, accuracy: 0.8602620087336245\n",
      "loss: 0.33166995644569397, accuracy: 0.8384279475982532\n",
      "loss: 0.30392685532569885, accuracy: 0.8951965065502183\n",
      "loss: 0.30939745903015137, accuracy: 0.8908296943231441\n",
      "loss: 0.29296791553497314, accuracy: 0.8908296943231441\n",
      "loss: 0.30712348222732544, accuracy: 0.8733624454148472\n",
      "loss: 0.31281447410583496, accuracy: 0.8427947598253275\n",
      "loss: 0.3195572793483734, accuracy: 0.8646288209606987\n",
      "loss: 0.3014571964740753, accuracy: 0.8777292576419214\n",
      "loss: 0.3122706413269043, accuracy: 0.8951965065502183\n",
      "loss: 0.3189203441143036, accuracy: 0.8820960698689956\n",
      "loss: 0.2997727692127228, accuracy: 0.8733624454148472\n",
      "loss: 0.2987397015094757, accuracy: 0.8733624454148472\n",
      "loss: 0.3104255497455597, accuracy: 0.8602620087336245\n",
      "loss: 0.363979697227478, accuracy: 0.834061135371179\n",
      "loss: 0.2995368242263794, accuracy: 0.8864628820960698\n",
      "loss: 0.308782160282135, accuracy: 0.8427947598253275\n",
      "loss: 0.3142813444137573, accuracy: 0.851528384279476\n",
      "loss: 0.29243895411491394, accuracy: 0.868995633187773\n",
      "loss: 0.3131467401981354, accuracy: 0.851528384279476\n",
      "loss: 0.31166693568229675, accuracy: 0.8820960698689956\n",
      "loss: 0.3388422727584839, accuracy: 0.8209606986899564\n",
      "loss: 0.30872154235839844, accuracy: 0.8602620087336245\n",
      "loss: 0.2997187077999115, accuracy: 0.8777292576419214\n",
      "loss: 0.2881273627281189, accuracy: 0.8995633187772926\n",
      "loss: 0.3046327233314514, accuracy: 0.8733624454148472\n",
      "loss: 0.32011863589286804, accuracy: 0.851528384279476\n",
      "loss: 0.31779471039772034, accuracy: 0.8646288209606987\n",
      "loss: 0.30361056327819824, accuracy: 0.8646288209606987\n",
      "loss: 0.2966291606426239, accuracy: 0.8602620087336245\n",
      "loss: 0.2972322106361389, accuracy: 0.8777292576419214\n",
      "loss: 0.31003180146217346, accuracy: 0.8908296943231441\n",
      "loss: 0.31620651483535767, accuracy: 0.8602620087336245\n",
      "loss: 0.3079407513141632, accuracy: 0.868995633187773\n",
      "loss: 0.30190759897232056, accuracy: 0.8864628820960698\n",
      "loss: 0.32313331961631775, accuracy: 0.8558951965065502\n",
      "loss: 0.29787468910217285, accuracy: 0.8864628820960698\n",
      "loss: 0.31644827127456665, accuracy: 0.8777292576419214\n",
      "loss: 0.2974257469177246, accuracy: 0.9039301310043668\n",
      "loss: 0.30883893370628357, accuracy: 0.8427947598253275\n",
      "loss: 0.29121342301368713, accuracy: 0.8908296943231441\n",
      "loss: 0.3025716543197632, accuracy: 0.8602620087336245\n",
      "loss: 0.306378036737442, accuracy: 0.8864628820960698\n",
      "loss: 0.29608476161956787, accuracy: 0.8820960698689956\n",
      "loss: 0.29345428943634033, accuracy: 0.8951965065502183\n",
      "loss: 0.30716967582702637, accuracy: 0.851528384279476\n",
      "loss: 0.3066287338733673, accuracy: 0.868995633187773\n",
      "loss: 0.29836076498031616, accuracy: 0.8733624454148472\n",
      "loss: 0.29344552755355835, accuracy: 0.8777292576419214\n",
      "max accuracy: 0.9213973799126638\n",
      "loss: 0.28087225556373596, accuracy: 0.9213973799126638\n",
      "loss: 0.3171027600765228, accuracy: 0.8296943231441049\n",
      "loss: 0.2921094596385956, accuracy: 0.8602620087336245\n",
      "loss: 0.29966938495635986, accuracy: 0.868995633187773\n",
      "loss: 0.28676217794418335, accuracy: 0.8995633187772926\n",
      "loss: 0.2927302420139313, accuracy: 0.8908296943231441\n",
      "loss: 0.33661794662475586, accuracy: 0.8558951965065502\n",
      "loss: 0.3037806451320648, accuracy: 0.8908296943231441\n",
      "loss: 0.304843544960022, accuracy: 0.8602620087336245\n",
      "loss: 0.30196624994277954, accuracy: 0.8558951965065502\n",
      "loss: 0.29684627056121826, accuracy: 0.8908296943231441\n",
      "loss: 0.3255462050437927, accuracy: 0.8558951965065502\n",
      "loss: 0.3148672580718994, accuracy: 0.8646288209606987\n",
      "loss: 0.28863778710365295, accuracy: 0.8951965065502183\n",
      "loss: 0.3000982403755188, accuracy: 0.8908296943231441\n",
      "loss: 0.2959316670894623, accuracy: 0.8777292576419214\n",
      "loss: 0.2844620943069458, accuracy: 0.8951965065502183\n",
      "loss: 0.2819843292236328, accuracy: 0.8908296943231441\n",
      "loss: 0.3598814010620117, accuracy: 0.8209606986899564\n",
      "loss: 0.3066841959953308, accuracy: 0.8908296943231441\n",
      "loss: 0.2929328680038452, accuracy: 0.8864628820960698\n",
      "loss: 0.28449535369873047, accuracy: 0.8995633187772926\n",
      "loss: 0.2966945171356201, accuracy: 0.8864628820960698\n",
      "loss: 0.29662418365478516, accuracy: 0.8951965065502183\n",
      "loss: 0.29468920826911926, accuracy: 0.8820960698689956\n",
      "loss: 0.2970375716686249, accuracy: 0.8995633187772926\n",
      "loss: 0.29357659816741943, accuracy: 0.868995633187773\n",
      "loss: 0.2936652600765228, accuracy: 0.868995633187773\n",
      "loss: 0.30887371301651, accuracy: 0.851528384279476\n",
      "loss: 0.29372474551200867, accuracy: 0.868995633187773\n",
      "loss: 0.2834167182445526, accuracy: 0.8951965065502183\n",
      "loss: 0.2745424211025238, accuracy: 0.8951965065502183\n",
      "loss: 0.29594770073890686, accuracy: 0.8820960698689956\n",
      "loss: 0.3059236407279968, accuracy: 0.8777292576419214\n",
      "loss: 0.2924158573150635, accuracy: 0.8864628820960698\n",
      "loss: 0.32479146122932434, accuracy: 0.8733624454148472\n",
      "loss: 0.31150978803634644, accuracy: 0.8602620087336245\n",
      "loss: 0.284527450799942, accuracy: 0.9126637554585153\n",
      "loss: 0.2796458601951599, accuracy: 0.8864628820960698\n",
      "loss: 0.29504722356796265, accuracy: 0.8908296943231441\n",
      "loss: 0.30146872997283936, accuracy: 0.8820960698689956\n",
      "loss: 0.32089534401893616, accuracy: 0.8427947598253275\n",
      "loss: 0.31373849511146545, accuracy: 0.8384279475982532\n",
      "loss: 0.2990313172340393, accuracy: 0.8777292576419214\n",
      "loss: 0.2938680052757263, accuracy: 0.8733624454148472\n",
      "loss: 0.27392929792404175, accuracy: 0.9126637554585153\n",
      "loss: 0.29742521047592163, accuracy: 0.8908296943231441\n",
      "loss: 0.2836344242095947, accuracy: 0.8995633187772926\n",
      "loss: 0.28741416335105896, accuracy: 0.8864628820960698\n",
      "loss: 0.28190556168556213, accuracy: 0.9039301310043668\n",
      "loss: 0.2954025864601135, accuracy: 0.868995633187773\n",
      "loss: 0.3099322021007538, accuracy: 0.8646288209606987\n",
      "loss: 0.28957128524780273, accuracy: 0.8864628820960698\n",
      "loss: 0.3049487769603729, accuracy: 0.8777292576419214\n",
      "loss: 0.3036549985408783, accuracy: 0.8820960698689956\n",
      "loss: 0.30458593368530273, accuracy: 0.868995633187773\n",
      "loss: 0.29410499334335327, accuracy: 0.8820960698689956\n",
      "loss: 0.3010723888874054, accuracy: 0.8427947598253275\n",
      "loss: 0.2833455502986908, accuracy: 0.8951965065502183\n",
      "loss: 0.310316264629364, accuracy: 0.868995633187773\n",
      "loss: 0.29149889945983887, accuracy: 0.8864628820960698\n",
      "loss: 0.28736916184425354, accuracy: 0.9170305676855895\n",
      "loss: 0.2846180200576782, accuracy: 0.8777292576419214\n",
      "loss: 0.2904110252857208, accuracy: 0.8602620087336245\n",
      "loss: 0.3008204400539398, accuracy: 0.8602620087336245\n",
      "loss: 0.3006477952003479, accuracy: 0.8777292576419214\n",
      "loss: 0.27754300832748413, accuracy: 0.8951965065502183\n",
      "loss: 0.28604012727737427, accuracy: 0.8951965065502183\n",
      "loss: 0.2988680303096771, accuracy: 0.868995633187773\n",
      "loss: 0.30624568462371826, accuracy: 0.8602620087336245\n",
      "loss: 0.30075639486312866, accuracy: 0.9039301310043668\n",
      "loss: 0.3076247274875641, accuracy: 0.8384279475982532\n",
      "loss: 0.2838878035545349, accuracy: 0.9039301310043668\n",
      "loss: 0.2823263108730316, accuracy: 0.8864628820960698\n",
      "loss: 0.2872008979320526, accuracy: 0.9082969432314411\n",
      "loss: 0.30065813660621643, accuracy: 0.8777292576419214\n",
      "loss: 0.2808392345905304, accuracy: 0.9039301310043668\n",
      "loss: 0.28189125657081604, accuracy: 0.8820960698689956\n",
      "loss: 0.30384281277656555, accuracy: 0.8602620087336245\n",
      "loss: 0.27543210983276367, accuracy: 0.9082969432314411\n",
      "loss: 0.3173055648803711, accuracy: 0.8384279475982532\n",
      "loss: 0.27916407585144043, accuracy: 0.8995633187772926\n",
      "loss: 0.28259095549583435, accuracy: 0.9039301310043668\n",
      "loss: 0.2915329933166504, accuracy: 0.8820960698689956\n",
      "loss: 0.28887927532196045, accuracy: 0.8864628820960698\n",
      "loss: 0.2853287160396576, accuracy: 0.9039301310043668\n",
      "loss: 0.30320072174072266, accuracy: 0.8602620087336245\n",
      "loss: 0.2777007222175598, accuracy: 0.9039301310043668\n",
      "loss: 0.2874281704425812, accuracy: 0.8864628820960698\n",
      "loss: 0.312093585729599, accuracy: 0.8864628820960698\n",
      "max accuracy: 0.9301310043668122\n",
      "loss: 0.2751980125904083, accuracy: 0.9301310043668122\n",
      "loss: 0.2751905620098114, accuracy: 0.8864628820960698\n",
      "loss: 0.3096793293952942, accuracy: 0.8427947598253275\n",
      "loss: 0.28916195034980774, accuracy: 0.8908296943231441\n",
      "loss: 0.28206172585487366, accuracy: 0.8864628820960698\n",
      "loss: 0.2684902548789978, accuracy: 0.9082969432314411\n",
      "loss: 0.2870919108390808, accuracy: 0.868995633187773\n",
      "loss: 0.2806890904903412, accuracy: 0.8864628820960698\n",
      "loss: 0.28285112977027893, accuracy: 0.9126637554585153\n",
      "loss: 0.27762076258659363, accuracy: 0.8951965065502183\n",
      "loss: 0.28415924310684204, accuracy: 0.8864628820960698\n",
      "loss: 0.3082730174064636, accuracy: 0.8820960698689956\n",
      "loss: 0.274491548538208, accuracy: 0.8995633187772926\n",
      "loss: 0.27041390538215637, accuracy: 0.8995633187772926\n",
      "loss: 0.29484108090400696, accuracy: 0.8908296943231441\n",
      "loss: 0.29296600818634033, accuracy: 0.8951965065502183\n",
      "loss: 0.27640289068222046, accuracy: 0.8908296943231441\n",
      "loss: 0.29687410593032837, accuracy: 0.8733624454148472\n",
      "loss: 0.29056593775749207, accuracy: 0.8820960698689956\n",
      "loss: 0.26461124420166016, accuracy: 0.9170305676855895\n",
      "loss: 0.30147621035575867, accuracy: 0.851528384279476\n",
      "loss: 0.2723822593688965, accuracy: 0.8864628820960698\n",
      "loss: 0.27284786105155945, accuracy: 0.8908296943231441\n",
      "loss: 0.3135727345943451, accuracy: 0.8646288209606987\n",
      "loss: 0.27312225103378296, accuracy: 0.8995633187772926\n",
      "loss: 0.26112493872642517, accuracy: 0.8908296943231441\n",
      "loss: 0.2721414864063263, accuracy: 0.9213973799126638\n",
      "loss: 0.2803894877433777, accuracy: 0.8951965065502183\n",
      "loss: 0.2836492955684662, accuracy: 0.8951965065502183\n",
      "loss: 0.32163864374160767, accuracy: 0.8558951965065502\n",
      "loss: 0.26095348596572876, accuracy: 0.9213973799126638\n",
      "loss: 0.3057994842529297, accuracy: 0.8602620087336245\n",
      "loss: 0.28817152976989746, accuracy: 0.8864628820960698\n",
      "loss: 0.28230684995651245, accuracy: 0.8995633187772926\n",
      "loss: 0.2816855311393738, accuracy: 0.9039301310043668\n",
      "loss: 0.2819260060787201, accuracy: 0.8733624454148472\n",
      "loss: 0.2914319336414337, accuracy: 0.8646288209606987\n",
      "loss: 0.25924986600875854, accuracy: 0.9039301310043668\n",
      "loss: 0.28896617889404297, accuracy: 0.8951965065502183\n",
      "loss: 0.26979056000709534, accuracy: 0.8908296943231441\n",
      "loss: 0.2801928222179413, accuracy: 0.8864628820960698\n",
      "loss: 0.2715086042881012, accuracy: 0.8951965065502183\n",
      "loss: 0.29219841957092285, accuracy: 0.8602620087336245\n",
      "loss: 0.2637142539024353, accuracy: 0.9126637554585153\n",
      "loss: 0.26876190304756165, accuracy: 0.8646288209606987\n",
      "loss: 0.27191752195358276, accuracy: 0.8951965065502183\n",
      "loss: 0.27926144003868103, accuracy: 0.8777292576419214\n",
      "loss: 0.27552330493927, accuracy: 0.8908296943231441\n",
      "loss: 0.28669700026512146, accuracy: 0.8951965065502183\n",
      "loss: 0.27667343616485596, accuracy: 0.8820960698689956\n",
      "loss: 0.260177344083786, accuracy: 0.8951965065502183\n",
      "loss: 0.27077949047088623, accuracy: 0.8951965065502183\n",
      "loss: 0.27078309655189514, accuracy: 0.9170305676855895\n",
      "loss: 0.27281737327575684, accuracy: 0.9213973799126638\n",
      "loss: 0.30606141686439514, accuracy: 0.868995633187773\n",
      "loss: 0.28135594725608826, accuracy: 0.8951965065502183\n",
      "loss: 0.27166926860809326, accuracy: 0.9039301310043668\n",
      "loss: 0.2782072126865387, accuracy: 0.8864628820960698\n",
      "loss: 0.2853003442287445, accuracy: 0.8908296943231441\n",
      "loss: 0.27975964546203613, accuracy: 0.8820960698689956\n",
      "loss: 0.27031850814819336, accuracy: 0.8995633187772926\n",
      "loss: 0.26065775752067566, accuracy: 0.9039301310043668\n",
      "loss: 0.266885906457901, accuracy: 0.9126637554585153\n",
      "loss: 0.2787683308124542, accuracy: 0.9039301310043668\n",
      "loss: 0.2860443890094757, accuracy: 0.8908296943231441\n",
      "loss: 0.28554728627204895, accuracy: 0.8864628820960698\n",
      "loss: 0.29898297786712646, accuracy: 0.8733624454148472\n",
      "loss: 0.2870601415634155, accuracy: 0.8733624454148472\n",
      "loss: 0.30115020275115967, accuracy: 0.868995633187773\n",
      "loss: 0.2647668123245239, accuracy: 0.9082969432314411\n",
      "loss: 0.2608555853366852, accuracy: 0.9082969432314411\n",
      "loss: 0.2927565276622772, accuracy: 0.8820960698689956\n",
      "max accuracy: 0.9344978165938864\n",
      "loss: 0.25638478994369507, accuracy: 0.9344978165938864\n",
      "loss: 0.25515908002853394, accuracy: 0.9126637554585153\n",
      "loss: 0.26672324538230896, accuracy: 0.8908296943231441\n",
      "loss: 0.26981645822525024, accuracy: 0.9126637554585153\n",
      "loss: 0.26337069272994995, accuracy: 0.9170305676855895\n",
      "loss: 0.26120293140411377, accuracy: 0.8995633187772926\n",
      "loss: 0.2958902418613434, accuracy: 0.8864628820960698\n",
      "loss: 0.2656475007534027, accuracy: 0.8951965065502183\n",
      "loss: 0.26463600993156433, accuracy: 0.8908296943231441\n",
      "loss: 0.2967425286769867, accuracy: 0.8951965065502183\n",
      "loss: 0.27998945116996765, accuracy: 0.8995633187772926\n",
      "loss: 0.2579198181629181, accuracy: 0.9213973799126638\n",
      "loss: 0.2905009984970093, accuracy: 0.8777292576419214\n",
      "loss: 0.2618705928325653, accuracy: 0.9082969432314411\n",
      "loss: 0.2595658302307129, accuracy: 0.9126637554585153\n",
      "loss: 0.2942565381526947, accuracy: 0.868995633187773\n",
      "loss: 0.29578524827957153, accuracy: 0.8777292576419214\n",
      "loss: 0.2975752055644989, accuracy: 0.8951965065502183\n",
      "loss: 0.25725606083869934, accuracy: 0.9170305676855895\n",
      "loss: 0.2632715702056885, accuracy: 0.8951965065502183\n",
      "loss: 0.26722460985183716, accuracy: 0.9213973799126638\n",
      "loss: 0.26677343249320984, accuracy: 0.9126637554585153\n",
      "loss: 0.2582465708255768, accuracy: 0.9170305676855895\n",
      "loss: 0.26155149936676025, accuracy: 0.9170305676855895\n",
      "loss: 0.2667222023010254, accuracy: 0.9039301310043668\n",
      "loss: 0.2594563364982605, accuracy: 0.9170305676855895\n",
      "loss: 0.26964399218559265, accuracy: 0.8908296943231441\n",
      "loss: 0.26336660981178284, accuracy: 0.9082969432314411\n",
      "loss: 0.26117005944252014, accuracy: 0.9082969432314411\n",
      "loss: 0.2588058114051819, accuracy: 0.9126637554585153\n",
      "loss: 0.25499868392944336, accuracy: 0.9082969432314411\n",
      "loss: 0.24953041970729828, accuracy: 0.9082969432314411\n",
      "loss: 0.2833653688430786, accuracy: 0.9126637554585153\n",
      "loss: 0.2543882131576538, accuracy: 0.925764192139738\n",
      "loss: 0.2607794404029846, accuracy: 0.9170305676855895\n",
      "loss: 0.2802985608577728, accuracy: 0.8864628820960698\n",
      "loss: 0.24537867307662964, accuracy: 0.925764192139738\n",
      "loss: 0.2766953706741333, accuracy: 0.8951965065502183\n",
      "loss: 0.2548864483833313, accuracy: 0.9126637554585153\n",
      "loss: 0.27096280455589294, accuracy: 0.8908296943231441\n",
      "loss: 0.32570111751556396, accuracy: 0.851528384279476\n",
      "loss: 0.3115401566028595, accuracy: 0.8558951965065502\n",
      "loss: 0.2567398250102997, accuracy: 0.9126637554585153\n",
      "loss: 0.25674229860305786, accuracy: 0.8820960698689956\n",
      "loss: 0.26385125517845154, accuracy: 0.9039301310043668\n",
      "loss: 0.27062079310417175, accuracy: 0.8864628820960698\n",
      "loss: 0.25709620118141174, accuracy: 0.8995633187772926\n",
      "loss: 0.2704336941242218, accuracy: 0.9170305676855895\n",
      "loss: 0.2767006456851959, accuracy: 0.9170305676855895\n",
      "loss: 0.2710728347301483, accuracy: 0.9039301310043668\n",
      "loss: 0.27849701046943665, accuracy: 0.8908296943231441\n",
      "loss: 0.254126638174057, accuracy: 0.9213973799126638\n",
      "loss: 0.27529221773147583, accuracy: 0.8820960698689956\n",
      "loss: 0.2608458399772644, accuracy: 0.8908296943231441\n",
      "loss: 0.2914760112762451, accuracy: 0.8733624454148472\n",
      "loss: 0.25313955545425415, accuracy: 0.925764192139738\n",
      "loss: 0.27064773440361023, accuracy: 0.8864628820960698\n",
      "loss: 0.27045485377311707, accuracy: 0.9039301310043668\n",
      "loss: 0.27236467599868774, accuracy: 0.8995633187772926\n",
      "loss: 0.259333074092865, accuracy: 0.9082969432314411\n",
      "loss: 0.2576296627521515, accuracy: 0.8951965065502183\n",
      "loss: 0.2566254734992981, accuracy: 0.9301310043668122\n",
      "loss: 0.2827991545200348, accuracy: 0.8995633187772926\n",
      "loss: 0.26173368096351624, accuracy: 0.9126637554585153\n",
      "loss: 0.2671768367290497, accuracy: 0.8908296943231441\n",
      "loss: 0.280770480632782, accuracy: 0.8951965065502183\n",
      "loss: 0.26240527629852295, accuracy: 0.8951965065502183\n",
      "loss: 0.26512694358825684, accuracy: 0.8951965065502183\n",
      "loss: 0.24488012492656708, accuracy: 0.9213973799126638\n",
      "loss: 0.2507227659225464, accuracy: 0.9213973799126638\n",
      "loss: 0.24566051363945007, accuracy: 0.925764192139738\n",
      "loss: 0.25193077325820923, accuracy: 0.9082969432314411\n",
      "loss: 0.25502121448516846, accuracy: 0.925764192139738\n",
      "loss: 0.2529464364051819, accuracy: 0.8908296943231441\n",
      "loss: 0.25271302461624146, accuracy: 0.9170305676855895\n",
      "loss: 0.27270573377609253, accuracy: 0.8995633187772926\n",
      "loss: 0.2471981644630432, accuracy: 0.9213973799126638\n",
      "loss: 0.2630511522293091, accuracy: 0.9126637554585153\n",
      "loss: 0.26469969749450684, accuracy: 0.8995633187772926\n",
      "loss: 0.2559838891029358, accuracy: 0.9082969432314411\n",
      "loss: 0.24836555123329163, accuracy: 0.925764192139738\n",
      "loss: 0.2584429681301117, accuracy: 0.8908296943231441\n",
      "loss: 0.24996896088123322, accuracy: 0.8908296943231441\n",
      "loss: 0.2574710547924042, accuracy: 0.8951965065502183\n",
      "loss: 0.24063733220100403, accuracy: 0.9126637554585153\n",
      "loss: 0.2485901266336441, accuracy: 0.9039301310043668\n",
      "loss: 0.2963668406009674, accuracy: 0.851528384279476\n",
      "loss: 0.2418309450149536, accuracy: 0.9344978165938864\n",
      "loss: 0.27053454518318176, accuracy: 0.8951965065502183\n",
      "loss: 0.26222994923591614, accuracy: 0.9082969432314411\n",
      "loss: 0.2586310803890228, accuracy: 0.8951965065502183\n",
      "loss: 0.25590255856513977, accuracy: 0.925764192139738\n",
      "loss: 0.25507640838623047, accuracy: 0.9126637554585153\n",
      "loss: 0.2516932189464569, accuracy: 0.9213973799126638\n",
      "loss: 0.24577085673809052, accuracy: 0.8951965065502183\n",
      "loss: 0.27147749066352844, accuracy: 0.9082969432314411\n",
      "loss: 0.23787106573581696, accuracy: 0.925764192139738\n",
      "loss: 0.2607414722442627, accuracy: 0.9039301310043668\n",
      "loss: 0.2610494792461395, accuracy: 0.9126637554585153\n",
      "loss: 0.26284152269363403, accuracy: 0.8951965065502183\n",
      "max accuracy: 0.9388646288209607\n",
      "loss: 0.23451077938079834, accuracy: 0.9388646288209607\n",
      "loss: 0.25477057695388794, accuracy: 0.925764192139738\n",
      "loss: 0.23758623003959656, accuracy: 0.925764192139738\n",
      "loss: 0.264462947845459, accuracy: 0.8864628820960698\n",
      "loss: 0.2440257966518402, accuracy: 0.9170305676855895\n",
      "loss: 0.2776934504508972, accuracy: 0.8995633187772926\n",
      "loss: 0.26391708850860596, accuracy: 0.8995633187772926\n",
      "loss: 0.269315242767334, accuracy: 0.8733624454148472\n",
      "loss: 0.2814195156097412, accuracy: 0.8951965065502183\n",
      "loss: 0.297919362783432, accuracy: 0.8908296943231441\n",
      "loss: 0.2475637048482895, accuracy: 0.9039301310043668\n",
      "loss: 0.27492040395736694, accuracy: 0.9082969432314411\n",
      "loss: 0.23715253174304962, accuracy: 0.9344978165938864\n",
      "loss: 0.24952340126037598, accuracy: 0.9126637554585153\n",
      "loss: 0.26098984479904175, accuracy: 0.9039301310043668\n",
      "loss: 0.28196099400520325, accuracy: 0.9039301310043668\n",
      "loss: 0.25073742866516113, accuracy: 0.8951965065502183\n",
      "loss: 0.2744452655315399, accuracy: 0.8777292576419214\n",
      "loss: 0.2593163549900055, accuracy: 0.9170305676855895\n",
      "loss: 0.2494824379682541, accuracy: 0.925764192139738\n",
      "loss: 0.2517123520374298, accuracy: 0.9039301310043668\n",
      "loss: 0.2406112402677536, accuracy: 0.925764192139738\n",
      "loss: 0.2574533224105835, accuracy: 0.9039301310043668\n",
      "loss: 0.2785685062408447, accuracy: 0.8864628820960698\n",
      "loss: 0.26684337854385376, accuracy: 0.8995633187772926\n",
      "loss: 0.2596696615219116, accuracy: 0.9039301310043668\n",
      "loss: 0.2628321051597595, accuracy: 0.9126637554585153\n",
      "loss: 0.2815193235874176, accuracy: 0.9039301310043668\n",
      "loss: 0.243735671043396, accuracy: 0.925764192139738\n",
      "loss: 0.2505872845649719, accuracy: 0.9126637554585153\n",
      "loss: 0.254489928483963, accuracy: 0.8908296943231441\n",
      "loss: 0.2554055452346802, accuracy: 0.8908296943231441\n",
      "loss: 0.2545854449272156, accuracy: 0.9301310043668122\n",
      "loss: 0.24090994894504547, accuracy: 0.9301310043668122\n",
      "loss: 0.2348635196685791, accuracy: 0.925764192139738\n",
      "loss: 0.2587258815765381, accuracy: 0.9039301310043668\n",
      "loss: 0.2942756116390228, accuracy: 0.8733624454148472\n",
      "loss: 0.2801390290260315, accuracy: 0.8777292576419214\n",
      "loss: 0.26403677463531494, accuracy: 0.8820960698689956\n",
      "loss: 0.23892450332641602, accuracy: 0.925764192139738\n",
      "loss: 0.24893712997436523, accuracy: 0.9082969432314411\n",
      "loss: 0.23779672384262085, accuracy: 0.9126637554585153\n",
      "loss: 0.25886231660842896, accuracy: 0.8951965065502183\n",
      "loss: 0.2582760751247406, accuracy: 0.8995633187772926\n",
      "loss: 0.2441639006137848, accuracy: 0.9344978165938864\n",
      "loss: 0.23737558722496033, accuracy: 0.925764192139738\n",
      "loss: 0.2470065951347351, accuracy: 0.9126637554585153\n",
      "loss: 0.258403480052948, accuracy: 0.9126637554585153\n",
      "loss: 0.2810419797897339, accuracy: 0.8908296943231441\n",
      "loss: 0.259097158908844, accuracy: 0.9082969432314411\n",
      "loss: 0.24150805175304413, accuracy: 0.9213973799126638\n",
      "loss: 0.26216477155685425, accuracy: 0.9082969432314411\n",
      "loss: 0.2796439826488495, accuracy: 0.9039301310043668\n",
      "max accuracy: 0.9432314410480349\n",
      "loss: 0.21545866131782532, accuracy: 0.9432314410480349\n",
      "loss: 0.2698397636413574, accuracy: 0.8777292576419214\n",
      "loss: 0.2554197907447815, accuracy: 0.9213973799126638\n",
      "loss: 0.2660278081893921, accuracy: 0.8908296943231441\n",
      "loss: 0.23425327241420746, accuracy: 0.9213973799126638\n",
      "loss: 0.23570840060710907, accuracy: 0.9301310043668122\n",
      "loss: 0.23323722183704376, accuracy: 0.925764192139738\n",
      "loss: 0.24227768182754517, accuracy: 0.9301310043668122\n",
      "loss: 0.24597182869911194, accuracy: 0.9039301310043668\n",
      "loss: 0.2298872023820877, accuracy: 0.9213973799126638\n",
      "loss: 0.2400582879781723, accuracy: 0.9126637554585153\n",
      "loss: 0.2379421889781952, accuracy: 0.9344978165938864\n",
      "loss: 0.2436043918132782, accuracy: 0.9301310043668122\n",
      "loss: 0.23949487507343292, accuracy: 0.9039301310043668\n",
      "loss: 0.2718140482902527, accuracy: 0.8908296943231441\n",
      "loss: 0.23926910758018494, accuracy: 0.9126637554585153\n",
      "loss: 0.2218743860721588, accuracy: 0.9344978165938864\n",
      "loss: 0.2549886107444763, accuracy: 0.8951965065502183\n",
      "loss: 0.264265775680542, accuracy: 0.8646288209606987\n",
      "loss: 0.263993501663208, accuracy: 0.8908296943231441\n",
      "loss: 0.2811639904975891, accuracy: 0.9082969432314411\n",
      "loss: 0.24587181210517883, accuracy: 0.9213973799126638\n",
      "loss: 0.2511916160583496, accuracy: 0.8995633187772926\n",
      "loss: 0.22885720431804657, accuracy: 0.925764192139738\n",
      "loss: 0.2318500429391861, accuracy: 0.9126637554585153\n",
      "loss: 0.2587761878967285, accuracy: 0.8777292576419214\n",
      "loss: 0.254036545753479, accuracy: 0.8995633187772926\n",
      "loss: 0.23853585124015808, accuracy: 0.9126637554585153\n",
      "loss: 0.22587348520755768, accuracy: 0.9344978165938864\n",
      "loss: 0.22822877764701843, accuracy: 0.9170305676855895\n",
      "loss: 0.2448408454656601, accuracy: 0.9170305676855895\n",
      "loss: 0.2396380603313446, accuracy: 0.9170305676855895\n",
      "loss: 0.23720383644104004, accuracy: 0.9170305676855895\n",
      "loss: 0.23238664865493774, accuracy: 0.9170305676855895\n",
      "loss: 0.23871736228466034, accuracy: 0.925764192139738\n",
      "loss: 0.2410064935684204, accuracy: 0.9082969432314411\n",
      "loss: 0.2466174066066742, accuracy: 0.9082969432314411\n",
      "loss: 0.25188684463500977, accuracy: 0.9039301310043668\n",
      "loss: 0.243741974234581, accuracy: 0.9126637554585153\n",
      "loss: 0.22935278713703156, accuracy: 0.9344978165938864\n",
      "loss: 0.24873709678649902, accuracy: 0.9213973799126638\n",
      "loss: 0.22464869916439056, accuracy: 0.9301310043668122\n",
      "loss: 0.25023606419563293, accuracy: 0.8951965065502183\n",
      "loss: 0.2447112500667572, accuracy: 0.9170305676855895\n",
      "max accuracy: 0.9475982532751092\n",
      "loss: 0.23022569715976715, accuracy: 0.9475982532751092\n",
      "loss: 0.23141084611415863, accuracy: 0.9388646288209607\n",
      "loss: 0.2680554687976837, accuracy: 0.8951965065502183\n",
      "loss: 0.24740418791770935, accuracy: 0.8951965065502183\n",
      "loss: 0.23874525725841522, accuracy: 0.9344978165938864\n",
      "loss: 0.2210410237312317, accuracy: 0.925764192139738\n",
      "loss: 0.22940286993980408, accuracy: 0.9301310043668122\n",
      "loss: 0.23162098228931427, accuracy: 0.9301310043668122\n",
      "loss: 0.2523142099380493, accuracy: 0.9126637554585153\n",
      "loss: 0.2568832337856293, accuracy: 0.9126637554585153\n",
      "loss: 0.25814327597618103, accuracy: 0.9082969432314411\n",
      "loss: 0.26433318853378296, accuracy: 0.8995633187772926\n",
      "loss: 0.24999499320983887, accuracy: 0.8995633187772926\n",
      "loss: 0.24694883823394775, accuracy: 0.9082969432314411\n",
      "loss: 0.22523793578147888, accuracy: 0.9301310043668122\n",
      "loss: 0.224948450922966, accuracy: 0.925764192139738\n",
      "loss: 0.22179482877254486, accuracy: 0.9432314410480349\n",
      "loss: 0.24816833436489105, accuracy: 0.8995633187772926\n",
      "loss: 0.23344655334949493, accuracy: 0.9213973799126638\n",
      "loss: 0.23651279509067535, accuracy: 0.9126637554585153\n",
      "loss: 0.23304659128189087, accuracy: 0.9388646288209607\n",
      "loss: 0.2576693296432495, accuracy: 0.8995633187772926\n",
      "loss: 0.24403314292430878, accuracy: 0.9082969432314411\n",
      "loss: 0.22140459716320038, accuracy: 0.925764192139738\n",
      "loss: 0.21689489483833313, accuracy: 0.9388646288209607\n",
      "loss: 0.2421637624502182, accuracy: 0.9126637554585153\n",
      "loss: 0.23261816799640656, accuracy: 0.9082969432314411\n",
      "loss: 0.22753816843032837, accuracy: 0.8995633187772926\n",
      "loss: 0.23350878059864044, accuracy: 0.9344978165938864\n",
      "loss: 0.23559272289276123, accuracy: 0.925764192139738\n",
      "loss: 0.2654164433479309, accuracy: 0.9039301310043668\n",
      "loss: 0.2222086489200592, accuracy: 0.9344978165938864\n",
      "loss: 0.22334346175193787, accuracy: 0.9388646288209607\n",
      "loss: 0.2532137930393219, accuracy: 0.9082969432314411\n",
      "loss: 0.22312189638614655, accuracy: 0.925764192139738\n",
      "loss: 0.222250297665596, accuracy: 0.925764192139738\n",
      "loss: 0.26418131589889526, accuracy: 0.8951965065502183\n",
      "loss: 0.25370341539382935, accuracy: 0.9039301310043668\n",
      "loss: 0.22662414610385895, accuracy: 0.9301310043668122\n",
      "loss: 0.2213132381439209, accuracy: 0.9344978165938864\n",
      "loss: 0.24204598367214203, accuracy: 0.9039301310043668\n",
      "loss: 0.23645122349262238, accuracy: 0.9126637554585153\n",
      "loss: 0.2508732080459595, accuracy: 0.9039301310043668\n",
      "loss: 0.24033203721046448, accuracy: 0.9082969432314411\n",
      "loss: 0.2342333197593689, accuracy: 0.9082969432314411\n",
      "loss: 0.2266121208667755, accuracy: 0.9126637554585153\n",
      "loss: 0.23143072426319122, accuracy: 0.9170305676855895\n",
      "loss: 0.23470282554626465, accuracy: 0.925764192139738\n",
      "loss: 0.2123190313577652, accuracy: 0.9388646288209607\n",
      "loss: 0.23706577718257904, accuracy: 0.9213973799126638\n",
      "loss: 0.22374942898750305, accuracy: 0.9301310043668122\n",
      "loss: 0.2269076406955719, accuracy: 0.9344978165938864\n",
      "loss: 0.21569819748401642, accuracy: 0.9388646288209607\n",
      "loss: 0.23792417347431183, accuracy: 0.9039301310043668\n",
      "loss: 0.2334483414888382, accuracy: 0.9213973799126638\n",
      "loss: 0.22941112518310547, accuracy: 0.9039301310043668\n",
      "loss: 0.2224319875240326, accuracy: 0.9344978165938864\n",
      "loss: 0.22899280488491058, accuracy: 0.9301310043668122\n",
      "loss: 0.22455111145973206, accuracy: 0.9301310043668122\n",
      "loss: 0.2208782136440277, accuracy: 0.9170305676855895\n",
      "loss: 0.217018723487854, accuracy: 0.9475982532751092\n",
      "loss: 0.24171032011508942, accuracy: 0.8995633187772926\n",
      "loss: 0.2349221110343933, accuracy: 0.9213973799126638\n",
      "loss: 0.2267296463251114, accuracy: 0.9213973799126638\n",
      "max accuracy: 0.9519650655021834\n",
      "loss: 0.21034055948257446, accuracy: 0.9519650655021834\n",
      "loss: 0.26116397976875305, accuracy: 0.8777292576419214\n",
      "loss: 0.25775808095932007, accuracy: 0.8864628820960698\n",
      "loss: 0.22624294459819794, accuracy: 0.9301310043668122\n",
      "loss: 0.2701733410358429, accuracy: 0.8864628820960698\n",
      "loss: 0.2181839644908905, accuracy: 0.925764192139738\n",
      "loss: 0.2284218668937683, accuracy: 0.925764192139738\n",
      "loss: 0.20924058556556702, accuracy: 0.9432314410480349\n",
      "loss: 0.2108381986618042, accuracy: 0.9344978165938864\n",
      "loss: 0.20989933609962463, accuracy: 0.9301310043668122\n",
      "loss: 0.2831462323665619, accuracy: 0.8908296943231441\n",
      "loss: 0.2254214584827423, accuracy: 0.9301310043668122\n",
      "loss: 0.22588947415351868, accuracy: 0.9126637554585153\n",
      "loss: 0.21142370998859406, accuracy: 0.9388646288209607\n",
      "loss: 0.22233201563358307, accuracy: 0.9213973799126638\n",
      "loss: 0.21584679186344147, accuracy: 0.9475982532751092\n",
      "loss: 0.23505687713623047, accuracy: 0.9126637554585153\n",
      "loss: 0.260929137468338, accuracy: 0.9039301310043668\n",
      "loss: 0.21179598569869995, accuracy: 0.9301310043668122\n",
      "loss: 0.22594338655471802, accuracy: 0.9301310043668122\n",
      "loss: 0.20962338149547577, accuracy: 0.9344978165938864\n",
      "loss: 0.2228342741727829, accuracy: 0.9301310043668122\n",
      "loss: 0.24178610742092133, accuracy: 0.9039301310043668\n",
      "loss: 0.20700062811374664, accuracy: 0.9301310043668122\n",
      "loss: 0.21985386312007904, accuracy: 0.9213973799126638\n",
      "loss: 0.21622496843338013, accuracy: 0.9213973799126638\n",
      "loss: 0.22226804494857788, accuracy: 0.9213973799126638\n",
      "loss: 0.21071185171604156, accuracy: 0.9388646288209607\n",
      "loss: 0.22629466652870178, accuracy: 0.925764192139738\n",
      "loss: 0.21945054829120636, accuracy: 0.9301310043668122\n",
      "loss: 0.2229686826467514, accuracy: 0.9301310043668122\n",
      "loss: 0.20720843970775604, accuracy: 0.9432314410480349\n",
      "loss: 0.229841947555542, accuracy: 0.925764192139738\n",
      "loss: 0.25971662998199463, accuracy: 0.9039301310043668\n",
      "loss: 0.2235962450504303, accuracy: 0.9126637554585153\n",
      "loss: 0.21359267830848694, accuracy: 0.9344978165938864\n",
      "loss: 0.22779850661754608, accuracy: 0.9170305676855895\n",
      "loss: 0.20667888224124908, accuracy: 0.9344978165938864\n",
      "loss: 0.23376068472862244, accuracy: 0.9082969432314411\n",
      "loss: 0.21135468780994415, accuracy: 0.9519650655021834\n",
      "loss: 0.20697630941867828, accuracy: 0.9344978165938864\n",
      "loss: 0.22201380133628845, accuracy: 0.9170305676855895\n",
      "loss: 0.24549493193626404, accuracy: 0.9082969432314411\n",
      "loss: 0.22914329171180725, accuracy: 0.925764192139738\n",
      "loss: 0.2179926037788391, accuracy: 0.9388646288209607\n",
      "loss: 0.21711640059947968, accuracy: 0.925764192139738\n",
      "loss: 0.2241702675819397, accuracy: 0.9126637554585153\n",
      "loss: 0.23445649445056915, accuracy: 0.9213973799126638\n",
      "loss: 0.22255189716815948, accuracy: 0.9082969432314411\n",
      "loss: 0.21498781442642212, accuracy: 0.9301310043668122\n",
      "loss: 0.20928984880447388, accuracy: 0.9432314410480349\n",
      "loss: 0.20200197398662567, accuracy: 0.9475982532751092\n",
      "loss: 0.21741531789302826, accuracy: 0.9344978165938864\n",
      "loss: 0.2778446674346924, accuracy: 0.8820960698689956\n",
      "loss: 0.21202097833156586, accuracy: 0.925764192139738\n",
      "loss: 0.2112228125333786, accuracy: 0.9344978165938864\n",
      "loss: 0.22042326629161835, accuracy: 0.9213973799126638\n",
      "loss: 0.24786023795604706, accuracy: 0.9082969432314411\n",
      "loss: 0.21085917949676514, accuracy: 0.9344978165938864\n",
      "loss: 0.20668986439704895, accuracy: 0.9344978165938864\n",
      "loss: 0.20369228720664978, accuracy: 0.9432314410480349\n",
      "loss: 0.2071957141160965, accuracy: 0.9519650655021834\n",
      "loss: 0.19433698058128357, accuracy: 0.9519650655021834\n",
      "loss: 0.2045283317565918, accuracy: 0.925764192139738\n",
      "loss: 0.2006485015153885, accuracy: 0.925764192139738\n",
      "loss: 0.23146124184131622, accuracy: 0.9170305676855895\n",
      "loss: 0.2097170650959015, accuracy: 0.9519650655021834\n",
      "loss: 0.21912093460559845, accuracy: 0.9344978165938864\n",
      "loss: 0.2080443799495697, accuracy: 0.925764192139738\n",
      "loss: 0.21548855304718018, accuracy: 0.9344978165938864\n",
      "loss: 0.2131025195121765, accuracy: 0.9344978165938864\n",
      "loss: 0.22150801122188568, accuracy: 0.9213973799126638\n",
      "loss: 0.21737024188041687, accuracy: 0.9301310043668122\n",
      "loss: 0.20618611574172974, accuracy: 0.9432314410480349\n",
      "loss: 0.21500810980796814, accuracy: 0.9301310043668122\n",
      "loss: 0.23211942613124847, accuracy: 0.9213973799126638\n",
      "loss: 0.20438694953918457, accuracy: 0.9519650655021834\n",
      "loss: 0.21553169190883636, accuracy: 0.9344978165938864\n",
      "loss: 0.21054978668689728, accuracy: 0.9213973799126638\n",
      "loss: 0.19439737498760223, accuracy: 0.9475982532751092\n",
      "loss: 0.2591496407985687, accuracy: 0.8908296943231441\n",
      "loss: 0.19512440264225006, accuracy: 0.9432314410480349\n",
      "loss: 0.20782609283924103, accuracy: 0.9344978165938864\n",
      "loss: 0.26067212224006653, accuracy: 0.9039301310043668\n",
      "loss: 0.21817508339881897, accuracy: 0.9301310043668122\n",
      "loss: 0.23144061863422394, accuracy: 0.925764192139738\n",
      "loss: 0.20175592601299286, accuracy: 0.9432314410480349\n",
      "loss: 0.21994920074939728, accuracy: 0.925764192139738\n",
      "loss: 0.20552371442317963, accuracy: 0.9388646288209607\n",
      "loss: 0.20859523117542267, accuracy: 0.9432314410480349\n",
      "loss: 0.20608307421207428, accuracy: 0.9388646288209607\n",
      "loss: 0.20500792562961578, accuracy: 0.9388646288209607\n",
      "loss: 0.21062134206295013, accuracy: 0.9213973799126638\n",
      "loss: 0.21526028215885162, accuracy: 0.9213973799126638\n",
      "loss: 0.19930604100227356, accuracy: 0.9432314410480349\n",
      "loss: 0.20311160385608673, accuracy: 0.9432314410480349\n",
      "loss: 0.21554771065711975, accuracy: 0.9344978165938864\n",
      "loss: 0.24444794654846191, accuracy: 0.8995633187772926\n",
      "loss: 0.20795568823814392, accuracy: 0.9388646288209607\n",
      "loss: 0.19959084689617157, accuracy: 0.9432314410480349\n",
      "loss: 0.21361000835895538, accuracy: 0.925764192139738\n",
      "loss: 0.20987747609615326, accuracy: 0.9344978165938864\n",
      "loss: 0.21143165230751038, accuracy: 0.9344978165938864\n",
      "loss: 0.2217129021883011, accuracy: 0.9126637554585153\n",
      "loss: 0.21496529877185822, accuracy: 0.9388646288209607\n",
      "loss: 0.20853304862976074, accuracy: 0.9344978165938864\n",
      "loss: 0.19672437012195587, accuracy: 0.9475982532751092\n",
      "loss: 0.19964976608753204, accuracy: 0.9432314410480349\n",
      "loss: 0.18864908814430237, accuracy: 0.9475982532751092\n",
      "loss: 0.19473111629486084, accuracy: 0.9344978165938864\n",
      "loss: 0.1920389086008072, accuracy: 0.9432314410480349\n",
      "loss: 0.22452056407928467, accuracy: 0.9082969432314411\n",
      "loss: 0.24758681654930115, accuracy: 0.9126637554585153\n",
      "loss: 0.22999802231788635, accuracy: 0.9039301310043668\n",
      "loss: 0.19596128165721893, accuracy: 0.9432314410480349\n",
      "loss: 0.1954895704984665, accuracy: 0.9475982532751092\n",
      "loss: 0.18701080977916718, accuracy: 0.9388646288209607\n",
      "loss: 0.2031410038471222, accuracy: 0.9388646288209607\n",
      "loss: 0.19850867986679077, accuracy: 0.9475982532751092\n",
      "loss: 0.19387778639793396, accuracy: 0.9475982532751092\n",
      "loss: 0.22757676243782043, accuracy: 0.9126637554585153\n",
      "loss: 0.20546947419643402, accuracy: 0.9039301310043668\n",
      "loss: 0.2006310224533081, accuracy: 0.9301310043668122\n",
      "loss: 0.19008108973503113, accuracy: 0.9519650655021834\n",
      "loss: 0.19206184148788452, accuracy: 0.9432314410480349\n",
      "loss: 0.20892713963985443, accuracy: 0.9170305676855895\n",
      "loss: 0.19352571666240692, accuracy: 0.9388646288209607\n",
      "loss: 0.2076091170310974, accuracy: 0.9344978165938864\n",
      "loss: 0.19040995836257935, accuracy: 0.9475982532751092\n",
      "loss: 0.1949690729379654, accuracy: 0.9475982532751092\n",
      "loss: 0.1963304728269577, accuracy: 0.9344978165938864\n",
      "loss: 0.19176745414733887, accuracy: 0.9432314410480349\n",
      "loss: 0.1915513128042221, accuracy: 0.9475982532751092\n",
      "loss: 0.2026652693748474, accuracy: 0.9213973799126638\n",
      "loss: 0.19807571172714233, accuracy: 0.9475982532751092\n",
      "loss: 0.19321666657924652, accuracy: 0.9301310043668122\n",
      "loss: 0.1857473999261856, accuracy: 0.9519650655021834\n",
      "loss: 0.2082052230834961, accuracy: 0.9344978165938864\n",
      "loss: 0.2224796712398529, accuracy: 0.9082969432314411\n",
      "loss: 0.19638392329216003, accuracy: 0.9301310043668122\n",
      "loss: 0.1922241896390915, accuracy: 0.9475982532751092\n",
      "loss: 0.19591081142425537, accuracy: 0.9301310043668122\n",
      "loss: 0.19235752522945404, accuracy: 0.9388646288209607\n",
      "loss: 0.19047212600708008, accuracy: 0.925764192139738\n",
      "loss: 0.19922508299350739, accuracy: 0.9432314410480349\n",
      "loss: 0.21450819075107574, accuracy: 0.925764192139738\n",
      "loss: 0.20816576480865479, accuracy: 0.9213973799126638\n",
      "loss: 0.22490526735782623, accuracy: 0.8951965065502183\n",
      "loss: 0.194656103849411, accuracy: 0.9475982532751092\n",
      "loss: 0.21166348457336426, accuracy: 0.9301310043668122\n",
      "loss: 0.22130951285362244, accuracy: 0.9126637554585153\n",
      "loss: 0.21124447882175446, accuracy: 0.9301310043668122\n",
      "loss: 0.18498967587947845, accuracy: 0.9519650655021834\n",
      "loss: 0.19609849154949188, accuracy: 0.9475982532751092\n",
      "loss: 0.20641762018203735, accuracy: 0.9432314410480349\n",
      "loss: 0.1860569417476654, accuracy: 0.9519650655021834\n",
      "loss: 0.2080472707748413, accuracy: 0.9213973799126638\n",
      "loss: 0.19597287476062775, accuracy: 0.9432314410480349\n",
      "loss: 0.2051895558834076, accuracy: 0.925764192139738\n",
      "loss: 0.2716008126735687, accuracy: 0.8908296943231441\n",
      "loss: 0.19820977747440338, accuracy: 0.9432314410480349\n",
      "max accuracy: 0.9563318777292577\n",
      "loss: 0.18448947370052338, accuracy: 0.9563318777292577\n",
      "loss: 0.19852441549301147, accuracy: 0.9170305676855895\n",
      "loss: 0.221500962972641, accuracy: 0.9213973799126638\n",
      "loss: 0.30918359756469727, accuracy: 0.8777292576419214\n",
      "loss: 0.1847536563873291, accuracy: 0.9432314410480349\n",
      "loss: 0.20711743831634521, accuracy: 0.9344978165938864\n",
      "loss: 0.23755469918251038, accuracy: 0.8951965065502183\n",
      "loss: 0.19039762020111084, accuracy: 0.9475982532751092\n",
      "loss: 0.2084706574678421, accuracy: 0.9126637554585153\n",
      "loss: 0.1937132030725479, accuracy: 0.9519650655021834\n",
      "loss: 0.20194163918495178, accuracy: 0.925764192139738\n",
      "loss: 0.2234792411327362, accuracy: 0.9170305676855895\n",
      "loss: 0.21330486238002777, accuracy: 0.9301310043668122\n",
      "loss: 0.19365738332271576, accuracy: 0.9344978165938864\n",
      "loss: 0.2037523239850998, accuracy: 0.9170305676855895\n",
      "loss: 0.17793475091457367, accuracy: 0.9432314410480349\n",
      "loss: 0.19660162925720215, accuracy: 0.9388646288209607\n",
      "loss: 0.19851890206336975, accuracy: 0.9432314410480349\n",
      "max accuracy: 0.9606986899563319\n",
      "loss: 0.1762251853942871, accuracy: 0.9606986899563319\n",
      "loss: 0.18351967632770538, accuracy: 0.9519650655021834\n",
      "loss: 0.18331004679203033, accuracy: 0.9519650655021834\n",
      "loss: 0.2297188937664032, accuracy: 0.8995633187772926\n",
      "loss: 0.18695978820323944, accuracy: 0.9519650655021834\n",
      "loss: 0.19333422183990479, accuracy: 0.9388646288209607\n",
      "loss: 0.1819441318511963, accuracy: 0.9388646288209607\n",
      "loss: 0.19316557049751282, accuracy: 0.9388646288209607\n",
      "loss: 0.24717456102371216, accuracy: 0.8777292576419214\n",
      "loss: 0.19965432584285736, accuracy: 0.9432314410480349\n",
      "loss: 0.2074485570192337, accuracy: 0.925764192139738\n",
      "loss: 0.18479391932487488, accuracy: 0.9432314410480349\n",
      "loss: 0.19313007593154907, accuracy: 0.9475982532751092\n",
      "loss: 0.22392821311950684, accuracy: 0.9126637554585153\n",
      "loss: 0.22012510895729065, accuracy: 0.9213973799126638\n",
      "loss: 0.18800748884677887, accuracy: 0.9432314410480349\n",
      "loss: 0.18137526512145996, accuracy: 0.9432314410480349\n",
      "loss: 0.1901433914899826, accuracy: 0.9344978165938864\n",
      "loss: 0.2197778970003128, accuracy: 0.9082969432314411\n",
      "loss: 0.1903396099805832, accuracy: 0.9388646288209607\n",
      "loss: 0.1860102117061615, accuracy: 0.9519650655021834\n",
      "loss: 0.17956554889678955, accuracy: 0.9563318777292577\n",
      "loss: 0.19864924252033234, accuracy: 0.9388646288209607\n",
      "loss: 0.18920378386974335, accuracy: 0.9388646288209607\n",
      "loss: 0.19032970070838928, accuracy: 0.9432314410480349\n",
      "loss: 0.19481901824474335, accuracy: 0.9432314410480349\n",
      "loss: 0.19107799232006073, accuracy: 0.9519650655021834\n",
      "loss: 0.18144340813159943, accuracy: 0.9519650655021834\n",
      "max accuracy: 0.9650655021834061\n",
      "loss: 0.17062430083751678, accuracy: 0.9650655021834061\n",
      "loss: 0.18168608844280243, accuracy: 0.9432314410480349\n",
      "loss: 0.18606293201446533, accuracy: 0.9519650655021834\n",
      "loss: 0.22192628681659698, accuracy: 0.9213973799126638\n",
      "loss: 0.20122650265693665, accuracy: 0.9344978165938864\n",
      "loss: 0.17992985248565674, accuracy: 0.9344978165938864\n",
      "loss: 0.17960768938064575, accuracy: 0.9519650655021834\n",
      "loss: 0.1853400021791458, accuracy: 0.9432314410480349\n",
      "loss: 0.17376740276813507, accuracy: 0.9563318777292577\n",
      "loss: 0.18402399122714996, accuracy: 0.9475982532751092\n",
      "loss: 0.1710900366306305, accuracy: 0.9475982532751092\n",
      "loss: 0.17734353244304657, accuracy: 0.9563318777292577\n",
      "loss: 0.18908873200416565, accuracy: 0.9388646288209607\n",
      "loss: 0.17959627509117126, accuracy: 0.9563318777292577\n",
      "loss: 0.1893729269504547, accuracy: 0.9301310043668122\n",
      "loss: 0.1847025454044342, accuracy: 0.9432314410480349\n",
      "loss: 0.19302408397197723, accuracy: 0.9344978165938864\n",
      "loss: 0.20334923267364502, accuracy: 0.9213973799126638\n",
      "loss: 0.18104742467403412, accuracy: 0.9388646288209607\n",
      "loss: 0.18978483974933624, accuracy: 0.9344978165938864\n",
      "loss: 0.1905101090669632, accuracy: 0.9344978165938864\n",
      "loss: 0.21469005942344666, accuracy: 0.9039301310043668\n",
      "loss: 0.19354850053787231, accuracy: 0.9344978165938864\n",
      "loss: 0.1885823756456375, accuracy: 0.9388646288209607\n",
      "loss: 0.17620904743671417, accuracy: 0.9432314410480349\n",
      "loss: 0.19573405385017395, accuracy: 0.9344978165938864\n",
      "loss: 0.19161196053028107, accuracy: 0.925764192139738\n",
      "loss: 0.17891335487365723, accuracy: 0.9519650655021834\n",
      "loss: 0.17869174480438232, accuracy: 0.9432314410480349\n",
      "loss: 0.18199960887432098, accuracy: 0.9388646288209607\n",
      "loss: 0.1820894479751587, accuracy: 0.9563318777292577\n",
      "loss: 0.16992205381393433, accuracy: 0.9388646288209607\n",
      "loss: 0.17117427289485931, accuracy: 0.9344978165938864\n",
      "loss: 0.18405281007289886, accuracy: 0.9432314410480349\n",
      "loss: 0.19980138540267944, accuracy: 0.9388646288209607\n",
      "loss: 0.17911161482334137, accuracy: 0.9519650655021834\n",
      "loss: 0.17998620867729187, accuracy: 0.9344978165938864\n",
      "loss: 0.1679031401872635, accuracy: 0.9475982532751092\n",
      "loss: 0.19924400746822357, accuracy: 0.925764192139738\n",
      "loss: 0.16825321316719055, accuracy: 0.9650655021834061\n",
      "loss: 0.19305013120174408, accuracy: 0.9388646288209607\n",
      "loss: 0.16999134421348572, accuracy: 0.9606986899563319\n",
      "loss: 0.1846606433391571, accuracy: 0.9344978165938864\n",
      "loss: 0.2161683887243271, accuracy: 0.925764192139738\n",
      "loss: 0.1763935387134552, accuracy: 0.9475982532751092\n",
      "loss: 0.17836114764213562, accuracy: 0.9563318777292577\n",
      "loss: 0.2047475427389145, accuracy: 0.9301310043668122\n",
      "loss: 0.21284370124340057, accuracy: 0.9170305676855895\n",
      "loss: 0.22451823949813843, accuracy: 0.9126637554585153\n",
      "loss: 0.21016423404216766, accuracy: 0.9301310043668122\n",
      "loss: 0.19106870889663696, accuracy: 0.925764192139738\n",
      "loss: 0.1856105774641037, accuracy: 0.9344978165938864\n",
      "loss: 0.16821739077568054, accuracy: 0.9563318777292577\n",
      "loss: 0.1747230589389801, accuracy: 0.9563318777292577\n",
      "loss: 0.1732293665409088, accuracy: 0.9432314410480349\n",
      "loss: 0.16973643004894257, accuracy: 0.9606986899563319\n",
      "loss: 0.17152424156665802, accuracy: 0.9606986899563319\n",
      "loss: 0.16611260175704956, accuracy: 0.9563318777292577\n",
      "loss: 0.17053832113742828, accuracy: 0.9475982532751092\n",
      "loss: 0.1775040477514267, accuracy: 0.9519650655021834\n",
      "loss: 0.16894569993019104, accuracy: 0.9563318777292577\n",
      "loss: 0.19761767983436584, accuracy: 0.9432314410480349\n",
      "loss: 0.17657622694969177, accuracy: 0.9432314410480349\n",
      "loss: 0.1791074424982071, accuracy: 0.9519650655021834\n",
      "loss: 0.17237414419651031, accuracy: 0.9563318777292577\n",
      "loss: 0.1649610847234726, accuracy: 0.9650655021834061\n",
      "loss: 0.16547466814517975, accuracy: 0.9563318777292577\n",
      "loss: 0.1822730004787445, accuracy: 0.9388646288209607\n",
      "loss: 0.16724532842636108, accuracy: 0.9606986899563319\n",
      "loss: 0.17172950506210327, accuracy: 0.9519650655021834\n",
      "loss: 0.16829483211040497, accuracy: 0.9432314410480349\n",
      "loss: 0.15781046450138092, accuracy: 0.9650655021834061\n",
      "loss: 0.16846156120300293, accuracy: 0.9519650655021834\n",
      "loss: 0.1771828532218933, accuracy: 0.9475982532751092\n",
      "loss: 0.25257790088653564, accuracy: 0.8864628820960698\n",
      "loss: 0.17672622203826904, accuracy: 0.9301310043668122\n",
      "loss: 0.169662207365036, accuracy: 0.9563318777292577\n",
      "loss: 0.18738344311714172, accuracy: 0.9432314410480349\n",
      "loss: 0.1635776311159134, accuracy: 0.9606986899563319\n",
      "loss: 0.1900920420885086, accuracy: 0.9432314410480349\n",
      "loss: 0.19673889875411987, accuracy: 0.9301310043668122\n",
      "max accuracy: 0.9737991266375546\n",
      "loss: 0.16011320054531097, accuracy: 0.9737991266375546\n",
      "loss: 0.17924636602401733, accuracy: 0.9388646288209607\n",
      "loss: 0.24463868141174316, accuracy: 0.8908296943231441\n",
      "loss: 0.18177169561386108, accuracy: 0.9388646288209607\n",
      "loss: 0.16341473162174225, accuracy: 0.9519650655021834\n",
      "loss: 0.1606166809797287, accuracy: 0.9432314410480349\n",
      "loss: 0.16304615139961243, accuracy: 0.9519650655021834\n",
      "loss: 0.17712867259979248, accuracy: 0.9432314410480349\n",
      "loss: 0.18100248277187347, accuracy: 0.9475982532751092\n",
      "loss: 0.19129642844200134, accuracy: 0.925764192139738\n",
      "loss: 0.1563243418931961, accuracy: 0.9650655021834061\n",
      "loss: 0.1939898282289505, accuracy: 0.9301310043668122\n",
      "loss: 0.1667313128709793, accuracy: 0.9563318777292577\n",
      "loss: 0.18203072249889374, accuracy: 0.9388646288209607\n",
      "loss: 0.19398833811283112, accuracy: 0.9344978165938864\n",
      "loss: 0.18055669963359833, accuracy: 0.9475982532751092\n",
      "loss: 0.17223334312438965, accuracy: 0.9388646288209607\n",
      "loss: 0.1955067366361618, accuracy: 0.9213973799126638\n",
      "loss: 0.19114144146442413, accuracy: 0.9388646288209607\n",
      "loss: 0.16579383611679077, accuracy: 0.9563318777292577\n",
      "loss: 0.15821929275989532, accuracy: 0.9563318777292577\n",
      "loss: 0.1569196730852127, accuracy: 0.9650655021834061\n",
      "loss: 0.16277873516082764, accuracy: 0.9563318777292577\n",
      "loss: 0.17012037336826324, accuracy: 0.9475982532751092\n",
      "loss: 0.17339643836021423, accuracy: 0.9475982532751092\n",
      "loss: 0.17216305434703827, accuracy: 0.9563318777292577\n",
      "loss: 0.15666702389717102, accuracy: 0.9650655021834061\n",
      "loss: 0.15519477427005768, accuracy: 0.9606986899563319\n",
      "loss: 0.17364351451396942, accuracy: 0.9475982532751092\n",
      "loss: 0.16885240375995636, accuracy: 0.9519650655021834\n",
      "loss: 0.15788975358009338, accuracy: 0.9606986899563319\n",
      "loss: 0.16276434063911438, accuracy: 0.9650655021834061\n",
      "loss: 0.17193366587162018, accuracy: 0.9432314410480349\n",
      "loss: 0.16934700310230255, accuracy: 0.9519650655021834\n",
      "loss: 0.15687955915927887, accuracy: 0.9606986899563319\n",
      "loss: 0.1594761461019516, accuracy: 0.9606986899563319\n",
      "loss: 0.16577735543251038, accuracy: 0.9563318777292577\n",
      "loss: 0.1606978178024292, accuracy: 0.9519650655021834\n",
      "loss: 0.17947454750537872, accuracy: 0.9475982532751092\n",
      "loss: 0.16048619151115417, accuracy: 0.9563318777292577\n",
      "loss: 0.17555567622184753, accuracy: 0.9475982532751092\n",
      "loss: 0.15877048671245575, accuracy: 0.9475982532751092\n",
      "loss: 0.29269787669181824, accuracy: 0.8777292576419214\n",
      "loss: 0.19539348781108856, accuracy: 0.9475982532751092\n",
      "loss: 0.16396138072013855, accuracy: 0.9650655021834061\n",
      "loss: 0.1598713994026184, accuracy: 0.9650655021834061\n",
      "loss: 0.1630459427833557, accuracy: 0.9650655021834061\n",
      "loss: 0.1657353639602661, accuracy: 0.9563318777292577\n",
      "loss: 0.15758292376995087, accuracy: 0.9475982532751092\n",
      "loss: 0.15943144261837006, accuracy: 0.9650655021834061\n",
      "loss: 0.15458449721336365, accuracy: 0.9650655021834061\n",
      "loss: 0.16452986001968384, accuracy: 0.9563318777292577\n",
      "loss: 0.16057009994983673, accuracy: 0.9606986899563319\n",
      "loss: 0.1645948588848114, accuracy: 0.9519650655021834\n",
      "loss: 0.18079587817192078, accuracy: 0.9301310043668122\n",
      "loss: 0.19501066207885742, accuracy: 0.9344978165938864\n",
      "loss: 0.15612944960594177, accuracy: 0.9606986899563319\n",
      "loss: 0.20914910733699799, accuracy: 0.9344978165938864\n",
      "loss: 0.17257775366306305, accuracy: 0.9388646288209607\n",
      "loss: 0.1645107865333557, accuracy: 0.9432314410480349\n",
      "loss: 0.16288478672504425, accuracy: 0.9563318777292577\n",
      "loss: 0.16771015524864197, accuracy: 0.9519650655021834\n",
      "loss: 0.1755416840314865, accuracy: 0.9563318777292577\n",
      "loss: 0.20649710297584534, accuracy: 0.9301310043668122\n",
      "loss: 0.17446216940879822, accuracy: 0.9475982532751092\n",
      "loss: 0.151272714138031, accuracy: 0.9650655021834061\n",
      "loss: 0.15675364434719086, accuracy: 0.9606986899563319\n",
      "loss: 0.1547633707523346, accuracy: 0.9563318777292577\n",
      "loss: 0.15172989666461945, accuracy: 0.9519650655021834\n",
      "loss: 0.19233369827270508, accuracy: 0.9213973799126638\n",
      "loss: 0.16144701838493347, accuracy: 0.9475982532751092\n",
      "loss: 0.16831451654434204, accuracy: 0.9519650655021834\n",
      "loss: 0.1917007714509964, accuracy: 0.9213973799126638\n",
      "loss: 0.16756798326969147, accuracy: 0.9388646288209607\n",
      "loss: 0.15776073932647705, accuracy: 0.9606986899563319\n",
      "loss: 0.14963500201702118, accuracy: 0.9606986899563319\n",
      "loss: 0.15445968508720398, accuracy: 0.9606986899563319\n",
      "loss: 0.17497782409191132, accuracy: 0.9519650655021834\n",
      "loss: 0.1637447327375412, accuracy: 0.9563318777292577\n",
      "loss: 0.15418259799480438, accuracy: 0.9563318777292577\n",
      "loss: 0.15109789371490479, accuracy: 0.9606986899563319\n",
      "loss: 0.1519327312707901, accuracy: 0.9606986899563319\n",
      "loss: 0.14410187304019928, accuracy: 0.9650655021834061\n",
      "loss: 0.16632285714149475, accuracy: 0.9344978165938864\n",
      "loss: 0.15697214007377625, accuracy: 0.9606986899563319\n",
      "loss: 0.16707780957221985, accuracy: 0.9475982532751092\n",
      "loss: 0.15442556142807007, accuracy: 0.9475982532751092\n",
      "loss: 0.1572079211473465, accuracy: 0.9606986899563319\n",
      "loss: 0.16605500876903534, accuracy: 0.9606986899563319\n",
      "loss: 0.17105531692504883, accuracy: 0.9519650655021834\n",
      "loss: 0.17123836278915405, accuracy: 0.9388646288209607\n",
      "loss: 0.1519860476255417, accuracy: 0.9519650655021834\n",
      "loss: 0.17148549854755402, accuracy: 0.9563318777292577\n",
      "loss: 0.17153917253017426, accuracy: 0.9519650655021834\n",
      "loss: 0.16118526458740234, accuracy: 0.9475982532751092\n",
      "loss: 0.19346646964550018, accuracy: 0.9475982532751092\n",
      "loss: 0.15887899696826935, accuracy: 0.9563318777292577\n",
      "loss: 0.1850578635931015, accuracy: 0.9475982532751092\n",
      "loss: 0.15707163512706757, accuracy: 0.9519650655021834\n",
      "loss: 0.14894481003284454, accuracy: 0.9694323144104804\n",
      "loss: 0.1698245406150818, accuracy: 0.9519650655021834\n",
      "loss: 0.18020731210708618, accuracy: 0.9388646288209607\n",
      "loss: 0.16087055206298828, accuracy: 0.9475982532751092\n",
      "loss: 0.1453612893819809, accuracy: 0.9694323144104804\n",
      "loss: 0.17075131833553314, accuracy: 0.9475982532751092\n",
      "loss: 0.16170217096805573, accuracy: 0.9606986899563319\n",
      "loss: 0.15492728352546692, accuracy: 0.9606986899563319\n",
      "loss: 0.14825057983398438, accuracy: 0.9694323144104804\n",
      "loss: 0.15020650625228882, accuracy: 0.9650655021834061\n",
      "loss: 0.15047666430473328, accuracy: 0.9519650655021834\n",
      "loss: 0.15315046906471252, accuracy: 0.9519650655021834\n",
      "loss: 0.20485083758831024, accuracy: 0.925764192139738\n",
      "loss: 0.16686974465847015, accuracy: 0.9432314410480349\n",
      "loss: 0.16189654171466827, accuracy: 0.9388646288209607\n",
      "loss: 0.1485762596130371, accuracy: 0.9650655021834061\n",
      "loss: 0.17843638360500336, accuracy: 0.9301310043668122\n",
      "loss: 0.1431007832288742, accuracy: 0.9694323144104804\n",
      "loss: 0.15786200761795044, accuracy: 0.9606986899563319\n",
      "loss: 0.18078702688217163, accuracy: 0.9475982532751092\n",
      "loss: 0.146158829331398, accuracy: 0.9650655021834061\n",
      "loss: 0.15111780166625977, accuracy: 0.9650655021834061\n",
      "loss: 0.19388651847839355, accuracy: 0.925764192139738\n",
      "loss: 0.1622917801141739, accuracy: 0.9563318777292577\n",
      "loss: 0.14329609274864197, accuracy: 0.9650655021834061\n",
      "loss: 0.15817943215370178, accuracy: 0.9563318777292577\n",
      "loss: 0.1504899114370346, accuracy: 0.9694323144104804\n",
      "loss: 0.1532839834690094, accuracy: 0.9606986899563319\n",
      "loss: 0.14682835340499878, accuracy: 0.9563318777292577\n",
      "loss: 0.15521679818630219, accuracy: 0.9519650655021834\n",
      "loss: 0.16380417346954346, accuracy: 0.9563318777292577\n",
      "loss: 0.14575453102588654, accuracy: 0.9694323144104804\n",
      "loss: 0.1752738356590271, accuracy: 0.9301310043668122\n",
      "loss: 0.15281005203723907, accuracy: 0.9563318777292577\n",
      "loss: 0.20409023761749268, accuracy: 0.9432314410480349\n",
      "loss: 0.15181593596935272, accuracy: 0.9563318777292577\n",
      "loss: 0.13946722447872162, accuracy: 0.9606986899563319\n",
      "loss: 0.17427702248096466, accuracy: 0.9432314410480349\n",
      "loss: 0.14572261273860931, accuracy: 0.9650655021834061\n",
      "loss: 0.1509418785572052, accuracy: 0.9606986899563319\n",
      "loss: 0.15603813529014587, accuracy: 0.9519650655021834\n",
      "loss: 0.15374279022216797, accuracy: 0.9519650655021834\n",
      "loss: 0.14179623126983643, accuracy: 0.9694323144104804\n",
      "loss: 0.13981927931308746, accuracy: 0.9650655021834061\n",
      "loss: 0.14497028291225433, accuracy: 0.9606986899563319\n",
      "loss: 0.17890122532844543, accuracy: 0.9388646288209607\n",
      "loss: 0.147070050239563, accuracy: 0.9650655021834061\n",
      "loss: 0.1455831080675125, accuracy: 0.9737991266375546\n",
      "loss: 0.1958632469177246, accuracy: 0.925764192139738\n",
      "loss: 0.15904030203819275, accuracy: 0.9650655021834061\n",
      "loss: 0.1702534407377243, accuracy: 0.9388646288209607\n",
      "loss: 0.15024535357952118, accuracy: 0.9650655021834061\n",
      "loss: 0.16512128710746765, accuracy: 0.9563318777292577\n",
      "loss: 0.13916799426078796, accuracy: 0.9606986899563319\n",
      "loss: 0.1400323361158371, accuracy: 0.9606986899563319\n",
      "loss: 0.13889315724372864, accuracy: 0.9606986899563319\n",
      "loss: 0.1502416580915451, accuracy: 0.9606986899563319\n",
      "loss: 0.15992312133312225, accuracy: 0.9563318777292577\n",
      "loss: 0.1461731493473053, accuracy: 0.9475982532751092\n",
      "loss: 0.14231200516223907, accuracy: 0.9694323144104804\n",
      "loss: 0.1402079313993454, accuracy: 0.9694323144104804\n",
      "loss: 0.15459810197353363, accuracy: 0.9388646288209607\n",
      "loss: 0.15057754516601562, accuracy: 0.9694323144104804\n",
      "loss: 0.1463756412267685, accuracy: 0.9563318777292577\n",
      "loss: 0.15944139659404755, accuracy: 0.9563318777292577\n",
      "loss: 0.16071555018424988, accuracy: 0.9563318777292577\n",
      "loss: 0.2939715087413788, accuracy: 0.8820960698689956\n",
      "loss: 0.26545214653015137, accuracy: 0.9039301310043668\n",
      "loss: 0.18581826984882355, accuracy: 0.925764192139738\n",
      "loss: 0.1553460955619812, accuracy: 0.9650655021834061\n",
      "loss: 0.15660691261291504, accuracy: 0.9475982532751092\n",
      "loss: 0.14373500645160675, accuracy: 0.9650655021834061\n",
      "loss: 0.14887459576129913, accuracy: 0.9563318777292577\n",
      "loss: 0.14975278079509735, accuracy: 0.9519650655021834\n",
      "loss: 0.1565118432044983, accuracy: 0.9563318777292577\n",
      "loss: 0.1370587944984436, accuracy: 0.9694323144104804\n",
      "loss: 0.14437367022037506, accuracy: 0.9650655021834061\n",
      "loss: 0.15906795859336853, accuracy: 0.9563318777292577\n",
      "loss: 0.16949400305747986, accuracy: 0.9344978165938864\n",
      "loss: 0.1382664144039154, accuracy: 0.9650655021834061\n",
      "loss: 0.13516534864902496, accuracy: 0.9737991266375546\n",
      "loss: 0.14490090310573578, accuracy: 0.9694323144104804\n",
      "loss: 0.14803627133369446, accuracy: 0.9606986899563319\n",
      "loss: 0.2638789713382721, accuracy: 0.8864628820960698\n",
      "loss: 0.15332509577274323, accuracy: 0.9737991266375546\n",
      "loss: 0.14257417619228363, accuracy: 0.9694323144104804\n",
      "loss: 0.14143241941928864, accuracy: 0.9606986899563319\n",
      "loss: 0.14433585107326508, accuracy: 0.9694323144104804\n",
      "loss: 0.13753873109817505, accuracy: 0.9650655021834061\n",
      "loss: 0.14996831119060516, accuracy: 0.9475982532751092\n",
      "loss: 0.13661277294158936, accuracy: 0.9694323144104804\n",
      "loss: 0.1706661581993103, accuracy: 0.9388646288209607\n",
      "loss: 0.15357984602451324, accuracy: 0.9606986899563319\n",
      "loss: 0.13942599296569824, accuracy: 0.9650655021834061\n",
      "loss: 0.16136592626571655, accuracy: 0.9475982532751092\n",
      "loss: 0.13576100766658783, accuracy: 0.9650655021834061\n",
      "loss: 0.14410121738910675, accuracy: 0.9694323144104804\n",
      "loss: 0.13694943487644196, accuracy: 0.9606986899563319\n",
      "loss: 0.1448729932308197, accuracy: 0.9694323144104804\n",
      "loss: 0.13718652725219727, accuracy: 0.9694323144104804\n",
      "loss: 0.14909577369689941, accuracy: 0.9563318777292577\n",
      "loss: 0.19183756411075592, accuracy: 0.9213973799126638\n",
      "loss: 0.14953163266181946, accuracy: 0.9475982532751092\n",
      "loss: 0.1344420462846756, accuracy: 0.9563318777292577\n",
      "loss: 0.14018814265727997, accuracy: 0.9650655021834061\n",
      "loss: 0.15392059087753296, accuracy: 0.9563318777292577\n",
      "loss: 0.13494542241096497, accuracy: 0.9606986899563319\n",
      "loss: 0.14857535064220428, accuracy: 0.9563318777292577\n",
      "loss: 0.16151928901672363, accuracy: 0.9388646288209607\n",
      "loss: 0.14173267781734467, accuracy: 0.9519650655021834\n",
      "loss: 0.14096733927726746, accuracy: 0.9694323144104804\n",
      "loss: 0.1400223970413208, accuracy: 0.9694323144104804\n",
      "loss: 0.14086662232875824, accuracy: 0.9694323144104804\n",
      "loss: 0.15006010234355927, accuracy: 0.9519650655021834\n",
      "loss: 0.15151990950107574, accuracy: 0.9606986899563319\n",
      "loss: 0.14212282001972198, accuracy: 0.9650655021834061\n",
      "loss: 0.15534549951553345, accuracy: 0.9606986899563319\n",
      "loss: 0.15306489169597626, accuracy: 0.9563318777292577\n",
      "loss: 0.1457270085811615, accuracy: 0.9432314410480349\n",
      "loss: 0.16083543002605438, accuracy: 0.9563318777292577\n",
      "loss: 0.13698545098304749, accuracy: 0.9694323144104804\n",
      "loss: 0.1292589157819748, accuracy: 0.9737991266375546\n",
      "loss: 0.14509856700897217, accuracy: 0.9563318777292577\n",
      "loss: 0.14677752554416656, accuracy: 0.9650655021834061\n",
      "loss: 0.14616507291793823, accuracy: 0.9475982532751092\n",
      "loss: 0.17574207484722137, accuracy: 0.9475982532751092\n",
      "loss: 0.14792397618293762, accuracy: 0.9606986899563319\n",
      "loss: 0.13323818147182465, accuracy: 0.9650655021834061\n",
      "loss: 0.1332491934299469, accuracy: 0.9650655021834061\n",
      "loss: 0.13402637839317322, accuracy: 0.9650655021834061\n",
      "loss: 0.1453230082988739, accuracy: 0.9563318777292577\n",
      "loss: 0.16985419392585754, accuracy: 0.9519650655021834\n",
      "loss: 0.15030749142169952, accuracy: 0.9563318777292577\n",
      "loss: 0.13503779470920563, accuracy: 0.9650655021834061\n",
      "loss: 0.14496538043022156, accuracy: 0.9606986899563319\n",
      "loss: 0.1305898129940033, accuracy: 0.9606986899563319\n",
      "loss: 0.13485097885131836, accuracy: 0.9519650655021834\n",
      "loss: 0.12854625284671783, accuracy: 0.9737991266375546\n",
      "loss: 0.12633709609508514, accuracy: 0.9737991266375546\n",
      "loss: 0.13532839715480804, accuracy: 0.9650655021834061\n",
      "loss: 0.12966392934322357, accuracy: 0.9694323144104804\n",
      "loss: 0.14603519439697266, accuracy: 0.9563318777292577\n",
      "loss: 0.12806305289268494, accuracy: 0.9650655021834061\n",
      "loss: 0.1373906433582306, accuracy: 0.9563318777292577\n",
      "loss: 0.14188005030155182, accuracy: 0.9519650655021834\n",
      "loss: 0.13811464607715607, accuracy: 0.9694323144104804\n",
      "loss: 0.13215957581996918, accuracy: 0.9606986899563319\n",
      "loss: 0.13854338228702545, accuracy: 0.9606986899563319\n",
      "loss: 0.1286582350730896, accuracy: 0.9694323144104804\n",
      "loss: 0.13681171834468842, accuracy: 0.9606986899563319\n",
      "loss: 0.13192681968212128, accuracy: 0.9694323144104804\n",
      "loss: 0.13004045188426971, accuracy: 0.9737991266375546\n",
      "loss: 0.13337741792201996, accuracy: 0.9650655021834061\n",
      "loss: 0.13666507601737976, accuracy: 0.9606986899563319\n",
      "loss: 0.1450847089290619, accuracy: 0.9519650655021834\n",
      "loss: 0.13869325816631317, accuracy: 0.9650655021834061\n",
      "loss: 0.15828141570091248, accuracy: 0.9650655021834061\n",
      "loss: 0.13347601890563965, accuracy: 0.9694323144104804\n",
      "loss: 0.12954656779766083, accuracy: 0.9694323144104804\n",
      "loss: 0.1251344531774521, accuracy: 0.9650655021834061\n",
      "loss: 0.13541854918003082, accuracy: 0.9563318777292577\n",
      "loss: 0.14583879709243774, accuracy: 0.9650655021834061\n",
      "loss: 0.1440267562866211, accuracy: 0.9650655021834061\n",
      "loss: 0.13301046192646027, accuracy: 0.9650655021834061\n",
      "loss: 0.13424734771251678, accuracy: 0.9606986899563319\n",
      "loss: 0.16536439955234528, accuracy: 0.9213973799126638\n",
      "loss: 0.1782217174768448, accuracy: 0.9388646288209607\n",
      "loss: 0.1411009281873703, accuracy: 0.9563318777292577\n",
      "loss: 0.12296696752309799, accuracy: 0.9694323144104804\n",
      "loss: 0.19584357738494873, accuracy: 0.9170305676855895\n",
      "loss: 0.16838619112968445, accuracy: 0.9475982532751092\n",
      "loss: 0.13016225397586823, accuracy: 0.9650655021834061\n",
      "loss: 0.1378825455904007, accuracy: 0.9563318777292577\n",
      "loss: 0.16042441129684448, accuracy: 0.9388646288209607\n",
      "loss: 0.14940719306468964, accuracy: 0.9432314410480349\n",
      "loss: 0.14997683465480804, accuracy: 0.9519650655021834\n",
      "loss: 0.1292181760072708, accuracy: 0.9694323144104804\n",
      "loss: 0.13619089126586914, accuracy: 0.9694323144104804\n",
      "loss: 0.13407649099826813, accuracy: 0.9650655021834061\n",
      "loss: 0.20892813801765442, accuracy: 0.9301310043668122\n",
      "loss: 0.14376519620418549, accuracy: 0.9519650655021834\n",
      "loss: 0.13646581768989563, accuracy: 0.9606986899563319\n",
      "loss: 0.1329827606678009, accuracy: 0.9737991266375546\n",
      "loss: 0.12832023203372955, accuracy: 0.9650655021834061\n",
      "loss: 0.14304374158382416, accuracy: 0.9606986899563319\n",
      "loss: 0.1361054927110672, accuracy: 0.9606986899563319\n",
      "loss: 0.12280310690402985, accuracy: 0.9737991266375546\n",
      "loss: 0.12880109250545502, accuracy: 0.9694323144104804\n",
      "loss: 0.12885072827339172, accuracy: 0.9650655021834061\n",
      "loss: 0.137911856174469, accuracy: 0.9737991266375546\n",
      "loss: 0.13001659512519836, accuracy: 0.9694323144104804\n",
      "loss: 0.1259845644235611, accuracy: 0.9650655021834061\n",
      "loss: 0.12131673842668533, accuracy: 0.9737991266375546\n",
      "loss: 0.12842334806919098, accuracy: 0.9606986899563319\n",
      "loss: 0.12910334765911102, accuracy: 0.9650655021834061\n",
      "loss: 0.1228686273097992, accuracy: 0.9650655021834061\n",
      "loss: 0.12618884444236755, accuracy: 0.9694323144104804\n",
      "loss: 0.13103587925434113, accuracy: 0.9606986899563319\n",
      "loss: 0.12465926259756088, accuracy: 0.9606986899563319\n",
      "loss: 0.13619589805603027, accuracy: 0.9606986899563319\n",
      "loss: 0.1346655637025833, accuracy: 0.9650655021834061\n",
      "loss: 0.13370461761951447, accuracy: 0.9606986899563319\n",
      "loss: 0.12145711481571198, accuracy: 0.9694323144104804\n",
      "loss: 0.14582359790802002, accuracy: 0.9563318777292577\n",
      "loss: 0.13416673243045807, accuracy: 0.9694323144104804\n",
      "loss: 0.13341212272644043, accuracy: 0.9606986899563319\n",
      "loss: 0.13352490961551666, accuracy: 0.9563318777292577\n",
      "loss: 0.140372633934021, accuracy: 0.9563318777292577\n",
      "loss: 0.1320582777261734, accuracy: 0.9606986899563319\n",
      "loss: 0.13311085104942322, accuracy: 0.9606986899563319\n",
      "loss: 0.1267017126083374, accuracy: 0.9737991266375546\n",
      "loss: 0.12365689128637314, accuracy: 0.9694323144104804\n",
      "loss: 0.12793545424938202, accuracy: 0.9650655021834061\n",
      "loss: 0.1291068196296692, accuracy: 0.9606986899563319\n",
      "loss: 0.12428271025419235, accuracy: 0.9694323144104804\n",
      "loss: 0.12756739556789398, accuracy: 0.9650655021834061\n",
      "loss: 0.134839728474617, accuracy: 0.9694323144104804\n",
      "loss: 0.1263720691204071, accuracy: 0.9694323144104804\n",
      "loss: 0.1310524195432663, accuracy: 0.9694323144104804\n",
      "loss: 0.12752357125282288, accuracy: 0.9694323144104804\n",
      "loss: 0.1304556131362915, accuracy: 0.9606986899563319\n",
      "loss: 0.18493223190307617, accuracy: 0.9388646288209607\n",
      "max accuracy: 0.9781659388646288\n",
      "loss: 0.12384224683046341, accuracy: 0.9781659388646288\n",
      "loss: 0.12754477560520172, accuracy: 0.9781659388646288\n",
      "loss: 0.12474095821380615, accuracy: 0.9737991266375546\n",
      "loss: 0.12314289063215256, accuracy: 0.9606986899563319\n",
      "loss: 0.15454304218292236, accuracy: 0.9344978165938864\n",
      "loss: 0.12384151667356491, accuracy: 0.9737991266375546\n",
      "loss: 0.12708063423633575, accuracy: 0.9606986899563319\n",
      "loss: 0.19300931692123413, accuracy: 0.9126637554585153\n",
      "loss: 0.1781725138425827, accuracy: 0.9475982532751092\n",
      "loss: 0.1486508846282959, accuracy: 0.9475982532751092\n",
      "loss: 0.1335509568452835, accuracy: 0.9694323144104804\n",
      "loss: 0.11583589017391205, accuracy: 0.9694323144104804\n",
      "loss: 0.13707883656024933, accuracy: 0.9563318777292577\n",
      "loss: 0.12268096208572388, accuracy: 0.9694323144104804\n",
      "loss: 0.12833017110824585, accuracy: 0.9737991266375546\n",
      "loss: 0.13181257247924805, accuracy: 0.9650655021834061\n",
      "loss: 0.13352972269058228, accuracy: 0.9606986899563319\n",
      "loss: 0.12180745601654053, accuracy: 0.9737991266375546\n",
      "loss: 0.12190325558185577, accuracy: 0.9737991266375546\n",
      "loss: 0.13076373934745789, accuracy: 0.9737991266375546\n",
      "loss: 0.12363982200622559, accuracy: 0.9694323144104804\n",
      "loss: 0.13830704987049103, accuracy: 0.9650655021834061\n",
      "loss: 0.13541923463344574, accuracy: 0.9650655021834061\n",
      "loss: 0.12501458823680878, accuracy: 0.9694323144104804\n",
      "loss: 0.11635938286781311, accuracy: 0.9650655021834061\n",
      "loss: 0.13221290707588196, accuracy: 0.9563318777292577\n",
      "loss: 0.12592418491840363, accuracy: 0.9606986899563319\n",
      "loss: 0.12255024909973145, accuracy: 0.9650655021834061\n",
      "loss: 0.11432638019323349, accuracy: 0.9737991266375546\n",
      "loss: 0.12368566542863846, accuracy: 0.9694323144104804\n",
      "loss: 0.12395916879177094, accuracy: 0.9650655021834061\n",
      "loss: 0.16506482660770416, accuracy: 0.9563318777292577\n",
      "loss: 0.14540210366249084, accuracy: 0.9432314410480349\n",
      "loss: 0.13046754896640778, accuracy: 0.9563318777292577\n",
      "loss: 0.12691764533519745, accuracy: 0.9650655021834061\n",
      "loss: 0.12204821407794952, accuracy: 0.9694323144104804\n",
      "loss: 0.1174255833029747, accuracy: 0.9606986899563319\n",
      "loss: 0.13163359463214874, accuracy: 0.9563318777292577\n",
      "loss: 0.12100324779748917, accuracy: 0.9694323144104804\n",
      "loss: 0.13041247427463531, accuracy: 0.9606986899563319\n",
      "loss: 0.12307918816804886, accuracy: 0.9694323144104804\n",
      "loss: 0.12888668477535248, accuracy: 0.9563318777292577\n",
      "loss: 0.11658252775669098, accuracy: 0.9737991266375546\n",
      "loss: 0.11779659241437912, accuracy: 0.9650655021834061\n",
      "loss: 0.12560489773750305, accuracy: 0.9694323144104804\n",
      "loss: 0.11491575092077255, accuracy: 0.9737991266375546\n",
      "loss: 0.1242414191365242, accuracy: 0.9606986899563319\n",
      "loss: 0.11967694759368896, accuracy: 0.9694323144104804\n",
      "loss: 0.11635315418243408, accuracy: 0.9694323144104804\n",
      "loss: 0.11246000230312347, accuracy: 0.9781659388646288\n",
      "loss: 0.133029043674469, accuracy: 0.9606986899563319\n",
      "loss: 0.1102943867444992, accuracy: 0.9781659388646288\n",
      "loss: 0.10934874415397644, accuracy: 0.9737991266375546\n",
      "loss: 0.11418578773736954, accuracy: 0.9737991266375546\n",
      "loss: 0.1414249688386917, accuracy: 0.9606986899563319\n",
      "loss: 0.13596639037132263, accuracy: 0.9694323144104804\n",
      "loss: 0.1108967587351799, accuracy: 0.9737991266375546\n",
      "loss: 0.13129165768623352, accuracy: 0.9650655021834061\n",
      "loss: 0.12249782681465149, accuracy: 0.9650655021834061\n",
      "loss: 0.1323755383491516, accuracy: 0.9606986899563319\n",
      "loss: 0.12079566717147827, accuracy: 0.9737991266375546\n",
      "loss: 0.14159761369228363, accuracy: 0.9563318777292577\n",
      "loss: 0.13663756847381592, accuracy: 0.9563318777292577\n",
      "loss: 0.11998522281646729, accuracy: 0.9694323144104804\n",
      "loss: 0.1291230320930481, accuracy: 0.9650655021834061\n",
      "loss: 0.1146334782242775, accuracy: 0.9694323144104804\n",
      "loss: 0.1211341992020607, accuracy: 0.9563318777292577\n",
      "loss: 0.1183512732386589, accuracy: 0.9694323144104804\n",
      "loss: 0.13516977429389954, accuracy: 0.9650655021834061\n",
      "loss: 0.12447181344032288, accuracy: 0.9606986899563319\n",
      "loss: 0.14669069647789001, accuracy: 0.9475982532751092\n",
      "loss: 0.1184510886669159, accuracy: 0.9694323144104804\n",
      "loss: 0.11643211543560028, accuracy: 0.9650655021834061\n",
      "loss: 0.12036780267953873, accuracy: 0.9563318777292577\n",
      "loss: 0.12195680290460587, accuracy: 0.9737991266375546\n",
      "loss: 0.11183860152959824, accuracy: 0.9737991266375546\n",
      "loss: 0.11928009986877441, accuracy: 0.9650655021834061\n",
      "loss: 0.1433725357055664, accuracy: 0.9563318777292577\n",
      "loss: 0.20122265815734863, accuracy: 0.9301310043668122\n",
      "loss: 0.2204257994890213, accuracy: 0.9301310043668122\n",
      "loss: 0.19954591989517212, accuracy: 0.9301310043668122\n",
      "loss: 0.12375403195619583, accuracy: 0.9650655021834061\n",
      "loss: 0.11922866106033325, accuracy: 0.9606986899563319\n",
      "loss: 0.11849551647901535, accuracy: 0.9781659388646288\n",
      "loss: 0.11577343195676804, accuracy: 0.9650655021834061\n",
      "loss: 0.12382801622152328, accuracy: 0.9737991266375546\n",
      "loss: 0.11858085542917252, accuracy: 0.9694323144104804\n",
      "loss: 0.11728576570749283, accuracy: 0.9737991266375546\n",
      "loss: 0.1267467737197876, accuracy: 0.9737991266375546\n",
      "loss: 0.1252574771642685, accuracy: 0.9650655021834061\n",
      "loss: 0.11511556804180145, accuracy: 0.9694323144104804\n",
      "loss: 0.18464139103889465, accuracy: 0.9344978165938864\n",
      "loss: 0.13752111792564392, accuracy: 0.9606986899563319\n",
      "loss: 0.1137385293841362, accuracy: 0.9694323144104804\n",
      "loss: 0.13489769399166107, accuracy: 0.9475982532751092\n",
      "loss: 0.12845692038536072, accuracy: 0.9606986899563319\n",
      "loss: 0.10760831832885742, accuracy: 0.9650655021834061\n",
      "loss: 0.1268148422241211, accuracy: 0.9694323144104804\n",
      "loss: 0.12665997445583344, accuracy: 0.9650655021834061\n",
      "loss: 0.11517763882875443, accuracy: 0.9737991266375546\n",
      "loss: 0.11789359897375107, accuracy: 0.9650655021834061\n",
      "loss: 0.10865629464387894, accuracy: 0.9650655021834061\n",
      "loss: 0.11321897059679031, accuracy: 0.9781659388646288\n",
      "loss: 0.12465769052505493, accuracy: 0.9606986899563319\n",
      "loss: 0.1214250773191452, accuracy: 0.9650655021834061\n",
      "loss: 0.12091398984193802, accuracy: 0.9694323144104804\n",
      "loss: 0.1245843842625618, accuracy: 0.9650655021834061\n",
      "loss: 0.1233162209391594, accuracy: 0.9606986899563319\n",
      "loss: 0.12557388842105865, accuracy: 0.9694323144104804\n",
      "loss: 0.1582353711128235, accuracy: 0.9344978165938864\n",
      "loss: 0.12153689563274384, accuracy: 0.9650655021834061\n",
      "loss: 0.12049802392721176, accuracy: 0.9650655021834061\n",
      "loss: 0.11857371777296066, accuracy: 0.9650655021834061\n",
      "loss: 0.11338577419519424, accuracy: 0.9650655021834061\n",
      "loss: 0.11659041047096252, accuracy: 0.9694323144104804\n",
      "loss: 0.11188201606273651, accuracy: 0.9694323144104804\n",
      "loss: 0.1120106503367424, accuracy: 0.9694323144104804\n",
      "loss: 0.10955046117305756, accuracy: 0.9694323144104804\n",
      "loss: 0.11060657352209091, accuracy: 0.9694323144104804\n",
      "loss: 0.1150459498167038, accuracy: 0.9694323144104804\n",
      "loss: 0.11823514103889465, accuracy: 0.9737991266375546\n",
      "loss: 0.1059374064207077, accuracy: 0.9781659388646288\n",
      "loss: 0.12110576778650284, accuracy: 0.9650655021834061\n",
      "loss: 0.12123176455497742, accuracy: 0.9694323144104804\n",
      "loss: 0.12256472557783127, accuracy: 0.9650655021834061\n",
      "loss: 0.11172622442245483, accuracy: 0.9694323144104804\n",
      "loss: 0.11218513548374176, accuracy: 0.9737991266375546\n",
      "loss: 0.14316144585609436, accuracy: 0.9519650655021834\n",
      "loss: 0.13148292899131775, accuracy: 0.9650655021834061\n",
      "loss: 0.11270295083522797, accuracy: 0.9650655021834061\n",
      "loss: 0.1133495420217514, accuracy: 0.9606986899563319\n",
      "loss: 0.10990354418754578, accuracy: 0.9606986899563319\n",
      "loss: 0.10652735084295273, accuracy: 0.9737991266375546\n",
      "loss: 0.11161734163761139, accuracy: 0.9694323144104804\n",
      "loss: 0.11151968687772751, accuracy: 0.9650655021834061\n",
      "loss: 0.11342976242303848, accuracy: 0.9606986899563319\n",
      "loss: 0.11851692199707031, accuracy: 0.9737991266375546\n",
      "loss: 0.11704342067241669, accuracy: 0.9650655021834061\n",
      "loss: 0.1140187606215477, accuracy: 0.9694323144104804\n",
      "loss: 0.11188367009162903, accuracy: 0.9737991266375546\n",
      "loss: 0.11126785725355148, accuracy: 0.9650655021834061\n",
      "loss: 0.1092616617679596, accuracy: 0.9694323144104804\n",
      "loss: 0.11793459951877594, accuracy: 0.9606986899563319\n",
      "loss: 0.11290597170591354, accuracy: 0.9694323144104804\n",
      "loss: 0.10346736758947372, accuracy: 0.9650655021834061\n",
      "loss: 0.11022977530956268, accuracy: 0.9650655021834061\n",
      "loss: 0.11995813995599747, accuracy: 0.9737991266375546\n",
      "loss: 0.11487164348363876, accuracy: 0.9737991266375546\n",
      "loss: 0.1152651309967041, accuracy: 0.9650655021834061\n",
      "loss: 0.10762050002813339, accuracy: 0.9737991266375546\n",
      "loss: 0.10926049947738647, accuracy: 0.9694323144104804\n",
      "loss: 0.15455162525177002, accuracy: 0.9519650655021834\n",
      "loss: 0.11429223418235779, accuracy: 0.9606986899563319\n",
      "loss: 0.10697194188833237, accuracy: 0.9650655021834061\n",
      "loss: 0.11817382276058197, accuracy: 0.9650655021834061\n",
      "loss: 0.1160229817032814, accuracy: 0.9650655021834061\n",
      "loss: 0.10903140902519226, accuracy: 0.9650655021834061\n",
      "loss: 0.11649687588214874, accuracy: 0.9694323144104804\n",
      "loss: 0.12114253640174866, accuracy: 0.9694323144104804\n",
      "loss: 0.10831326991319656, accuracy: 0.9694323144104804\n",
      "loss: 0.11363472044467926, accuracy: 0.9606986899563319\n",
      "loss: 0.11022823303937912, accuracy: 0.9650655021834061\n",
      "loss: 0.11072076112031937, accuracy: 0.9737991266375546\n",
      "loss: 0.11341512948274612, accuracy: 0.9694323144104804\n",
      "loss: 0.1027374267578125, accuracy: 0.9694323144104804\n",
      "loss: 0.10478010773658752, accuracy: 0.9694323144104804\n",
      "loss: 0.10701949894428253, accuracy: 0.9694323144104804\n",
      "loss: 0.1062074676156044, accuracy: 0.9694323144104804\n",
      "loss: 0.10639633983373642, accuracy: 0.9694323144104804\n",
      "loss: 0.1100408285856247, accuracy: 0.9694323144104804\n",
      "loss: 0.12573467195034027, accuracy: 0.9606986899563319\n",
      "loss: 0.11251457780599594, accuracy: 0.9694323144104804\n",
      "loss: 0.11039360612630844, accuracy: 0.9650655021834061\n",
      "loss: 0.10949800908565521, accuracy: 0.9650655021834061\n",
      "loss: 0.11959889531135559, accuracy: 0.9563318777292577\n",
      "loss: 0.11286085098981857, accuracy: 0.9650655021834061\n",
      "loss: 0.11033058166503906, accuracy: 0.9650655021834061\n",
      "loss: 0.11169838160276413, accuracy: 0.9606986899563319\n",
      "loss: 0.10786101967096329, accuracy: 0.9650655021834061\n",
      "loss: 0.10775796324014664, accuracy: 0.9737991266375546\n",
      "loss: 0.10996681451797485, accuracy: 0.9737991266375546\n",
      "loss: 0.1793346256017685, accuracy: 0.9388646288209607\n",
      "loss: 0.12963856756687164, accuracy: 0.9694323144104804\n",
      "loss: 0.11146729439496994, accuracy: 0.9694323144104804\n",
      "loss: 0.10763005912303925, accuracy: 0.9650655021834061\n",
      "loss: 0.1128493919968605, accuracy: 0.9650655021834061\n",
      "loss: 0.11158782243728638, accuracy: 0.9694323144104804\n",
      "loss: 0.1048271432518959, accuracy: 0.9737991266375546\n",
      "loss: 0.10765068978071213, accuracy: 0.9694323144104804\n",
      "loss: 0.10419776290655136, accuracy: 0.9781659388646288\n",
      "loss: 0.10153955966234207, accuracy: 0.9650655021834061\n",
      "loss: 0.11786055564880371, accuracy: 0.9563318777292577\n",
      "loss: 0.11040621995925903, accuracy: 0.9781659388646288\n",
      "loss: 0.11850280314683914, accuracy: 0.9650655021834061\n",
      "loss: 0.12039284408092499, accuracy: 0.9694323144104804\n",
      "loss: 0.11644160002470016, accuracy: 0.9737991266375546\n",
      "loss: 0.11035118997097015, accuracy: 0.9694323144104804\n",
      "loss: 0.10261069238185883, accuracy: 0.9737991266375546\n",
      "loss: 0.10358680784702301, accuracy: 0.9694323144104804\n",
      "loss: 0.11176453530788422, accuracy: 0.9606986899563319\n",
      "loss: 0.10212468355894089, accuracy: 0.9650655021834061\n",
      "loss: 0.10059073567390442, accuracy: 0.9781659388646288\n",
      "loss: 0.11379962414503098, accuracy: 0.9606986899563319\n",
      "loss: 0.10643979161977768, accuracy: 0.9694323144104804\n",
      "loss: 0.1191544458270073, accuracy: 0.9606986899563319\n",
      "loss: 0.11402100324630737, accuracy: 0.9563318777292577\n",
      "loss: 0.11714591830968857, accuracy: 0.9606986899563319\n",
      "loss: 0.10520494729280472, accuracy: 0.9563318777292577\n",
      "loss: 0.10300630331039429, accuracy: 0.9737991266375546\n",
      "loss: 0.10439535975456238, accuracy: 0.9650655021834061\n",
      "loss: 0.1038580909371376, accuracy: 0.9606986899563319\n",
      "loss: 0.11634701490402222, accuracy: 0.9650655021834061\n",
      "loss: 0.11226095259189606, accuracy: 0.9650655021834061\n",
      "loss: 0.10282662510871887, accuracy: 0.9650655021834061\n",
      "loss: 0.10242532193660736, accuracy: 0.9694323144104804\n",
      "loss: 0.10471829771995544, accuracy: 0.9694323144104804\n",
      "loss: 0.11981238424777985, accuracy: 0.9563318777292577\n",
      "loss: 0.10613726824522018, accuracy: 0.9694323144104804\n",
      "loss: 0.10611781477928162, accuracy: 0.9737991266375546\n",
      "loss: 0.12059570103883743, accuracy: 0.9563318777292577\n",
      "loss: 0.09744144231081009, accuracy: 0.9737991266375546\n",
      "loss: 0.1246853619813919, accuracy: 0.9694323144104804\n",
      "loss: 0.10778307169675827, accuracy: 0.9694323144104804\n",
      "loss: 0.10280946642160416, accuracy: 0.9650655021834061\n",
      "loss: 0.10924266278743744, accuracy: 0.9563318777292577\n",
      "loss: 0.09789836406707764, accuracy: 0.9737991266375546\n",
      "loss: 0.1105465516448021, accuracy: 0.9650655021834061\n",
      "loss: 0.0988912507891655, accuracy: 0.9737991266375546\n",
      "loss: 0.17989268898963928, accuracy: 0.9475982532751092\n",
      "loss: 0.12208495289087296, accuracy: 0.9563318777292577\n",
      "loss: 0.11195499449968338, accuracy: 0.9606986899563319\n",
      "loss: 0.11771117150783539, accuracy: 0.9650655021834061\n",
      "loss: 0.10345791280269623, accuracy: 0.9694323144104804\n",
      "loss: 0.1024472713470459, accuracy: 0.9737991266375546\n",
      "loss: 0.105906642973423, accuracy: 0.9737991266375546\n",
      "loss: 0.10175739973783493, accuracy: 0.9781659388646288\n",
      "loss: 0.1041371300816536, accuracy: 0.9737991266375546\n",
      "loss: 0.11505506932735443, accuracy: 0.9563318777292577\n",
      "loss: 0.10275089740753174, accuracy: 0.9737991266375546\n",
      "loss: 0.11207874119281769, accuracy: 0.9606986899563319\n",
      "loss: 0.1061578169465065, accuracy: 0.9650655021834061\n",
      "loss: 0.10109950602054596, accuracy: 0.9606986899563319\n",
      "loss: 0.09965302050113678, accuracy: 0.9694323144104804\n",
      "loss: 0.10062799602746964, accuracy: 0.9694323144104804\n",
      "loss: 0.11517076939344406, accuracy: 0.9475982532751092\n",
      "loss: 0.11155255883932114, accuracy: 0.9606986899563319\n",
      "loss: 0.10247348994016647, accuracy: 0.9781659388646288\n",
      "loss: 0.1085900366306305, accuracy: 0.9737991266375546\n",
      "loss: 0.09976236522197723, accuracy: 0.9737991266375546\n",
      "loss: 0.1125224232673645, accuracy: 0.9694323144104804\n",
      "loss: 0.11768176406621933, accuracy: 0.9475982532751092\n",
      "loss: 0.10550057888031006, accuracy: 0.9737991266375546\n",
      "max accuracy: 0.982532751091703\n",
      "loss: 0.0950586274266243, accuracy: 0.982532751091703\n",
      "loss: 0.1043340191245079, accuracy: 0.9606986899563319\n",
      "loss: 0.10313413292169571, accuracy: 0.9694323144104804\n",
      "loss: 0.0993402749300003, accuracy: 0.9694323144104804\n",
      "loss: 0.12120372802019119, accuracy: 0.9737991266375546\n",
      "loss: 0.143228217959404, accuracy: 0.9519650655021834\n",
      "loss: 0.10406183451414108, accuracy: 0.9650655021834061\n",
      "loss: 0.10706540942192078, accuracy: 0.9694323144104804\n",
      "loss: 0.09553591907024384, accuracy: 0.9737991266375546\n",
      "loss: 0.11893053352832794, accuracy: 0.9606986899563319\n",
      "loss: 0.10825447738170624, accuracy: 0.9606986899563319\n",
      "loss: 0.09649891406297684, accuracy: 0.9650655021834061\n",
      "loss: 0.11747894436120987, accuracy: 0.9694323144104804\n",
      "loss: 0.10307011008262634, accuracy: 0.9694323144104804\n",
      "loss: 0.10575748234987259, accuracy: 0.9694323144104804\n",
      "loss: 0.11005870997905731, accuracy: 0.9694323144104804\n",
      "loss: 0.0961286649107933, accuracy: 0.9781659388646288\n",
      "loss: 0.10898183286190033, accuracy: 0.9694323144104804\n",
      "loss: 0.14801891148090363, accuracy: 0.9475982532751092\n",
      "loss: 0.12309340387582779, accuracy: 0.9563318777292577\n",
      "loss: 0.11124046891927719, accuracy: 0.9606986899563319\n",
      "loss: 0.09823299199342728, accuracy: 0.9737991266375546\n",
      "loss: 0.09844402968883514, accuracy: 0.9694323144104804\n",
      "loss: 0.094777911901474, accuracy: 0.9694323144104804\n",
      "loss: 0.103519007563591, accuracy: 0.9737991266375546\n",
      "loss: 0.09746380895376205, accuracy: 0.9650655021834061\n",
      "loss: 0.09998636692762375, accuracy: 0.9694323144104804\n",
      "loss: 0.09800513833761215, accuracy: 0.9737991266375546\n",
      "loss: 0.1007838025689125, accuracy: 0.9781659388646288\n",
      "loss: 0.09829846769571304, accuracy: 0.9737991266375546\n",
      "loss: 0.10501997917890549, accuracy: 0.9650655021834061\n",
      "loss: 0.10110228508710861, accuracy: 0.9650655021834061\n",
      "loss: 0.09759495407342911, accuracy: 0.9694323144104804\n",
      "loss: 0.10123196244239807, accuracy: 0.9737991266375546\n",
      "loss: 0.10463415086269379, accuracy: 0.9650655021834061\n",
      "loss: 0.0999915823340416, accuracy: 0.9781659388646288\n",
      "loss: 0.10367949306964874, accuracy: 0.9781659388646288\n",
      "loss: 0.09122323244810104, accuracy: 0.9737991266375546\n",
      "loss: 0.10845734179019928, accuracy: 0.9650655021834061\n",
      "loss: 0.09182079136371613, accuracy: 0.9781659388646288\n",
      "loss: 0.1110752746462822, accuracy: 0.9519650655021834\n",
      "loss: 0.09646180272102356, accuracy: 0.9694323144104804\n",
      "loss: 0.09512211382389069, accuracy: 0.9737991266375546\n",
      "loss: 0.0966222733259201, accuracy: 0.9694323144104804\n",
      "loss: 0.11292664706707001, accuracy: 0.9650655021834061\n",
      "loss: 0.10799853503704071, accuracy: 0.9781659388646288\n",
      "loss: 0.10120072215795517, accuracy: 0.9650655021834061\n",
      "loss: 0.11656603217124939, accuracy: 0.9737991266375546\n",
      "loss: 0.0993829071521759, accuracy: 0.9781659388646288\n",
      "loss: 0.13384410738945007, accuracy: 0.9563318777292577\n",
      "loss: 0.1057315543293953, accuracy: 0.9606986899563319\n",
      "loss: 0.10179003328084946, accuracy: 0.9650655021834061\n",
      "loss: 0.10588008910417557, accuracy: 0.9694323144104804\n",
      "loss: 0.1083366647362709, accuracy: 0.9694323144104804\n",
      "loss: 0.10485851019620895, accuracy: 0.9694323144104804\n",
      "loss: 0.09919237345457077, accuracy: 0.9781659388646288\n",
      "loss: 0.10117164999246597, accuracy: 0.9650655021834061\n",
      "loss: 0.10148413479328156, accuracy: 0.9737991266375546\n",
      "loss: 0.09615565836429596, accuracy: 0.9781659388646288\n",
      "loss: 0.0950278490781784, accuracy: 0.9781659388646288\n",
      "loss: 0.10268286615610123, accuracy: 0.9737991266375546\n",
      "loss: 0.09358394145965576, accuracy: 0.9781659388646288\n",
      "loss: 0.09262765198945999, accuracy: 0.9694323144104804\n",
      "loss: 0.09388579428195953, accuracy: 0.9737991266375546\n",
      "loss: 0.09463874995708466, accuracy: 0.9650655021834061\n",
      "loss: 0.1113961935043335, accuracy: 0.9694323144104804\n",
      "loss: 0.10721568763256073, accuracy: 0.9650655021834061\n",
      "loss: 0.09644036740064621, accuracy: 0.9694323144104804\n",
      "loss: 0.10265477001667023, accuracy: 0.9694323144104804\n",
      "loss: 0.10246024280786514, accuracy: 0.9737991266375546\n",
      "loss: 0.11796323210000992, accuracy: 0.9606986899563319\n",
      "loss: 0.10416321456432343, accuracy: 0.9694323144104804\n",
      "loss: 0.09698119759559631, accuracy: 0.9781659388646288\n",
      "loss: 0.09606339037418365, accuracy: 0.9781659388646288\n",
      "loss: 0.09934936463832855, accuracy: 0.9737991266375546\n",
      "loss: 0.09876464307308197, accuracy: 0.9694323144104804\n",
      "loss: 0.092906653881073, accuracy: 0.9781659388646288\n",
      "loss: 0.09519493579864502, accuracy: 0.9650655021834061\n",
      "loss: 0.09791842848062515, accuracy: 0.9781659388646288\n",
      "loss: 0.09259660542011261, accuracy: 0.9737991266375546\n",
      "loss: 0.10105636715888977, accuracy: 0.9781659388646288\n",
      "loss: 0.09045883268117905, accuracy: 0.9737991266375546\n",
      "loss: 0.09106285870075226, accuracy: 0.9694323144104804\n",
      "loss: 0.09502115100622177, accuracy: 0.9694323144104804\n",
      "loss: 0.10066774487495422, accuracy: 0.9694323144104804\n",
      "loss: 0.0957065001130104, accuracy: 0.9781659388646288\n",
      "loss: 0.10049499571323395, accuracy: 0.9694323144104804\n",
      "loss: 0.09109801799058914, accuracy: 0.9781659388646288\n",
      "loss: 0.09449444711208344, accuracy: 0.9694323144104804\n",
      "loss: 0.0999336987733841, accuracy: 0.9650655021834061\n",
      "loss: 0.09456136077642441, accuracy: 0.9737991266375546\n",
      "loss: 0.10135235637426376, accuracy: 0.9650655021834061\n",
      "loss: 0.11460909992456436, accuracy: 0.9650655021834061\n",
      "loss: 0.10521984845399857, accuracy: 0.9650655021834061\n",
      "loss: 0.10239312052726746, accuracy: 0.9650655021834061\n",
      "loss: 0.0898490771651268, accuracy: 0.9781659388646288\n",
      "loss: 0.09401135891675949, accuracy: 0.9650655021834061\n",
      "loss: 0.10019475966691971, accuracy: 0.9694323144104804\n",
      "loss: 0.10946989804506302, accuracy: 0.9781659388646288\n",
      "loss: 0.10349370539188385, accuracy: 0.9650655021834061\n",
      "loss: 0.08807585388422012, accuracy: 0.9737991266375546\n",
      "loss: 0.08845671266317368, accuracy: 0.9737991266375546\n",
      "loss: 0.09379790723323822, accuracy: 0.9737991266375546\n",
      "loss: 0.09295815974473953, accuracy: 0.9781659388646288\n",
      "loss: 0.09804566204547882, accuracy: 0.9694323144104804\n",
      "loss: 0.09723891317844391, accuracy: 0.9781659388646288\n",
      "loss: 0.0953495055437088, accuracy: 0.9694323144104804\n",
      "loss: 0.09711413085460663, accuracy: 0.9781659388646288\n",
      "loss: 0.09237664192914963, accuracy: 0.9694323144104804\n",
      "loss: 0.09257527440786362, accuracy: 0.9737991266375546\n",
      "loss: 0.09672574698925018, accuracy: 0.9737991266375546\n",
      "loss: 0.09230538457632065, accuracy: 0.9781659388646288\n",
      "loss: 0.09833858162164688, accuracy: 0.9737991266375546\n",
      "loss: 0.08663588017225266, accuracy: 0.9781659388646288\n",
      "loss: 0.08922687917947769, accuracy: 0.9737991266375546\n",
      "loss: 0.09021691232919693, accuracy: 0.9737991266375546\n",
      "loss: 0.10393835604190826, accuracy: 0.9650655021834061\n",
      "loss: 0.10539321601390839, accuracy: 0.9606986899563319\n",
      "loss: 0.1110612079501152, accuracy: 0.9694323144104804\n",
      "loss: 0.1615903526544571, accuracy: 0.9344978165938864\n",
      "loss: 0.13835114240646362, accuracy: 0.9519650655021834\n",
      "loss: 0.12383902072906494, accuracy: 0.9650655021834061\n",
      "loss: 0.09634460508823395, accuracy: 0.982532751091703\n",
      "loss: 0.08827902376651764, accuracy: 0.982532751091703\n",
      "loss: 0.08776199817657471, accuracy: 0.9737991266375546\n",
      "loss: 0.09778189659118652, accuracy: 0.9737991266375546\n",
      "loss: 0.10327843576669693, accuracy: 0.9650655021834061\n",
      "loss: 0.0998230129480362, accuracy: 0.9694323144104804\n",
      "loss: 0.10100512206554413, accuracy: 0.9694323144104804\n",
      "loss: 0.09180057048797607, accuracy: 0.9694323144104804\n",
      "loss: 0.09573957324028015, accuracy: 0.9737991266375546\n",
      "loss: 0.08989560604095459, accuracy: 0.9737991266375546\n",
      "loss: 0.09395816177129745, accuracy: 0.9606986899563319\n",
      "loss: 0.09824123978614807, accuracy: 0.9737991266375546\n",
      "loss: 0.11262234300374985, accuracy: 0.9606986899563319\n",
      "loss: 0.0978393629193306, accuracy: 0.9694323144104804\n",
      "loss: 0.08669544756412506, accuracy: 0.9781659388646288\n",
      "loss: 0.08849174529314041, accuracy: 0.9737991266375546\n",
      "loss: 0.08877725899219513, accuracy: 0.9737991266375546\n",
      "loss: 0.09196358174085617, accuracy: 0.9650655021834061\n",
      "loss: 0.09587609767913818, accuracy: 0.9694323144104804\n",
      "loss: 0.08643007278442383, accuracy: 0.9781659388646288\n",
      "loss: 0.08980398625135422, accuracy: 0.9694323144104804\n",
      "loss: 0.08826567232608795, accuracy: 0.9781659388646288\n",
      "loss: 0.08706270158290863, accuracy: 0.982532751091703\n",
      "loss: 0.0836501196026802, accuracy: 0.9694323144104804\n",
      "loss: 0.08256684988737106, accuracy: 0.9781659388646288\n",
      "loss: 0.10013975203037262, accuracy: 0.9606986899563319\n",
      "loss: 0.09738803654909134, accuracy: 0.9650655021834061\n",
      "loss: 0.10353947430849075, accuracy: 0.9737991266375546\n",
      "loss: 0.18479225039482117, accuracy: 0.925764192139738\n",
      "loss: 0.09368541836738586, accuracy: 0.9737991266375546\n",
      "loss: 0.0872691348195076, accuracy: 0.9781659388646288\n",
      "loss: 0.10248416662216187, accuracy: 0.9737991266375546\n",
      "loss: 0.08541874587535858, accuracy: 0.9781659388646288\n",
      "loss: 0.09451449662446976, accuracy: 0.9781659388646288\n",
      "loss: 0.08621061593294144, accuracy: 0.9737991266375546\n",
      "loss: 0.08755897730588913, accuracy: 0.9694323144104804\n",
      "loss: 0.09588073939085007, accuracy: 0.9694323144104804\n",
      "loss: 0.1008744016289711, accuracy: 0.9650655021834061\n",
      "loss: 0.0865786224603653, accuracy: 0.9781659388646288\n",
      "loss: 0.08920151740312576, accuracy: 0.9737991266375546\n",
      "max accuracy: 0.9868995633187773\n",
      "loss: 0.08544669300317764, accuracy: 0.9868995633187773\n",
      "loss: 0.09659025818109512, accuracy: 0.9694323144104804\n",
      "loss: 0.09391642361879349, accuracy: 0.9737991266375546\n",
      "loss: 0.09033219516277313, accuracy: 0.9737991266375546\n",
      "loss: 0.09054116159677505, accuracy: 0.9781659388646288\n",
      "loss: 0.0893402174115181, accuracy: 0.9694323144104804\n",
      "loss: 0.10160698741674423, accuracy: 0.9737991266375546\n",
      "loss: 0.08602738380432129, accuracy: 0.9737991266375546\n",
      "loss: 0.09256159514188766, accuracy: 0.9694323144104804\n",
      "loss: 0.09351513534784317, accuracy: 0.9650655021834061\n",
      "loss: 0.09913818538188934, accuracy: 0.9781659388646288\n",
      "loss: 0.08892050385475159, accuracy: 0.9781659388646288\n",
      "loss: 0.0900835394859314, accuracy: 0.9781659388646288\n",
      "loss: 0.10231130570173264, accuracy: 0.9694323144104804\n",
      "loss: 0.09020306169986725, accuracy: 0.9694323144104804\n",
      "loss: 0.08643002063035965, accuracy: 0.9650655021834061\n",
      "loss: 0.09169268608093262, accuracy: 0.9781659388646288\n",
      "loss: 0.10937130451202393, accuracy: 0.9650655021834061\n",
      "loss: 0.10668233782052994, accuracy: 0.9650655021834061\n",
      "loss: 0.09722588211297989, accuracy: 0.9737991266375546\n",
      "loss: 0.11652011424303055, accuracy: 0.9519650655021834\n",
      "loss: 0.09028948098421097, accuracy: 0.9694323144104804\n",
      "loss: 0.09192656725645065, accuracy: 0.9737991266375546\n",
      "loss: 0.08633165806531906, accuracy: 0.9737991266375546\n",
      "loss: 0.118787482380867, accuracy: 0.9606986899563319\n",
      "loss: 0.11942098289728165, accuracy: 0.9475982532751092\n",
      "loss: 0.08847139775753021, accuracy: 0.9694323144104804\n",
      "loss: 0.0901380404829979, accuracy: 0.9781659388646288\n",
      "loss: 0.09192143380641937, accuracy: 0.9781659388646288\n",
      "loss: 0.09317610412836075, accuracy: 0.9650655021834061\n",
      "loss: 0.10149790346622467, accuracy: 0.9650655021834061\n",
      "loss: 0.08674982935190201, accuracy: 0.9781659388646288\n",
      "loss: 0.10256364196538925, accuracy: 0.9650655021834061\n",
      "loss: 0.10055307298898697, accuracy: 0.9650655021834061\n",
      "loss: 0.10395094007253647, accuracy: 0.9650655021834061\n",
      "loss: 0.08176585286855698, accuracy: 0.982532751091703\n",
      "loss: 0.09044545888900757, accuracy: 0.9737991266375546\n",
      "loss: 0.08817142248153687, accuracy: 0.982532751091703\n",
      "loss: 0.08649543672800064, accuracy: 0.9781659388646288\n",
      "loss: 0.09251868724822998, accuracy: 0.9737991266375546\n",
      "loss: 0.08993624895811081, accuracy: 0.9737991266375546\n",
      "loss: 0.09976618736982346, accuracy: 0.9650655021834061\n",
      "loss: 0.08315952122211456, accuracy: 0.9737991266375546\n",
      "loss: 0.08757632970809937, accuracy: 0.9781659388646288\n",
      "loss: 0.08959028869867325, accuracy: 0.982532751091703\n",
      "loss: 0.08547917008399963, accuracy: 0.9737991266375546\n",
      "loss: 0.09732357412576675, accuracy: 0.9650655021834061\n",
      "loss: 0.07496225833892822, accuracy: 0.9868995633187773\n",
      "loss: 0.08546985685825348, accuracy: 0.9781659388646288\n",
      "loss: 0.09047015011310577, accuracy: 0.982532751091703\n",
      "loss: 0.09566902369260788, accuracy: 0.9737991266375546\n",
      "loss: 0.09823981672525406, accuracy: 0.9650655021834061\n",
      "loss: 0.08606986701488495, accuracy: 0.9781659388646288\n",
      "loss: 0.08703530579805374, accuracy: 0.9781659388646288\n",
      "loss: 0.08510617911815643, accuracy: 0.9781659388646288\n",
      "loss: 0.08295273780822754, accuracy: 0.9694323144104804\n",
      "loss: 0.10246852040290833, accuracy: 0.9650655021834061\n",
      "loss: 0.0891040563583374, accuracy: 0.9781659388646288\n",
      "loss: 0.0827033668756485, accuracy: 0.982532751091703\n",
      "loss: 0.08507706224918365, accuracy: 0.9694323144104804\n",
      "loss: 0.08434980362653732, accuracy: 0.9650655021834061\n",
      "loss: 0.07999688386917114, accuracy: 0.9781659388646288\n",
      "loss: 0.0869385227560997, accuracy: 0.9737991266375546\n",
      "loss: 0.09109831601381302, accuracy: 0.9781659388646288\n",
      "loss: 0.09570152312517166, accuracy: 0.9650655021834061\n",
      "loss: 0.09875571727752686, accuracy: 0.9694323144104804\n",
      "loss: 0.09535109996795654, accuracy: 0.9737991266375546\n",
      "loss: 0.08896264433860779, accuracy: 0.9737991266375546\n",
      "loss: 0.08686196804046631, accuracy: 0.9650655021834061\n",
      "loss: 0.08311419188976288, accuracy: 0.9781659388646288\n",
      "loss: 0.08362297713756561, accuracy: 0.9737991266375546\n",
      "loss: 0.09109802544116974, accuracy: 0.9781659388646288\n",
      "loss: 0.0876377522945404, accuracy: 0.9781659388646288\n",
      "loss: 0.08659868687391281, accuracy: 0.9694323144104804\n",
      "loss: 0.08382033556699753, accuracy: 0.9737991266375546\n",
      "loss: 0.08420059084892273, accuracy: 0.9781659388646288\n",
      "loss: 0.08559483289718628, accuracy: 0.9694323144104804\n",
      "loss: 0.08751951903104782, accuracy: 0.9694323144104804\n",
      "loss: 0.0826461985707283, accuracy: 0.9737991266375546\n",
      "loss: 0.07783479988574982, accuracy: 0.9781659388646288\n",
      "loss: 0.08262745290994644, accuracy: 0.9781659388646288\n",
      "loss: 0.08689341694116592, accuracy: 0.9781659388646288\n",
      "loss: 0.0885734036564827, accuracy: 0.9781659388646288\n",
      "loss: 0.07726829499006271, accuracy: 0.9781659388646288\n",
      "loss: 0.0844351202249527, accuracy: 0.9737991266375546\n",
      "loss: 0.10128576308488846, accuracy: 0.9737991266375546\n",
      "loss: 0.08029554039239883, accuracy: 0.9737991266375546\n",
      "loss: 0.08417796343564987, accuracy: 0.9737991266375546\n",
      "loss: 0.08725708723068237, accuracy: 0.9694323144104804\n",
      "loss: 0.082815021276474, accuracy: 0.9781659388646288\n",
      "loss: 0.09397110342979431, accuracy: 0.9737991266375546\n",
      "loss: 0.0783035010099411, accuracy: 0.9694323144104804\n",
      "loss: 0.08509036153554916, accuracy: 0.9737991266375546\n",
      "loss: 0.08860935270786285, accuracy: 0.9737991266375546\n",
      "loss: 0.07812055200338364, accuracy: 0.9781659388646288\n",
      "loss: 0.08927259594202042, accuracy: 0.9737991266375546\n",
      "loss: 0.08279654383659363, accuracy: 0.9694323144104804\n",
      "loss: 0.080431267619133, accuracy: 0.9781659388646288\n",
      "loss: 0.08937948942184448, accuracy: 0.9650655021834061\n",
      "loss: 0.1064988523721695, accuracy: 0.9737991266375546\n",
      "loss: 0.0868232250213623, accuracy: 0.9737991266375546\n",
      "loss: 0.08528421074151993, accuracy: 0.9694323144104804\n",
      "loss: 0.08417615294456482, accuracy: 0.9781659388646288\n",
      "loss: 0.096371129155159, accuracy: 0.9781659388646288\n",
      "loss: 0.10025962442159653, accuracy: 0.9781659388646288\n",
      "loss: 0.20134754478931427, accuracy: 0.925764192139738\n",
      "loss: 0.08722975850105286, accuracy: 0.9650655021834061\n",
      "loss: 0.0925050675868988, accuracy: 0.9694323144104804\n",
      "loss: 0.08457855880260468, accuracy: 0.9737991266375546\n",
      "loss: 0.07823722809553146, accuracy: 0.9781659388646288\n",
      "loss: 0.0870954766869545, accuracy: 0.9737991266375546\n",
      "loss: 0.08088495582342148, accuracy: 0.9737991266375546\n",
      "loss: 0.08349131792783737, accuracy: 0.9694323144104804\n",
      "loss: 0.08877831697463989, accuracy: 0.9737991266375546\n",
      "loss: 0.09124654531478882, accuracy: 0.9781659388646288\n",
      "loss: 0.0751006230711937, accuracy: 0.9781659388646288\n",
      "loss: 0.11276057362556458, accuracy: 0.9563318777292577\n",
      "loss: 0.08680326491594315, accuracy: 0.9781659388646288\n",
      "loss: 0.08365114033222198, accuracy: 0.9737991266375546\n",
      "loss: 0.07945115119218826, accuracy: 0.9694323144104804\n",
      "loss: 0.08065971732139587, accuracy: 0.9781659388646288\n",
      "loss: 0.08077065646648407, accuracy: 0.9868995633187773\n",
      "loss: 0.08529782295227051, accuracy: 0.9737991266375546\n",
      "loss: 0.0891178548336029, accuracy: 0.9737991266375546\n",
      "loss: 0.08206835389137268, accuracy: 0.9781659388646288\n",
      "loss: 0.11158159375190735, accuracy: 0.9650655021834061\n",
      "loss: 0.09524109959602356, accuracy: 0.9737991266375546\n",
      "loss: 0.07961292564868927, accuracy: 0.9737991266375546\n",
      "loss: 0.07570968568325043, accuracy: 0.9781659388646288\n",
      "loss: 0.08554928004741669, accuracy: 0.9737991266375546\n",
      "loss: 0.07771241664886475, accuracy: 0.9737991266375546\n",
      "loss: 0.09786352515220642, accuracy: 0.9694323144104804\n",
      "loss: 0.08083877712488174, accuracy: 0.9781659388646288\n",
      "loss: 0.08270037174224854, accuracy: 0.9781659388646288\n",
      "loss: 0.07632897049188614, accuracy: 0.9781659388646288\n",
      "loss: 0.07977006584405899, accuracy: 0.9781659388646288\n",
      "loss: 0.08236891031265259, accuracy: 0.9737991266375546\n",
      "loss: 0.09258025884628296, accuracy: 0.982532751091703\n",
      "loss: 0.09126352518796921, accuracy: 0.9737991266375546\n",
      "loss: 0.07926397770643234, accuracy: 0.982532751091703\n",
      "loss: 0.08569665253162384, accuracy: 0.9737991266375546\n",
      "loss: 0.09221930801868439, accuracy: 0.9737991266375546\n",
      "loss: 0.07489275932312012, accuracy: 0.9781659388646288\n",
      "loss: 0.07655314356088638, accuracy: 0.9781659388646288\n",
      "loss: 0.1201607957482338, accuracy: 0.9519650655021834\n",
      "loss: 0.08575671911239624, accuracy: 0.982532751091703\n",
      "loss: 0.0812179297208786, accuracy: 0.9781659388646288\n",
      "loss: 0.08583623915910721, accuracy: 0.9737991266375546\n",
      "loss: 0.07848836481571198, accuracy: 0.9781659388646288\n",
      "loss: 0.07984770089387894, accuracy: 0.982532751091703\n",
      "loss: 0.07862880825996399, accuracy: 0.9737991266375546\n",
      "loss: 0.09903118014335632, accuracy: 0.9563318777292577\n",
      "loss: 0.09216140955686569, accuracy: 0.9781659388646288\n",
      "loss: 0.07973470538854599, accuracy: 0.9781659388646288\n",
      "loss: 0.08725778013467789, accuracy: 0.9694323144104804\n",
      "loss: 0.07496757060289383, accuracy: 0.9737991266375546\n",
      "loss: 0.0824422687292099, accuracy: 0.9781659388646288\n",
      "loss: 0.0800720751285553, accuracy: 0.9781659388646288\n",
      "loss: 0.08196107298135757, accuracy: 0.9781659388646288\n",
      "loss: 0.07690311968326569, accuracy: 0.9737991266375546\n",
      "loss: 0.09001854062080383, accuracy: 0.9737991266375546\n",
      "loss: 0.07502854615449905, accuracy: 0.9737991266375546\n",
      "loss: 0.08122501522302628, accuracy: 0.9737991266375546\n",
      "loss: 0.0896088108420372, accuracy: 0.9737991266375546\n",
      "loss: 0.07670437544584274, accuracy: 0.9737991266375546\n",
      "loss: 0.0717683956027031, accuracy: 0.982532751091703\n",
      "loss: 0.07889389246702194, accuracy: 0.9694323144104804\n",
      "loss: 0.08760979026556015, accuracy: 0.9737991266375546\n",
      "loss: 0.09762772917747498, accuracy: 0.9694323144104804\n",
      "loss: 0.09190239012241364, accuracy: 0.9606986899563319\n",
      "loss: 0.08678878843784332, accuracy: 0.9781659388646288\n",
      "loss: 0.09194016456604004, accuracy: 0.9737991266375546\n",
      "loss: 0.08217311650514603, accuracy: 0.9781659388646288\n",
      "loss: 0.08135256916284561, accuracy: 0.9737991266375546\n",
      "loss: 0.07888823747634888, accuracy: 0.9781659388646288\n",
      "loss: 0.07680456340312958, accuracy: 0.9737991266375546\n",
      "loss: 0.08414027094841003, accuracy: 0.9737991266375546\n",
      "loss: 0.08711666613817215, accuracy: 0.9737991266375546\n",
      "loss: 0.08537032455205917, accuracy: 0.9694323144104804\n",
      "loss: 0.07920331507921219, accuracy: 0.9694323144104804\n",
      "loss: 0.08611401170492172, accuracy: 0.9737991266375546\n",
      "loss: 0.07899998873472214, accuracy: 0.9737991266375546\n",
      "loss: 0.08238165825605392, accuracy: 0.9737991266375546\n",
      "loss: 0.07821308821439743, accuracy: 0.9694323144104804\n",
      "loss: 0.07402238994836807, accuracy: 0.9868995633187773\n",
      "loss: 0.07619542628526688, accuracy: 0.9781659388646288\n",
      "loss: 0.08464536815881729, accuracy: 0.9737991266375546\n",
      "loss: 0.07666423916816711, accuracy: 0.9737991266375546\n",
      "loss: 0.08437135815620422, accuracy: 0.9650655021834061\n",
      "loss: 0.07187693566083908, accuracy: 0.982532751091703\n",
      "loss: 0.08690553158521652, accuracy: 0.9781659388646288\n",
      "loss: 0.09050191938877106, accuracy: 0.9781659388646288\n",
      "loss: 0.09171176701784134, accuracy: 0.9694323144104804\n",
      "loss: 0.08309892565011978, accuracy: 0.982532751091703\n",
      "loss: 0.07349548488855362, accuracy: 0.9781659388646288\n",
      "loss: 0.07955600321292877, accuracy: 0.9737991266375546\n",
      "loss: 0.07601731270551682, accuracy: 0.9781659388646288\n",
      "loss: 0.09083230048418045, accuracy: 0.9737991266375546\n",
      "loss: 0.08037163317203522, accuracy: 0.982532751091703\n",
      "loss: 0.07588089257478714, accuracy: 0.982532751091703\n",
      "loss: 0.07856667786836624, accuracy: 0.982532751091703\n",
      "loss: 0.09865319728851318, accuracy: 0.9694323144104804\n",
      "loss: 0.09569113701581955, accuracy: 0.9694323144104804\n",
      "loss: 0.07584433257579803, accuracy: 0.9781659388646288\n",
      "loss: 0.07565437257289886, accuracy: 0.9781659388646288\n",
      "loss: 0.07616394758224487, accuracy: 0.982532751091703\n",
      "loss: 0.07392330467700958, accuracy: 0.9868995633187773\n",
      "loss: 0.07900556176900864, accuracy: 0.9781659388646288\n",
      "loss: 0.09375521540641785, accuracy: 0.982532751091703\n",
      "loss: 0.07973220944404602, accuracy: 0.9868995633187773\n",
      "loss: 0.0733395516872406, accuracy: 0.982532751091703\n",
      "loss: 0.08076716214418411, accuracy: 0.9781659388646288\n",
      "loss: 0.08291294425725937, accuracy: 0.9781659388646288\n",
      "loss: 0.075754813849926, accuracy: 0.9781659388646288\n",
      "loss: 0.08344518393278122, accuracy: 0.9781659388646288\n",
      "loss: 0.08704228699207306, accuracy: 0.982532751091703\n",
      "loss: 0.07750192284584045, accuracy: 0.9737991266375546\n",
      "loss: 0.07520613819360733, accuracy: 0.9781659388646288\n",
      "loss: 0.07438889890909195, accuracy: 0.9781659388646288\n",
      "loss: 0.08109476417303085, accuracy: 0.9781659388646288\n",
      "loss: 0.08536895364522934, accuracy: 0.9737991266375546\n",
      "loss: 0.07463061809539795, accuracy: 0.9781659388646288\n",
      "loss: 0.08339068293571472, accuracy: 0.9737991266375546\n",
      "loss: 0.08110659569501877, accuracy: 0.9781659388646288\n",
      "loss: 0.08187676966190338, accuracy: 0.9737991266375546\n",
      "loss: 0.07764488458633423, accuracy: 0.982532751091703\n",
      "loss: 0.0748651996254921, accuracy: 0.982532751091703\n",
      "loss: 0.07149577885866165, accuracy: 0.9781659388646288\n",
      "loss: 0.08589750528335571, accuracy: 0.9737991266375546\n",
      "loss: 0.10406811535358429, accuracy: 0.9694323144104804\n",
      "loss: 0.08122274279594421, accuracy: 0.9737991266375546\n",
      "loss: 0.08055456727743149, accuracy: 0.9781659388646288\n",
      "loss: 0.1045474037528038, accuracy: 0.9694323144104804\n",
      "loss: 0.08212185651063919, accuracy: 0.9781659388646288\n",
      "loss: 0.07679733633995056, accuracy: 0.982532751091703\n",
      "loss: 0.07490876317024231, accuracy: 0.9737991266375546\n",
      "loss: 0.07863099873065948, accuracy: 0.982532751091703\n",
      "loss: 0.07273741811513901, accuracy: 0.9868995633187773\n",
      "loss: 0.08072500675916672, accuracy: 0.9737991266375546\n",
      "loss: 0.07437991350889206, accuracy: 0.982532751091703\n",
      "loss: 0.07917198538780212, accuracy: 0.9781659388646288\n",
      "loss: 0.0673796609044075, accuracy: 0.9868995633187773\n",
      "loss: 0.06906018406152725, accuracy: 0.9868995633187773\n",
      "loss: 0.07495870441198349, accuracy: 0.9737991266375546\n",
      "loss: 0.09164482355117798, accuracy: 0.9781659388646288\n",
      "loss: 0.09452520310878754, accuracy: 0.9737991266375546\n",
      "loss: 0.07236848771572113, accuracy: 0.9781659388646288\n",
      "loss: 0.07398182898759842, accuracy: 0.982532751091703\n",
      "loss: 0.07572004199028015, accuracy: 0.982532751091703\n",
      "loss: 0.07351537048816681, accuracy: 0.9868995633187773\n",
      "loss: 0.07056590169668198, accuracy: 0.9868995633187773\n",
      "loss: 0.07108787447214127, accuracy: 0.9781659388646288\n",
      "loss: 0.0725696012377739, accuracy: 0.982532751091703\n",
      "loss: 0.0744379386305809, accuracy: 0.9781659388646288\n",
      "loss: 0.07057704031467438, accuracy: 0.982532751091703\n",
      "loss: 0.071479931473732, accuracy: 0.9781659388646288\n",
      "loss: 0.07272715866565704, accuracy: 0.982532751091703\n",
      "loss: 0.07162099331617355, accuracy: 0.982532751091703\n",
      "loss: 0.09871017187833786, accuracy: 0.9650655021834061\n",
      "loss: 0.07037325203418732, accuracy: 0.9868995633187773\n",
      "loss: 0.0744064599275589, accuracy: 0.982532751091703\n",
      "loss: 0.07905317097902298, accuracy: 0.9781659388646288\n",
      "loss: 0.11939533799886703, accuracy: 0.9475982532751092\n",
      "loss: 0.08675330132246017, accuracy: 0.9694323144104804\n",
      "loss: 0.07555928081274033, accuracy: 0.9737991266375546\n",
      "loss: 0.0820605456829071, accuracy: 0.9737991266375546\n",
      "loss: 0.07675010710954666, accuracy: 0.9781659388646288\n",
      "loss: 0.06937485188245773, accuracy: 0.982532751091703\n",
      "loss: 0.07511473447084427, accuracy: 0.982532751091703\n",
      "loss: 0.0928812175989151, accuracy: 0.9650655021834061\n",
      "loss: 0.07914175093173981, accuracy: 0.982532751091703\n",
      "loss: 0.08243419975042343, accuracy: 0.9694323144104804\n",
      "loss: 0.07736700028181076, accuracy: 0.9781659388646288\n",
      "loss: 0.08398502320051193, accuracy: 0.9650655021834061\n",
      "loss: 0.08728746324777603, accuracy: 0.9694323144104804\n",
      "loss: 0.07574356347322464, accuracy: 0.9737991266375546\n",
      "loss: 0.07347139716148376, accuracy: 0.982532751091703\n",
      "loss: 0.07484980672597885, accuracy: 0.9781659388646288\n",
      "loss: 0.07143478095531464, accuracy: 0.982532751091703\n",
      "loss: 0.08773442357778549, accuracy: 0.9737991266375546\n",
      "loss: 0.07287751138210297, accuracy: 0.9781659388646288\n",
      "loss: 0.07526031881570816, accuracy: 0.9781659388646288\n",
      "loss: 0.08709914982318878, accuracy: 0.982532751091703\n",
      "loss: 0.08311183750629425, accuracy: 0.9694323144104804\n",
      "loss: 0.07782433182001114, accuracy: 0.9781659388646288\n",
      "loss: 0.08281803876161575, accuracy: 0.9737991266375546\n",
      "loss: 0.06751988083124161, accuracy: 0.9737991266375546\n",
      "loss: 0.07750904560089111, accuracy: 0.9781659388646288\n",
      "loss: 0.08048021793365479, accuracy: 0.9781659388646288\n",
      "loss: 0.07303867489099503, accuracy: 0.982532751091703\n",
      "loss: 0.06881948560476303, accuracy: 0.9781659388646288\n",
      "loss: 0.08274666965007782, accuracy: 0.9781659388646288\n",
      "loss: 0.06835078448057175, accuracy: 0.9781659388646288\n",
      "loss: 0.09021375328302383, accuracy: 0.9694323144104804\n",
      "loss: 0.07440801709890366, accuracy: 0.9781659388646288\n",
      "loss: 0.07767877727746964, accuracy: 0.982532751091703\n",
      "loss: 0.08490893244743347, accuracy: 0.9781659388646288\n",
      "loss: 0.0731460228562355, accuracy: 0.9781659388646288\n",
      "loss: 0.07518314570188522, accuracy: 0.982532751091703\n",
      "loss: 0.0737995132803917, accuracy: 0.9868995633187773\n",
      "loss: 0.07538791000843048, accuracy: 0.9781659388646288\n",
      "loss: 0.08168395608663559, accuracy: 0.9737991266375546\n",
      "loss: 0.07063046097755432, accuracy: 0.982532751091703\n",
      "loss: 0.07937010377645493, accuracy: 0.982532751091703\n",
      "loss: 0.07126998901367188, accuracy: 0.982532751091703\n",
      "loss: 0.0698973536491394, accuracy: 0.9868995633187773\n",
      "loss: 0.08053857088088989, accuracy: 0.9868995633187773\n",
      "loss: 0.0746205598115921, accuracy: 0.9781659388646288\n",
      "loss: 0.07347999513149261, accuracy: 0.9781659388646288\n",
      "loss: 0.06638588011264801, accuracy: 0.9737991266375546\n",
      "loss: 0.08171727508306503, accuracy: 0.982532751091703\n",
      "loss: 0.07308591902256012, accuracy: 0.9781659388646288\n",
      "loss: 0.06584982573986053, accuracy: 0.982532751091703\n",
      "loss: 0.06764703243970871, accuracy: 0.9781659388646288\n",
      "loss: 0.0801367536187172, accuracy: 0.9781659388646288\n",
      "loss: 0.07862953096628189, accuracy: 0.9781659388646288\n",
      "max accuracy: 0.9912663755458515\n",
      "loss: 0.062326639890670776, accuracy: 0.9912663755458515\n",
      "loss: 0.07731805741786957, accuracy: 0.982532751091703\n",
      "loss: 0.07323004305362701, accuracy: 0.9781659388646288\n",
      "loss: 0.07350260019302368, accuracy: 0.9737991266375546\n",
      "loss: 0.07161709666252136, accuracy: 0.982532751091703\n",
      "loss: 0.06907172501087189, accuracy: 0.9868995633187773\n",
      "loss: 0.06162114813923836, accuracy: 0.9912663755458515\n",
      "loss: 0.07029780000448227, accuracy: 0.9781659388646288\n",
      "loss: 0.07876451313495636, accuracy: 0.9650655021834061\n",
      "loss: 0.07012030482292175, accuracy: 0.982532751091703\n",
      "loss: 0.06658786535263062, accuracy: 0.9868995633187773\n",
      "loss: 0.08693890273571014, accuracy: 0.9737991266375546\n",
      "loss: 0.06964117288589478, accuracy: 0.9737991266375546\n",
      "loss: 0.06604301929473877, accuracy: 0.982532751091703\n",
      "loss: 0.07399305701255798, accuracy: 0.9912663755458515\n",
      "loss: 0.06332890689373016, accuracy: 0.9868995633187773\n",
      "loss: 0.07250847667455673, accuracy: 0.982532751091703\n",
      "loss: 0.0669572502374649, accuracy: 0.9868995633187773\n",
      "loss: 0.06743328273296356, accuracy: 0.9912663755458515\n",
      "loss: 0.06855747103691101, accuracy: 0.9868995633187773\n",
      "loss: 0.09345880895853043, accuracy: 0.9694323144104804\n",
      "loss: 0.09243079274892807, accuracy: 0.9694323144104804\n",
      "loss: 0.08450920879840851, accuracy: 0.9781659388646288\n",
      "loss: 0.07630258798599243, accuracy: 0.9737991266375546\n",
      "loss: 0.08009418100118637, accuracy: 0.9737991266375546\n",
      "loss: 0.09409202635288239, accuracy: 0.9694323144104804\n",
      "loss: 0.07187201082706451, accuracy: 0.9781659388646288\n",
      "loss: 0.06617073714733124, accuracy: 0.982532751091703\n",
      "loss: 0.06721484661102295, accuracy: 0.982532751091703\n",
      "loss: 0.06522385776042938, accuracy: 0.9868995633187773\n",
      "loss: 0.10096217691898346, accuracy: 0.9650655021834061\n",
      "loss: 0.09200272709131241, accuracy: 0.9737991266375546\n",
      "loss: 0.07448907941579819, accuracy: 0.9737991266375546\n",
      "loss: 0.06884445995092392, accuracy: 0.982532751091703\n",
      "loss: 0.06482679396867752, accuracy: 0.982532751091703\n",
      "loss: 0.06800027191638947, accuracy: 0.982532751091703\n",
      "loss: 0.06836619973182678, accuracy: 0.9868995633187773\n",
      "loss: 0.06635067611932755, accuracy: 0.9781659388646288\n",
      "loss: 0.07803288102149963, accuracy: 0.9694323144104804\n",
      "loss: 0.06822086125612259, accuracy: 0.9781659388646288\n",
      "loss: 0.0690106749534607, accuracy: 0.982532751091703\n",
      "loss: 0.06498092412948608, accuracy: 0.982532751091703\n",
      "loss: 0.06772683560848236, accuracy: 0.9868995633187773\n",
      "loss: 0.07146406918764114, accuracy: 0.982532751091703\n",
      "loss: 0.06844712048768997, accuracy: 0.982532751091703\n",
      "loss: 0.0734487920999527, accuracy: 0.9781659388646288\n",
      "loss: 0.08595012128353119, accuracy: 0.9737991266375546\n",
      "loss: 0.07441309839487076, accuracy: 0.982532751091703\n",
      "loss: 0.07199177145957947, accuracy: 0.9737991266375546\n",
      "loss: 0.07467237114906311, accuracy: 0.9737991266375546\n",
      "loss: 0.09344489872455597, accuracy: 0.9694323144104804\n",
      "loss: 0.07114889472723007, accuracy: 0.982532751091703\n",
      "loss: 0.07047215104103088, accuracy: 0.9781659388646288\n",
      "loss: 0.0681450292468071, accuracy: 0.9781659388646288\n",
      "loss: 0.07173959910869598, accuracy: 0.9781659388646288\n",
      "loss: 0.0650220736861229, accuracy: 0.9868995633187773\n",
      "loss: 0.06755559891462326, accuracy: 0.9781659388646288\n",
      "loss: 0.0705263614654541, accuracy: 0.9781659388646288\n",
      "loss: 0.07128799706697464, accuracy: 0.9781659388646288\n",
      "loss: 0.07140394300222397, accuracy: 0.9868995633187773\n",
      "loss: 0.06798475980758667, accuracy: 0.982532751091703\n",
      "loss: 0.06833374500274658, accuracy: 0.982532751091703\n",
      "loss: 0.07231561839580536, accuracy: 0.9868995633187773\n",
      "loss: 0.06526520103216171, accuracy: 0.9781659388646288\n",
      "loss: 0.08730519562959671, accuracy: 0.9737991266375546\n",
      "loss: 0.07059594243764877, accuracy: 0.982532751091703\n",
      "loss: 0.08640613406896591, accuracy: 0.9737991266375546\n",
      "loss: 0.07127272337675095, accuracy: 0.9781659388646288\n",
      "loss: 0.07355239987373352, accuracy: 0.9781659388646288\n",
      "loss: 0.07214944064617157, accuracy: 0.982532751091703\n",
      "loss: 0.08268539607524872, accuracy: 0.9781659388646288\n",
      "loss: 0.06563658267259598, accuracy: 0.982532751091703\n",
      "loss: 0.06953351199626923, accuracy: 0.9781659388646288\n",
      "loss: 0.07171747833490372, accuracy: 0.9781659388646288\n",
      "loss: 0.06636996567249298, accuracy: 0.9912663755458515\n",
      "loss: 0.06718863546848297, accuracy: 0.982532751091703\n",
      "loss: 0.06769080460071564, accuracy: 0.9868995633187773\n",
      "loss: 0.06799686700105667, accuracy: 0.9737991266375546\n",
      "loss: 0.07465724647045135, accuracy: 0.9868995633187773\n",
      "loss: 0.06593381613492966, accuracy: 0.982532751091703\n",
      "loss: 0.06415267288684845, accuracy: 0.982532751091703\n",
      "loss: 0.06496521830558777, accuracy: 0.982532751091703\n",
      "loss: 0.06326587498188019, accuracy: 0.9868995633187773\n",
      "loss: 0.08001701533794403, accuracy: 0.9781659388646288\n",
      "loss: 0.06289755553007126, accuracy: 0.982532751091703\n",
      "loss: 0.06609685719013214, accuracy: 0.9737991266375546\n",
      "loss: 0.081476129591465, accuracy: 0.9781659388646288\n",
      "loss: 0.07099944353103638, accuracy: 0.9737991266375546\n",
      "loss: 0.06593552976846695, accuracy: 0.9781659388646288\n",
      "loss: 0.068301722407341, accuracy: 0.9737991266375546\n",
      "loss: 0.08487613499164581, accuracy: 0.9737991266375546\n",
      "loss: 0.11381667107343674, accuracy: 0.9563318777292577\n",
      "loss: 0.0799485445022583, accuracy: 0.9781659388646288\n",
      "loss: 0.07298686355352402, accuracy: 0.982532751091703\n",
      "loss: 0.08591482788324356, accuracy: 0.9781659388646288\n",
      "loss: 0.07670924067497253, accuracy: 0.9737991266375546\n",
      "loss: 0.06807025521993637, accuracy: 0.9737991266375546\n",
      "loss: 0.06643711030483246, accuracy: 0.982532751091703\n",
      "loss: 0.06060683727264404, accuracy: 0.9868995633187773\n",
      "loss: 0.06337207555770874, accuracy: 0.982532751091703\n",
      "loss: 0.06658931821584702, accuracy: 0.9781659388646288\n",
      "loss: 0.06334701925516129, accuracy: 0.982532751091703\n",
      "loss: 0.06355785578489304, accuracy: 0.9781659388646288\n",
      "loss: 0.05949409678578377, accuracy: 0.9868995633187773\n",
      "loss: 0.061624109745025635, accuracy: 0.982532751091703\n",
      "loss: 0.06646845489740372, accuracy: 0.9781659388646288\n",
      "loss: 0.06929876655340195, accuracy: 0.982532751091703\n",
      "loss: 0.06153566762804985, accuracy: 0.9781659388646288\n",
      "loss: 0.0638665184378624, accuracy: 0.982532751091703\n",
      "loss: 0.07664185017347336, accuracy: 0.982532751091703\n",
      "loss: 0.06781632453203201, accuracy: 0.982532751091703\n",
      "loss: 0.06943054497241974, accuracy: 0.982532751091703\n",
      "loss: 0.07788726687431335, accuracy: 0.9694323144104804\n",
      "loss: 0.06439977139234543, accuracy: 0.982532751091703\n",
      "loss: 0.06406040489673615, accuracy: 0.9781659388646288\n",
      "loss: 0.06291864812374115, accuracy: 0.982532751091703\n",
      "loss: 0.06456387042999268, accuracy: 0.9781659388646288\n",
      "loss: 0.07692727446556091, accuracy: 0.9737991266375546\n",
      "loss: 0.0591312050819397, accuracy: 0.982532751091703\n",
      "loss: 0.06434742361307144, accuracy: 0.982532751091703\n",
      "loss: 0.07303280383348465, accuracy: 0.982532751091703\n",
      "loss: 0.09392446279525757, accuracy: 0.9694323144104804\n",
      "loss: 0.09185020625591278, accuracy: 0.9694323144104804\n",
      "loss: 0.06240617111325264, accuracy: 0.982532751091703\n",
      "loss: 0.07147399336099625, accuracy: 0.9737991266375546\n",
      "loss: 0.06248057261109352, accuracy: 0.982532751091703\n",
      "loss: 0.07430651783943176, accuracy: 0.9781659388646288\n",
      "loss: 0.07121092081069946, accuracy: 0.982532751091703\n",
      "loss: 0.06384830176830292, accuracy: 0.9781659388646288\n",
      "loss: 0.0669068768620491, accuracy: 0.982532751091703\n",
      "loss: 0.08152937889099121, accuracy: 0.9737991266375546\n",
      "loss: 0.08062950521707535, accuracy: 0.9694323144104804\n",
      "loss: 0.07034029066562653, accuracy: 0.982532751091703\n",
      "loss: 0.06384341418743134, accuracy: 0.982532751091703\n",
      "loss: 0.07118399441242218, accuracy: 0.982532751091703\n",
      "loss: 0.06283450871706009, accuracy: 0.982532751091703\n",
      "loss: 0.07420555502176285, accuracy: 0.9737991266375546\n",
      "loss: 0.059636399149894714, accuracy: 0.982532751091703\n",
      "loss: 0.06310448795557022, accuracy: 0.9868995633187773\n",
      "loss: 0.06380663067102432, accuracy: 0.982532751091703\n",
      "loss: 0.06687093526124954, accuracy: 0.982532751091703\n",
      "loss: 0.0775313675403595, accuracy: 0.9781659388646288\n",
      "loss: 0.06270056962966919, accuracy: 0.982532751091703\n",
      "loss: 0.06815517693758011, accuracy: 0.982532751091703\n",
      "loss: 0.0793052464723587, accuracy: 0.9781659388646288\n",
      "loss: 0.09732933342456818, accuracy: 0.9694323144104804\n",
      "loss: 0.05900844559073448, accuracy: 0.9781659388646288\n",
      "loss: 0.06007428094744682, accuracy: 0.9868995633187773\n",
      "loss: 0.06246793642640114, accuracy: 0.9868995633187773\n",
      "loss: 0.05839315801858902, accuracy: 0.9868995633187773\n",
      "loss: 0.058353014290332794, accuracy: 0.9868995633187773\n",
      "loss: 0.05458084121346474, accuracy: 0.9912663755458515\n",
      "loss: 0.0672934502363205, accuracy: 0.982532751091703\n",
      "loss: 0.058152347803115845, accuracy: 0.9781659388646288\n",
      "loss: 0.0703837051987648, accuracy: 0.9781659388646288\n",
      "loss: 0.07119773328304291, accuracy: 0.9781659388646288\n",
      "loss: 0.06126086041331291, accuracy: 0.982532751091703\n",
      "loss: 0.06429492682218552, accuracy: 0.9868995633187773\n",
      "loss: 0.07060211151838303, accuracy: 0.982532751091703\n",
      "loss: 0.11199825257062912, accuracy: 0.9519650655021834\n",
      "loss: 0.06446153670549393, accuracy: 0.982532751091703\n",
      "loss: 0.0780918151140213, accuracy: 0.9737991266375546\n",
      "loss: 0.07350490987300873, accuracy: 0.982532751091703\n",
      "loss: 0.06657233834266663, accuracy: 0.982532751091703\n",
      "loss: 0.066649429500103, accuracy: 0.982532751091703\n",
      "loss: 0.06180107593536377, accuracy: 0.9868995633187773\n",
      "loss: 0.06776010245084763, accuracy: 0.982532751091703\n",
      "loss: 0.06745407730340958, accuracy: 0.982532751091703\n",
      "loss: 0.058566123247146606, accuracy: 0.982532751091703\n",
      "loss: 0.06174292415380478, accuracy: 0.9868995633187773\n",
      "loss: 0.06949105858802795, accuracy: 0.982532751091703\n",
      "loss: 0.0578034482896328, accuracy: 0.9868995633187773\n",
      "loss: 0.05614221841096878, accuracy: 0.9868995633187773\n",
      "loss: 0.09991604834794998, accuracy: 0.9606986899563319\n",
      "loss: 0.0667286291718483, accuracy: 0.9781659388646288\n",
      "loss: 0.0640912875533104, accuracy: 0.982532751091703\n",
      "loss: 0.06283154338598251, accuracy: 0.982532751091703\n",
      "loss: 0.06747470796108246, accuracy: 0.9868995633187773\n",
      "loss: 0.05990790203213692, accuracy: 0.982532751091703\n",
      "loss: 0.06309393793344498, accuracy: 0.9781659388646288\n",
      "loss: 0.06399910151958466, accuracy: 0.982532751091703\n",
      "loss: 0.06864763796329498, accuracy: 0.9650655021834061\n",
      "loss: 0.06727210432291031, accuracy: 0.9781659388646288\n",
      "loss: 0.0598999559879303, accuracy: 0.982532751091703\n",
      "loss: 0.06753410398960114, accuracy: 0.9781659388646288\n",
      "loss: 0.06563260406255722, accuracy: 0.982532751091703\n",
      "loss: 0.057289641350507736, accuracy: 0.9868995633187773\n",
      "loss: 0.06318990141153336, accuracy: 0.982532751091703\n",
      "loss: 0.06940895318984985, accuracy: 0.982532751091703\n",
      "loss: 0.06073959916830063, accuracy: 0.9868995633187773\n",
      "loss: 0.058959346264600754, accuracy: 0.9912663755458515\n",
      "loss: 0.06428021192550659, accuracy: 0.9781659388646288\n",
      "loss: 0.07413379848003387, accuracy: 0.982532751091703\n",
      "loss: 0.05636969581246376, accuracy: 0.9781659388646288\n",
      "loss: 0.07224240899085999, accuracy: 0.9781659388646288\n",
      "loss: 0.056687746196985245, accuracy: 0.982532751091703\n",
      "loss: 0.05855635926127434, accuracy: 0.9868995633187773\n",
      "loss: 0.05946037173271179, accuracy: 0.9912663755458515\n",
      "loss: 0.06930437684059143, accuracy: 0.9781659388646288\n",
      "loss: 0.06939344108104706, accuracy: 0.9868995633187773\n",
      "loss: 0.06476804614067078, accuracy: 0.982532751091703\n",
      "loss: 0.05962780490517616, accuracy: 0.9912663755458515\n",
      "loss: 0.06245879456400871, accuracy: 0.982532751091703\n",
      "loss: 0.05854334682226181, accuracy: 0.9781659388646288\n",
      "loss: 0.06465691328048706, accuracy: 0.9781659388646288\n",
      "loss: 0.06070787459611893, accuracy: 0.9781659388646288\n",
      "loss: 0.05867381393909454, accuracy: 0.982532751091703\n",
      "loss: 0.055696193128824234, accuracy: 0.9868995633187773\n",
      "loss: 0.05448520928621292, accuracy: 0.982532751091703\n",
      "loss: 0.05930258706212044, accuracy: 0.982532751091703\n",
      "loss: 0.06658781319856644, accuracy: 0.9868995633187773\n",
      "loss: 0.06312467902898788, accuracy: 0.982532751091703\n",
      "loss: 0.05652313679456711, accuracy: 0.9868995633187773\n",
      "loss: 0.0603710375726223, accuracy: 0.982532751091703\n",
      "loss: 0.05888572707772255, accuracy: 0.9868995633187773\n",
      "loss: 0.06790556013584137, accuracy: 0.982532751091703\n",
      "loss: 0.06894734501838684, accuracy: 0.9781659388646288\n",
      "loss: 0.08399928361177444, accuracy: 0.9737991266375546\n",
      "loss: 0.0628008246421814, accuracy: 0.9868995633187773\n",
      "loss: 0.06050180643796921, accuracy: 0.9781659388646288\n",
      "loss: 0.061062462627887726, accuracy: 0.9868995633187773\n",
      "loss: 0.057411111891269684, accuracy: 0.982532751091703\n",
      "loss: 0.0597078762948513, accuracy: 0.9868995633187773\n",
      "loss: 0.0662822499871254, accuracy: 0.9868995633187773\n",
      "loss: 0.0680735632777214, accuracy: 0.982532751091703\n",
      "loss: 0.06799056380987167, accuracy: 0.9737991266375546\n",
      "loss: 0.06265436112880707, accuracy: 0.9868995633187773\n",
      "loss: 0.06138372793793678, accuracy: 0.9868995633187773\n",
      "loss: 0.06614445149898529, accuracy: 0.9781659388646288\n",
      "loss: 0.05657709017395973, accuracy: 0.9868995633187773\n",
      "loss: 0.058825306594371796, accuracy: 0.982532751091703\n",
      "loss: 0.06080497428774834, accuracy: 0.982532751091703\n",
      "loss: 0.06917700171470642, accuracy: 0.982532751091703\n",
      "loss: 0.05613391846418381, accuracy: 0.9868995633187773\n",
      "loss: 0.05832017585635185, accuracy: 0.9781659388646288\n",
      "loss: 0.05599857494235039, accuracy: 0.982532751091703\n",
      "loss: 0.061515841633081436, accuracy: 0.9781659388646288\n",
      "loss: 0.05943545699119568, accuracy: 0.9868995633187773\n",
      "loss: 0.14344623684883118, accuracy: 0.9519650655021834\n",
      "loss: 0.07950714230537415, accuracy: 0.9737991266375546\n",
      "loss: 0.06418926268815994, accuracy: 0.9868995633187773\n",
      "loss: 0.05357621610164642, accuracy: 0.982532751091703\n",
      "loss: 0.06266637146472931, accuracy: 0.9868995633187773\n",
      "loss: 0.05415492504835129, accuracy: 0.9868995633187773\n",
      "loss: 0.060624249279499054, accuracy: 0.982532751091703\n",
      "loss: 0.05224922299385071, accuracy: 0.9912663755458515\n",
      "loss: 0.0668235495686531, accuracy: 0.9781659388646288\n",
      "loss: 0.05537806451320648, accuracy: 0.9912663755458515\n",
      "loss: 0.0621124804019928, accuracy: 0.9781659388646288\n",
      "loss: 0.05754106119275093, accuracy: 0.9868995633187773\n",
      "loss: 0.054575029760599136, accuracy: 0.9868995633187773\n",
      "loss: 0.07294635474681854, accuracy: 0.9781659388646288\n",
      "loss: 0.05600852519273758, accuracy: 0.982532751091703\n",
      "loss: 0.052595965564250946, accuracy: 0.9868995633187773\n",
      "loss: 0.050900861620903015, accuracy: 0.9912663755458515\n",
      "loss: 0.062252793461084366, accuracy: 0.9868995633187773\n",
      "loss: 0.05808795243501663, accuracy: 0.9868995633187773\n",
      "loss: 0.057896118611097336, accuracy: 0.9868995633187773\n",
      "loss: 0.060595933347940445, accuracy: 0.982532751091703\n",
      "loss: 0.06631220877170563, accuracy: 0.9737991266375546\n",
      "loss: 0.05473998188972473, accuracy: 0.9868995633187773\n",
      "loss: 0.056941911578178406, accuracy: 0.9868995633187773\n",
      "loss: 0.07540969550609589, accuracy: 0.9694323144104804\n",
      "loss: 0.05949131026864052, accuracy: 0.982532751091703\n",
      "loss: 0.056852906942367554, accuracy: 0.982532751091703\n",
      "loss: 0.05501246452331543, accuracy: 0.982532751091703\n",
      "loss: 0.0629768818616867, accuracy: 0.9781659388646288\n",
      "loss: 0.05930266156792641, accuracy: 0.982532751091703\n",
      "loss: 0.05270417034626007, accuracy: 0.9912663755458515\n",
      "loss: 0.05970544368028641, accuracy: 0.9868995633187773\n",
      "loss: 0.061630383133888245, accuracy: 0.982532751091703\n",
      "loss: 0.060467805713415146, accuracy: 0.9868995633187773\n",
      "loss: 0.053889911621809006, accuracy: 0.9868995633187773\n",
      "loss: 0.059114452451467514, accuracy: 0.9868995633187773\n",
      "loss: 0.06547611206769943, accuracy: 0.982532751091703\n",
      "loss: 0.06603378057479858, accuracy: 0.982532751091703\n",
      "loss: 0.05964343994855881, accuracy: 0.982532751091703\n",
      "loss: 0.055811554193496704, accuracy: 0.9868995633187773\n",
      "loss: 0.05595739930868149, accuracy: 0.9868995633187773\n",
      "loss: 0.057279471307992935, accuracy: 0.982532751091703\n",
      "loss: 0.05477134883403778, accuracy: 0.9868995633187773\n",
      "loss: 0.05774553492665291, accuracy: 0.9868995633187773\n",
      "loss: 0.05753235146403313, accuracy: 0.982532751091703\n",
      "loss: 0.05796530470252037, accuracy: 0.9912663755458515\n",
      "loss: 0.05503304675221443, accuracy: 0.9868995633187773\n",
      "loss: 0.056601833552122116, accuracy: 0.982532751091703\n",
      "loss: 0.05939090996980667, accuracy: 0.982532751091703\n",
      "loss: 0.062213826924562454, accuracy: 0.982532751091703\n",
      "loss: 0.05472107604146004, accuracy: 0.9868995633187773\n",
      "loss: 0.06821171939373016, accuracy: 0.9868995633187773\n",
      "loss: 0.07020807266235352, accuracy: 0.9781659388646288\n",
      "loss: 0.06829682737588882, accuracy: 0.9781659388646288\n",
      "loss: 0.05855966731905937, accuracy: 0.982532751091703\n",
      "loss: 0.055483534932136536, accuracy: 0.9868995633187773\n",
      "loss: 0.06285477429628372, accuracy: 0.982532751091703\n",
      "loss: 0.05624621734023094, accuracy: 0.9912663755458515\n",
      "loss: 0.05976591259241104, accuracy: 0.9781659388646288\n",
      "loss: 0.05832241475582123, accuracy: 0.9868995633187773\n",
      "loss: 0.0515301339328289, accuracy: 0.9868995633187773\n",
      "loss: 0.06059429794549942, accuracy: 0.982532751091703\n",
      "loss: 0.05129318684339523, accuracy: 0.9868995633187773\n",
      "loss: 0.054650865495204926, accuracy: 0.982532751091703\n",
      "loss: 0.07094012945890427, accuracy: 0.9781659388646288\n",
      "loss: 0.05115960165858269, accuracy: 0.982532751091703\n",
      "loss: 0.06244613975286484, accuracy: 0.9868995633187773\n",
      "loss: 0.053877055644989014, accuracy: 0.982532751091703\n",
      "loss: 0.0628238245844841, accuracy: 0.982532751091703\n",
      "loss: 0.0654376819729805, accuracy: 0.982532751091703\n",
      "loss: 0.05474570021033287, accuracy: 0.9868995633187773\n",
      "loss: 0.05987069755792618, accuracy: 0.982532751091703\n",
      "loss: 0.05290423706173897, accuracy: 0.9912663755458515\n",
      "loss: 0.055303458124399185, accuracy: 0.982532751091703\n",
      "loss: 0.06110289320349693, accuracy: 0.982532751091703\n",
      "loss: 0.06347541511058807, accuracy: 0.9868995633187773\n",
      "loss: 0.06299959868192673, accuracy: 0.982532751091703\n",
      "loss: 0.050136249512434006, accuracy: 0.9868995633187773\n",
      "loss: 0.06348351389169693, accuracy: 0.982532751091703\n",
      "loss: 0.06087387725710869, accuracy: 0.982532751091703\n",
      "loss: 0.057688288390636444, accuracy: 0.982532751091703\n",
      "loss: 0.059374045580625534, accuracy: 0.982532751091703\n",
      "loss: 0.06492850929498672, accuracy: 0.9781659388646288\n",
      "loss: 0.05803513526916504, accuracy: 0.982532751091703\n",
      "loss: 0.05061598867177963, accuracy: 0.9868995633187773\n",
      "loss: 0.055605679750442505, accuracy: 0.9868995633187773\n",
      "loss: 0.050433721393346786, accuracy: 0.9868995633187773\n",
      "loss: 0.048952993005514145, accuracy: 0.9912663755458515\n",
      "loss: 0.05556439235806465, accuracy: 0.982532751091703\n",
      "loss: 0.05248734727501869, accuracy: 0.982532751091703\n",
      "loss: 0.060894254595041275, accuracy: 0.9868995633187773\n",
      "loss: 0.06120564788579941, accuracy: 0.982532751091703\n",
      "loss: 0.06058724597096443, accuracy: 0.982532751091703\n",
      "loss: 0.062425531446933746, accuracy: 0.982532751091703\n",
      "loss: 0.051272597163915634, accuracy: 0.9912663755458515\n",
      "loss: 0.05507683753967285, accuracy: 0.9868995633187773\n",
      "loss: 0.05003798380494118, accuracy: 0.9868995633187773\n",
      "loss: 0.05336322262883186, accuracy: 0.982532751091703\n",
      "loss: 0.06075083464384079, accuracy: 0.982532751091703\n",
      "loss: 0.052514974027872086, accuracy: 0.9868995633187773\n",
      "loss: 0.05267906188964844, accuracy: 0.9868995633187773\n",
      "max accuracy: 0.9956331877729258\n",
      "loss: 0.04988199472427368, accuracy: 0.9956331877729258\n",
      "loss: 0.05794711783528328, accuracy: 0.9868995633187773\n",
      "loss: 0.05157189071178436, accuracy: 0.9912663755458515\n",
      "loss: 0.05500297620892525, accuracy: 0.9868995633187773\n",
      "loss: 0.05573347583413124, accuracy: 0.9868995633187773\n",
      "loss: 0.05487901717424393, accuracy: 0.982532751091703\n",
      "loss: 0.05179174989461899, accuracy: 0.982532751091703\n",
      "loss: 0.05358448997139931, accuracy: 0.982532751091703\n",
      "loss: 0.05497457832098007, accuracy: 0.9868995633187773\n",
      "loss: 0.05498998239636421, accuracy: 0.9868995633187773\n",
      "loss: 0.049751996994018555, accuracy: 0.9868995633187773\n",
      "loss: 0.052251461893320084, accuracy: 0.9868995633187773\n",
      "loss: 0.0601743720471859, accuracy: 0.982532751091703\n",
      "loss: 0.05163074657320976, accuracy: 0.9868995633187773\n",
      "loss: 0.05562066286802292, accuracy: 0.9912663755458515\n",
      "loss: 0.05249461531639099, accuracy: 0.9868995633187773\n",
      "loss: 0.050571508705616, accuracy: 0.9912663755458515\n",
      "loss: 0.05273841693997383, accuracy: 0.9912663755458515\n",
      "loss: 0.049974244087934494, accuracy: 0.9868995633187773\n",
      "loss: 0.055054306983947754, accuracy: 0.9912663755458515\n",
      "loss: 0.058108530938625336, accuracy: 0.982532751091703\n",
      "loss: 0.049150340259075165, accuracy: 0.9912663755458515\n",
      "loss: 0.054305363446474075, accuracy: 0.9868995633187773\n",
      "loss: 0.04717889800667763, accuracy: 0.9868995633187773\n",
      "loss: 0.0665285512804985, accuracy: 0.9781659388646288\n",
      "loss: 0.06200422719120979, accuracy: 0.9868995633187773\n",
      "loss: 0.05451270565390587, accuracy: 0.9868995633187773\n",
      "loss: 0.054994504898786545, accuracy: 0.982532751091703\n",
      "loss: 0.05619886890053749, accuracy: 0.982532751091703\n",
      "loss: 0.049516402184963226, accuracy: 0.9868995633187773\n",
      "loss: 0.0542435422539711, accuracy: 0.9868995633187773\n",
      "loss: 0.052451685070991516, accuracy: 0.9868995633187773\n",
      "loss: 0.05619017779827118, accuracy: 0.9868995633187773\n",
      "loss: 0.07309149205684662, accuracy: 0.9737991266375546\n",
      "loss: 0.07125607132911682, accuracy: 0.9781659388646288\n",
      "loss: 0.05081873759627342, accuracy: 0.9868995633187773\n",
      "loss: 0.05270185321569443, accuracy: 0.9868995633187773\n",
      "loss: 0.05324890837073326, accuracy: 0.9868995633187773\n",
      "loss: 0.048763178288936615, accuracy: 0.9912663755458515\n",
      "loss: 0.05738653987646103, accuracy: 0.9868995633187773\n",
      "loss: 0.04946185648441315, accuracy: 0.9868995633187773\n",
      "loss: 0.05580514669418335, accuracy: 0.9781659388646288\n",
      "loss: 0.06393012404441833, accuracy: 0.9781659388646288\n",
      "loss: 0.04979698359966278, accuracy: 0.9868995633187773\n",
      "loss: 0.061053816229104996, accuracy: 0.9781659388646288\n",
      "loss: 0.05798779055476189, accuracy: 0.9912663755458515\n",
      "loss: 0.05246277526021004, accuracy: 0.982532751091703\n",
      "loss: 0.05926364287734032, accuracy: 0.982532751091703\n",
      "loss: 0.04953071102499962, accuracy: 0.9912663755458515\n",
      "loss: 0.058539360761642456, accuracy: 0.9737991266375546\n",
      "loss: 0.05008966103196144, accuracy: 0.9912663755458515\n",
      "loss: 0.05545223504304886, accuracy: 0.9868995633187773\n",
      "loss: 0.05167577043175697, accuracy: 0.9868995633187773\n",
      "loss: 0.054944682866334915, accuracy: 0.982532751091703\n",
      "loss: 0.05044545233249664, accuracy: 0.9868995633187773\n",
      "loss: 0.0710645541548729, accuracy: 0.9781659388646288\n",
      "loss: 0.07612868398427963, accuracy: 0.9737991266375546\n",
      "loss: 0.05906473845243454, accuracy: 0.9868995633187773\n",
      "loss: 0.06143477186560631, accuracy: 0.982532751091703\n",
      "loss: 0.054511550813913345, accuracy: 0.982532751091703\n",
      "loss: 0.05140936002135277, accuracy: 0.9868995633187773\n",
      "loss: 0.049654506146907806, accuracy: 0.9868995633187773\n",
      "loss: 0.05485082417726517, accuracy: 0.982532751091703\n",
      "loss: 0.05171551927924156, accuracy: 0.9912663755458515\n",
      "loss: 0.05165303871035576, accuracy: 0.9868995633187773\n",
      "loss: 0.060553424060344696, accuracy: 0.9868995633187773\n",
      "loss: 0.05113975703716278, accuracy: 0.9868995633187773\n",
      "loss: 0.0574382022023201, accuracy: 0.982532751091703\n",
      "loss: 0.069170743227005, accuracy: 0.982532751091703\n",
      "loss: 0.059224896132946014, accuracy: 0.9868995633187773\n",
      "loss: 0.050656508654356, accuracy: 0.9868995633187773\n",
      "loss: 0.06143922358751297, accuracy: 0.9868995633187773\n",
      "loss: 0.04982232302427292, accuracy: 0.9868995633187773\n",
      "loss: 0.05784320458769798, accuracy: 0.982532751091703\n",
      "loss: 0.05218413844704628, accuracy: 0.9868995633187773\n",
      "loss: 0.05081340670585632, accuracy: 0.9868995633187773\n",
      "loss: 0.0664382353425026, accuracy: 0.9781659388646288\n",
      "loss: 0.052025292068719864, accuracy: 0.9956331877729258\n",
      "loss: 0.05185062810778618, accuracy: 0.982532751091703\n",
      "loss: 0.05852949619293213, accuracy: 0.982532751091703\n",
      "loss: 0.05880995839834213, accuracy: 0.9781659388646288\n",
      "loss: 0.051964327692985535, accuracy: 0.9868995633187773\n",
      "loss: 0.05545271188020706, accuracy: 0.9868995633187773\n",
      "loss: 0.05247316136956215, accuracy: 0.9868995633187773\n",
      "loss: 0.053313158452510834, accuracy: 0.9868995633187773\n",
      "loss: 0.05573611706495285, accuracy: 0.9912663755458515\n",
      "loss: 0.06685630977153778, accuracy: 0.9737991266375546\n",
      "loss: 0.05473402887582779, accuracy: 0.982532751091703\n",
      "loss: 0.049046024680137634, accuracy: 0.9912663755458515\n",
      "loss: 0.053598802536726, accuracy: 0.9868995633187773\n",
      "loss: 0.04932890832424164, accuracy: 0.9868995633187773\n",
      "loss: 0.051835399121046066, accuracy: 0.982532751091703\n",
      "loss: 0.05847659334540367, accuracy: 0.9868995633187773\n",
      "loss: 0.04620804265141487, accuracy: 0.9868995633187773\n",
      "loss: 0.045318275690078735, accuracy: 0.9868995633187773\n",
      "loss: 0.055052075535058975, accuracy: 0.982532751091703\n",
      "loss: 0.05012279003858566, accuracy: 0.9868995633187773\n",
      "loss: 0.04926497861742973, accuracy: 0.9912663755458515\n",
      "loss: 0.04515782371163368, accuracy: 0.9868995633187773\n",
      "loss: 0.05696671083569527, accuracy: 0.9868995633187773\n",
      "loss: 0.05164496973156929, accuracy: 0.9868995633187773\n",
      "loss: 0.050025615841150284, accuracy: 0.982532751091703\n",
      "loss: 0.04699178785085678, accuracy: 0.9868995633187773\n",
      "loss: 0.05529274046421051, accuracy: 0.9868995633187773\n",
      "loss: 0.06877096742391586, accuracy: 0.9737991266375546\n",
      "loss: 0.05277488753199577, accuracy: 0.9868995633187773\n",
      "loss: 0.04899195581674576, accuracy: 0.9912663755458515\n",
      "loss: 0.06092775613069534, accuracy: 0.9737991266375546\n",
      "loss: 0.05323714762926102, accuracy: 0.9868995633187773\n",
      "loss: 0.05413270741701126, accuracy: 0.9868995633187773\n",
      "loss: 0.06003151461482048, accuracy: 0.9781659388646288\n",
      "loss: 0.05270983651280403, accuracy: 0.9868995633187773\n",
      "loss: 0.05204561725258827, accuracy: 0.982532751091703\n",
      "loss: 0.06321084499359131, accuracy: 0.9781659388646288\n",
      "loss: 0.053312964737415314, accuracy: 0.9868995633187773\n",
      "loss: 0.048977386206388474, accuracy: 0.9912663755458515\n",
      "loss: 0.05661731958389282, accuracy: 0.9868995633187773\n",
      "loss: 0.050479333847761154, accuracy: 0.982532751091703\n",
      "loss: 0.05299505963921547, accuracy: 0.982532751091703\n",
      "loss: 0.05392463505268097, accuracy: 0.9868995633187773\n",
      "loss: 0.054791465401649475, accuracy: 0.982532751091703\n",
      "loss: 0.051956430077552795, accuracy: 0.9868995633187773\n",
      "loss: 0.054450929164886475, accuracy: 0.9868995633187773\n",
      "loss: 0.07720699161291122, accuracy: 0.9737991266375546\n",
      "loss: 0.054637301713228226, accuracy: 0.982532751091703\n",
      "loss: 0.04814932867884636, accuracy: 0.9912663755458515\n",
      "loss: 0.05542159453034401, accuracy: 0.982532751091703\n",
      "loss: 0.05417327210307121, accuracy: 0.9868995633187773\n",
      "loss: 0.06041216105222702, accuracy: 0.9868995633187773\n",
      "loss: 0.04900883510708809, accuracy: 0.982532751091703\n",
      "loss: 0.05144305154681206, accuracy: 0.982532751091703\n",
      "loss: 0.05588272586464882, accuracy: 0.982532751091703\n",
      "loss: 0.053226031363010406, accuracy: 0.9868995633187773\n",
      "loss: 0.0494278185069561, accuracy: 0.9868995633187773\n",
      "loss: 0.0449998565018177, accuracy: 0.9868995633187773\n",
      "loss: 0.06264258176088333, accuracy: 0.9781659388646288\n",
      "loss: 0.06300953030586243, accuracy: 0.982532751091703\n",
      "loss: 0.04578038305044174, accuracy: 0.9868995633187773\n",
      "loss: 0.05283935368061066, accuracy: 0.982532751091703\n",
      "loss: 0.11322439461946487, accuracy: 0.9475982532751092\n",
      "loss: 0.054562002420425415, accuracy: 0.9912663755458515\n",
      "loss: 0.04954883083701134, accuracy: 0.9868995633187773\n",
      "loss: 0.061327338218688965, accuracy: 0.9781659388646288\n",
      "loss: 0.0600476898252964, accuracy: 0.982532751091703\n",
      "loss: 0.04909232631325722, accuracy: 0.9912663755458515\n",
      "loss: 0.04682988300919533, accuracy: 0.9868995633187773\n",
      "loss: 0.05548693612217903, accuracy: 0.982532751091703\n",
      "loss: 0.05126957595348358, accuracy: 0.9868995633187773\n",
      "loss: 0.05203506350517273, accuracy: 0.982532751091703\n",
      "loss: 0.04888686165213585, accuracy: 0.9868995633187773\n",
      "loss: 0.04963560402393341, accuracy: 0.9868995633187773\n",
      "loss: 0.057534195482730865, accuracy: 0.9868995633187773\n",
      "loss: 0.051080361008644104, accuracy: 0.9912663755458515\n",
      "loss: 0.04914580658078194, accuracy: 0.9868995633187773\n",
      "loss: 0.049961790442466736, accuracy: 0.9868995633187773\n",
      "loss: 0.04864206537604332, accuracy: 0.9868995633187773\n",
      "loss: 0.05671922117471695, accuracy: 0.9868995633187773\n",
      "loss: 0.05291208252310753, accuracy: 0.9868995633187773\n",
      "loss: 0.05638740211725235, accuracy: 0.9868995633187773\n",
      "loss: 0.04578045755624771, accuracy: 0.9868995633187773\n",
      "loss: 0.044261544942855835, accuracy: 0.9956331877729258\n",
      "loss: 0.04947471246123314, accuracy: 0.9868995633187773\n",
      "loss: 0.045697201043367386, accuracy: 0.9868995633187773\n",
      "loss: 0.04753025993704796, accuracy: 0.9912663755458515\n",
      "loss: 0.047834791243076324, accuracy: 0.9912663755458515\n",
      "loss: 0.05303863808512688, accuracy: 0.982532751091703\n",
      "loss: 0.04587244614958763, accuracy: 0.9912663755458515\n",
      "loss: 0.054168835282325745, accuracy: 0.9868995633187773\n",
      "loss: 0.05665541812777519, accuracy: 0.9868995633187773\n",
      "loss: 0.05114884674549103, accuracy: 0.9868995633187773\n",
      "loss: 0.04726700112223625, accuracy: 0.982532751091703\n",
      "loss: 0.06642813980579376, accuracy: 0.9737991266375546\n",
      "loss: 0.04568280652165413, accuracy: 0.9912663755458515\n",
      "loss: 0.04873495548963547, accuracy: 0.9868995633187773\n",
      "loss: 0.04790255427360535, accuracy: 0.9912663755458515\n",
      "loss: 0.0440078042447567, accuracy: 0.9912663755458515\n",
      "loss: 0.04603150114417076, accuracy: 0.9868995633187773\n",
      "loss: 0.05164467170834541, accuracy: 0.9781659388646288\n",
      "loss: 0.04662615433335304, accuracy: 0.9912663755458515\n",
      "loss: 0.04690995812416077, accuracy: 0.9868995633187773\n",
      "loss: 0.04623790085315704, accuracy: 0.9868995633187773\n",
      "loss: 0.045074377208948135, accuracy: 0.9912663755458515\n",
      "loss: 0.055943556129932404, accuracy: 0.982532751091703\n",
      "loss: 0.045214273035526276, accuracy: 0.9868995633187773\n",
      "loss: 0.04867390915751457, accuracy: 0.982532751091703\n",
      "loss: 0.04847268387675285, accuracy: 0.9868995633187773\n",
      "loss: 0.05940970033407211, accuracy: 0.982532751091703\n",
      "loss: 0.05087735503911972, accuracy: 0.982532751091703\n",
      "loss: 0.047215841710567474, accuracy: 0.9868995633187773\n",
      "loss: 0.04524437338113785, accuracy: 0.9868995633187773\n",
      "loss: 0.05569975823163986, accuracy: 0.982532751091703\n",
      "loss: 0.05083942040801048, accuracy: 0.9868995633187773\n",
      "loss: 0.04795165732502937, accuracy: 0.9868995633187773\n",
      "loss: 0.044057972729206085, accuracy: 0.9868995633187773\n",
      "loss: 0.038023777306079865, accuracy: 0.9956331877729258\n",
      "loss: 0.049806054681539536, accuracy: 0.9868995633187773\n",
      "loss: 0.04487454891204834, accuracy: 0.9912663755458515\n",
      "loss: 0.04303194582462311, accuracy: 0.9912663755458515\n",
      "loss: 0.04286354035139084, accuracy: 0.9912663755458515\n",
      "loss: 0.04351768642663956, accuracy: 0.9912663755458515\n",
      "loss: 0.04965873807668686, accuracy: 0.982532751091703\n",
      "loss: 0.043584976345300674, accuracy: 0.9956331877729258\n",
      "loss: 0.04864291101694107, accuracy: 0.9868995633187773\n",
      "loss: 0.04872572422027588, accuracy: 0.982532751091703\n",
      "loss: 0.048626504838466644, accuracy: 0.9912663755458515\n",
      "loss: 0.04363346844911575, accuracy: 0.9868995633187773\n",
      "loss: 0.04216054826974869, accuracy: 0.9868995633187773\n",
      "loss: 0.046665750443935394, accuracy: 0.9912663755458515\n",
      "loss: 0.04592170938849449, accuracy: 0.9868995633187773\n",
      "loss: 0.043602265417575836, accuracy: 0.9912663755458515\n",
      "loss: 0.04644348472356796, accuracy: 0.982532751091703\n",
      "loss: 0.04760110378265381, accuracy: 0.982532751091703\n",
      "loss: 0.044658366590738297, accuracy: 0.9912663755458515\n",
      "loss: 0.04582268372178078, accuracy: 0.9868995633187773\n",
      "loss: 0.043359577655792236, accuracy: 0.9868995633187773\n",
      "loss: 0.04329155385494232, accuracy: 0.9868995633187773\n",
      "loss: 0.05049259588122368, accuracy: 0.982532751091703\n",
      "loss: 0.045141447335481644, accuracy: 0.9912663755458515\n",
      "loss: 0.046435024589300156, accuracy: 0.982532751091703\n",
      "loss: 0.04697524756193161, accuracy: 0.982532751091703\n",
      "loss: 0.10119488090276718, accuracy: 0.9606986899563319\n",
      "loss: 0.05051590874791145, accuracy: 0.9868995633187773\n",
      "loss: 0.05227227881550789, accuracy: 0.9868995633187773\n",
      "loss: 0.049430202692747116, accuracy: 0.9912663755458515\n",
      "loss: 0.04469975456595421, accuracy: 0.9912663755458515\n",
      "loss: 0.041401997208595276, accuracy: 0.9912663755458515\n",
      "loss: 0.057578589767217636, accuracy: 0.982532751091703\n",
      "loss: 0.07321314513683319, accuracy: 0.9781659388646288\n",
      "loss: 0.05073040723800659, accuracy: 0.9868995633187773\n",
      "loss: 0.04659058898687363, accuracy: 0.9868995633187773\n",
      "loss: 0.045264922082424164, accuracy: 0.9912663755458515\n",
      "loss: 0.04412449151277542, accuracy: 0.9912663755458515\n",
      "loss: 0.04290662333369255, accuracy: 0.9868995633187773\n",
      "loss: 0.05170557275414467, accuracy: 0.982532751091703\n",
      "loss: 0.0584123432636261, accuracy: 0.9868995633187773\n",
      "loss: 0.05319149047136307, accuracy: 0.9912663755458515\n",
      "loss: 0.04275629296898842, accuracy: 0.9912663755458515\n",
      "loss: 0.04511956498026848, accuracy: 0.9912663755458515\n",
      "loss: 0.04191048815846443, accuracy: 0.9956331877729258\n",
      "loss: 0.05388978123664856, accuracy: 0.982532751091703\n",
      "loss: 0.045863665640354156, accuracy: 0.9912663755458515\n",
      "loss: 0.044097594916820526, accuracy: 0.9912663755458515\n",
      "loss: 0.044388700276613235, accuracy: 0.9912663755458515\n",
      "loss: 0.04451228305697441, accuracy: 0.9868995633187773\n",
      "loss: 0.048058848828077316, accuracy: 0.982532751091703\n",
      "loss: 0.06396027654409409, accuracy: 0.9781659388646288\n",
      "loss: 0.04514583572745323, accuracy: 0.9912663755458515\n",
      "loss: 0.052090950310230255, accuracy: 0.9912663755458515\n",
      "loss: 0.047048285603523254, accuracy: 0.9868995633187773\n",
      "loss: 0.04439667984843254, accuracy: 0.982532751091703\n",
      "loss: 0.04805098846554756, accuracy: 0.9912663755458515\n",
      "loss: 0.042935699224472046, accuracy: 0.9912663755458515\n",
      "loss: 0.04864659905433655, accuracy: 0.982532751091703\n",
      "loss: 0.04882652312517166, accuracy: 0.9868995633187773\n",
      "loss: 0.04983760043978691, accuracy: 0.982532751091703\n",
      "loss: 0.04740121588110924, accuracy: 0.9912663755458515\n",
      "loss: 0.04235631972551346, accuracy: 0.9912663755458515\n",
      "loss: 0.043503593653440475, accuracy: 0.9868995633187773\n",
      "loss: 0.04177650809288025, accuracy: 0.9912663755458515\n",
      "loss: 0.06882559508085251, accuracy: 0.9737991266375546\n",
      "loss: 0.04331464320421219, accuracy: 0.9868995633187773\n",
      "loss: 0.0429365374147892, accuracy: 0.9868995633187773\n",
      "loss: 0.041678536683321, accuracy: 0.9868995633187773\n",
      "loss: 0.04398529976606369, accuracy: 0.9868995633187773\n",
      "loss: 0.06730533391237259, accuracy: 0.9781659388646288\n",
      "loss: 0.05048733949661255, accuracy: 0.9781659388646288\n",
      "loss: 0.048676520586013794, accuracy: 0.982532751091703\n",
      "loss: 0.042799483984708786, accuracy: 0.9912663755458515\n",
      "loss: 0.04619499295949936, accuracy: 0.9912663755458515\n",
      "loss: 0.04233478382229805, accuracy: 0.9912663755458515\n",
      "loss: 0.04032304510474205, accuracy: 0.9868995633187773\n",
      "loss: 0.0449780710041523, accuracy: 0.9868995633187773\n",
      "loss: 0.045396871864795685, accuracy: 0.9868995633187773\n",
      "loss: 0.05217300355434418, accuracy: 0.982532751091703\n",
      "loss: 0.046826187521219254, accuracy: 0.9868995633187773\n",
      "loss: 0.045512646436691284, accuracy: 0.9912663755458515\n",
      "loss: 0.048752155154943466, accuracy: 0.982532751091703\n",
      "loss: 0.04360578954219818, accuracy: 0.9912663755458515\n",
      "loss: 0.03951839730143547, accuracy: 0.9912663755458515\n",
      "loss: 0.038835760205984116, accuracy: 0.9912663755458515\n",
      "loss: 0.044707875698804855, accuracy: 0.9868995633187773\n",
      "loss: 0.046206921339035034, accuracy: 0.9868995633187773\n",
      "loss: 0.039976052939891815, accuracy: 0.9956331877729258\n",
      "loss: 0.05172256752848625, accuracy: 0.982532751091703\n",
      "loss: 0.0562954805791378, accuracy: 0.982532751091703\n",
      "loss: 0.04142168536782265, accuracy: 0.9868995633187773\n",
      "loss: 0.04109644889831543, accuracy: 0.9912663755458515\n",
      "loss: 0.04144122451543808, accuracy: 0.9912663755458515\n",
      "loss: 0.04563003033399582, accuracy: 0.9912663755458515\n",
      "loss: 0.044831424951553345, accuracy: 0.9868995633187773\n",
      "loss: 0.04560841992497444, accuracy: 0.9868995633187773\n",
      "loss: 0.04450355842709541, accuracy: 0.9868995633187773\n",
      "loss: 0.043904587626457214, accuracy: 0.982532751091703\n",
      "loss: 0.04455867409706116, accuracy: 0.9868995633187773\n",
      "loss: 0.05621955916285515, accuracy: 0.982532751091703\n",
      "loss: 0.052859559655189514, accuracy: 0.9868995633187773\n",
      "loss: 0.045367274433374405, accuracy: 0.9912663755458515\n",
      "loss: 0.04703998565673828, accuracy: 0.9868995633187773\n",
      "loss: 0.04205676168203354, accuracy: 0.9912663755458515\n",
      "loss: 0.04235633462667465, accuracy: 0.9868995633187773\n",
      "loss: 0.05132994055747986, accuracy: 0.9868995633187773\n",
      "loss: 0.05229388549923897, accuracy: 0.9868995633187773\n",
      "loss: 0.03960341960191727, accuracy: 0.9912663755458515\n",
      "loss: 0.04391960799694061, accuracy: 0.9868995633187773\n",
      "loss: 0.04479075223207474, accuracy: 0.9868995633187773\n",
      "loss: 0.04442855343222618, accuracy: 0.9912663755458515\n",
      "loss: 0.04446693882346153, accuracy: 0.982532751091703\n",
      "loss: 0.04352524131536484, accuracy: 0.9912663755458515\n",
      "loss: 0.04151246324181557, accuracy: 0.9868995633187773\n",
      "loss: 0.048237867653369904, accuracy: 0.9868995633187773\n",
      "loss: 0.045317523181438446, accuracy: 0.9912663755458515\n",
      "loss: 0.04271752014756203, accuracy: 0.9912663755458515\n",
      "loss: 0.04824978858232498, accuracy: 0.982532751091703\n",
      "loss: 0.04187971353530884, accuracy: 0.9868995633187773\n",
      "loss: 0.05083893984556198, accuracy: 0.982532751091703\n",
      "loss: 0.04622427001595497, accuracy: 0.9912663755458515\n",
      "loss: 0.047388847917318344, accuracy: 0.9912663755458515\n",
      "loss: 0.046767208725214005, accuracy: 0.9912663755458515\n",
      "loss: 0.07552700489759445, accuracy: 0.9737991266375546\n",
      "loss: 0.047118671238422394, accuracy: 0.9868995633187773\n",
      "loss: 0.04595422372221947, accuracy: 0.982532751091703\n",
      "loss: 0.04172338545322418, accuracy: 0.9868995633187773\n",
      "loss: 0.0400439091026783, accuracy: 0.9912663755458515\n",
      "loss: 0.04716631770133972, accuracy: 0.9868995633187773\n",
      "loss: 0.045648034662008286, accuracy: 0.9912663755458515\n",
      "loss: 0.04701640084385872, accuracy: 0.9912663755458515\n",
      "loss: 0.042619489133358, accuracy: 0.9868995633187773\n",
      "loss: 0.04051395505666733, accuracy: 0.9912663755458515\n",
      "loss: 0.04713728651404381, accuracy: 0.982532751091703\n",
      "loss: 0.04301365464925766, accuracy: 0.9868995633187773\n",
      "loss: 0.03997354209423065, accuracy: 0.9912663755458515\n",
      "loss: 0.04536975175142288, accuracy: 0.982532751091703\n",
      "loss: 0.04787128046154976, accuracy: 0.9868995633187773\n",
      "loss: 0.051351435482501984, accuracy: 0.9781659388646288\n",
      "loss: 0.04418206214904785, accuracy: 0.9956331877729258\n",
      "loss: 0.04676823690533638, accuracy: 0.9868995633187773\n",
      "loss: 0.05026591196656227, accuracy: 0.9868995633187773\n",
      "loss: 0.04557761922478676, accuracy: 0.9912663755458515\n",
      "loss: 0.04345504194498062, accuracy: 0.9912663755458515\n",
      "loss: 0.04192071035504341, accuracy: 0.9912663755458515\n",
      "loss: 0.039231859147548676, accuracy: 0.9912663755458515\n",
      "loss: 0.04663720726966858, accuracy: 0.9868995633187773\n",
      "loss: 0.04895121976733208, accuracy: 0.982532751091703\n",
      "loss: 0.03749800845980644, accuracy: 0.9956331877729258\n",
      "loss: 0.04841223731637001, accuracy: 0.9868995633187773\n",
      "loss: 0.04154222086071968, accuracy: 0.9912663755458515\n",
      "loss: 0.038584139198064804, accuracy: 0.9956331877729258\n",
      "loss: 0.04469719156622887, accuracy: 0.9868995633187773\n",
      "loss: 0.04147348180413246, accuracy: 0.9868995633187773\n",
      "loss: 0.04265397787094116, accuracy: 0.9868995633187773\n",
      "loss: 0.03895800933241844, accuracy: 0.9912663755458515\n",
      "loss: 0.03988844156265259, accuracy: 0.9912663755458515\n",
      "loss: 0.0471663661301136, accuracy: 0.9868995633187773\n",
      "loss: 0.039794545620679855, accuracy: 0.9912663755458515\n",
      "loss: 0.037143778055906296, accuracy: 0.9912663755458515\n",
      "loss: 0.04265124723315239, accuracy: 0.9868995633187773\n",
      "loss: 0.03997254744172096, accuracy: 0.9912663755458515\n",
      "loss: 0.045563917607069016, accuracy: 0.9912663755458515\n",
      "loss: 0.039269138127565384, accuracy: 0.9956331877729258\n",
      "loss: 0.05928098037838936, accuracy: 0.982532751091703\n",
      "loss: 0.0482257604598999, accuracy: 0.9912663755458515\n",
      "loss: 0.04738469794392586, accuracy: 0.982532751091703\n",
      "loss: 0.040411029011011124, accuracy: 0.9868995633187773\n",
      "loss: 0.03981008380651474, accuracy: 0.9912663755458515\n",
      "loss: 0.04154055938124657, accuracy: 0.9868995633187773\n",
      "loss: 0.04250717535614967, accuracy: 0.9912663755458515\n",
      "loss: 0.03985437750816345, accuracy: 0.9912663755458515\n",
      "loss: 0.04273470863699913, accuracy: 0.9912663755458515\n",
      "loss: 0.04062400385737419, accuracy: 0.9868995633187773\n",
      "loss: 0.038075078278779984, accuracy: 0.9956331877729258\n",
      "loss: 0.04646879807114601, accuracy: 0.982532751091703\n",
      "loss: 0.04099743068218231, accuracy: 0.9912663755458515\n",
      "loss: 0.03928187116980553, accuracy: 0.9912663755458515\n",
      "loss: 0.03960764408111572, accuracy: 0.9912663755458515\n",
      "loss: 0.04191896319389343, accuracy: 0.982532751091703\n",
      "loss: 0.03731909766793251, accuracy: 0.9912663755458515\n",
      "loss: 0.04081239551305771, accuracy: 0.9868995633187773\n",
      "loss: 0.04036227986216545, accuracy: 0.9868995633187773\n",
      "loss: 0.038573626428842545, accuracy: 0.9868995633187773\n",
      "loss: 0.042825985699892044, accuracy: 0.9868995633187773\n",
      "loss: 0.04308627173304558, accuracy: 0.9868995633187773\n",
      "loss: 0.04328259080648422, accuracy: 0.9912663755458515\n",
      "loss: 0.03874784708023071, accuracy: 0.9912663755458515\n",
      "loss: 0.04068006947636604, accuracy: 0.9956331877729258\n",
      "loss: 0.042236801236867905, accuracy: 0.982532751091703\n",
      "loss: 0.03776611015200615, accuracy: 0.9956331877729258\n",
      "loss: 0.04178605228662491, accuracy: 0.9912663755458515\n",
      "loss: 0.039381105452775955, accuracy: 0.9912663755458515\n",
      "loss: 0.04049116373062134, accuracy: 0.9912663755458515\n",
      "loss: 0.03951610252261162, accuracy: 0.9912663755458515\n",
      "loss: 0.039373673498630524, accuracy: 0.9868995633187773\n",
      "loss: 0.040810827165842056, accuracy: 0.9868995633187773\n",
      "loss: 0.043366093188524246, accuracy: 0.982532751091703\n",
      "loss: 0.05109351873397827, accuracy: 0.9912663755458515\n",
      "loss: 0.041649214923381805, accuracy: 0.9912663755458515\n",
      "loss: 0.03993764892220497, accuracy: 0.9912663755458515\n",
      "loss: 0.034376878291368484, accuracy: 0.9956331877729258\n",
      "loss: 0.03792143240571022, accuracy: 0.9956331877729258\n",
      "loss: 0.044014155864715576, accuracy: 0.9868995633187773\n",
      "loss: 0.0471544973552227, accuracy: 0.9868995633187773\n",
      "loss: 0.05122333765029907, accuracy: 0.982532751091703\n",
      "loss: 0.03732749819755554, accuracy: 0.9912663755458515\n",
      "loss: 0.043196067214012146, accuracy: 0.9868995633187773\n",
      "loss: 0.034710630774497986, accuracy: 0.9956331877729258\n",
      "loss: 0.03984697908163071, accuracy: 0.9956331877729258\n",
      "loss: 0.03908422961831093, accuracy: 0.9868995633187773\n",
      "loss: 0.03602982684969902, accuracy: 0.9912663755458515\n",
      "loss: 0.04112222045660019, accuracy: 0.9868995633187773\n",
      "loss: 0.04574928432703018, accuracy: 0.9868995633187773\n",
      "loss: 0.04225577041506767, accuracy: 0.9868995633187773\n",
      "loss: 0.045268964022397995, accuracy: 0.982532751091703\n",
      "loss: 0.037294331938028336, accuracy: 0.9912663755458515\n",
      "loss: 0.04116535186767578, accuracy: 0.9868995633187773\n",
      "loss: 0.039701420813798904, accuracy: 0.9912663755458515\n",
      "loss: 0.0442633181810379, accuracy: 0.9912663755458515\n",
      "loss: 0.04532532021403313, accuracy: 0.9868995633187773\n",
      "loss: 0.040888670831918716, accuracy: 0.9912663755458515\n",
      "loss: 0.03955826908349991, accuracy: 0.9868995633187773\n",
      "loss: 0.03836355358362198, accuracy: 0.9912663755458515\n",
      "loss: 0.03881092742085457, accuracy: 0.9956331877729258\n",
      "loss: 0.04034893959760666, accuracy: 0.9956331877729258\n",
      "loss: 0.03800490126013756, accuracy: 0.9912663755458515\n",
      "loss: 0.041723184287548065, accuracy: 0.9868995633187773\n",
      "loss: 0.03853275999426842, accuracy: 0.9868995633187773\n",
      "loss: 0.04897598177194595, accuracy: 0.982532751091703\n",
      "loss: 0.04130569100379944, accuracy: 0.9868995633187773\n",
      "loss: 0.04481878504157066, accuracy: 0.9781659388646288\n",
      "loss: 0.04455133527517319, accuracy: 0.9912663755458515\n",
      "loss: 0.03715464472770691, accuracy: 0.9912663755458515\n",
      "loss: 0.04301522299647331, accuracy: 0.9868995633187773\n",
      "loss: 0.03625091165304184, accuracy: 0.9912663755458515\n",
      "loss: 0.035538144409656525, accuracy: 0.9956331877729258\n",
      "loss: 0.03708022087812424, accuracy: 0.9912663755458515\n",
      "loss: 0.03926524147391319, accuracy: 0.9868995633187773\n",
      "loss: 0.0403665155172348, accuracy: 0.9912663755458515\n",
      "loss: 0.03755202144384384, accuracy: 0.9956331877729258\n",
      "loss: 0.04088853672146797, accuracy: 0.9868995633187773\n",
      "loss: 0.03899375721812248, accuracy: 0.9912663755458515\n",
      "loss: 0.039377085864543915, accuracy: 0.9912663755458515\n",
      "loss: 0.05096883699297905, accuracy: 0.982532751091703\n",
      "loss: 0.03899578005075455, accuracy: 0.9868995633187773\n",
      "loss: 0.046723783016204834, accuracy: 0.982532751091703\n",
      "loss: 0.03864940255880356, accuracy: 0.9912663755458515\n",
      "loss: 0.04134213551878929, accuracy: 0.9912663755458515\n",
      "loss: 0.04663696512579918, accuracy: 0.9912663755458515\n",
      "loss: 0.044126782566308975, accuracy: 0.9868995633187773\n",
      "loss: 0.050127819180488586, accuracy: 0.9912663755458515\n",
      "loss: 0.03743359073996544, accuracy: 0.9868995633187773\n",
      "loss: 0.03682802617549896, accuracy: 0.9868995633187773\n",
      "loss: 0.0400441437959671, accuracy: 0.9912663755458515\n",
      "loss: 0.04191475361585617, accuracy: 0.9912663755458515\n",
      "loss: 0.03979339078068733, accuracy: 0.9912663755458515\n",
      "loss: 0.038289207965135574, accuracy: 0.9912663755458515\n",
      "loss: 0.03957004472613335, accuracy: 0.9912663755458515\n",
      "loss: 0.03618194907903671, accuracy: 0.9956331877729258\n",
      "loss: 0.04317657649517059, accuracy: 0.9912663755458515\n",
      "loss: 0.03776278719305992, accuracy: 0.9912663755458515\n",
      "loss: 0.03703967109322548, accuracy: 0.9912663755458515\n",
      "loss: 0.041459567844867706, accuracy: 0.9868995633187773\n",
      "loss: 0.0352046974003315, accuracy: 0.9912663755458515\n",
      "loss: 0.041910357773303986, accuracy: 0.9912663755458515\n",
      "loss: 0.03739104047417641, accuracy: 0.9912663755458515\n",
      "loss: 0.0367036834359169, accuracy: 0.9912663755458515\n",
      "loss: 0.03836667165160179, accuracy: 0.9868995633187773\n",
      "loss: 0.03763117641210556, accuracy: 0.9868995633187773\n",
      "loss: 0.03608688712120056, accuracy: 0.9912663755458515\n",
      "loss: 0.04116383567452431, accuracy: 0.982532751091703\n",
      "loss: 0.05286931246519089, accuracy: 0.9868995633187773\n",
      "loss: 0.03875952214002609, accuracy: 0.9868995633187773\n",
      "loss: 0.039286039769649506, accuracy: 0.9956331877729258\n",
      "loss: 0.0376521535217762, accuracy: 0.9912663755458515\n",
      "loss: 0.03799925744533539, accuracy: 0.9868995633187773\n",
      "loss: 0.038940586149692535, accuracy: 0.9912663755458515\n",
      "loss: 0.042860161513090134, accuracy: 0.9868995633187773\n",
      "loss: 0.04195041209459305, accuracy: 0.982532751091703\n",
      "loss: 0.03936900943517685, accuracy: 0.9912663755458515\n",
      "loss: 0.040205635130405426, accuracy: 0.9868995633187773\n",
      "loss: 0.04033257067203522, accuracy: 0.9912663755458515\n",
      "loss: 0.03737154230475426, accuracy: 0.9912663755458515\n",
      "loss: 0.03575337678194046, accuracy: 0.9956331877729258\n",
      "loss: 0.03430429473519325, accuracy: 0.9912663755458515\n",
      "loss: 0.037073757499456406, accuracy: 0.9868995633187773\n",
      "loss: 0.03973573073744774, accuracy: 0.9868995633187773\n",
      "loss: 0.03731663525104523, accuracy: 0.9912663755458515\n",
      "loss: 0.08715608716011047, accuracy: 0.9606986899563319\n",
      "loss: 0.26313814520835876, accuracy: 0.9213973799126638\n",
      "loss: 0.055186305195093155, accuracy: 0.9868995633187773\n",
      "loss: 0.04210304096341133, accuracy: 0.9912663755458515\n",
      "loss: 0.0388757660984993, accuracy: 0.9912663755458515\n",
      "loss: 0.03917970880866051, accuracy: 0.9868995633187773\n",
      "loss: 0.0354478545486927, accuracy: 0.9956331877729258\n",
      "loss: 0.04015835374593735, accuracy: 0.9912663755458515\n",
      "loss: 0.039079952985048294, accuracy: 0.9912663755458515\n",
      "loss: 0.03744763508439064, accuracy: 0.9912663755458515\n",
      "loss: 0.03588184341788292, accuracy: 0.9956331877729258\n",
      "loss: 0.043164074420928955, accuracy: 0.982532751091703\n",
      "loss: 0.03692016750574112, accuracy: 0.9956331877729258\n",
      "loss: 0.03934157267212868, accuracy: 0.9868995633187773\n",
      "loss: 0.035889994353055954, accuracy: 0.9956331877729258\n",
      "loss: 0.036688968539237976, accuracy: 0.9912663755458515\n",
      "loss: 0.03952553868293762, accuracy: 0.9912663755458515\n",
      "loss: 0.045221950858831406, accuracy: 0.9912663755458515\n",
      "loss: 0.036310676485300064, accuracy: 0.9912663755458515\n",
      "loss: 0.03733232989907265, accuracy: 0.9912663755458515\n",
      "loss: 0.04111523553729057, accuracy: 0.9912663755458515\n",
      "loss: 0.035497765988111496, accuracy: 0.9956331877729258\n",
      "loss: 0.03706231713294983, accuracy: 0.9912663755458515\n",
      "loss: 0.09325526654720306, accuracy: 0.9650655021834061\n",
      "loss: 0.04316297918558121, accuracy: 0.9912663755458515\n",
      "loss: 0.039015453308820724, accuracy: 0.9956331877729258\n",
      "loss: 0.04664851725101471, accuracy: 0.9912663755458515\n",
      "loss: 0.03728819638490677, accuracy: 0.9912663755458515\n",
      "loss: 0.039619117975234985, accuracy: 0.982532751091703\n",
      "loss: 0.0412314273416996, accuracy: 0.9868995633187773\n",
      "loss: 0.047420818358659744, accuracy: 0.9868995633187773\n",
      "loss: 0.03592436760663986, accuracy: 0.9956331877729258\n",
      "loss: 0.03368980810046196, accuracy: 0.9912663755458515\n",
      "loss: 0.05251641944050789, accuracy: 0.9868995633187773\n",
      "loss: 0.036309123039245605, accuracy: 0.9912663755458515\n",
      "loss: 0.038647353649139404, accuracy: 0.9868995633187773\n",
      "loss: 0.03403268754482269, accuracy: 0.9912663755458515\n",
      "loss: 0.034823980182409286, accuracy: 0.9956331877729258\n",
      "loss: 0.04178003966808319, accuracy: 0.9956331877729258\n",
      "loss: 0.03559645265340805, accuracy: 0.9912663755458515\n",
      "loss: 0.03723054751753807, accuracy: 0.9868995633187773\n",
      "loss: 0.03616538271307945, accuracy: 0.9912663755458515\n",
      "loss: 0.04037730023264885, accuracy: 0.9956331877729258\n",
      "loss: 0.040597595274448395, accuracy: 0.982532751091703\n",
      "loss: 0.03307504206895828, accuracy: 0.9912663755458515\n",
      "loss: 0.03853214532136917, accuracy: 0.9868995633187773\n",
      "loss: 0.03577461093664169, accuracy: 0.9912663755458515\n",
      "loss: 0.03260112181305885, accuracy: 0.9956331877729258\n",
      "loss: 0.03471948206424713, accuracy: 0.9912663755458515\n",
      "loss: 0.03351932391524315, accuracy: 0.9912663755458515\n",
      "loss: 0.051526352763175964, accuracy: 0.982532751091703\n",
      "loss: 0.04139120876789093, accuracy: 0.9912663755458515\n",
      "loss: 0.03808494657278061, accuracy: 0.9912663755458515\n",
      "loss: 0.037949591875076294, accuracy: 0.9868995633187773\n",
      "loss: 0.03877045586705208, accuracy: 0.9868995633187773\n",
      "loss: 0.034543611109256744, accuracy: 0.9912663755458515\n",
      "loss: 0.037279561161994934, accuracy: 0.9956331877729258\n",
      "loss: 0.03565859794616699, accuracy: 0.9912663755458515\n",
      "loss: 0.03517995774745941, accuracy: 0.9956331877729258\n",
      "loss: 0.034563444554805756, accuracy: 0.9956331877729258\n",
      "loss: 0.03573865815997124, accuracy: 0.9868995633187773\n",
      "loss: 0.03812086954712868, accuracy: 0.982532751091703\n",
      "loss: 0.037237174808979034, accuracy: 0.9912663755458515\n",
      "loss: 0.0377366840839386, accuracy: 0.9912663755458515\n",
      "loss: 0.03426823019981384, accuracy: 0.9912663755458515\n",
      "loss: 0.04300861433148384, accuracy: 0.9868995633187773\n",
      "loss: 0.039615072309970856, accuracy: 0.9868995633187773\n",
      "loss: 0.037807565182447433, accuracy: 0.9912663755458515\n",
      "loss: 0.033685460686683655, accuracy: 0.9956331877729258\n",
      "loss: 0.03577325493097305, accuracy: 0.9912663755458515\n",
      "loss: 0.03851253539323807, accuracy: 0.9956331877729258\n",
      "loss: 0.03759017586708069, accuracy: 0.9868995633187773\n",
      "loss: 0.036477793008089066, accuracy: 0.9956331877729258\n",
      "loss: 0.033846378326416016, accuracy: 0.9912663755458515\n",
      "loss: 0.03730502352118492, accuracy: 0.9912663755458515\n",
      "loss: 0.0440700463950634, accuracy: 0.9912663755458515\n",
      "loss: 0.034843090921640396, accuracy: 0.9956331877729258\n",
      "loss: 0.032536618411540985, accuracy: 0.9956331877729258\n",
      "loss: 0.03419721499085426, accuracy: 0.9956331877729258\n",
      "loss: 0.03448870778083801, accuracy: 0.9868995633187773\n",
      "loss: 0.03167266026139259, accuracy: 0.9956331877729258\n",
      "loss: 0.03236475959420204, accuracy: 0.9956331877729258\n",
      "loss: 0.03759963810443878, accuracy: 0.9912663755458515\n",
      "loss: 0.04405366629362106, accuracy: 0.9868995633187773\n",
      "loss: 0.03708828613162041, accuracy: 0.9912663755458515\n",
      "loss: 0.034949351102113724, accuracy: 0.9956331877729258\n",
      "loss: 0.03957236558198929, accuracy: 0.982532751091703\n",
      "loss: 0.033010996878147125, accuracy: 0.9912663755458515\n",
      "loss: 0.035673145204782486, accuracy: 0.9912663755458515\n",
      "loss: 0.03583242744207382, accuracy: 0.9912663755458515\n",
      "loss: 0.03415390104055405, accuracy: 0.9868995633187773\n",
      "loss: 0.03644056245684624, accuracy: 0.9912663755458515\n",
      "loss: 0.03229885548353195, accuracy: 0.9868995633187773\n",
      "loss: 0.039613787084817886, accuracy: 0.9868995633187773\n",
      "loss: 0.03516042232513428, accuracy: 0.9912663755458515\n",
      "loss: 0.03443443402647972, accuracy: 0.9868995633187773\n",
      "loss: 0.03684210404753685, accuracy: 0.9868995633187773\n",
      "loss: 0.03255259245634079, accuracy: 0.9956331877729258\n",
      "loss: 0.041401203721761703, accuracy: 0.9868995633187773\n",
      "loss: 0.035460010170936584, accuracy: 0.9956331877729258\n",
      "loss: 0.040431372821331024, accuracy: 0.9912663755458515\n",
      "loss: 0.03917555883526802, accuracy: 0.9912663755458515\n",
      "loss: 0.03384541720151901, accuracy: 0.9956331877729258\n",
      "loss: 0.03667354956269264, accuracy: 0.9912663755458515\n",
      "loss: 0.033844273537397385, accuracy: 0.9912663755458515\n",
      "loss: 0.03681013733148575, accuracy: 0.9912663755458515\n",
      "loss: 0.035153064876794815, accuracy: 0.9912663755458515\n",
      "loss: 0.036840394139289856, accuracy: 0.9868995633187773\n",
      "loss: 0.038147617131471634, accuracy: 0.9912663755458515\n",
      "loss: 0.04015682265162468, accuracy: 0.9912663755458515\n",
      "loss: 0.04039433225989342, accuracy: 0.9912663755458515\n",
      "loss: 0.0385797843337059, accuracy: 0.9956331877729258\n",
      "loss: 0.034602608531713486, accuracy: 0.9912663755458515\n",
      "loss: 0.033994000405073166, accuracy: 0.9956331877729258\n",
      "loss: 0.032538410276174545, accuracy: 0.9956331877729258\n",
      "loss: 0.03432990238070488, accuracy: 0.9912663755458515\n",
      "loss: 0.04149249568581581, accuracy: 0.9912663755458515\n",
      "loss: 0.03647678345441818, accuracy: 0.9868995633187773\n",
      "loss: 0.03471555933356285, accuracy: 0.9912663755458515\n",
      "loss: 0.030331548303365707, accuracy: 0.9912663755458515\n",
      "loss: 0.03539774566888809, accuracy: 0.9956331877729258\n",
      "loss: 0.036723095923662186, accuracy: 0.9912663755458515\n",
      "loss: 0.03713565319776535, accuracy: 0.9956331877729258\n",
      "loss: 0.034166738390922546, accuracy: 0.9912663755458515\n",
      "loss: 0.03389706835150719, accuracy: 0.9912663755458515\n",
      "loss: 0.03332902118563652, accuracy: 0.9912663755458515\n",
      "loss: 0.03071219101548195, accuracy: 0.9912663755458515\n",
      "max accuracy: 1.0\n",
      "loss: 0.03040120005607605, accuracy: 1.0\n",
      "loss: 0.03382685035467148, accuracy: 0.9912663755458515\n",
      "loss: 0.03396361321210861, accuracy: 0.9912663755458515\n",
      "loss: 0.03367622196674347, accuracy: 0.9912663755458515\n",
      "loss: 0.032349806278944016, accuracy: 0.9956331877729258\n",
      "loss: 0.03535120189189911, accuracy: 0.9868995633187773\n",
      "loss: 0.04097382724285126, accuracy: 0.9868995633187773\n",
      "loss: 0.03289920091629028, accuracy: 1.0\n",
      "loss: 0.04918603599071503, accuracy: 0.9781659388646288\n",
      "loss: 0.038627948611974716, accuracy: 0.9912663755458515\n",
      "loss: 0.040297962725162506, accuracy: 0.9868995633187773\n",
      "loss: 0.034359801560640335, accuracy: 0.9956331877729258\n",
      "loss: 0.03758036345243454, accuracy: 0.9912663755458515\n",
      "loss: 0.02982589602470398, accuracy: 0.9956331877729258\n",
      "loss: 0.0361168272793293, accuracy: 0.9912663755458515\n",
      "loss: 0.03306983411312103, accuracy: 0.9956331877729258\n",
      "loss: 0.02917809598147869, accuracy: 1.0\n",
      "loss: 0.05086842179298401, accuracy: 0.9912663755458515\n",
      "loss: 0.03463464975357056, accuracy: 0.9912663755458515\n",
      "loss: 0.04183058813214302, accuracy: 0.9868995633187773\n",
      "loss: 0.036212027072906494, accuracy: 0.9912663755458515\n",
      "loss: 0.03880493715405464, accuracy: 0.9912663755458515\n",
      "loss: 0.034111637622117996, accuracy: 0.9956331877729258\n",
      "loss: 0.03330768272280693, accuracy: 0.9912663755458515\n",
      "loss: 0.031651824712753296, accuracy: 0.9868995633187773\n",
      "loss: 0.03583308681845665, accuracy: 0.9956331877729258\n",
      "loss: 0.03505636006593704, accuracy: 0.9868995633187773\n",
      "loss: 0.03571000322699547, accuracy: 0.9912663755458515\n",
      "loss: 0.03330663964152336, accuracy: 1.0\n",
      "loss: 0.036789849400520325, accuracy: 0.9956331877729258\n",
      "loss: 0.0451219268143177, accuracy: 0.982532751091703\n",
      "loss: 0.033712223172187805, accuracy: 0.9912663755458515\n",
      "loss: 0.030191102996468544, accuracy: 1.0\n",
      "loss: 0.033582188189029694, accuracy: 0.9912663755458515\n",
      "loss: 0.03462604060769081, accuracy: 0.9912663755458515\n",
      "loss: 0.03182592988014221, accuracy: 0.9956331877729258\n",
      "loss: 0.038677264004945755, accuracy: 0.9868995633187773\n",
      "loss: 0.04757463186979294, accuracy: 0.9868995633187773\n",
      "loss: 0.036947306245565414, accuracy: 0.9956331877729258\n",
      "loss: 0.03439792990684509, accuracy: 0.9956331877729258\n",
      "loss: 0.03196936100721359, accuracy: 0.9956331877729258\n",
      "loss: 0.029948513954877853, accuracy: 0.9956331877729258\n",
      "loss: 0.03239816054701805, accuracy: 0.9912663755458515\n",
      "loss: 0.032567113637924194, accuracy: 0.9956331877729258\n",
      "loss: 0.03199338540434837, accuracy: 1.0\n",
      "loss: 0.0347217358648777, accuracy: 0.9912663755458515\n",
      "loss: 0.040735505521297455, accuracy: 0.9956331877729258\n",
      "loss: 0.038470376282930374, accuracy: 0.9912663755458515\n",
      "loss: 0.03698478639125824, accuracy: 0.9956331877729258\n",
      "loss: 0.032120734453201294, accuracy: 0.9956331877729258\n",
      "loss: 0.030952811241149902, accuracy: 0.9912663755458515\n",
      "loss: 0.03184545785188675, accuracy: 0.9912663755458515\n",
      "loss: 0.03185609355568886, accuracy: 0.9912663755458515\n",
      "loss: 0.03054650127887726, accuracy: 0.9956331877729258\n",
      "loss: 0.03160657733678818, accuracy: 0.9956331877729258\n",
      "loss: 0.032341260462999344, accuracy: 0.9912663755458515\n",
      "loss: 0.03566242381930351, accuracy: 0.9956331877729258\n",
      "loss: 0.03317667916417122, accuracy: 0.9956331877729258\n",
      "loss: 0.0297648124396801, accuracy: 0.9912663755458515\n",
      "loss: 0.03578231856226921, accuracy: 0.9912663755458515\n",
      "loss: 0.03290591388940811, accuracy: 0.9956331877729258\n",
      "loss: 0.03713398426771164, accuracy: 0.9956331877729258\n",
      "loss: 0.029695622622966766, accuracy: 0.9956331877729258\n",
      "loss: 0.03318649157881737, accuracy: 0.9912663755458515\n",
      "loss: 0.033570338040590286, accuracy: 0.9956331877729258\n",
      "loss: 0.030038956552743912, accuracy: 0.9912663755458515\n",
      "loss: 0.03551216796040535, accuracy: 0.9912663755458515\n",
      "loss: 0.031182989478111267, accuracy: 0.9912663755458515\n",
      "loss: 0.0318637453019619, accuracy: 0.9912663755458515\n",
      "loss: 0.03292267024517059, accuracy: 0.9956331877729258\n",
      "loss: 0.028948193415999413, accuracy: 1.0\n",
      "loss: 0.031030988320708275, accuracy: 0.9912663755458515\n",
      "loss: 0.030245712026953697, accuracy: 0.9912663755458515\n",
      "loss: 0.031443193554878235, accuracy: 0.9912663755458515\n",
      "loss: 0.03153698518872261, accuracy: 0.9912663755458515\n",
      "loss: 0.03055117093026638, accuracy: 0.9912663755458515\n",
      "loss: 0.03004448488354683, accuracy: 0.9912663755458515\n",
      "loss: 0.03076743707060814, accuracy: 1.0\n",
      "loss: 0.03315137326717377, accuracy: 0.9912663755458515\n",
      "loss: 0.031467799097299576, accuracy: 0.9956331877729258\n",
      "loss: 0.029525132849812508, accuracy: 0.9912663755458515\n",
      "loss: 0.0373365692794323, accuracy: 0.9868995633187773\n",
      "loss: 0.033073123544454575, accuracy: 0.9912663755458515\n",
      "loss: 0.03398500755429268, accuracy: 0.9956331877729258\n",
      "loss: 0.03752509132027626, accuracy: 0.9868995633187773\n",
      "loss: 0.03308301046490669, accuracy: 0.9912663755458515\n",
      "loss: 0.027413737028837204, accuracy: 0.9956331877729258\n",
      "loss: 0.036790650337934494, accuracy: 0.9956331877729258\n",
      "loss: 0.029614776372909546, accuracy: 0.9956331877729258\n",
      "loss: 0.03135812282562256, accuracy: 0.9956331877729258\n",
      "loss: 0.029095998033881187, accuracy: 0.9912663755458515\n",
      "loss: 0.0325654074549675, accuracy: 0.9912663755458515\n",
      "loss: 0.03169103339314461, accuracy: 0.9912663755458515\n",
      "loss: 0.03274586424231529, accuracy: 0.9868995633187773\n",
      "loss: 0.034092120826244354, accuracy: 0.9868995633187773\n",
      "loss: 0.03298775106668472, accuracy: 0.9912663755458515\n",
      "loss: 0.031853534281253815, accuracy: 0.9956331877729258\n",
      "loss: 0.035123199224472046, accuracy: 0.9912663755458515\n",
      "loss: 0.027943773195147514, accuracy: 1.0\n",
      "loss: 0.03129329904913902, accuracy: 1.0\n",
      "loss: 0.031143508851528168, accuracy: 0.9912663755458515\n",
      "loss: 0.027256792411208153, accuracy: 1.0\n",
      "loss: 0.030553383752703667, accuracy: 0.9956331877729258\n",
      "loss: 0.03847203031182289, accuracy: 0.9868995633187773\n",
      "loss: 0.03153764083981514, accuracy: 0.9956331877729258\n",
      "loss: 0.03656209260225296, accuracy: 0.9912663755458515\n",
      "loss: 0.031922291964292526, accuracy: 0.9956331877729258\n",
      "loss: 0.029483066871762276, accuracy: 1.0\n",
      "loss: 0.030083104968070984, accuracy: 0.9956331877729258\n",
      "loss: 0.030129114165902138, accuracy: 0.9912663755458515\n",
      "loss: 0.029938513413071632, accuracy: 0.9956331877729258\n",
      "loss: 0.02989809401333332, accuracy: 0.9956331877729258\n",
      "loss: 0.028276605531573296, accuracy: 0.9956331877729258\n",
      "loss: 0.034235939383506775, accuracy: 0.9912663755458515\n",
      "loss: 0.029911182820796967, accuracy: 0.9956331877729258\n",
      "loss: 0.029007088392972946, accuracy: 0.9912663755458515\n",
      "loss: 0.03360147029161453, accuracy: 0.9912663755458515\n",
      "loss: 0.03491000458598137, accuracy: 0.9912663755458515\n",
      "loss: 0.030615834519267082, accuracy: 0.9912663755458515\n",
      "loss: 0.0353676900267601, accuracy: 0.9956331877729258\n",
      "loss: 0.03118429146707058, accuracy: 0.9912663755458515\n",
      "loss: 0.03241655230522156, accuracy: 0.9912663755458515\n",
      "loss: 0.03172463923692703, accuracy: 0.9912663755458515\n",
      "loss: 0.03147222846746445, accuracy: 0.9956331877729258\n",
      "loss: 0.0345853790640831, accuracy: 0.9912663755458515\n",
      "loss: 0.029015960171818733, accuracy: 1.0\n",
      "loss: 0.031177876517176628, accuracy: 0.9912663755458515\n",
      "loss: 0.03244331479072571, accuracy: 0.9956331877729258\n",
      "loss: 0.032352276146411896, accuracy: 0.9912663755458515\n",
      "loss: 0.03320910781621933, accuracy: 0.9956331877729258\n",
      "loss: 0.02712724171578884, accuracy: 0.9956331877729258\n",
      "loss: 0.029316788539290428, accuracy: 0.9956331877729258\n",
      "loss: 0.03310133516788483, accuracy: 0.9912663755458515\n",
      "loss: 0.03039879910647869, accuracy: 0.9956331877729258\n",
      "loss: 0.033526577055454254, accuracy: 0.9956331877729258\n",
      "loss: 0.03328896686434746, accuracy: 0.9912663755458515\n",
      "loss: 0.0300536397844553, accuracy: 0.9956331877729258\n",
      "loss: 0.029942363500595093, accuracy: 0.9956331877729258\n",
      "loss: 0.029950212687253952, accuracy: 0.9912663755458515\n",
      "loss: 0.034406933933496475, accuracy: 0.9912663755458515\n",
      "loss: 0.032995209097862244, accuracy: 0.9956331877729258\n",
      "loss: 0.02989579364657402, accuracy: 0.9912663755458515\n",
      "loss: 0.032768815755844116, accuracy: 0.9912663755458515\n",
      "loss: 0.02911117486655712, accuracy: 0.9956331877729258\n",
      "loss: 0.03218032047152519, accuracy: 0.9912663755458515\n",
      "loss: 0.033818408846855164, accuracy: 0.9868995633187773\n",
      "loss: 0.02920166589319706, accuracy: 0.9956331877729258\n",
      "loss: 0.03110010176897049, accuracy: 1.0\n",
      "loss: 0.02931058593094349, accuracy: 0.9956331877729258\n",
      "loss: 0.032819945365190506, accuracy: 0.9912663755458515\n",
      "loss: 0.03210623189806938, accuracy: 0.9956331877729258\n",
      "loss: 0.029565982520580292, accuracy: 0.9912663755458515\n",
      "loss: 0.03365027531981468, accuracy: 0.9868995633187773\n",
      "loss: 0.03036937303841114, accuracy: 0.9956331877729258\n",
      "loss: 0.02518417313694954, accuracy: 1.0\n",
      "loss: 0.029912173748016357, accuracy: 0.9912663755458515\n",
      "loss: 0.028114182874560356, accuracy: 1.0\n",
      "loss: 0.03392576426267624, accuracy: 0.9912663755458515\n",
      "loss: 0.03194309398531914, accuracy: 0.9912663755458515\n",
      "loss: 0.03098434768617153, accuracy: 0.9912663755458515\n",
      "loss: 0.03034493327140808, accuracy: 0.9956331877729258\n",
      "loss: 0.032874464988708496, accuracy: 0.9912663755458515\n",
      "loss: 0.02987680211663246, accuracy: 0.9956331877729258\n",
      "loss: 0.028787847608327866, accuracy: 0.9956331877729258\n",
      "loss: 0.02907130867242813, accuracy: 0.9912663755458515\n",
      "loss: 0.026119619607925415, accuracy: 0.9956331877729258\n",
      "loss: 0.0344512052834034, accuracy: 0.9868995633187773\n",
      "loss: 0.03174076974391937, accuracy: 0.9956331877729258\n",
      "loss: 0.028967874124646187, accuracy: 0.9912663755458515\n",
      "loss: 0.02790670096874237, accuracy: 0.9956331877729258\n",
      "loss: 0.03604534640908241, accuracy: 0.9912663755458515\n",
      "loss: 0.03231191635131836, accuracy: 0.9956331877729258\n",
      "loss: 0.03547627478837967, accuracy: 0.9912663755458515\n",
      "loss: 0.033839963376522064, accuracy: 0.9956331877729258\n",
      "loss: 0.03244724124670029, accuracy: 0.9956331877729258\n",
      "loss: 0.02883022092282772, accuracy: 0.9956331877729258\n",
      "loss: 0.03372717276215553, accuracy: 0.9912663755458515\n",
      "loss: 0.028457636013627052, accuracy: 0.9956331877729258\n",
      "loss: 0.030876409262418747, accuracy: 0.9912663755458515\n",
      "loss: 0.02793765999376774, accuracy: 0.9956331877729258\n",
      "loss: 0.035745542496442795, accuracy: 0.9956331877729258\n",
      "loss: 0.031146440654993057, accuracy: 0.9912663755458515\n",
      "loss: 0.0293730441480875, accuracy: 0.9956331877729258\n",
      "loss: 0.0288423802703619, accuracy: 0.9912663755458515\n",
      "loss: 0.03361108526587486, accuracy: 0.9912663755458515\n",
      "loss: 0.031189018860459328, accuracy: 0.9956331877729258\n",
      "loss: 0.028849266469478607, accuracy: 0.9956331877729258\n",
      "loss: 0.03807813301682472, accuracy: 0.9912663755458515\n",
      "loss: 0.034457311034202576, accuracy: 0.9912663755458515\n",
      "loss: 0.029643023386597633, accuracy: 0.9912663755458515\n",
      "loss: 0.03296476602554321, accuracy: 0.9956331877729258\n",
      "loss: 0.032043080776929855, accuracy: 0.9868995633187773\n",
      "loss: 0.03607077896595001, accuracy: 0.9912663755458515\n",
      "loss: 0.030110105872154236, accuracy: 0.9912663755458515\n",
      "loss: 0.02989625371992588, accuracy: 0.9912663755458515\n",
      "loss: 0.03188527002930641, accuracy: 0.9956331877729258\n",
      "loss: 0.029349245131015778, accuracy: 1.0\n",
      "loss: 0.0338418185710907, accuracy: 0.9868995633187773\n",
      "loss: 0.027712276205420494, accuracy: 1.0\n",
      "loss: 0.027058975771069527, accuracy: 0.9956331877729258\n",
      "loss: 0.03404337540268898, accuracy: 0.9868995633187773\n",
      "loss: 0.03359571099281311, accuracy: 0.9956331877729258\n",
      "loss: 0.031696874648332596, accuracy: 0.9912663755458515\n",
      "loss: 0.030274847522377968, accuracy: 0.9912663755458515\n",
      "loss: 0.028046973049640656, accuracy: 0.9956331877729258\n",
      "loss: 0.026198329403996468, accuracy: 1.0\n",
      "loss: 0.02991059236228466, accuracy: 0.9912663755458515\n",
      "loss: 0.026850804686546326, accuracy: 1.0\n",
      "loss: 0.03399742767214775, accuracy: 0.9912663755458515\n",
      "loss: 0.028888316825032234, accuracy: 1.0\n",
      "loss: 0.030608730390667915, accuracy: 0.9956331877729258\n",
      "loss: 0.028179876506328583, accuracy: 0.9912663755458515\n",
      "loss: 0.028555283322930336, accuracy: 0.9956331877729258\n",
      "loss: 0.028400972485542297, accuracy: 0.9956331877729258\n",
      "loss: 0.031431082636117935, accuracy: 0.9912663755458515\n",
      "loss: 0.03134343773126602, accuracy: 0.9912663755458515\n",
      "loss: 0.02665059082210064, accuracy: 1.0\n",
      "loss: 0.028557172045111656, accuracy: 0.9956331877729258\n",
      "loss: 0.029147548601031303, accuracy: 0.9956331877729258\n",
      "loss: 0.030046291649341583, accuracy: 0.9956331877729258\n",
      "loss: 0.03182371333241463, accuracy: 1.0\n",
      "loss: 0.031216025352478027, accuracy: 0.9956331877729258\n",
      "loss: 0.02974955551326275, accuracy: 0.9956331877729258\n",
      "loss: 0.027109289541840553, accuracy: 0.9956331877729258\n",
      "loss: 0.03241103142499924, accuracy: 0.9912663755458515\n",
      "loss: 0.02601085603237152, accuracy: 0.9956331877729258\n",
      "loss: 0.026258328929543495, accuracy: 0.9956331877729258\n",
      "loss: 0.026422783732414246, accuracy: 0.9956331877729258\n",
      "loss: 0.03725596144795418, accuracy: 0.9868995633187773\n",
      "loss: 0.0316421203315258, accuracy: 0.9912663755458515\n",
      "loss: 0.03249938413500786, accuracy: 0.9868995633187773\n",
      "loss: 0.029321378096938133, accuracy: 0.9912663755458515\n",
      "loss: 0.02930704690515995, accuracy: 0.9956331877729258\n",
      "loss: 0.028882768005132675, accuracy: 0.9956331877729258\n",
      "loss: 0.04286050423979759, accuracy: 0.9912663755458515\n",
      "loss: 0.03128562495112419, accuracy: 0.9956331877729258\n",
      "loss: 0.04005315899848938, accuracy: 0.9868995633187773\n",
      "loss: 0.0293626319617033, accuracy: 0.9956331877729258\n",
      "loss: 0.03237292543053627, accuracy: 0.9956331877729258\n",
      "loss: 0.029446182772517204, accuracy: 0.9956331877729258\n",
      "loss: 0.02854899689555168, accuracy: 0.9956331877729258\n",
      "loss: 0.029102807864546776, accuracy: 0.9956331877729258\n",
      "loss: 0.02673591673374176, accuracy: 1.0\n",
      "loss: 0.028967594727873802, accuracy: 0.9956331877729258\n",
      "loss: 0.02988619916141033, accuracy: 0.9912663755458515\n",
      "loss: 0.028558338060975075, accuracy: 0.9956331877729258\n",
      "loss: 0.0322127565741539, accuracy: 0.9956331877729258\n",
      "loss: 0.03249869868159294, accuracy: 0.9956331877729258\n",
      "loss: 0.032444387674331665, accuracy: 0.9956331877729258\n",
      "loss: 0.03282596170902252, accuracy: 0.9868995633187773\n",
      "loss: 0.030389636754989624, accuracy: 0.9956331877729258\n",
      "loss: 0.028776710852980614, accuracy: 1.0\n",
      "loss: 0.03094647079706192, accuracy: 0.9956331877729258\n",
      "loss: 0.029305635020136833, accuracy: 0.9912663755458515\n",
      "loss: 0.03058726340532303, accuracy: 0.9956331877729258\n",
      "loss: 0.02809976227581501, accuracy: 0.9956331877729258\n",
      "loss: 0.029995758086442947, accuracy: 0.9956331877729258\n",
      "loss: 0.030254127457737923, accuracy: 1.0\n",
      "loss: 0.03146619722247124, accuracy: 0.9956331877729258\n",
      "loss: 0.030278360471129417, accuracy: 0.9956331877729258\n",
      "loss: 0.028608078137040138, accuracy: 0.9956331877729258\n",
      "loss: 0.026638727635145187, accuracy: 0.9956331877729258\n",
      "loss: 0.028084909543395042, accuracy: 0.9956331877729258\n",
      "loss: 0.030318740755319595, accuracy: 0.9956331877729258\n",
      "loss: 0.02746368758380413, accuracy: 0.9956331877729258\n",
      "loss: 0.029198387637734413, accuracy: 0.9912663755458515\n",
      "loss: 0.029961006715893745, accuracy: 0.9912663755458515\n",
      "loss: 0.03141395375132561, accuracy: 0.9956331877729258\n",
      "loss: 0.029352078214287758, accuracy: 0.9956331877729258\n",
      "loss: 0.02816513553261757, accuracy: 0.9912663755458515\n",
      "loss: 0.025415392592549324, accuracy: 1.0\n",
      "loss: 0.026530252769589424, accuracy: 1.0\n",
      "loss: 0.026727059856057167, accuracy: 0.9956331877729258\n",
      "loss: 0.02737167477607727, accuracy: 0.9912663755458515\n",
      "loss: 0.027260178700089455, accuracy: 0.9868995633187773\n",
      "loss: 0.027648117393255234, accuracy: 0.9912663755458515\n",
      "loss: 0.025385761633515358, accuracy: 1.0\n",
      "loss: 0.025780893862247467, accuracy: 0.9956331877729258\n",
      "loss: 0.028065409511327744, accuracy: 0.9956331877729258\n",
      "loss: 0.02574470452964306, accuracy: 0.9956331877729258\n",
      "loss: 0.02896418422460556, accuracy: 0.9912663755458515\n",
      "loss: 0.030191008001565933, accuracy: 0.9956331877729258\n",
      "loss: 0.029453614726662636, accuracy: 0.9956331877729258\n",
      "loss: 0.028208978474140167, accuracy: 0.9956331877729258\n",
      "loss: 0.027994154021143913, accuracy: 0.9956331877729258\n",
      "loss: 0.02755260281264782, accuracy: 0.9956331877729258\n",
      "loss: 0.02529841661453247, accuracy: 0.9956331877729258\n",
      "loss: 0.02768007479608059, accuracy: 0.9912663755458515\n",
      "loss: 0.03178306296467781, accuracy: 0.9956331877729258\n",
      "loss: 0.026304299011826515, accuracy: 0.9956331877729258\n",
      "loss: 0.025527730584144592, accuracy: 1.0\n",
      "loss: 0.027235882356762886, accuracy: 0.9912663755458515\n",
      "loss: 0.02578328549861908, accuracy: 1.0\n",
      "loss: 0.02585548534989357, accuracy: 1.0\n",
      "loss: 0.025521406903862953, accuracy: 1.0\n",
      "loss: 0.03680574893951416, accuracy: 0.982532751091703\n",
      "loss: 0.032564908266067505, accuracy: 0.9912663755458515\n",
      "loss: 0.026933841407299042, accuracy: 0.9956331877729258\n",
      "loss: 0.028544746339321136, accuracy: 1.0\n",
      "loss: 0.0322408452630043, accuracy: 0.9956331877729258\n",
      "loss: 0.027240686118602753, accuracy: 1.0\n",
      "loss: 0.026059892028570175, accuracy: 1.0\n",
      "loss: 0.028163470327854156, accuracy: 0.9912663755458515\n",
      "loss: 0.026025492697954178, accuracy: 0.9956331877729258\n",
      "loss: 0.031779851764440536, accuracy: 0.9912663755458515\n",
      "loss: 0.040816619992256165, accuracy: 0.9912663755458515\n",
      "loss: 0.026144176721572876, accuracy: 0.9956331877729258\n",
      "loss: 0.024243056774139404, accuracy: 1.0\n",
      "loss: 0.024646006524562836, accuracy: 0.9956331877729258\n",
      "loss: 0.028625188395380974, accuracy: 0.9912663755458515\n",
      "loss: 0.02683877758681774, accuracy: 0.9956331877729258\n",
      "loss: 0.02862987108528614, accuracy: 0.9956331877729258\n",
      "loss: 0.029754962772130966, accuracy: 0.9956331877729258\n",
      "loss: 0.027855951339006424, accuracy: 0.9956331877729258\n",
      "loss: 0.028123697265982628, accuracy: 0.9956331877729258\n",
      "loss: 0.03004297986626625, accuracy: 0.9912663755458515\n",
      "loss: 0.023843837901949883, accuracy: 1.0\n",
      "loss: 0.03096262365579605, accuracy: 0.9912663755458515\n",
      "loss: 0.023449363186955452, accuracy: 0.9956331877729258\n",
      "loss: 0.02832740731537342, accuracy: 0.9956331877729258\n",
      "loss: 0.02491607889533043, accuracy: 1.0\n",
      "loss: 0.02495012804865837, accuracy: 1.0\n",
      "loss: 0.026224223896861076, accuracy: 0.9956331877729258\n",
      "loss: 0.027647925540804863, accuracy: 0.9956331877729258\n",
      "loss: 0.024795301258563995, accuracy: 0.9956331877729258\n",
      "loss: 0.029997384175658226, accuracy: 1.0\n",
      "loss: 0.025904474779963493, accuracy: 0.9956331877729258\n",
      "loss: 0.025507666170597076, accuracy: 0.9956331877729258\n",
      "loss: 0.024610958993434906, accuracy: 0.9956331877729258\n",
      "loss: 0.025777850300073624, accuracy: 0.9956331877729258\n",
      "loss: 0.023405281826853752, accuracy: 0.9956331877729258\n",
      "loss: 0.028904801234602928, accuracy: 0.9912663755458515\n",
      "loss: 0.027776937931776047, accuracy: 0.9956331877729258\n",
      "loss: 0.029641101136803627, accuracy: 1.0\n",
      "loss: 0.024581117555499077, accuracy: 0.9956331877729258\n",
      "loss: 0.025744017213582993, accuracy: 1.0\n",
      "loss: 0.027588166296482086, accuracy: 0.9912663755458515\n",
      "loss: 0.028092367574572563, accuracy: 0.9912663755458515\n",
      "loss: 0.026145776733756065, accuracy: 0.9956331877729258\n",
      "loss: 0.02510250359773636, accuracy: 1.0\n",
      "loss: 0.025360265746712685, accuracy: 1.0\n",
      "loss: 0.02485319972038269, accuracy: 0.9956331877729258\n",
      "loss: 0.025023767724633217, accuracy: 0.9956331877729258\n",
      "loss: 0.02872023917734623, accuracy: 1.0\n",
      "loss: 0.03533513844013214, accuracy: 0.9912663755458515\n",
      "loss: 0.029104173183441162, accuracy: 0.9956331877729258\n",
      "loss: 0.024274101480841637, accuracy: 0.9956331877729258\n",
      "loss: 0.025685207918286324, accuracy: 0.9912663755458515\n",
      "loss: 0.03086126782000065, accuracy: 0.9956331877729258\n",
      "loss: 0.023834986612200737, accuracy: 1.0\n",
      "loss: 0.025741329416632652, accuracy: 1.0\n",
      "loss: 0.03139309212565422, accuracy: 0.9956331877729258\n",
      "loss: 0.02833670936524868, accuracy: 0.9912663755458515\n",
      "loss: 0.024543466046452522, accuracy: 1.0\n",
      "loss: 0.02930621989071369, accuracy: 0.9956331877729258\n",
      "loss: 0.026995651423931122, accuracy: 0.9956331877729258\n",
      "loss: 0.025945130735635757, accuracy: 0.9956331877729258\n",
      "loss: 0.026026271283626556, accuracy: 0.9912663755458515\n",
      "loss: 0.028321968391537666, accuracy: 0.9956331877729258\n",
      "loss: 0.025199370458722115, accuracy: 1.0\n",
      "loss: 0.025327879935503006, accuracy: 1.0\n",
      "loss: 0.02761879190802574, accuracy: 0.9956331877729258\n",
      "loss: 0.02742798812687397, accuracy: 1.0\n",
      "loss: 0.024565815925598145, accuracy: 0.9956331877729258\n",
      "loss: 0.027715172618627548, accuracy: 0.9912663755458515\n",
      "loss: 0.02903187833726406, accuracy: 0.9912663755458515\n",
      "loss: 0.0259288027882576, accuracy: 0.9956331877729258\n",
      "loss: 0.023463020101189613, accuracy: 1.0\n",
      "loss: 0.02299853041768074, accuracy: 1.0\n",
      "loss: 0.022247834131121635, accuracy: 1.0\n",
      "loss: 0.025329938158392906, accuracy: 1.0\n",
      "loss: 0.02441459521651268, accuracy: 0.9956331877729258\n",
      "loss: 0.024384303018450737, accuracy: 1.0\n",
      "loss: 0.025418195873498917, accuracy: 1.0\n",
      "loss: 0.02300567738711834, accuracy: 1.0\n",
      "loss: 0.023502811789512634, accuracy: 1.0\n",
      "loss: 0.027048392221331596, accuracy: 0.9956331877729258\n",
      "loss: 0.03620273992419243, accuracy: 0.9956331877729258\n",
      "loss: 0.02813033200800419, accuracy: 0.9912663755458515\n",
      "loss: 0.02831224538385868, accuracy: 0.9956331877729258\n",
      "loss: 0.027386922389268875, accuracy: 0.9956331877729258\n",
      "loss: 0.025081850588321686, accuracy: 0.9956331877729258\n",
      "loss: 0.028034204617142677, accuracy: 0.9956331877729258\n",
      "loss: 0.031008215621113777, accuracy: 0.9912663755458515\n",
      "loss: 0.027724847197532654, accuracy: 1.0\n",
      "loss: 0.02379726618528366, accuracy: 1.0\n",
      "loss: 0.024956127628684044, accuracy: 1.0\n",
      "loss: 0.023953532800078392, accuracy: 0.9956331877729258\n",
      "loss: 0.02474445290863514, accuracy: 1.0\n",
      "loss: 0.022438183426856995, accuracy: 1.0\n",
      "loss: 0.031040703877806664, accuracy: 0.9912663755458515\n",
      "loss: 0.026806332170963287, accuracy: 0.9912663755458515\n",
      "loss: 0.0263995248824358, accuracy: 0.9956331877729258\n",
      "loss: 0.023147013038396835, accuracy: 1.0\n",
      "loss: 0.02504236064851284, accuracy: 1.0\n",
      "loss: 0.03028990887105465, accuracy: 0.9956331877729258\n",
      "loss: 0.03078596666455269, accuracy: 0.9956331877729258\n",
      "loss: 0.02499799057841301, accuracy: 0.9956331877729258\n",
      "loss: 0.02352391555905342, accuracy: 0.9956331877729258\n",
      "loss: 0.021778594702482224, accuracy: 1.0\n",
      "loss: 0.028020206838846207, accuracy: 0.9956331877729258\n",
      "loss: 0.02646666392683983, accuracy: 0.9956331877729258\n",
      "loss: 0.025529038161039352, accuracy: 1.0\n",
      "loss: 0.024579742923378944, accuracy: 0.9912663755458515\n",
      "loss: 0.025871438905596733, accuracy: 0.9956331877729258\n",
      "loss: 0.024327432736754417, accuracy: 0.9956331877729258\n",
      "loss: 0.02690821886062622, accuracy: 0.9956331877729258\n",
      "loss: 0.024780817329883575, accuracy: 1.0\n",
      "loss: 0.025043508037924767, accuracy: 0.9956331877729258\n",
      "loss: 0.02683183178305626, accuracy: 0.9912663755458515\n",
      "loss: 0.02556949108839035, accuracy: 1.0\n",
      "loss: 0.024790581315755844, accuracy: 0.9956331877729258\n",
      "loss: 0.023797549307346344, accuracy: 1.0\n",
      "loss: 0.02631455659866333, accuracy: 0.9956331877729258\n",
      "loss: 0.026474028825759888, accuracy: 0.9956331877729258\n",
      "loss: 0.02456587925553322, accuracy: 0.9956331877729258\n",
      "loss: 0.025589313358068466, accuracy: 1.0\n",
      "loss: 0.030352415516972542, accuracy: 0.9956331877729258\n",
      "loss: 0.025391602888703346, accuracy: 0.9956331877729258\n",
      "loss: 0.02483205311000347, accuracy: 0.9956331877729258\n",
      "loss: 0.02418144978582859, accuracy: 1.0\n",
      "loss: 0.022607412189245224, accuracy: 1.0\n",
      "loss: 0.03220167011022568, accuracy: 0.9912663755458515\n",
      "loss: 0.029395921155810356, accuracy: 0.9912663755458515\n",
      "loss: 0.025220230221748352, accuracy: 0.9956331877729258\n",
      "loss: 0.023517750203609467, accuracy: 1.0\n",
      "loss: 0.028646064922213554, accuracy: 1.0\n",
      "loss: 0.024420326575636864, accuracy: 0.9956331877729258\n",
      "loss: 0.024429213255643845, accuracy: 1.0\n",
      "loss: 0.024388063699007034, accuracy: 1.0\n",
      "loss: 0.021870087832212448, accuracy: 1.0\n",
      "loss: 0.023320144042372704, accuracy: 1.0\n",
      "loss: 0.024366114288568497, accuracy: 0.9956331877729258\n",
      "loss: 0.022773602977395058, accuracy: 1.0\n",
      "loss: 0.024739941582083702, accuracy: 0.9956331877729258\n",
      "loss: 0.02276293747127056, accuracy: 0.9956331877729258\n",
      "loss: 0.023424286395311356, accuracy: 0.9956331877729258\n",
      "loss: 0.02543599344789982, accuracy: 0.9956331877729258\n",
      "loss: 0.021922249346971512, accuracy: 1.0\n",
      "loss: 0.021536167711019516, accuracy: 1.0\n",
      "loss: 0.022656384855508804, accuracy: 1.0\n",
      "loss: 0.023977192118763924, accuracy: 0.9956331877729258\n",
      "loss: 0.025736916810274124, accuracy: 0.9956331877729258\n",
      "loss: 0.023455185815691948, accuracy: 1.0\n",
      "loss: 0.023613283410668373, accuracy: 0.9956331877729258\n",
      "loss: 0.02374895103275776, accuracy: 1.0\n",
      "loss: 0.022567840293049812, accuracy: 1.0\n",
      "loss: 0.02470318228006363, accuracy: 1.0\n",
      "loss: 0.023376092314720154, accuracy: 1.0\n",
      "loss: 0.024435533210635185, accuracy: 0.9956331877729258\n",
      "loss: 0.025115840137004852, accuracy: 1.0\n",
      "loss: 0.021306749433279037, accuracy: 1.0\n",
      "loss: 0.0234592966735363, accuracy: 1.0\n",
      "loss: 0.020903998985886574, accuracy: 1.0\n",
      "loss: 0.037921421229839325, accuracy: 0.9912663755458515\n",
      "loss: 0.03165470436215401, accuracy: 0.9956331877729258\n",
      "loss: 0.026744147762656212, accuracy: 1.0\n",
      "loss: 0.02387499064207077, accuracy: 0.9956331877729258\n",
      "loss: 0.02641131356358528, accuracy: 0.9956331877729258\n",
      "loss: 0.024027295410633087, accuracy: 1.0\n",
      "loss: 0.022207386791706085, accuracy: 1.0\n",
      "loss: 0.02681473083794117, accuracy: 1.0\n",
      "loss: 0.022253477945923805, accuracy: 1.0\n",
      "loss: 0.022142531350255013, accuracy: 1.0\n",
      "loss: 0.022725841030478477, accuracy: 1.0\n",
      "loss: 0.025689544156193733, accuracy: 0.9956331877729258\n",
      "loss: 0.023989267647266388, accuracy: 1.0\n",
      "loss: 0.024289673194289207, accuracy: 0.9956331877729258\n",
      "loss: 0.02738827094435692, accuracy: 1.0\n",
      "loss: 0.02305489405989647, accuracy: 1.0\n",
      "loss: 0.02343844808638096, accuracy: 0.9956331877729258\n",
      "loss: 0.022017689421772957, accuracy: 1.0\n",
      "loss: 0.023133011534810066, accuracy: 1.0\n",
      "loss: 0.023007484152913094, accuracy: 1.0\n",
      "loss: 0.02109481766819954, accuracy: 1.0\n",
      "loss: 0.024514395743608475, accuracy: 1.0\n",
      "loss: 0.02244546264410019, accuracy: 1.0\n",
      "loss: 0.025200895965099335, accuracy: 0.9956331877729258\n",
      "loss: 0.028297405689954758, accuracy: 0.9912663755458515\n",
      "loss: 0.024440910667181015, accuracy: 0.9956331877729258\n",
      "loss: 0.024894526228308678, accuracy: 1.0\n",
      "loss: 0.023903600871562958, accuracy: 1.0\n",
      "loss: 0.029532743617892265, accuracy: 0.9912663755458515\n",
      "loss: 0.021714523434638977, accuracy: 1.0\n",
      "loss: 0.025277642533183098, accuracy: 1.0\n",
      "loss: 0.023161575198173523, accuracy: 0.9956331877729258\n",
      "loss: 0.026838630437850952, accuracy: 0.9956331877729258\n",
      "loss: 0.02361198514699936, accuracy: 0.9956331877729258\n",
      "loss: 0.0252876915037632, accuracy: 0.9956331877729258\n",
      "loss: 0.022571280598640442, accuracy: 1.0\n",
      "loss: 0.02097640000283718, accuracy: 1.0\n",
      "loss: 0.02261568233370781, accuracy: 1.0\n",
      "loss: 0.022903231903910637, accuracy: 0.9956331877729258\n",
      "loss: 0.021483367308974266, accuracy: 1.0\n",
      "loss: 0.022623881697654724, accuracy: 0.9956331877729258\n",
      "loss: 0.02163548953831196, accuracy: 1.0\n",
      "loss: 0.022110652178525925, accuracy: 0.9956331877729258\n",
      "loss: 0.0220656618475914, accuracy: 1.0\n",
      "loss: 0.022338006645441055, accuracy: 1.0\n",
      "loss: 0.02351483516395092, accuracy: 0.9912663755458515\n",
      "loss: 0.023122452199459076, accuracy: 1.0\n",
      "loss: 0.022832345217466354, accuracy: 1.0\n",
      "loss: 0.020909056067466736, accuracy: 1.0\n",
      "loss: 0.023958707228302956, accuracy: 1.0\n",
      "loss: 0.022148041054606438, accuracy: 1.0\n",
      "loss: 0.02139335125684738, accuracy: 1.0\n",
      "loss: 0.021068330854177475, accuracy: 1.0\n",
      "loss: 0.021444672718644142, accuracy: 1.0\n",
      "loss: 0.02225719764828682, accuracy: 1.0\n",
      "loss: 0.022738875821232796, accuracy: 1.0\n",
      "loss: 0.023410217836499214, accuracy: 1.0\n",
      "loss: 0.021887153387069702, accuracy: 1.0\n",
      "loss: 0.024973735213279724, accuracy: 1.0\n",
      "loss: 0.026304740458726883, accuracy: 0.9956331877729258\n",
      "loss: 0.027435140684247017, accuracy: 1.0\n",
      "loss: 0.02197391167283058, accuracy: 1.0\n",
      "loss: 0.02545788884162903, accuracy: 0.9956331877729258\n",
      "loss: 0.02169550582766533, accuracy: 1.0\n",
      "loss: 0.02208559587597847, accuracy: 0.9912663755458515\n",
      "loss: 0.026042455807328224, accuracy: 1.0\n",
      "loss: 0.025342954322695732, accuracy: 0.9956331877729258\n",
      "loss: 0.02382955513894558, accuracy: 0.9912663755458515\n",
      "loss: 0.025269053876399994, accuracy: 0.9956331877729258\n",
      "loss: 0.021279217675328255, accuracy: 1.0\n",
      "loss: 0.02295423299074173, accuracy: 1.0\n",
      "loss: 0.02365327626466751, accuracy: 0.9956331877729258\n",
      "loss: 0.020761817693710327, accuracy: 0.9956331877729258\n",
      "loss: 0.023181142285466194, accuracy: 1.0\n",
      "loss: 0.02087448351085186, accuracy: 1.0\n",
      "loss: 0.019765788689255714, accuracy: 1.0\n",
      "loss: 0.024479003623127937, accuracy: 0.9956331877729258\n",
      "loss: 0.024393348023295403, accuracy: 0.9956331877729258\n",
      "loss: 0.02411818690598011, accuracy: 1.0\n",
      "loss: 0.02131902426481247, accuracy: 1.0\n",
      "loss: 0.023998714983463287, accuracy: 0.9956331877729258\n",
      "loss: 0.02173265814781189, accuracy: 0.9956331877729258\n",
      "loss: 0.02387099340558052, accuracy: 0.9956331877729258\n",
      "loss: 0.02107045240700245, accuracy: 1.0\n",
      "loss: 0.021765392273664474, accuracy: 0.9956331877729258\n",
      "loss: 0.022127149626612663, accuracy: 0.9956331877729258\n",
      "loss: 0.02129235491156578, accuracy: 1.0\n",
      "loss: 0.03252996504306793, accuracy: 0.9912663755458515\n",
      "loss: 0.02280178852379322, accuracy: 1.0\n",
      "loss: 0.022330477833747864, accuracy: 0.9956331877729258\n",
      "loss: 0.019945094361901283, accuracy: 1.0\n",
      "loss: 0.01983383670449257, accuracy: 1.0\n",
      "loss: 0.02251490205526352, accuracy: 1.0\n",
      "loss: 0.021427791565656662, accuracy: 1.0\n",
      "loss: 0.025068040937185287, accuracy: 1.0\n",
      "loss: 0.027957066893577576, accuracy: 0.9956331877729258\n",
      "loss: 0.023170094937086105, accuracy: 1.0\n",
      "loss: 0.021887462586164474, accuracy: 1.0\n",
      "loss: 0.022161835804581642, accuracy: 1.0\n",
      "loss: 0.022605616599321365, accuracy: 1.0\n",
      "loss: 0.02295130491256714, accuracy: 0.9956331877729258\n",
      "loss: 0.02037891373038292, accuracy: 1.0\n",
      "loss: 0.020467760041356087, accuracy: 1.0\n",
      "loss: 0.02483336254954338, accuracy: 0.9956331877729258\n",
      "loss: 0.02322522923350334, accuracy: 0.9956331877729258\n",
      "loss: 0.02440263144671917, accuracy: 0.9956331877729258\n",
      "loss: 0.021247895434498787, accuracy: 0.9956331877729258\n",
      "loss: 0.023096458986401558, accuracy: 0.9956331877729258\n",
      "loss: 0.020840901881456375, accuracy: 1.0\n",
      "loss: 0.020650601014494896, accuracy: 1.0\n",
      "loss: 0.022651612758636475, accuracy: 1.0\n",
      "loss: 0.021668309345841408, accuracy: 1.0\n",
      "loss: 0.021284978836774826, accuracy: 0.9956331877729258\n",
      "loss: 0.019935106858611107, accuracy: 1.0\n",
      "loss: 0.02113526500761509, accuracy: 1.0\n",
      "loss: 0.024212107062339783, accuracy: 1.0\n",
      "loss: 0.026027699932456017, accuracy: 0.9912663755458515\n",
      "loss: 0.021265774965286255, accuracy: 1.0\n",
      "loss: 0.021319955587387085, accuracy: 1.0\n",
      "loss: 0.02010216750204563, accuracy: 1.0\n",
      "loss: 0.020388631150126457, accuracy: 1.0\n",
      "loss: 0.024342874065041542, accuracy: 1.0\n",
      "loss: 0.0198027603328228, accuracy: 1.0\n",
      "loss: 0.02378045581281185, accuracy: 1.0\n",
      "loss: 0.02316122315824032, accuracy: 0.9956331877729258\n",
      "loss: 0.021236272528767586, accuracy: 1.0\n",
      "loss: 0.018483605235815048, accuracy: 1.0\n",
      "loss: 0.027251338586211205, accuracy: 0.9956331877729258\n",
      "loss: 0.021137641742825508, accuracy: 1.0\n",
      "loss: 0.020268773660063744, accuracy: 1.0\n",
      "loss: 0.019199198111891747, accuracy: 1.0\n",
      "loss: 0.020214300602674484, accuracy: 1.0\n",
      "loss: 0.021235518157482147, accuracy: 1.0\n",
      "loss: 0.021543266251683235, accuracy: 1.0\n",
      "loss: 0.01959811896085739, accuracy: 1.0\n",
      "loss: 0.023127390071749687, accuracy: 0.9956331877729258\n",
      "loss: 0.02135760523378849, accuracy: 1.0\n",
      "loss: 0.021463440731167793, accuracy: 1.0\n",
      "loss: 0.023125052452087402, accuracy: 1.0\n",
      "loss: 0.023557469248771667, accuracy: 1.0\n",
      "loss: 0.02712036296725273, accuracy: 0.9956331877729258\n",
      "loss: 0.021450133994221687, accuracy: 1.0\n",
      "loss: 0.02057519182562828, accuracy: 1.0\n",
      "loss: 0.01971679925918579, accuracy: 1.0\n",
      "loss: 0.021532634273171425, accuracy: 1.0\n",
      "loss: 0.023136964067816734, accuracy: 0.9956331877729258\n",
      "loss: 0.020293891429901123, accuracy: 0.9956331877729258\n",
      "loss: 0.021092738956212997, accuracy: 1.0\n",
      "loss: 0.0233576949685812, accuracy: 0.9956331877729258\n",
      "loss: 0.02031911164522171, accuracy: 1.0\n",
      "loss: 0.019064458087086678, accuracy: 0.9956331877729258\n",
      "loss: 0.02051731012761593, accuracy: 1.0\n",
      "loss: 0.023297201842069626, accuracy: 1.0\n",
      "loss: 0.019089877605438232, accuracy: 1.0\n",
      "loss: 0.019803546369075775, accuracy: 1.0\n",
      "loss: 0.02058384381234646, accuracy: 1.0\n",
      "loss: 0.021053262054920197, accuracy: 0.9956331877729258\n",
      "loss: 0.018309317529201508, accuracy: 1.0\n",
      "loss: 0.021094799041748047, accuracy: 1.0\n",
      "loss: 0.01879645697772503, accuracy: 1.0\n",
      "loss: 0.018541038036346436, accuracy: 1.0\n",
      "loss: 0.02074659802019596, accuracy: 1.0\n",
      "loss: 0.02211766690015793, accuracy: 1.0\n",
      "loss: 0.019571086391806602, accuracy: 1.0\n",
      "loss: 0.024277562275528908, accuracy: 0.9956331877729258\n",
      "loss: 0.022919826209545135, accuracy: 1.0\n",
      "loss: 0.02766614407300949, accuracy: 0.9956331877729258\n",
      "loss: 0.025058817118406296, accuracy: 0.9956331877729258\n",
      "loss: 0.021613987162709236, accuracy: 0.9956331877729258\n",
      "loss: 0.02021157369017601, accuracy: 1.0\n",
      "loss: 0.01951184868812561, accuracy: 1.0\n",
      "loss: 0.02273249812424183, accuracy: 1.0\n",
      "loss: 0.026133408769965172, accuracy: 0.9912663755458515\n",
      "loss: 0.027112089097499847, accuracy: 0.9956331877729258\n",
      "loss: 0.020722752436995506, accuracy: 1.0\n",
      "loss: 0.020154880359768867, accuracy: 1.0\n",
      "loss: 0.020545797422528267, accuracy: 1.0\n",
      "loss: 0.020194370299577713, accuracy: 1.0\n",
      "loss: 0.019240550696849823, accuracy: 1.0\n",
      "loss: 0.02207222767174244, accuracy: 1.0\n",
      "loss: 0.020933955907821655, accuracy: 1.0\n",
      "loss: 0.0207529217004776, accuracy: 1.0\n",
      "loss: 0.02038617990911007, accuracy: 1.0\n",
      "loss: 0.020208533853292465, accuracy: 1.0\n",
      "loss: 0.01928306743502617, accuracy: 1.0\n",
      "loss: 0.02422773838043213, accuracy: 0.9956331877729258\n",
      "loss: 0.019581900909543037, accuracy: 1.0\n",
      "loss: 0.0208749882876873, accuracy: 1.0\n",
      "loss: 0.021728286519646645, accuracy: 1.0\n",
      "loss: 0.024245241656899452, accuracy: 0.9956331877729258\n",
      "loss: 0.021299581974744797, accuracy: 0.9956331877729258\n",
      "loss: 0.02079041674733162, accuracy: 1.0\n",
      "loss: 0.01891672983765602, accuracy: 1.0\n",
      "loss: 0.01892043836414814, accuracy: 1.0\n",
      "loss: 0.019606688991189003, accuracy: 1.0\n",
      "loss: 0.01869923807680607, accuracy: 1.0\n",
      "loss: 0.02075386792421341, accuracy: 1.0\n",
      "loss: 0.021341776475310326, accuracy: 0.9956331877729258\n",
      "loss: 0.019159885123372078, accuracy: 1.0\n",
      "loss: 0.023325005546212196, accuracy: 1.0\n",
      "loss: 0.02073974907398224, accuracy: 1.0\n",
      "loss: 0.02201029658317566, accuracy: 1.0\n",
      "loss: 0.019645705819129944, accuracy: 1.0\n",
      "loss: 0.019214998930692673, accuracy: 1.0\n",
      "loss: 0.01846178062260151, accuracy: 1.0\n",
      "loss: 0.0199419055134058, accuracy: 1.0\n",
      "loss: 0.018705660477280617, accuracy: 1.0\n",
      "loss: 0.02034662663936615, accuracy: 1.0\n",
      "loss: 0.01839262619614601, accuracy: 1.0\n",
      "loss: 0.021811429411172867, accuracy: 0.9956331877729258\n",
      "loss: 0.022859279066324234, accuracy: 0.9956331877729258\n",
      "loss: 0.01997017115354538, accuracy: 1.0\n",
      "loss: 0.01870977319777012, accuracy: 1.0\n",
      "loss: 0.020535863935947418, accuracy: 1.0\n",
      "loss: 0.020596807822585106, accuracy: 1.0\n",
      "loss: 0.02091863751411438, accuracy: 1.0\n",
      "loss: 0.021283039823174477, accuracy: 1.0\n",
      "loss: 0.020183362066745758, accuracy: 0.9956331877729258\n",
      "loss: 0.02088909223675728, accuracy: 1.0\n",
      "loss: 0.02096552960574627, accuracy: 1.0\n",
      "loss: 0.019405806437134743, accuracy: 1.0\n",
      "loss: 0.02019183151423931, accuracy: 1.0\n",
      "loss: 0.021582530811429024, accuracy: 0.9956331877729258\n",
      "loss: 0.020024385303258896, accuracy: 1.0\n",
      "loss: 0.020305462181568146, accuracy: 1.0\n",
      "loss: 0.023145848885178566, accuracy: 1.0\n",
      "loss: 0.021663399413228035, accuracy: 1.0\n",
      "loss: 0.023323141038417816, accuracy: 0.9956331877729258\n",
      "loss: 0.019308550283312798, accuracy: 1.0\n",
      "loss: 0.01984797790646553, accuracy: 1.0\n",
      "loss: 0.019228005781769753, accuracy: 1.0\n",
      "loss: 0.020384281873703003, accuracy: 1.0\n",
      "loss: 0.02474079467356205, accuracy: 0.9956331877729258\n",
      "loss: 0.018040088936686516, accuracy: 1.0\n",
      "loss: 0.01961640641093254, accuracy: 1.0\n",
      "loss: 0.02055283449590206, accuracy: 1.0\n",
      "loss: 0.018521390855312347, accuracy: 1.0\n",
      "loss: 0.019636519253253937, accuracy: 1.0\n",
      "loss: 0.018998831510543823, accuracy: 1.0\n",
      "loss: 0.018908454105257988, accuracy: 1.0\n",
      "loss: 0.020974088460206985, accuracy: 1.0\n",
      "loss: 0.018693048506975174, accuracy: 1.0\n",
      "loss: 0.020300641655921936, accuracy: 1.0\n",
      "loss: 0.017964692786335945, accuracy: 1.0\n",
      "loss: 0.01860775798559189, accuracy: 1.0\n",
      "loss: 0.018322236835956573, accuracy: 1.0\n",
      "loss: 0.01713988371193409, accuracy: 1.0\n",
      "loss: 0.018520621582865715, accuracy: 1.0\n",
      "loss: 0.019946295768022537, accuracy: 1.0\n",
      "loss: 0.01853395439684391, accuracy: 1.0\n",
      "loss: 0.01898123323917389, accuracy: 1.0\n",
      "loss: 0.018523382022976875, accuracy: 1.0\n",
      "loss: 0.02123149484395981, accuracy: 1.0\n",
      "loss: 0.020290479063987732, accuracy: 1.0\n",
      "loss: 0.018536733463406563, accuracy: 1.0\n",
      "loss: 0.019834693521261215, accuracy: 1.0\n",
      "loss: 0.019977133721113205, accuracy: 1.0\n",
      "loss: 0.02194017358124256, accuracy: 1.0\n",
      "loss: 0.020520629361271858, accuracy: 0.9956331877729258\n",
      "loss: 0.017864258959889412, accuracy: 1.0\n",
      "loss: 0.019110074266791344, accuracy: 1.0\n",
      "loss: 0.01740582473576069, accuracy: 1.0\n",
      "loss: 0.020221805199980736, accuracy: 1.0\n",
      "loss: 0.01763133518397808, accuracy: 1.0\n",
      "loss: 0.018239637836813927, accuracy: 1.0\n",
      "loss: 0.01852290891110897, accuracy: 1.0\n",
      "loss: 0.018027164041996002, accuracy: 1.0\n",
      "loss: 0.018135370686650276, accuracy: 1.0\n",
      "loss: 0.01835494115948677, accuracy: 1.0\n",
      "loss: 0.01803446374833584, accuracy: 1.0\n",
      "loss: 0.018690964207053185, accuracy: 1.0\n",
      "loss: 0.018610848113894463, accuracy: 1.0\n",
      "loss: 0.019642725586891174, accuracy: 1.0\n",
      "loss: 0.02304566465318203, accuracy: 0.9956331877729258\n",
      "loss: 0.02036578767001629, accuracy: 1.0\n",
      "loss: 0.018546419218182564, accuracy: 1.0\n",
      "loss: 0.017640570178627968, accuracy: 1.0\n",
      "loss: 0.019323647022247314, accuracy: 1.0\n",
      "loss: 0.020330408588051796, accuracy: 1.0\n",
      "loss: 0.01812809333205223, accuracy: 1.0\n",
      "loss: 0.020216496661305428, accuracy: 1.0\n",
      "loss: 0.018671559169888496, accuracy: 0.9956331877729258\n",
      "loss: 0.018182281404733658, accuracy: 1.0\n",
      "loss: 0.020396148785948753, accuracy: 1.0\n",
      "loss: 0.018815429881215096, accuracy: 1.0\n",
      "loss: 0.018298618495464325, accuracy: 1.0\n",
      "loss: 0.024260127916932106, accuracy: 0.9956331877729258\n",
      "loss: 0.02083936706185341, accuracy: 1.0\n",
      "loss: 0.01795588992536068, accuracy: 1.0\n",
      "loss: 0.01939133182168007, accuracy: 1.0\n",
      "loss: 0.019564274698495865, accuracy: 1.0\n",
      "loss: 0.018079861998558044, accuracy: 1.0\n",
      "loss: 0.019371751695871353, accuracy: 1.0\n",
      "loss: 0.01864183507859707, accuracy: 1.0\n",
      "loss: 0.01808975450694561, accuracy: 1.0\n",
      "loss: 0.017099665477871895, accuracy: 1.0\n",
      "loss: 0.019886115565896034, accuracy: 1.0\n",
      "loss: 0.017882484942674637, accuracy: 1.0\n",
      "loss: 0.016859160736203194, accuracy: 1.0\n",
      "loss: 0.020714331418275833, accuracy: 1.0\n",
      "loss: 0.020628510043025017, accuracy: 0.9956331877729258\n",
      "loss: 0.01713675633072853, accuracy: 1.0\n",
      "loss: 0.01736779697239399, accuracy: 1.0\n",
      "loss: 0.018697114661335945, accuracy: 1.0\n",
      "loss: 0.018215032294392586, accuracy: 1.0\n",
      "loss: 0.01851014420390129, accuracy: 1.0\n",
      "loss: 0.018869642168283463, accuracy: 0.9956331877729258\n",
      "loss: 0.017052263021469116, accuracy: 1.0\n",
      "loss: 0.016859466210007668, accuracy: 1.0\n",
      "loss: 0.018779467791318893, accuracy: 1.0\n",
      "loss: 0.019050028175115585, accuracy: 1.0\n",
      "loss: 0.019315024837851524, accuracy: 1.0\n",
      "loss: 0.016939595341682434, accuracy: 1.0\n",
      "loss: 0.01922212913632393, accuracy: 1.0\n",
      "loss: 0.01811552792787552, accuracy: 1.0\n",
      "loss: 0.018388040363788605, accuracy: 1.0\n",
      "loss: 0.01856502890586853, accuracy: 1.0\n",
      "loss: 0.016195550560951233, accuracy: 1.0\n",
      "loss: 0.018412671983242035, accuracy: 1.0\n",
      "loss: 0.022976305335760117, accuracy: 0.9956331877729258\n",
      "loss: 0.021809285506606102, accuracy: 0.9956331877729258\n",
      "loss: 0.018438316881656647, accuracy: 1.0\n",
      "loss: 0.017331549897789955, accuracy: 1.0\n",
      "loss: 0.018692348152399063, accuracy: 1.0\n",
      "loss: 0.01886683702468872, accuracy: 1.0\n",
      "loss: 0.017860746011137962, accuracy: 1.0\n",
      "loss: 0.017057478427886963, accuracy: 1.0\n",
      "loss: 0.020245792344212532, accuracy: 1.0\n",
      "loss: 0.016599059104919434, accuracy: 1.0\n",
      "loss: 0.017069239169359207, accuracy: 1.0\n",
      "loss: 0.01733638346195221, accuracy: 1.0\n",
      "loss: 0.017476269975304604, accuracy: 1.0\n",
      "loss: 0.018422847613692284, accuracy: 1.0\n",
      "loss: 0.017555641010403633, accuracy: 1.0\n",
      "loss: 0.017692014575004578, accuracy: 1.0\n",
      "loss: 0.02055356651544571, accuracy: 0.9956331877729258\n",
      "loss: 0.017631713300943375, accuracy: 1.0\n",
      "loss: 0.016754833981394768, accuracy: 1.0\n",
      "loss: 0.017533984035253525, accuracy: 1.0\n",
      "loss: 0.01619996875524521, accuracy: 1.0\n",
      "loss: 0.018652373924851418, accuracy: 1.0\n",
      "loss: 0.02086017094552517, accuracy: 0.9956331877729258\n",
      "loss: 0.021998604759573936, accuracy: 0.9956331877729258\n",
      "loss: 0.017185546457767487, accuracy: 1.0\n",
      "loss: 0.01783585362136364, accuracy: 1.0\n",
      "loss: 0.01707248017191887, accuracy: 1.0\n",
      "loss: 0.01783336140215397, accuracy: 1.0\n",
      "loss: 0.017240962013602257, accuracy: 1.0\n",
      "loss: 0.017331672832369804, accuracy: 1.0\n",
      "loss: 0.02016361430287361, accuracy: 1.0\n",
      "loss: 0.01898811012506485, accuracy: 1.0\n",
      "loss: 0.018562214449048042, accuracy: 1.0\n",
      "loss: 0.020296255126595497, accuracy: 1.0\n",
      "loss: 0.01774296909570694, accuracy: 1.0\n",
      "loss: 0.01757797971367836, accuracy: 1.0\n",
      "loss: 0.016197558492422104, accuracy: 1.0\n",
      "loss: 0.017810622230172157, accuracy: 1.0\n",
      "loss: 0.016607102006673813, accuracy: 1.0\n",
      "loss: 0.01670757494866848, accuracy: 1.0\n",
      "loss: 0.018564825877547264, accuracy: 1.0\n",
      "loss: 0.01693739928305149, accuracy: 1.0\n",
      "loss: 0.01641840487718582, accuracy: 1.0\n",
      "loss: 0.01770051196217537, accuracy: 1.0\n",
      "loss: 0.016912225633859634, accuracy: 1.0\n",
      "loss: 0.017236093059182167, accuracy: 1.0\n",
      "loss: 0.01676926203072071, accuracy: 1.0\n",
      "loss: 0.018059372901916504, accuracy: 1.0\n",
      "loss: 0.018322279676795006, accuracy: 1.0\n",
      "loss: 0.01689598150551319, accuracy: 1.0\n",
      "loss: 0.01730094850063324, accuracy: 1.0\n",
      "loss: 0.018461812287569046, accuracy: 1.0\n",
      "loss: 0.0173896923661232, accuracy: 1.0\n",
      "loss: 0.01863940991461277, accuracy: 1.0\n",
      "loss: 0.017165953293442726, accuracy: 1.0\n",
      "loss: 0.01823321357369423, accuracy: 1.0\n",
      "loss: 0.018344968557357788, accuracy: 1.0\n",
      "loss: 0.017174573615193367, accuracy: 1.0\n",
      "loss: 0.019044984132051468, accuracy: 1.0\n",
      "loss: 0.01735614240169525, accuracy: 1.0\n",
      "loss: 0.01619235798716545, accuracy: 1.0\n",
      "loss: 0.01729080267250538, accuracy: 1.0\n",
      "loss: 0.01698676310479641, accuracy: 1.0\n",
      "loss: 0.01607375219464302, accuracy: 1.0\n",
      "loss: 0.016449717804789543, accuracy: 1.0\n",
      "loss: 0.02015954814851284, accuracy: 1.0\n",
      "loss: 0.017976179718971252, accuracy: 1.0\n",
      "loss: 0.016377056017518044, accuracy: 1.0\n",
      "loss: 0.01906096190214157, accuracy: 0.9956331877729258\n",
      "loss: 0.017167527228593826, accuracy: 1.0\n",
      "loss: 0.016825998201966286, accuracy: 1.0\n",
      "loss: 0.01592029631137848, accuracy: 1.0\n",
      "loss: 0.016104446724057198, accuracy: 1.0\n",
      "loss: 0.01742389053106308, accuracy: 1.0\n",
      "loss: 0.015477176755666733, accuracy: 1.0\n",
      "loss: 0.01643434911966324, accuracy: 1.0\n",
      "loss: 0.02177274413406849, accuracy: 1.0\n",
      "loss: 0.020177271217107773, accuracy: 1.0\n",
      "loss: 0.019419634714722633, accuracy: 0.9956331877729258\n",
      "loss: 0.016329096630215645, accuracy: 1.0\n",
      "loss: 0.01723148673772812, accuracy: 1.0\n",
      "loss: 0.01691875047981739, accuracy: 1.0\n",
      "loss: 0.01582716777920723, accuracy: 1.0\n",
      "loss: 0.017667079344391823, accuracy: 1.0\n",
      "loss: 0.016020020470023155, accuracy: 1.0\n",
      "loss: 0.018153585493564606, accuracy: 1.0\n",
      "loss: 0.01735994964838028, accuracy: 1.0\n",
      "loss: 0.019471289590001106, accuracy: 1.0\n",
      "loss: 0.01838918961584568, accuracy: 1.0\n",
      "loss: 0.019330354407429695, accuracy: 1.0\n",
      "loss: 0.016354896128177643, accuracy: 1.0\n",
      "loss: 0.015339638106524944, accuracy: 1.0\n",
      "loss: 0.017198922112584114, accuracy: 1.0\n",
      "loss: 0.017351746559143066, accuracy: 1.0\n",
      "loss: 0.017475787550210953, accuracy: 1.0\n",
      "loss: 0.01624966226518154, accuracy: 1.0\n",
      "loss: 0.01660247892141342, accuracy: 1.0\n",
      "loss: 0.01655239798128605, accuracy: 1.0\n",
      "loss: 0.017572814598679543, accuracy: 1.0\n",
      "loss: 0.017285030335187912, accuracy: 1.0\n",
      "loss: 0.015951761975884438, accuracy: 1.0\n",
      "loss: 0.016042344272136688, accuracy: 1.0\n",
      "loss: 0.016840262338519096, accuracy: 1.0\n",
      "loss: 0.01652565784752369, accuracy: 1.0\n",
      "loss: 0.016795827075839043, accuracy: 1.0\n",
      "loss: 0.016312606632709503, accuracy: 1.0\n",
      "loss: 0.016281500458717346, accuracy: 1.0\n",
      "loss: 0.01655222661793232, accuracy: 1.0\n",
      "loss: 0.018353862687945366, accuracy: 1.0\n",
      "loss: 0.024665869772434235, accuracy: 0.9912663755458515\n",
      "loss: 0.022881172597408295, accuracy: 0.9956331877729258\n",
      "loss: 0.017694497480988503, accuracy: 1.0\n",
      "loss: 0.015428022481501102, accuracy: 1.0\n",
      "loss: 0.016132066026329994, accuracy: 1.0\n",
      "loss: 0.016048692166805267, accuracy: 1.0\n",
      "loss: 0.016150902956724167, accuracy: 1.0\n",
      "loss: 0.0169485192745924, accuracy: 1.0\n",
      "loss: 0.017606936395168304, accuracy: 1.0\n",
      "loss: 0.017324578016996384, accuracy: 1.0\n",
      "loss: 0.01594303548336029, accuracy: 1.0\n",
      "loss: 0.015503082424402237, accuracy: 1.0\n",
      "loss: 0.018364205956459045, accuracy: 1.0\n",
      "loss: 0.015278071165084839, accuracy: 1.0\n",
      "loss: 0.016021423041820526, accuracy: 1.0\n",
      "loss: 0.016032161191105843, accuracy: 1.0\n",
      "loss: 0.015014400705695152, accuracy: 1.0\n",
      "loss: 0.01606123335659504, accuracy: 1.0\n",
      "loss: 0.01540642324835062, accuracy: 1.0\n",
      "loss: 0.016095692291855812, accuracy: 1.0\n",
      "loss: 0.016472043469548225, accuracy: 1.0\n",
      "loss: 0.01871887966990471, accuracy: 1.0\n",
      "loss: 0.017909640446305275, accuracy: 1.0\n",
      "loss: 0.01596658118069172, accuracy: 1.0\n",
      "loss: 0.015089819207787514, accuracy: 1.0\n",
      "loss: 0.016013791784644127, accuracy: 1.0\n",
      "loss: 0.01700875349342823, accuracy: 1.0\n",
      "loss: 0.017571989446878433, accuracy: 1.0\n",
      "loss: 0.01608259789645672, accuracy: 1.0\n",
      "loss: 0.016166411340236664, accuracy: 1.0\n",
      "loss: 0.016023248434066772, accuracy: 1.0\n",
      "loss: 0.01619575172662735, accuracy: 1.0\n",
      "loss: 0.015846682712435722, accuracy: 1.0\n",
      "loss: 0.016361426562070847, accuracy: 1.0\n",
      "loss: 0.01468735933303833, accuracy: 1.0\n",
      "loss: 0.016687892377376556, accuracy: 1.0\n",
      "loss: 0.015866858884692192, accuracy: 1.0\n",
      "loss: 0.018434662371873856, accuracy: 1.0\n",
      "loss: 0.020244205370545387, accuracy: 0.9956331877729258\n",
      "loss: 0.01719135232269764, accuracy: 1.0\n",
      "loss: 0.018291791900992393, accuracy: 1.0\n",
      "loss: 0.022771064192056656, accuracy: 0.9956331877729258\n",
      "loss: 0.01660950854420662, accuracy: 1.0\n",
      "loss: 0.016555964946746826, accuracy: 1.0\n",
      "loss: 0.017924968153238297, accuracy: 1.0\n",
      "loss: 0.016181791201233864, accuracy: 1.0\n",
      "loss: 0.015327336266636848, accuracy: 1.0\n",
      "loss: 0.017358282580971718, accuracy: 1.0\n",
      "loss: 0.015146801248192787, accuracy: 1.0\n",
      "loss: 0.016040507704019547, accuracy: 1.0\n",
      "loss: 0.014202388003468513, accuracy: 1.0\n",
      "loss: 0.017608243972063065, accuracy: 1.0\n",
      "loss: 0.01512861903756857, accuracy: 1.0\n",
      "loss: 0.016871890053153038, accuracy: 1.0\n",
      "loss: 0.016023913398385048, accuracy: 1.0\n",
      "loss: 0.015123236924409866, accuracy: 1.0\n",
      "loss: 0.016695939004421234, accuracy: 1.0\n",
      "loss: 0.016495948657393456, accuracy: 1.0\n",
      "loss: 0.015753939747810364, accuracy: 1.0\n",
      "loss: 0.01678706891834736, accuracy: 0.9956331877729258\n",
      "loss: 0.016575098037719727, accuracy: 1.0\n",
      "loss: 0.017080942168831825, accuracy: 1.0\n",
      "loss: 0.015615181066095829, accuracy: 1.0\n",
      "loss: 0.015608190558850765, accuracy: 1.0\n",
      "loss: 0.016375156119465828, accuracy: 1.0\n",
      "loss: 0.016106629744172096, accuracy: 1.0\n",
      "loss: 0.015560835599899292, accuracy: 1.0\n",
      "loss: 0.016013022512197495, accuracy: 1.0\n",
      "loss: 0.015633055940270424, accuracy: 1.0\n",
      "loss: 0.016461540013551712, accuracy: 1.0\n",
      "loss: 0.0201814454048872, accuracy: 0.9956331877729258\n",
      "loss: 0.01762249879539013, accuracy: 1.0\n",
      "loss: 0.014790615998208523, accuracy: 1.0\n",
      "loss: 0.014920037239789963, accuracy: 1.0\n",
      "loss: 0.016064168885350227, accuracy: 1.0\n",
      "loss: 0.015833143144845963, accuracy: 1.0\n",
      "loss: 0.014865926466882229, accuracy: 1.0\n",
      "loss: 0.016222957521677017, accuracy: 1.0\n",
      "loss: 0.014782578684389591, accuracy: 1.0\n",
      "loss: 0.015603470616042614, accuracy: 1.0\n",
      "loss: 0.015775181353092194, accuracy: 1.0\n",
      "loss: 0.015925506129860878, accuracy: 1.0\n",
      "loss: 0.015811290591955185, accuracy: 1.0\n",
      "loss: 0.016005123034119606, accuracy: 1.0\n",
      "loss: 0.015287505462765694, accuracy: 1.0\n",
      "loss: 0.0150517039000988, accuracy: 1.0\n",
      "loss: 0.01634887047111988, accuracy: 1.0\n",
      "loss: 0.01633756048977375, accuracy: 1.0\n",
      "loss: 0.014591095969080925, accuracy: 1.0\n",
      "loss: 0.014785747975111008, accuracy: 1.0\n",
      "loss: 0.014696231111884117, accuracy: 1.0\n",
      "loss: 0.014263640157878399, accuracy: 1.0\n",
      "loss: 0.014958279207348824, accuracy: 1.0\n",
      "loss: 0.015741853043437004, accuracy: 1.0\n",
      "loss: 0.01497630588710308, accuracy: 1.0\n",
      "loss: 0.016417091712355614, accuracy: 1.0\n",
      "loss: 0.017609262838959694, accuracy: 1.0\n",
      "loss: 0.019986595958471298, accuracy: 0.9956331877729258\n",
      "loss: 0.01549491472542286, accuracy: 1.0\n",
      "loss: 0.016786929219961166, accuracy: 1.0\n",
      "loss: 0.01507360115647316, accuracy: 1.0\n",
      "loss: 0.01514707412570715, accuracy: 1.0\n",
      "loss: 0.015468140132725239, accuracy: 1.0\n",
      "loss: 0.01760018803179264, accuracy: 1.0\n",
      "loss: 0.019254788756370544, accuracy: 0.9956331877729258\n",
      "loss: 0.015395171009004116, accuracy: 1.0\n",
      "loss: 0.015370707027614117, accuracy: 1.0\n",
      "loss: 0.014665541239082813, accuracy: 1.0\n",
      "loss: 0.015046234242618084, accuracy: 1.0\n",
      "loss: 0.01528683677315712, accuracy: 1.0\n",
      "loss: 0.016162002459168434, accuracy: 1.0\n",
      "loss: 0.014532552100718021, accuracy: 1.0\n",
      "loss: 0.015371524728834629, accuracy: 1.0\n",
      "loss: 0.01414906233549118, accuracy: 1.0\n",
      "loss: 0.015466458164155483, accuracy: 1.0\n",
      "loss: 0.015845345333218575, accuracy: 1.0\n",
      "loss: 0.015171492472290993, accuracy: 1.0\n",
      "loss: 0.014678603038191795, accuracy: 1.0\n",
      "loss: 0.014438467100262642, accuracy: 1.0\n",
      "loss: 0.014856643974781036, accuracy: 1.0\n",
      "loss: 0.014674084261059761, accuracy: 1.0\n",
      "loss: 0.014453980140388012, accuracy: 1.0\n",
      "loss: 0.01523556187748909, accuracy: 1.0\n",
      "loss: 0.01669834926724434, accuracy: 1.0\n",
      "loss: 0.014491531066596508, accuracy: 1.0\n",
      "loss: 0.015709441155195236, accuracy: 1.0\n",
      "loss: 0.01993107981979847, accuracy: 1.0\n",
      "loss: 0.017200950533151627, accuracy: 0.9956331877729258\n",
      "loss: 0.016007309779524803, accuracy: 1.0\n",
      "loss: 0.015161233022809029, accuracy: 1.0\n",
      "loss: 0.0155037147924304, accuracy: 1.0\n",
      "loss: 0.016264157369732857, accuracy: 1.0\n",
      "loss: 0.016151096671819687, accuracy: 1.0\n",
      "loss: 0.014156238175928593, accuracy: 1.0\n",
      "loss: 0.014820622280240059, accuracy: 1.0\n",
      "loss: 0.013658398762345314, accuracy: 1.0\n",
      "loss: 0.01396231260150671, accuracy: 1.0\n",
      "loss: 0.014315604232251644, accuracy: 1.0\n",
      "loss: 0.015597901307046413, accuracy: 1.0\n",
      "loss: 0.01582934334874153, accuracy: 1.0\n",
      "loss: 0.015203684568405151, accuracy: 1.0\n",
      "loss: 0.01415283977985382, accuracy: 1.0\n",
      "loss: 0.014223422855138779, accuracy: 1.0\n",
      "loss: 0.015872251242399216, accuracy: 1.0\n",
      "loss: 0.01523992232978344, accuracy: 1.0\n",
      "loss: 0.015050070360302925, accuracy: 1.0\n",
      "loss: 0.014934789389371872, accuracy: 1.0\n",
      "loss: 0.014677990227937698, accuracy: 1.0\n",
      "loss: 0.01387940812855959, accuracy: 1.0\n",
      "loss: 0.020207257941365242, accuracy: 1.0\n",
      "loss: 0.01674855500459671, accuracy: 1.0\n",
      "loss: 0.015185539610683918, accuracy: 1.0\n",
      "loss: 0.017915334552526474, accuracy: 1.0\n",
      "loss: 0.015112880617380142, accuracy: 1.0\n",
      "loss: 0.013942088931798935, accuracy: 1.0\n",
      "loss: 0.015524555929005146, accuracy: 1.0\n",
      "loss: 0.014996535144746304, accuracy: 1.0\n",
      "loss: 0.014558468014001846, accuracy: 1.0\n",
      "loss: 0.014805665239691734, accuracy: 1.0\n",
      "loss: 0.01534387469291687, accuracy: 1.0\n",
      "loss: 0.014850201085209846, accuracy: 1.0\n",
      "loss: 0.014317921362817287, accuracy: 1.0\n",
      "loss: 0.014202969148755074, accuracy: 1.0\n",
      "loss: 0.016129976138472557, accuracy: 1.0\n",
      "loss: 0.013798821717500687, accuracy: 1.0\n",
      "loss: 0.014323248527944088, accuracy: 1.0\n",
      "loss: 0.014096676371991634, accuracy: 1.0\n",
      "loss: 0.01579561084508896, accuracy: 1.0\n",
      "loss: 0.015180322341620922, accuracy: 1.0\n",
      "loss: 0.014542536810040474, accuracy: 1.0\n",
      "loss: 0.014467845670878887, accuracy: 1.0\n",
      "loss: 0.01552790217101574, accuracy: 1.0\n",
      "loss: 0.014549249783158302, accuracy: 1.0\n",
      "loss: 0.013933970592916012, accuracy: 1.0\n",
      "loss: 0.01553652249276638, accuracy: 1.0\n",
      "loss: 0.014362855814397335, accuracy: 1.0\n",
      "loss: 0.013780564069747925, accuracy: 1.0\n",
      "loss: 0.014567065052688122, accuracy: 1.0\n",
      "loss: 0.014027751050889492, accuracy: 1.0\n",
      "loss: 0.014976183883845806, accuracy: 1.0\n",
      "loss: 0.01343739777803421, accuracy: 1.0\n",
      "loss: 0.014783026650547981, accuracy: 1.0\n",
      "loss: 0.013740019872784615, accuracy: 1.0\n",
      "loss: 0.013412817381322384, accuracy: 1.0\n",
      "loss: 0.013369663618505001, accuracy: 1.0\n",
      "loss: 0.014611747115850449, accuracy: 1.0\n",
      "loss: 0.01411300990730524, accuracy: 1.0\n",
      "loss: 0.01631843112409115, accuracy: 1.0\n",
      "loss: 0.013767859898507595, accuracy: 1.0\n",
      "loss: 0.013985471799969673, accuracy: 1.0\n",
      "loss: 0.014444706961512566, accuracy: 1.0\n",
      "loss: 0.014336741529405117, accuracy: 1.0\n",
      "loss: 0.014228441752493382, accuracy: 1.0\n",
      "loss: 0.014769780449569225, accuracy: 1.0\n",
      "loss: 0.014198752120137215, accuracy: 1.0\n",
      "loss: 0.015285223722457886, accuracy: 1.0\n",
      "loss: 0.013982330448925495, accuracy: 1.0\n",
      "loss: 0.013970826752483845, accuracy: 1.0\n",
      "loss: 0.013578329235315323, accuracy: 1.0\n",
      "loss: 0.013430680148303509, accuracy: 1.0\n",
      "loss: 0.013940941542387009, accuracy: 1.0\n",
      "loss: 0.01376316323876381, accuracy: 1.0\n",
      "loss: 0.01416933722794056, accuracy: 1.0\n",
      "loss: 0.015050480142235756, accuracy: 1.0\n",
      "loss: 0.014790553599596024, accuracy: 1.0\n",
      "loss: 0.015636678785085678, accuracy: 1.0\n",
      "loss: 0.014406319707632065, accuracy: 1.0\n",
      "loss: 0.014442142099142075, accuracy: 1.0\n",
      "loss: 0.014221074990928173, accuracy: 1.0\n",
      "loss: 0.01563287153840065, accuracy: 1.0\n",
      "loss: 0.014229791238904, accuracy: 1.0\n",
      "loss: 0.01409108005464077, accuracy: 1.0\n",
      "loss: 0.016899893060326576, accuracy: 0.9956331877729258\n",
      "loss: 0.013421718962490559, accuracy: 1.0\n",
      "loss: 0.013337850570678711, accuracy: 1.0\n",
      "loss: 0.014320417307317257, accuracy: 1.0\n",
      "loss: 0.01401431579142809, accuracy: 1.0\n",
      "loss: 0.013243614695966244, accuracy: 1.0\n",
      "loss: 0.013516981154680252, accuracy: 1.0\n",
      "loss: 0.014424344524741173, accuracy: 1.0\n",
      "loss: 0.014153699390590191, accuracy: 1.0\n",
      "loss: 0.01313693355768919, accuracy: 1.0\n",
      "loss: 0.014364922419190407, accuracy: 1.0\n",
      "loss: 0.013088172301650047, accuracy: 1.0\n",
      "loss: 0.014417569153010845, accuracy: 1.0\n",
      "loss: 0.014243285171687603, accuracy: 1.0\n",
      "loss: 0.013658116571605206, accuracy: 1.0\n",
      "loss: 0.01507325004786253, accuracy: 1.0\n",
      "loss: 0.01323577482253313, accuracy: 1.0\n",
      "loss: 0.014046499505639076, accuracy: 1.0\n",
      "loss: 0.01385559979826212, accuracy: 1.0\n",
      "loss: 0.012711701914668083, accuracy: 1.0\n",
      "loss: 0.013641365803778172, accuracy: 1.0\n",
      "loss: 0.014583255164325237, accuracy: 1.0\n",
      "loss: 0.0142270028591156, accuracy: 1.0\n",
      "loss: 0.013834425248205662, accuracy: 1.0\n",
      "loss: 0.017648523673415184, accuracy: 1.0\n",
      "loss: 0.017125315964221954, accuracy: 0.9956331877729258\n",
      "loss: 0.013275775127112865, accuracy: 1.0\n",
      "loss: 0.013613476417958736, accuracy: 1.0\n",
      "loss: 0.014982952736318111, accuracy: 1.0\n",
      "loss: 0.013768159784376621, accuracy: 1.0\n",
      "loss: 0.013641872443258762, accuracy: 1.0\n",
      "loss: 0.014378598891198635, accuracy: 1.0\n",
      "loss: 0.012805843725800514, accuracy: 1.0\n",
      "loss: 0.015532014891505241, accuracy: 1.0\n",
      "loss: 0.013403545133769512, accuracy: 1.0\n",
      "loss: 0.014142125844955444, accuracy: 1.0\n",
      "loss: 0.013414508663117886, accuracy: 1.0\n",
      "loss: 0.01481909491121769, accuracy: 1.0\n",
      "loss: 0.013462319038808346, accuracy: 1.0\n",
      "loss: 0.013599361293017864, accuracy: 1.0\n",
      "loss: 0.012930723838508129, accuracy: 1.0\n",
      "loss: 0.013186168856918812, accuracy: 1.0\n",
      "loss: 0.014175012707710266, accuracy: 1.0\n",
      "loss: 0.014633567072451115, accuracy: 1.0\n",
      "loss: 0.013471126556396484, accuracy: 1.0\n",
      "loss: 0.013397829607129097, accuracy: 1.0\n",
      "loss: 0.013305886648595333, accuracy: 1.0\n",
      "loss: 0.013443348929286003, accuracy: 1.0\n",
      "loss: 0.01312887854874134, accuracy: 1.0\n",
      "loss: 0.012751058675348759, accuracy: 1.0\n",
      "loss: 0.013691131956875324, accuracy: 1.0\n",
      "loss: 0.012694794684648514, accuracy: 1.0\n",
      "loss: 0.014132460579276085, accuracy: 1.0\n",
      "loss: 0.013518891297280788, accuracy: 1.0\n",
      "loss: 0.013728629797697067, accuracy: 1.0\n",
      "loss: 0.014692403376102448, accuracy: 1.0\n",
      "loss: 0.01366161648184061, accuracy: 1.0\n",
      "loss: 0.014124151319265366, accuracy: 1.0\n",
      "loss: 0.014300127513706684, accuracy: 1.0\n",
      "loss: 0.013962289318442345, accuracy: 1.0\n",
      "loss: 0.01425712276250124, accuracy: 1.0\n",
      "loss: 0.015477055683732033, accuracy: 0.9956331877729258\n",
      "loss: 0.012715617194771767, accuracy: 1.0\n",
      "loss: 0.01366288773715496, accuracy: 1.0\n",
      "loss: 0.014040220528841019, accuracy: 1.0\n",
      "loss: 0.01310625672340393, accuracy: 1.0\n",
      "loss: 0.01337199006229639, accuracy: 1.0\n",
      "loss: 0.013474859297275543, accuracy: 1.0\n",
      "loss: 0.01369300577789545, accuracy: 1.0\n",
      "loss: 0.01355519238859415, accuracy: 1.0\n",
      "loss: 0.012370575219392776, accuracy: 1.0\n",
      "loss: 0.01243573147803545, accuracy: 1.0\n",
      "loss: 0.012969383969902992, accuracy: 1.0\n",
      "loss: 0.013008511625230312, accuracy: 1.0\n",
      "loss: 0.013198629021644592, accuracy: 1.0\n",
      "loss: 0.014629099518060684, accuracy: 1.0\n",
      "loss: 0.013675494119524956, accuracy: 1.0\n",
      "loss: 0.013299289159476757, accuracy: 1.0\n",
      "loss: 0.013208233751356602, accuracy: 1.0\n",
      "loss: 0.012445406056940556, accuracy: 1.0\n",
      "loss: 0.01372036524116993, accuracy: 1.0\n",
      "loss: 0.014087366871535778, accuracy: 1.0\n",
      "loss: 0.01329551637172699, accuracy: 1.0\n",
      "loss: 0.012374036945402622, accuracy: 1.0\n",
      "loss: 0.01255345344543457, accuracy: 1.0\n",
      "loss: 0.013100573793053627, accuracy: 1.0\n",
      "loss: 0.012875841930508614, accuracy: 1.0\n",
      "loss: 0.01628378964960575, accuracy: 1.0\n",
      "loss: 0.01263712253421545, accuracy: 1.0\n",
      "loss: 0.01257216464728117, accuracy: 1.0\n",
      "loss: 0.013623916544020176, accuracy: 1.0\n",
      "loss: 0.013575228862464428, accuracy: 1.0\n",
      "loss: 0.013290159404277802, accuracy: 1.0\n",
      "loss: 0.013421475887298584, accuracy: 1.0\n",
      "loss: 0.012911367230117321, accuracy: 1.0\n",
      "loss: 0.013204343616962433, accuracy: 1.0\n",
      "loss: 0.012560873292386532, accuracy: 1.0\n",
      "loss: 0.013687492348253727, accuracy: 1.0\n",
      "loss: 0.014151723124086857, accuracy: 1.0\n",
      "loss: 0.014097911305725574, accuracy: 1.0\n",
      "loss: 0.01287562120705843, accuracy: 1.0\n",
      "loss: 0.012939149513840675, accuracy: 1.0\n",
      "loss: 0.013671334832906723, accuracy: 1.0\n",
      "loss: 0.014253123663365841, accuracy: 1.0\n",
      "loss: 0.01272006705403328, accuracy: 1.0\n",
      "loss: 0.013081640936434269, accuracy: 1.0\n",
      "loss: 0.012689069844782352, accuracy: 1.0\n",
      "loss: 0.013072854839265347, accuracy: 1.0\n",
      "loss: 0.012500453740358353, accuracy: 1.0\n",
      "loss: 0.012914364226162434, accuracy: 1.0\n",
      "loss: 0.013649205677211285, accuracy: 1.0\n",
      "loss: 0.013390611857175827, accuracy: 1.0\n",
      "loss: 0.013543962500989437, accuracy: 1.0\n",
      "loss: 0.013831662945449352, accuracy: 1.0\n",
      "loss: 0.012804321944713593, accuracy: 1.0\n",
      "loss: 0.013256033882498741, accuracy: 1.0\n",
      "loss: 0.013272770680487156, accuracy: 1.0\n",
      "loss: 0.011935130693018436, accuracy: 1.0\n",
      "loss: 0.013758348301053047, accuracy: 1.0\n",
      "loss: 0.012529279105365276, accuracy: 1.0\n",
      "loss: 0.012859168462455273, accuracy: 1.0\n",
      "loss: 0.012634373269975185, accuracy: 1.0\n",
      "loss: 0.012823655270040035, accuracy: 1.0\n",
      "loss: 0.012999170459806919, accuracy: 1.0\n",
      "loss: 0.012177658267319202, accuracy: 1.0\n",
      "loss: 0.013486290350556374, accuracy: 1.0\n",
      "loss: 0.012422448955476284, accuracy: 1.0\n",
      "loss: 0.01264921110123396, accuracy: 1.0\n",
      "loss: 0.013073109090328217, accuracy: 1.0\n",
      "loss: 0.014244364574551582, accuracy: 1.0\n",
      "loss: 0.01388325821608305, accuracy: 1.0\n",
      "loss: 0.012102681212127209, accuracy: 1.0\n",
      "loss: 0.013284417800605297, accuracy: 1.0\n",
      "loss: 0.012903990224003792, accuracy: 1.0\n",
      "loss: 0.012242335826158524, accuracy: 1.0\n",
      "loss: 0.012549239210784435, accuracy: 1.0\n",
      "loss: 0.012913462705910206, accuracy: 1.0\n",
      "loss: 0.012685190886259079, accuracy: 1.0\n",
      "loss: 0.013894175179302692, accuracy: 1.0\n",
      "loss: 0.012993105687201023, accuracy: 1.0\n",
      "loss: 0.013329840265214443, accuracy: 1.0\n",
      "loss: 0.012528740800917149, accuracy: 1.0\n",
      "loss: 0.013112026266753674, accuracy: 1.0\n",
      "loss: 0.013103315606713295, accuracy: 1.0\n",
      "loss: 0.01270150113850832, accuracy: 1.0\n",
      "loss: 0.013075507245957851, accuracy: 1.0\n",
      "loss: 0.012194541282951832, accuracy: 1.0\n",
      "loss: 0.012450193986296654, accuracy: 1.0\n",
      "loss: 0.012797646224498749, accuracy: 1.0\n",
      "loss: 0.012909555807709694, accuracy: 1.0\n",
      "loss: 0.012517866678535938, accuracy: 1.0\n",
      "loss: 0.012345420196652412, accuracy: 1.0\n",
      "loss: 0.012675544247031212, accuracy: 1.0\n",
      "loss: 0.012534425593912601, accuracy: 1.0\n",
      "loss: 0.013290751725435257, accuracy: 1.0\n",
      "loss: 0.012418055906891823, accuracy: 1.0\n",
      "loss: 0.012187199667096138, accuracy: 1.0\n",
      "loss: 0.012184168212115765, accuracy: 1.0\n",
      "loss: 0.013486264273524284, accuracy: 1.0\n",
      "loss: 0.012449752539396286, accuracy: 1.0\n",
      "loss: 0.013617012649774551, accuracy: 1.0\n",
      "loss: 0.013765271753072739, accuracy: 1.0\n",
      "loss: 0.012228239327669144, accuracy: 1.0\n",
      "loss: 0.012829706072807312, accuracy: 1.0\n",
      "loss: 0.011911898851394653, accuracy: 1.0\n",
      "loss: 0.013078576885163784, accuracy: 1.0\n",
      "loss: 0.012936819344758987, accuracy: 1.0\n",
      "loss: 0.012232893146574497, accuracy: 1.0\n",
      "loss: 0.013367678038775921, accuracy: 1.0\n",
      "loss: 0.013794389553368092, accuracy: 1.0\n",
      "loss: 0.012481695041060448, accuracy: 1.0\n",
      "loss: 0.012506472878158092, accuracy: 1.0\n",
      "loss: 0.012884174473583698, accuracy: 1.0\n",
      "loss: 0.012631075456738472, accuracy: 1.0\n",
      "loss: 0.011989028193056583, accuracy: 1.0\n",
      "loss: 0.013044612482190132, accuracy: 1.0\n",
      "loss: 0.012584521435201168, accuracy: 1.0\n",
      "loss: 0.012265373952686787, accuracy: 1.0\n",
      "loss: 0.012612841092050076, accuracy: 1.0\n",
      "loss: 0.012140441685914993, accuracy: 1.0\n",
      "loss: 0.013724355958402157, accuracy: 1.0\n",
      "loss: 0.011764860711991787, accuracy: 1.0\n",
      "loss: 0.01311381347477436, accuracy: 1.0\n",
      "loss: 0.012177548371255398, accuracy: 1.0\n",
      "loss: 0.01223537977784872, accuracy: 1.0\n",
      "loss: 0.011756256222724915, accuracy: 1.0\n",
      "loss: 0.012867107056081295, accuracy: 1.0\n",
      "loss: 0.012682637199759483, accuracy: 1.0\n",
      "loss: 0.013332469388842583, accuracy: 1.0\n",
      "loss: 0.012287145480513573, accuracy: 1.0\n",
      "loss: 0.011456119827926159, accuracy: 1.0\n",
      "loss: 0.012147628702223301, accuracy: 1.0\n",
      "loss: 0.012005000375211239, accuracy: 1.0\n",
      "loss: 0.012172851711511612, accuracy: 1.0\n",
      "loss: 0.012469098903238773, accuracy: 1.0\n",
      "loss: 0.011973702348768711, accuracy: 1.0\n",
      "loss: 0.013815837912261486, accuracy: 1.0\n",
      "loss: 0.012224454432725906, accuracy: 1.0\n",
      "loss: 0.011477701365947723, accuracy: 1.0\n",
      "loss: 0.013029798865318298, accuracy: 1.0\n",
      "loss: 0.013117769733071327, accuracy: 1.0\n",
      "loss: 0.011442639864981174, accuracy: 1.0\n",
      "loss: 0.013469168916344643, accuracy: 1.0\n",
      "loss: 0.011998523958027363, accuracy: 1.0\n",
      "loss: 0.0130479009822011, accuracy: 1.0\n",
      "loss: 0.012139763683080673, accuracy: 1.0\n",
      "loss: 0.012675105594098568, accuracy: 1.0\n",
      "loss: 0.011926583014428616, accuracy: 1.0\n",
      "loss: 0.012655334547162056, accuracy: 1.0\n",
      "loss: 0.011699817143380642, accuracy: 1.0\n",
      "loss: 0.013669058680534363, accuracy: 1.0\n",
      "loss: 0.012472203932702541, accuracy: 1.0\n",
      "loss: 0.01192362979054451, accuracy: 1.0\n",
      "loss: 0.01172980759292841, accuracy: 1.0\n",
      "loss: 0.011921368539333344, accuracy: 1.0\n",
      "loss: 0.011893355287611485, accuracy: 1.0\n",
      "loss: 0.012273640371859074, accuracy: 1.0\n",
      "loss: 0.011903107166290283, accuracy: 1.0\n",
      "loss: 0.011595958843827248, accuracy: 1.0\n",
      "loss: 0.013174972496926785, accuracy: 1.0\n",
      "loss: 0.012013200670480728, accuracy: 1.0\n",
      "loss: 0.012026708573102951, accuracy: 1.0\n",
      "loss: 0.013614710420370102, accuracy: 1.0\n",
      "loss: 0.012645384296774864, accuracy: 1.0\n",
      "loss: 0.012004073709249496, accuracy: 1.0\n",
      "loss: 0.012140821665525436, accuracy: 1.0\n",
      "loss: 0.011441964656114578, accuracy: 1.0\n",
      "loss: 0.013119146227836609, accuracy: 1.0\n",
      "loss: 0.011767924763262272, accuracy: 1.0\n",
      "loss: 0.012044333852827549, accuracy: 1.0\n",
      "loss: 0.012296205386519432, accuracy: 1.0\n",
      "loss: 0.012789949774742126, accuracy: 1.0\n",
      "loss: 0.012064553797245026, accuracy: 1.0\n",
      "loss: 0.011800953187048435, accuracy: 1.0\n",
      "loss: 0.011783885769546032, accuracy: 1.0\n",
      "loss: 0.012922878377139568, accuracy: 1.0\n",
      "loss: 0.01286045741289854, accuracy: 1.0\n",
      "loss: 0.014447418972849846, accuracy: 1.0\n",
      "loss: 0.012806507758796215, accuracy: 1.0\n",
      "loss: 0.013029900379478931, accuracy: 1.0\n",
      "loss: 0.012586032040417194, accuracy: 1.0\n",
      "loss: 0.011486315168440342, accuracy: 1.0\n",
      "loss: 0.011706003919243813, accuracy: 1.0\n",
      "loss: 0.012260070070624352, accuracy: 1.0\n",
      "loss: 0.011982700787484646, accuracy: 1.0\n",
      "loss: 0.01122856792062521, accuracy: 1.0\n",
      "loss: 0.012516978196799755, accuracy: 1.0\n",
      "loss: 0.011980046518146992, accuracy: 1.0\n",
      "loss: 0.012101499363780022, accuracy: 1.0\n",
      "loss: 0.011691155843436718, accuracy: 1.0\n",
      "loss: 0.011556873098015785, accuracy: 1.0\n",
      "loss: 0.011495813727378845, accuracy: 1.0\n",
      "loss: 0.012997332029044628, accuracy: 1.0\n",
      "loss: 0.011696353554725647, accuracy: 1.0\n",
      "loss: 0.011308349668979645, accuracy: 1.0\n",
      "loss: 0.012490546330809593, accuracy: 1.0\n",
      "loss: 0.011407078243792057, accuracy: 1.0\n",
      "loss: 0.011949872598052025, accuracy: 1.0\n",
      "loss: 0.011593030765652657, accuracy: 1.0\n",
      "loss: 0.012082149274647236, accuracy: 1.0\n",
      "loss: 0.012973351404070854, accuracy: 1.0\n",
      "loss: 0.01228538528084755, accuracy: 1.0\n",
      "loss: 0.011522302404046059, accuracy: 1.0\n",
      "loss: 0.011843143031001091, accuracy: 1.0\n",
      "loss: 0.012086264789104462, accuracy: 1.0\n",
      "loss: 0.011269881390035152, accuracy: 1.0\n",
      "loss: 0.011106357909739017, accuracy: 1.0\n",
      "loss: 0.011552561074495316, accuracy: 1.0\n",
      "loss: 0.011339177377521992, accuracy: 1.0\n",
      "loss: 0.01132259238511324, accuracy: 1.0\n",
      "loss: 0.011907579377293587, accuracy: 1.0\n",
      "loss: 0.011998324654996395, accuracy: 1.0\n",
      "loss: 0.013602884486317635, accuracy: 1.0\n",
      "loss: 0.012213819660246372, accuracy: 1.0\n",
      "loss: 0.011128808371722698, accuracy: 1.0\n",
      "loss: 0.01137987244874239, accuracy: 1.0\n",
      "loss: 0.01166889350861311, accuracy: 1.0\n",
      "loss: 0.012316239066421986, accuracy: 1.0\n",
      "loss: 0.011360688135027885, accuracy: 1.0\n",
      "loss: 0.011992181651294231, accuracy: 1.0\n",
      "loss: 0.01088230311870575, accuracy: 1.0\n",
      "loss: 0.012178041972219944, accuracy: 1.0\n",
      "loss: 0.012275935150682926, accuracy: 1.0\n",
      "loss: 0.012196356430649757, accuracy: 1.0\n",
      "loss: 0.012118729762732983, accuracy: 1.0\n",
      "loss: 0.01177721843123436, accuracy: 1.0\n",
      "loss: 0.012075955048203468, accuracy: 1.0\n",
      "loss: 0.01109747588634491, accuracy: 1.0\n",
      "loss: 0.012391087599098682, accuracy: 1.0\n",
      "loss: 0.011851062066853046, accuracy: 1.0\n",
      "loss: 0.012011954560875893, accuracy: 1.0\n",
      "loss: 0.011501428671181202, accuracy: 1.0\n",
      "loss: 0.010969680733978748, accuracy: 1.0\n",
      "loss: 0.012196035124361515, accuracy: 1.0\n",
      "loss: 0.011675742454826832, accuracy: 1.0\n",
      "loss: 0.01178146805614233, accuracy: 1.0\n",
      "loss: 0.010629191063344479, accuracy: 1.0\n",
      "loss: 0.012247682549059391, accuracy: 1.0\n",
      "loss: 0.011146701872348785, accuracy: 1.0\n",
      "loss: 0.01133780274540186, accuracy: 1.0\n",
      "loss: 0.011752679944038391, accuracy: 1.0\n",
      "loss: 0.011232386343181133, accuracy: 1.0\n",
      "loss: 0.010702608153223991, accuracy: 1.0\n",
      "loss: 0.012438328936696053, accuracy: 1.0\n",
      "loss: 0.012954012490808964, accuracy: 1.0\n",
      "loss: 0.01105327345430851, accuracy: 1.0\n",
      "loss: 0.011425038799643517, accuracy: 1.0\n",
      "loss: 0.011244403198361397, accuracy: 1.0\n",
      "loss: 0.01111414935439825, accuracy: 1.0\n",
      "loss: 0.011561199091374874, accuracy: 1.0\n",
      "loss: 0.0116653423756361, accuracy: 1.0\n",
      "loss: 0.01092890277504921, accuracy: 1.0\n",
      "loss: 0.011630244553089142, accuracy: 1.0\n",
      "loss: 0.012076953426003456, accuracy: 1.0\n",
      "loss: 0.011791002936661243, accuracy: 1.0\n",
      "loss: 0.011183909140527248, accuracy: 1.0\n",
      "loss: 0.011091080494225025, accuracy: 1.0\n",
      "loss: 0.01245503593236208, accuracy: 1.0\n",
      "loss: 0.011313546448946, accuracy: 1.0\n",
      "loss: 0.011085230857133865, accuracy: 1.0\n",
      "loss: 0.011095929890871048, accuracy: 1.0\n",
      "loss: 0.010883004404604435, accuracy: 1.0\n",
      "loss: 0.010591698810458183, accuracy: 1.0\n",
      "loss: 0.011241666972637177, accuracy: 1.0\n",
      "loss: 0.010944551788270473, accuracy: 1.0\n",
      "loss: 0.01135338842868805, accuracy: 1.0\n",
      "loss: 0.011081097647547722, accuracy: 1.0\n",
      "loss: 0.011904250830411911, accuracy: 1.0\n",
      "loss: 0.013332493603229523, accuracy: 1.0\n",
      "loss: 0.011614288203418255, accuracy: 1.0\n",
      "loss: 0.011846667155623436, accuracy: 1.0\n",
      "loss: 0.011839760467410088, accuracy: 1.0\n",
      "loss: 0.011267500929534435, accuracy: 1.0\n",
      "loss: 0.01099166739732027, accuracy: 1.0\n",
      "loss: 0.011251796968281269, accuracy: 1.0\n",
      "loss: 0.011034771800041199, accuracy: 1.0\n",
      "loss: 0.011612700298428535, accuracy: 1.0\n",
      "loss: 0.01208579633384943, accuracy: 1.0\n",
      "loss: 0.011554432101547718, accuracy: 1.0\n",
      "loss: 0.010992794297635555, accuracy: 1.0\n",
      "loss: 0.010708850808441639, accuracy: 1.0\n",
      "loss: 0.011490565724670887, accuracy: 1.0\n",
      "loss: 0.012070728465914726, accuracy: 1.0\n",
      "loss: 0.010935336351394653, accuracy: 1.0\n",
      "loss: 0.010609729215502739, accuracy: 1.0\n",
      "loss: 0.011146977543830872, accuracy: 1.0\n",
      "loss: 0.011598998680710793, accuracy: 1.0\n",
      "loss: 0.011906330473721027, accuracy: 1.0\n",
      "loss: 0.011264335364103317, accuracy: 1.0\n",
      "loss: 0.012443619780242443, accuracy: 1.0\n",
      "loss: 0.011089354753494263, accuracy: 1.0\n",
      "loss: 0.011034357361495495, accuracy: 1.0\n",
      "loss: 0.01140069030225277, accuracy: 1.0\n",
      "loss: 0.011297836899757385, accuracy: 1.0\n",
      "loss: 0.01079578697681427, accuracy: 1.0\n",
      "loss: 0.011052222922444344, accuracy: 1.0\n",
      "loss: 0.011202320456504822, accuracy: 1.0\n",
      "loss: 0.011141794733703136, accuracy: 1.0\n",
      "loss: 0.01140257902443409, accuracy: 1.0\n",
      "loss: 0.011221600696444511, accuracy: 1.0\n",
      "loss: 0.010820072144269943, accuracy: 1.0\n",
      "loss: 0.010497969575226307, accuracy: 1.0\n",
      "loss: 0.010932392440736294, accuracy: 1.0\n",
      "loss: 0.01064850389957428, accuracy: 1.0\n",
      "loss: 0.010908097960054874, accuracy: 1.0\n",
      "loss: 0.011161357164382935, accuracy: 1.0\n",
      "loss: 0.011208605952560902, accuracy: 1.0\n",
      "loss: 0.010626575909554958, accuracy: 1.0\n",
      "loss: 0.010562638752162457, accuracy: 1.0\n",
      "loss: 0.011960702948272228, accuracy: 1.0\n",
      "loss: 0.011429001577198505, accuracy: 1.0\n",
      "loss: 0.01117213536053896, accuracy: 1.0\n",
      "loss: 0.010725112631917, accuracy: 1.0\n",
      "loss: 0.011073855683207512, accuracy: 1.0\n",
      "loss: 0.01138511672616005, accuracy: 1.0\n",
      "loss: 0.011140484362840652, accuracy: 1.0\n",
      "loss: 0.010784072801470757, accuracy: 1.0\n",
      "loss: 0.011711349710822105, accuracy: 1.0\n",
      "loss: 0.010552315972745419, accuracy: 1.0\n",
      "loss: 0.010425243526697159, accuracy: 1.0\n",
      "loss: 0.011856576427817345, accuracy: 1.0\n",
      "loss: 0.01175894308835268, accuracy: 1.0\n",
      "loss: 0.010289384052157402, accuracy: 1.0\n",
      "loss: 0.010680307634174824, accuracy: 1.0\n",
      "loss: 0.010980234481394291, accuracy: 1.0\n",
      "loss: 0.010519037954509258, accuracy: 1.0\n",
      "loss: 0.010351025499403477, accuracy: 1.0\n",
      "loss: 0.011453591287136078, accuracy: 1.0\n",
      "loss: 0.010391606949269772, accuracy: 1.0\n",
      "loss: 0.011085829697549343, accuracy: 1.0\n",
      "loss: 0.010841865092515945, accuracy: 1.0\n",
      "loss: 0.010859562084078789, accuracy: 1.0\n",
      "loss: 0.010617793537676334, accuracy: 1.0\n",
      "loss: 0.010660621337592602, accuracy: 1.0\n",
      "loss: 0.011842955835163593, accuracy: 1.0\n",
      "loss: 0.011200431734323502, accuracy: 1.0\n",
      "loss: 0.011615179479122162, accuracy: 1.0\n",
      "loss: 0.010573563165962696, accuracy: 1.0\n",
      "loss: 0.011175369843840599, accuracy: 1.0\n",
      "loss: 0.011843868531286716, accuracy: 1.0\n",
      "loss: 0.010924757458269596, accuracy: 1.0\n",
      "loss: 0.010504331439733505, accuracy: 1.0\n",
      "loss: 0.010373666882514954, accuracy: 1.0\n",
      "loss: 0.010305826552212238, accuracy: 1.0\n",
      "loss: 0.010844831354916096, accuracy: 1.0\n",
      "loss: 0.010636973194777966, accuracy: 1.0\n",
      "loss: 0.011069572530686855, accuracy: 1.0\n",
      "loss: 0.011159677058458328, accuracy: 1.0\n",
      "loss: 0.010480844415724277, accuracy: 1.0\n",
      "loss: 0.010301659815013409, accuracy: 1.0\n",
      "loss: 0.011136049404740334, accuracy: 1.0\n",
      "loss: 0.010809635743498802, accuracy: 1.0\n",
      "loss: 0.010760178789496422, accuracy: 1.0\n",
      "loss: 0.011745109222829342, accuracy: 1.0\n",
      "loss: 0.011457459069788456, accuracy: 1.0\n",
      "loss: 0.01159574743360281, accuracy: 1.0\n",
      "loss: 0.012463030405342579, accuracy: 1.0\n",
      "loss: 0.01071151439100504, accuracy: 1.0\n",
      "loss: 0.010820024646818638, accuracy: 1.0\n",
      "loss: 0.010557304136455059, accuracy: 1.0\n",
      "loss: 0.010192733258008957, accuracy: 1.0\n",
      "loss: 0.010820324532687664, accuracy: 1.0\n",
      "loss: 0.010870274156332016, accuracy: 1.0\n",
      "loss: 0.011061344295740128, accuracy: 1.0\n",
      "loss: 0.01018055435270071, accuracy: 1.0\n",
      "loss: 0.010603140108287334, accuracy: 1.0\n",
      "loss: 0.010601848363876343, accuracy: 1.0\n",
      "loss: 0.010373563505709171, accuracy: 1.0\n",
      "loss: 0.01078468281775713, accuracy: 1.0\n",
      "loss: 0.011286945082247257, accuracy: 1.0\n",
      "loss: 0.010831053368747234, accuracy: 1.0\n",
      "loss: 0.010562238283455372, accuracy: 1.0\n",
      "loss: 0.010358436964452267, accuracy: 1.0\n",
      "loss: 0.009825263172388077, accuracy: 1.0\n",
      "loss: 0.010750561952590942, accuracy: 1.0\n",
      "loss: 0.010373477824032307, accuracy: 1.0\n",
      "loss: 0.01052527129650116, accuracy: 1.0\n",
      "loss: 0.011750797741115093, accuracy: 1.0\n",
      "loss: 0.011334608308970928, accuracy: 1.0\n",
      "loss: 0.01047008391469717, accuracy: 1.0\n",
      "loss: 0.010665375739336014, accuracy: 1.0\n",
      "loss: 0.01068367250263691, accuracy: 1.0\n",
      "loss: 0.010624241083860397, accuracy: 1.0\n",
      "loss: 0.011195972561836243, accuracy: 1.0\n",
      "loss: 0.011402996256947517, accuracy: 1.0\n",
      "loss: 0.011220226064324379, accuracy: 1.0\n",
      "loss: 0.010299311950802803, accuracy: 1.0\n",
      "loss: 0.010200327262282372, accuracy: 1.0\n",
      "loss: 0.010229776613414288, accuracy: 1.0\n",
      "loss: 0.00990741141140461, accuracy: 1.0\n",
      "loss: 0.011227901093661785, accuracy: 1.0\n",
      "loss: 0.010452158749103546, accuracy: 1.0\n",
      "loss: 0.010648258961737156, accuracy: 1.0\n",
      "loss: 0.010789305903017521, accuracy: 1.0\n",
      "loss: 0.010725721716880798, accuracy: 1.0\n",
      "loss: 0.010732350870966911, accuracy: 1.0\n",
      "loss: 0.010399668477475643, accuracy: 1.0\n",
      "loss: 0.00997651182115078, accuracy: 1.0\n",
      "loss: 0.010323865339159966, accuracy: 1.0\n",
      "loss: 0.009958435781300068, accuracy: 1.0\n",
      "loss: 0.010231123305857182, accuracy: 1.0\n",
      "loss: 0.010923430323600769, accuracy: 1.0\n",
      "loss: 0.01010144967585802, accuracy: 1.0\n",
      "loss: 0.010935175232589245, accuracy: 1.0\n",
      "loss: 0.010369724594056606, accuracy: 1.0\n",
      "loss: 0.010043228976428509, accuracy: 1.0\n",
      "loss: 0.010292213410139084, accuracy: 1.0\n",
      "loss: 0.01005821954458952, accuracy: 1.0\n",
      "loss: 0.01016395166516304, accuracy: 1.0\n",
      "loss: 0.012052005156874657, accuracy: 1.0\n",
      "loss: 0.01026315987110138, accuracy: 1.0\n",
      "loss: 0.009864463470876217, accuracy: 1.0\n",
      "loss: 0.010645316913723946, accuracy: 1.0\n",
      "loss: 0.01029694452881813, accuracy: 1.0\n",
      "loss: 0.010388595052063465, accuracy: 1.0\n",
      "loss: 0.009833017364144325, accuracy: 1.0\n",
      "loss: 0.01047531422227621, accuracy: 1.0\n",
      "loss: 0.010031900368630886, accuracy: 1.0\n",
      "loss: 0.00985814817249775, accuracy: 1.0\n",
      "loss: 0.009880340658128262, accuracy: 1.0\n",
      "loss: 0.010772263631224632, accuracy: 1.0\n",
      "loss: 0.010456465184688568, accuracy: 1.0\n",
      "loss: 0.010147977620363235, accuracy: 1.0\n",
      "loss: 0.010113302618265152, accuracy: 1.0\n",
      "loss: 0.010220177471637726, accuracy: 1.0\n",
      "loss: 0.00956276711076498, accuracy: 1.0\n",
      "loss: 0.010339764878153801, accuracy: 1.0\n",
      "loss: 0.010341832414269447, accuracy: 1.0\n",
      "loss: 0.010180865414440632, accuracy: 1.0\n",
      "loss: 0.009954534471035004, accuracy: 1.0\n",
      "loss: 0.010229922831058502, accuracy: 1.0\n",
      "loss: 0.00995673332363367, accuracy: 1.0\n",
      "loss: 0.010252281092107296, accuracy: 1.0\n",
      "loss: 0.00991443358361721, accuracy: 1.0\n",
      "loss: 0.009856435470283031, accuracy: 1.0\n",
      "loss: 0.010039214044809341, accuracy: 1.0\n",
      "loss: 0.009950279258191586, accuracy: 1.0\n",
      "loss: 0.009798499755561352, accuracy: 1.0\n",
      "loss: 0.010576257482171059, accuracy: 1.0\n",
      "loss: 0.01044616848230362, accuracy: 1.0\n",
      "loss: 0.011378408409655094, accuracy: 1.0\n",
      "loss: 0.010867079719901085, accuracy: 1.0\n",
      "loss: 0.010116220451891422, accuracy: 1.0\n",
      "loss: 0.009672644548118114, accuracy: 1.0\n",
      "loss: 0.010463454760611057, accuracy: 1.0\n",
      "loss: 0.010237382724881172, accuracy: 1.0\n",
      "loss: 0.010120442137122154, accuracy: 1.0\n",
      "loss: 0.009900636970996857, accuracy: 1.0\n",
      "loss: 0.009598182514309883, accuracy: 1.0\n",
      "loss: 0.009950780309736729, accuracy: 1.0\n",
      "loss: 0.009931105189025402, accuracy: 1.0\n",
      "loss: 0.01027773693203926, accuracy: 1.0\n",
      "loss: 0.009968535043299198, accuracy: 1.0\n",
      "loss: 0.009664906188845634, accuracy: 1.0\n",
      "loss: 0.010684757493436337, accuracy: 1.0\n",
      "loss: 0.01023098174482584, accuracy: 1.0\n",
      "loss: 0.009857028722763062, accuracy: 1.0\n",
      "loss: 0.009872964583337307, accuracy: 1.0\n",
      "loss: 0.01035452913492918, accuracy: 1.0\n",
      "loss: 0.010390827432274818, accuracy: 1.0\n",
      "loss: 0.009839070029556751, accuracy: 1.0\n",
      "loss: 0.010249273851513863, accuracy: 1.0\n",
      "loss: 0.009745914489030838, accuracy: 1.0\n",
      "loss: 0.01050549279898405, accuracy: 1.0\n",
      "loss: 0.010219047777354717, accuracy: 1.0\n",
      "loss: 0.009861242026090622, accuracy: 1.0\n",
      "loss: 0.011021976359188557, accuracy: 1.0\n",
      "loss: 0.010350339114665985, accuracy: 1.0\n",
      "loss: 0.010905339382588863, accuracy: 1.0\n",
      "loss: 0.01023628655821085, accuracy: 1.0\n",
      "loss: 0.010282590985298157, accuracy: 1.0\n",
      "loss: 0.010622079484164715, accuracy: 1.0\n",
      "loss: 0.009544107131659985, accuracy: 1.0\n",
      "loss: 0.010236643254756927, accuracy: 1.0\n",
      "loss: 0.010465397499501705, accuracy: 1.0\n",
      "loss: 0.00963857676833868, accuracy: 1.0\n",
      "loss: 0.009632324799895287, accuracy: 1.0\n",
      "loss: 0.009593582712113857, accuracy: 1.0\n",
      "loss: 0.010039652697741985, accuracy: 1.0\n",
      "loss: 0.01014170702546835, accuracy: 1.0\n",
      "loss: 0.009905814193189144, accuracy: 1.0\n",
      "loss: 0.010034514591097832, accuracy: 1.0\n",
      "loss: 0.009570201858878136, accuracy: 1.0\n",
      "loss: 0.009776842780411243, accuracy: 1.0\n",
      "loss: 0.009831227362155914, accuracy: 1.0\n",
      "loss: 0.00955806765705347, accuracy: 1.0\n",
      "loss: 0.010158462449908257, accuracy: 1.0\n",
      "loss: 0.009512938559055328, accuracy: 1.0\n",
      "loss: 0.009541192092001438, accuracy: 1.0\n",
      "loss: 0.009533917531371117, accuracy: 1.0\n",
      "loss: 0.01005226094275713, accuracy: 1.0\n",
      "loss: 0.009319702163338661, accuracy: 1.0\n",
      "loss: 0.009886644780635834, accuracy: 1.0\n",
      "loss: 0.009796063415706158, accuracy: 1.0\n",
      "loss: 0.009749005548655987, accuracy: 1.0\n",
      "loss: 0.009745568968355656, accuracy: 1.0\n",
      "loss: 0.00939886923879385, accuracy: 1.0\n",
      "loss: 0.009699709713459015, accuracy: 1.0\n",
      "loss: 0.009573789313435555, accuracy: 1.0\n",
      "loss: 0.009516690857708454, accuracy: 1.0\n",
      "loss: 0.009761380031704903, accuracy: 1.0\n",
      "loss: 0.009865590371191502, accuracy: 1.0\n",
      "loss: 0.009748359210789204, accuracy: 1.0\n",
      "loss: 0.009536117315292358, accuracy: 1.0\n",
      "loss: 0.009612875990569592, accuracy: 1.0\n",
      "loss: 0.009617353789508343, accuracy: 1.0\n",
      "loss: 0.010015949606895447, accuracy: 1.0\n",
      "loss: 0.00983852706849575, accuracy: 1.0\n",
      "loss: 0.01017811056226492, accuracy: 1.0\n",
      "loss: 0.00952452328056097, accuracy: 1.0\n",
      "loss: 0.00945020467042923, accuracy: 1.0\n",
      "loss: 0.010297900065779686, accuracy: 1.0\n",
      "loss: 0.01026401948183775, accuracy: 1.0\n",
      "loss: 0.009503645822405815, accuracy: 1.0\n",
      "loss: 0.010671918280422688, accuracy: 1.0\n",
      "loss: 0.009981399402022362, accuracy: 1.0\n",
      "loss: 0.009391158819198608, accuracy: 1.0\n",
      "loss: 0.009626176208257675, accuracy: 1.0\n",
      "loss: 0.00975697860121727, accuracy: 1.0\n",
      "loss: 0.010613597929477692, accuracy: 1.0\n",
      "loss: 0.01055665872991085, accuracy: 1.0\n",
      "loss: 0.009825310669839382, accuracy: 1.0\n",
      "loss: 0.009734501130878925, accuracy: 1.0\n",
      "loss: 0.009573397226631641, accuracy: 1.0\n",
      "loss: 0.009562834165990353, accuracy: 1.0\n",
      "loss: 0.010260925628244877, accuracy: 1.0\n",
      "loss: 0.010369641706347466, accuracy: 1.0\n",
      "loss: 0.009700528346002102, accuracy: 1.0\n",
      "loss: 0.010278258472681046, accuracy: 1.0\n",
      "loss: 0.009532387368381023, accuracy: 1.0\n",
      "loss: 0.00983564555644989, accuracy: 1.0\n",
      "loss: 0.009389055892825127, accuracy: 1.0\n",
      "loss: 0.009986716322600842, accuracy: 1.0\n",
      "loss: 0.009468204341828823, accuracy: 1.0\n",
      "loss: 0.009593673050403595, accuracy: 1.0\n",
      "loss: 0.009630715474486351, accuracy: 1.0\n",
      "loss: 0.009560897946357727, accuracy: 1.0\n",
      "loss: 0.009420844726264477, accuracy: 1.0\n",
      "loss: 0.009693155065178871, accuracy: 1.0\n",
      "loss: 0.009451717138290405, accuracy: 1.0\n",
      "loss: 0.008964831940829754, accuracy: 1.0\n",
      "loss: 0.009396648965775967, accuracy: 1.0\n",
      "loss: 0.009075138717889786, accuracy: 1.0\n",
      "loss: 0.00989897083491087, accuracy: 1.0\n",
      "loss: 0.010050266981124878, accuracy: 1.0\n",
      "loss: 0.010393216274678707, accuracy: 1.0\n",
      "loss: 0.009918104857206345, accuracy: 1.0\n",
      "loss: 0.010270143859088421, accuracy: 1.0\n",
      "loss: 0.009668650105595589, accuracy: 1.0\n",
      "loss: 0.009098879992961884, accuracy: 1.0\n",
      "loss: 0.00967771839350462, accuracy: 1.0\n",
      "loss: 0.009195531718432903, accuracy: 1.0\n",
      "loss: 0.009497041814029217, accuracy: 1.0\n",
      "loss: 0.009820258244872093, accuracy: 1.0\n",
      "loss: 0.009115376509726048, accuracy: 1.0\n",
      "loss: 0.010151343420147896, accuracy: 1.0\n",
      "loss: 0.009216965176165104, accuracy: 1.0\n",
      "loss: 0.009682894684374332, accuracy: 1.0\n",
      "loss: 0.009258551523089409, accuracy: 1.0\n",
      "loss: 0.009996103122830391, accuracy: 1.0\n",
      "loss: 0.010141361504793167, accuracy: 1.0\n",
      "loss: 0.010404001921415329, accuracy: 1.0\n",
      "loss: 0.009453771635890007, accuracy: 1.0\n",
      "loss: 0.009106868878006935, accuracy: 1.0\n",
      "loss: 0.009424371644854546, accuracy: 1.0\n",
      "loss: 0.009487784467637539, accuracy: 1.0\n",
      "loss: 0.009298360906541348, accuracy: 1.0\n",
      "loss: 0.00956732127815485, accuracy: 1.0\n",
      "loss: 0.009575449861586094, accuracy: 1.0\n",
      "loss: 0.009735389612615108, accuracy: 1.0\n",
      "loss: 0.009277412667870522, accuracy: 1.0\n",
      "loss: 0.009463874623179436, accuracy: 1.0\n",
      "loss: 0.009012890048325062, accuracy: 1.0\n",
      "loss: 0.009448115713894367, accuracy: 1.0\n",
      "loss: 0.00958489440381527, accuracy: 1.0\n",
      "loss: 0.009282179176807404, accuracy: 1.0\n",
      "loss: 0.009335201233625412, accuracy: 1.0\n",
      "loss: 0.009687326848506927, accuracy: 1.0\n",
      "loss: 0.00904222670942545, accuracy: 1.0\n",
      "loss: 0.008916383609175682, accuracy: 1.0\n",
      "loss: 0.00939222052693367, accuracy: 1.0\n",
      "loss: 0.009519103914499283, accuracy: 1.0\n",
      "loss: 0.00942959077656269, accuracy: 1.0\n",
      "loss: 0.009250801056623459, accuracy: 1.0\n",
      "loss: 0.009500933811068535, accuracy: 1.0\n",
      "loss: 0.009057249873876572, accuracy: 1.0\n",
      "loss: 0.009529945440590382, accuracy: 1.0\n",
      "loss: 0.009581969119608402, accuracy: 1.0\n",
      "loss: 0.008984862826764584, accuracy: 1.0\n",
      "loss: 0.009346479550004005, accuracy: 1.0\n",
      "loss: 0.009128113277256489, accuracy: 1.0\n",
      "loss: 0.008930816315114498, accuracy: 1.0\n",
      "loss: 0.009100587107241154, accuracy: 1.0\n",
      "loss: 0.00902542658150196, accuracy: 1.0\n",
      "loss: 0.009348777122795582, accuracy: 1.0\n",
      "loss: 0.00964764878153801, accuracy: 1.0\n",
      "loss: 0.00931490771472454, accuracy: 1.0\n",
      "loss: 0.009448738768696785, accuracy: 1.0\n",
      "loss: 0.008991002105176449, accuracy: 1.0\n",
      "loss: 0.00998182687908411, accuracy: 1.0\n",
      "loss: 0.009070605970919132, accuracy: 1.0\n",
      "loss: 0.009210764430463314, accuracy: 1.0\n",
      "loss: 0.009539407677948475, accuracy: 1.0\n",
      "loss: 0.009114336222410202, accuracy: 1.0\n",
      "loss: 0.009101124480366707, accuracy: 1.0\n",
      "loss: 0.009103990159928799, accuracy: 1.0\n",
      "loss: 0.009337042458355427, accuracy: 1.0\n",
      "loss: 0.009120894595980644, accuracy: 1.0\n",
      "loss: 0.009700192138552666, accuracy: 1.0\n",
      "loss: 0.009795550256967545, accuracy: 1.0\n",
      "loss: 0.009470416232943535, accuracy: 1.0\n",
      "loss: 0.00906566996127367, accuracy: 1.0\n",
      "loss: 0.009981507435441017, accuracy: 1.0\n",
      "loss: 0.009056896902620792, accuracy: 1.0\n",
      "loss: 0.00907783955335617, accuracy: 1.0\n",
      "loss: 0.009114780463278294, accuracy: 1.0\n",
      "loss: 0.009345537051558495, accuracy: 1.0\n",
      "loss: 0.009394937194883823, accuracy: 1.0\n",
      "loss: 0.009195858612656593, accuracy: 1.0\n",
      "loss: 0.009248229674994946, accuracy: 1.0\n",
      "loss: 0.009239447303116322, accuracy: 1.0\n",
      "loss: 0.009178252890706062, accuracy: 1.0\n",
      "loss: 0.009499379433691502, accuracy: 1.0\n",
      "loss: 0.00942978449165821, accuracy: 1.0\n",
      "loss: 0.008823338896036148, accuracy: 1.0\n",
      "loss: 0.009329311549663544, accuracy: 1.0\n",
      "loss: 0.009039029479026794, accuracy: 1.0\n",
      "loss: 0.008769281208515167, accuracy: 1.0\n",
      "loss: 0.009483539499342442, accuracy: 1.0\n",
      "loss: 0.00892353244125843, accuracy: 1.0\n",
      "loss: 0.009570765309035778, accuracy: 1.0\n",
      "loss: 0.008852628991007805, accuracy: 1.0\n",
      "loss: 0.009080816060304642, accuracy: 1.0\n",
      "loss: 0.009515566751360893, accuracy: 1.0\n",
      "loss: 0.009072746150195599, accuracy: 1.0\n",
      "loss: 0.009021829813718796, accuracy: 1.0\n",
      "loss: 0.008850072510540485, accuracy: 1.0\n",
      "loss: 0.008723094128072262, accuracy: 1.0\n",
      "loss: 0.008814208209514618, accuracy: 1.0\n",
      "loss: 0.009625005535781384, accuracy: 1.0\n",
      "loss: 0.009151839651167393, accuracy: 1.0\n",
      "loss: 0.008820638060569763, accuracy: 1.0\n",
      "loss: 0.008648321963846684, accuracy: 1.0\n",
      "loss: 0.009048432111740112, accuracy: 1.0\n",
      "loss: 0.008812736719846725, accuracy: 1.0\n",
      "loss: 0.009061495773494244, accuracy: 1.0\n",
      "loss: 0.008878310211002827, accuracy: 1.0\n",
      "loss: 0.00947713851928711, accuracy: 1.0\n",
      "loss: 0.009733614511787891, accuracy: 1.0\n",
      "loss: 0.008876231499016285, accuracy: 1.0\n",
      "loss: 0.008810018189251423, accuracy: 1.0\n",
      "loss: 0.00895027257502079, accuracy: 1.0\n",
      "loss: 0.009624555706977844, accuracy: 1.0\n",
      "loss: 0.009161168709397316, accuracy: 1.0\n",
      "loss: 0.008704395964741707, accuracy: 1.0\n",
      "loss: 0.009007351472973824, accuracy: 1.0\n",
      "loss: 0.009249004535377026, accuracy: 1.0\n",
      "loss: 0.00896670762449503, accuracy: 1.0\n",
      "loss: 0.00877368077635765, accuracy: 1.0\n",
      "loss: 0.008780875243246555, accuracy: 1.0\n",
      "loss: 0.009665249846875668, accuracy: 1.0\n",
      "loss: 0.008813017047941685, accuracy: 1.0\n",
      "loss: 0.0090770423412323, accuracy: 1.0\n",
      "loss: 0.009246788918972015, accuracy: 1.0\n",
      "loss: 0.00860096886754036, accuracy: 1.0\n",
      "loss: 0.008674487471580505, accuracy: 1.0\n",
      "loss: 0.008951226249337196, accuracy: 1.0\n",
      "loss: 0.008625736460089684, accuracy: 1.0\n",
      "loss: 0.008871563710272312, accuracy: 1.0\n",
      "loss: 0.009150594472885132, accuracy: 1.0\n",
      "loss: 0.008790994994342327, accuracy: 1.0\n",
      "loss: 0.008765587583184242, accuracy: 1.0\n",
      "loss: 0.00888210441917181, accuracy: 1.0\n",
      "loss: 0.009355840273201466, accuracy: 1.0\n",
      "loss: 0.008611665107309818, accuracy: 1.0\n",
      "loss: 0.008822795003652573, accuracy: 1.0\n",
      "loss: 0.008977800607681274, accuracy: 1.0\n",
      "loss: 0.008635219186544418, accuracy: 1.0\n",
      "loss: 0.008924105204641819, accuracy: 1.0\n",
      "loss: 0.008700910024344921, accuracy: 1.0\n",
      "loss: 0.008727753534913063, accuracy: 1.0\n",
      "loss: 0.009178061969578266, accuracy: 1.0\n",
      "loss: 0.00861817505210638, accuracy: 1.0\n",
      "loss: 0.008671209216117859, accuracy: 1.0\n",
      "loss: 0.009015020914375782, accuracy: 1.0\n",
      "loss: 0.008701321668922901, accuracy: 1.0\n",
      "loss: 0.00932119321078062, accuracy: 1.0\n",
      "loss: 0.008812378160655499, accuracy: 1.0\n",
      "loss: 0.008612948469817638, accuracy: 1.0\n",
      "loss: 0.009389505721628666, accuracy: 1.0\n",
      "loss: 0.009180587716400623, accuracy: 1.0\n",
      "loss: 0.00839833915233612, accuracy: 1.0\n",
      "loss: 0.008930114097893238, accuracy: 1.0\n",
      "loss: 0.009114760905504227, accuracy: 1.0\n",
      "loss: 0.009766018949449062, accuracy: 1.0\n",
      "loss: 0.008594676852226257, accuracy: 1.0\n",
      "loss: 0.008906758390367031, accuracy: 1.0\n",
      "loss: 0.008871647529304028, accuracy: 1.0\n",
      "loss: 0.008554501459002495, accuracy: 1.0\n",
      "loss: 0.008601526729762554, accuracy: 1.0\n",
      "loss: 0.00874185562133789, accuracy: 1.0\n",
      "loss: 0.008885283954441547, accuracy: 1.0\n",
      "loss: 0.00879832822829485, accuracy: 1.0\n",
      "loss: 0.008692173287272453, accuracy: 1.0\n",
      "loss: 0.008726616390049458, accuracy: 1.0\n",
      "loss: 0.00911800842732191, accuracy: 1.0\n",
      "loss: 0.008803373202681541, accuracy: 1.0\n",
      "loss: 0.008812721818685532, accuracy: 1.0\n",
      "loss: 0.008539016358554363, accuracy: 1.0\n",
      "loss: 0.008621188811957836, accuracy: 1.0\n",
      "loss: 0.008651398122310638, accuracy: 1.0\n",
      "loss: 0.008485053665935993, accuracy: 1.0\n",
      "loss: 0.00937702227383852, accuracy: 1.0\n",
      "loss: 0.008621571585536003, accuracy: 1.0\n",
      "loss: 0.008859213441610336, accuracy: 1.0\n",
      "loss: 0.008761466480791569, accuracy: 1.0\n",
      "loss: 0.009128573350608349, accuracy: 1.0\n",
      "loss: 0.00867829192429781, accuracy: 1.0\n",
      "loss: 0.008387397043406963, accuracy: 1.0\n",
      "loss: 0.008947106078267097, accuracy: 1.0\n",
      "loss: 0.008355299010872841, accuracy: 1.0\n",
      "loss: 0.008639487437903881, accuracy: 1.0\n",
      "loss: 0.00889467354863882, accuracy: 1.0\n",
      "loss: 0.008222419768571854, accuracy: 1.0\n",
      "loss: 0.009019577875733376, accuracy: 1.0\n",
      "loss: 0.008681260980665684, accuracy: 1.0\n",
      "loss: 0.008494657464325428, accuracy: 1.0\n",
      "loss: 0.008309068158268929, accuracy: 1.0\n",
      "loss: 0.008799035102128983, accuracy: 1.0\n",
      "loss: 0.008485020138323307, accuracy: 1.0\n",
      "loss: 0.008718255907297134, accuracy: 1.0\n",
      "loss: 0.008307893760502338, accuracy: 1.0\n",
      "loss: 0.008217976428568363, accuracy: 1.0\n",
      "loss: 0.00844741053879261, accuracy: 1.0\n",
      "loss: 0.008748776279389858, accuracy: 1.0\n",
      "loss: 0.008380593731999397, accuracy: 1.0\n",
      "loss: 0.008414170704782009, accuracy: 1.0\n",
      "loss: 0.008497399277985096, accuracy: 1.0\n",
      "loss: 0.008585535921156406, accuracy: 1.0\n",
      "loss: 0.00907060969620943, accuracy: 1.0\n",
      "loss: 0.008210856467485428, accuracy: 1.0\n",
      "loss: 0.009295782074332237, accuracy: 1.0\n",
      "loss: 0.008573665283620358, accuracy: 1.0\n",
      "loss: 0.008515266701579094, accuracy: 1.0\n",
      "loss: 0.008204501122236252, accuracy: 1.0\n",
      "loss: 0.008472012355923653, accuracy: 1.0\n",
      "loss: 0.008529547601938248, accuracy: 1.0\n",
      "loss: 0.008286949247121811, accuracy: 1.0\n",
      "loss: 0.008528348058462143, accuracy: 1.0\n",
      "loss: 0.008355841040611267, accuracy: 1.0\n",
      "loss: 0.008981622755527496, accuracy: 1.0\n",
      "loss: 0.008369159884750843, accuracy: 1.0\n",
      "loss: 0.008549939841032028, accuracy: 1.0\n",
      "loss: 0.00837930478155613, accuracy: 1.0\n",
      "loss: 0.008728638291358948, accuracy: 1.0\n",
      "loss: 0.008568511344492435, accuracy: 1.0\n",
      "loss: 0.008301005698740482, accuracy: 1.0\n",
      "loss: 0.008438877761363983, accuracy: 1.0\n",
      "loss: 0.008706191554665565, accuracy: 1.0\n",
      "loss: 0.008466286584734917, accuracy: 1.0\n",
      "loss: 0.008583340793848038, accuracy: 1.0\n",
      "loss: 0.008418424986302853, accuracy: 1.0\n",
      "loss: 0.008445479907095432, accuracy: 1.0\n",
      "loss: 0.00891912542283535, accuracy: 1.0\n",
      "loss: 0.008184853009879589, accuracy: 1.0\n",
      "loss: 0.008259863592684269, accuracy: 1.0\n",
      "loss: 0.00843221228569746, accuracy: 1.0\n",
      "loss: 0.007894373498857021, accuracy: 1.0\n",
      "loss: 0.008205500431358814, accuracy: 1.0\n",
      "loss: 0.008119883015751839, accuracy: 1.0\n",
      "loss: 0.008460840210318565, accuracy: 1.0\n",
      "loss: 0.00827623438090086, accuracy: 1.0\n",
      "loss: 0.0084553062915802, accuracy: 1.0\n",
      "loss: 0.008943881839513779, accuracy: 1.0\n",
      "loss: 0.00858660601079464, accuracy: 1.0\n",
      "loss: 0.008407232351601124, accuracy: 1.0\n",
      "loss: 0.008318563923239708, accuracy: 1.0\n",
      "loss: 0.008599938824772835, accuracy: 1.0\n",
      "loss: 0.008551078848540783, accuracy: 1.0\n",
      "loss: 0.008281840942800045, accuracy: 1.0\n",
      "loss: 0.008227157406508923, accuracy: 1.0\n",
      "loss: 0.008195500820875168, accuracy: 1.0\n",
      "loss: 0.008201896212995052, accuracy: 1.0\n",
      "loss: 0.008180506527423859, accuracy: 1.0\n",
      "loss: 0.00842729676514864, accuracy: 1.0\n",
      "loss: 0.008702692575752735, accuracy: 1.0\n",
      "loss: 0.008382878266274929, accuracy: 1.0\n",
      "loss: 0.008344312198460102, accuracy: 1.0\n",
      "loss: 0.008477000519633293, accuracy: 1.0\n",
      "loss: 0.008352097123861313, accuracy: 1.0\n",
      "loss: 0.008568073622882366, accuracy: 1.0\n",
      "loss: 0.008279599249362946, accuracy: 1.0\n",
      "loss: 0.00842765998095274, accuracy: 1.0\n",
      "loss: 0.00857993308454752, accuracy: 1.0\n",
      "loss: 0.008444465696811676, accuracy: 1.0\n",
      "loss: 0.008222504518926144, accuracy: 1.0\n",
      "loss: 0.008466738276183605, accuracy: 1.0\n",
      "loss: 0.008542103692889214, accuracy: 1.0\n",
      "loss: 0.008278331719338894, accuracy: 1.0\n",
      "loss: 0.008261116221547127, accuracy: 1.0\n",
      "loss: 0.008052164688706398, accuracy: 1.0\n",
      "loss: 0.008530951105058193, accuracy: 1.0\n",
      "loss: 0.008068583905696869, accuracy: 1.0\n",
      "loss: 0.008621761575341225, accuracy: 1.0\n",
      "loss: 0.00831543281674385, accuracy: 1.0\n",
      "loss: 0.008062337525188923, accuracy: 1.0\n",
      "loss: 0.008376376703381538, accuracy: 1.0\n",
      "loss: 0.008420752361416817, accuracy: 1.0\n",
      "loss: 0.007946188561618328, accuracy: 1.0\n",
      "loss: 0.008215781301259995, accuracy: 1.0\n",
      "loss: 0.008318960666656494, accuracy: 1.0\n",
      "loss: 0.008497875183820724, accuracy: 1.0\n",
      "loss: 0.008381165564060211, accuracy: 1.0\n",
      "loss: 0.008198092691600323, accuracy: 1.0\n",
      "loss: 0.007736086379736662, accuracy: 1.0\n",
      "loss: 0.008773082867264748, accuracy: 1.0\n",
      "loss: 0.008216206915676594, accuracy: 1.0\n",
      "loss: 0.008343823254108429, accuracy: 1.0\n",
      "loss: 0.007948041893541813, accuracy: 1.0\n",
      "loss: 0.007960937917232513, accuracy: 1.0\n",
      "loss: 0.008553844876587391, accuracy: 1.0\n",
      "loss: 0.008016719482839108, accuracy: 1.0\n",
      "loss: 0.00803524348884821, accuracy: 1.0\n",
      "loss: 0.008291596546769142, accuracy: 1.0\n",
      "loss: 0.008397315628826618, accuracy: 1.0\n",
      "loss: 0.008579351007938385, accuracy: 1.0\n",
      "loss: 0.008389136753976345, accuracy: 1.0\n",
      "loss: 0.00834257435053587, accuracy: 1.0\n",
      "loss: 0.008405540138483047, accuracy: 1.0\n",
      "loss: 0.007955866865813732, accuracy: 1.0\n",
      "loss: 0.007858073338866234, accuracy: 1.0\n",
      "loss: 0.00826331414282322, accuracy: 1.0\n",
      "loss: 0.00806698389351368, accuracy: 1.0\n",
      "loss: 0.008275198750197887, accuracy: 1.0\n",
      "loss: 0.007915191352367401, accuracy: 1.0\n",
      "loss: 0.008482677862048149, accuracy: 1.0\n",
      "loss: 0.008056620135903358, accuracy: 1.0\n",
      "loss: 0.008008492179214954, accuracy: 1.0\n",
      "loss: 0.00813517440110445, accuracy: 1.0\n",
      "loss: 0.008542398922145367, accuracy: 1.0\n",
      "loss: 0.007974830456078053, accuracy: 1.0\n",
      "loss: 0.008316446095705032, accuracy: 1.0\n",
      "loss: 0.008252005092799664, accuracy: 1.0\n",
      "loss: 0.007843276485800743, accuracy: 1.0\n",
      "loss: 0.008287625387310982, accuracy: 1.0\n",
      "loss: 0.007977268658578396, accuracy: 1.0\n",
      "loss: 0.008164350874722004, accuracy: 1.0\n",
      "loss: 0.0082174027338624, accuracy: 1.0\n",
      "loss: 0.007993984036147594, accuracy: 1.0\n",
      "loss: 0.008206035010516644, accuracy: 1.0\n",
      "loss: 0.008223704062402248, accuracy: 1.0\n",
      "loss: 0.007974229753017426, accuracy: 1.0\n",
      "loss: 0.007888061925768852, accuracy: 1.0\n",
      "loss: 0.008350406773388386, accuracy: 1.0\n",
      "loss: 0.007924098521471024, accuracy: 1.0\n",
      "loss: 0.008214941248297691, accuracy: 1.0\n",
      "loss: 0.007846415974199772, accuracy: 1.0\n",
      "loss: 0.00789330992847681, accuracy: 1.0\n",
      "loss: 0.008028129115700722, accuracy: 1.0\n",
      "loss: 0.008325641043484211, accuracy: 1.0\n",
      "loss: 0.007994898594915867, accuracy: 1.0\n",
      "loss: 0.008072370663285255, accuracy: 1.0\n",
      "loss: 0.008130227215588093, accuracy: 1.0\n",
      "loss: 0.007961753755807877, accuracy: 1.0\n",
      "loss: 0.008046910166740417, accuracy: 1.0\n",
      "loss: 0.008130023255944252, accuracy: 1.0\n",
      "loss: 0.007843968458473682, accuracy: 1.0\n",
      "loss: 0.007839925587177277, accuracy: 1.0\n",
      "loss: 0.008027317933738232, accuracy: 1.0\n",
      "loss: 0.008324268274009228, accuracy: 1.0\n",
      "loss: 0.007928622886538506, accuracy: 1.0\n",
      "loss: 0.008033704943954945, accuracy: 1.0\n",
      "loss: 0.007842661812901497, accuracy: 1.0\n",
      "loss: 0.007967045530676842, accuracy: 1.0\n",
      "loss: 0.007869935594499111, accuracy: 1.0\n",
      "loss: 0.008135858923196793, accuracy: 1.0\n",
      "loss: 0.00800352543592453, accuracy: 1.0\n",
      "loss: 0.008074276149272919, accuracy: 1.0\n",
      "loss: 0.007747937925159931, accuracy: 1.0\n",
      "loss: 0.007832338102161884, accuracy: 1.0\n",
      "loss: 0.007994767278432846, accuracy: 1.0\n",
      "loss: 0.008029279299080372, accuracy: 1.0\n",
      "loss: 0.008069111034274101, accuracy: 1.0\n",
      "loss: 0.007951131090521812, accuracy: 1.0\n",
      "loss: 0.008052143268287182, accuracy: 1.0\n",
      "loss: 0.008061947301030159, accuracy: 1.0\n",
      "loss: 0.007848143577575684, accuracy: 1.0\n",
      "loss: 0.007722076494246721, accuracy: 1.0\n",
      "loss: 0.007690847851336002, accuracy: 1.0\n",
      "loss: 0.008474590256810188, accuracy: 1.0\n",
      "loss: 0.007883076556026936, accuracy: 1.0\n",
      "loss: 0.008151533082127571, accuracy: 1.0\n",
      "loss: 0.00818764790892601, accuracy: 1.0\n",
      "loss: 0.007820847444236279, accuracy: 1.0\n",
      "loss: 0.007553191855549812, accuracy: 1.0\n",
      "loss: 0.007965756580233574, accuracy: 1.0\n",
      "loss: 0.007871977984905243, accuracy: 1.0\n",
      "loss: 0.008147841319441795, accuracy: 1.0\n",
      "loss: 0.007848168723285198, accuracy: 1.0\n",
      "loss: 0.007738540414720774, accuracy: 1.0\n",
      "loss: 0.008099622093141079, accuracy: 1.0\n",
      "loss: 0.007979418151080608, accuracy: 1.0\n",
      "loss: 0.008281799033284187, accuracy: 1.0\n",
      "loss: 0.007922954857349396, accuracy: 1.0\n",
      "loss: 0.00749139953404665, accuracy: 1.0\n",
      "loss: 0.008081786334514618, accuracy: 1.0\n",
      "loss: 0.007818005979061127, accuracy: 1.0\n",
      "loss: 0.008166775107383728, accuracy: 1.0\n",
      "loss: 0.008129015564918518, accuracy: 1.0\n",
      "loss: 0.007634417153894901, accuracy: 1.0\n",
      "loss: 0.007885323837399483, accuracy: 1.0\n",
      "loss: 0.007810742128640413, accuracy: 1.0\n",
      "loss: 0.008120866492390633, accuracy: 1.0\n",
      "loss: 0.008091818541288376, accuracy: 1.0\n",
      "loss: 0.00806630589067936, accuracy: 1.0\n",
      "loss: 0.0076250857673585415, accuracy: 1.0\n",
      "loss: 0.007983976975083351, accuracy: 1.0\n",
      "loss: 0.007878650911152363, accuracy: 1.0\n",
      "loss: 0.007995721884071827, accuracy: 1.0\n",
      "loss: 0.007639636285603046, accuracy: 1.0\n",
      "loss: 0.00816715694963932, accuracy: 1.0\n",
      "loss: 0.007698823232203722, accuracy: 1.0\n",
      "loss: 0.007707864046096802, accuracy: 1.0\n",
      "loss: 0.008006935939192772, accuracy: 1.0\n",
      "loss: 0.007830093614757061, accuracy: 1.0\n",
      "loss: 0.007802787236869335, accuracy: 1.0\n",
      "loss: 0.007913283072412014, accuracy: 1.0\n",
      "loss: 0.007801357191056013, accuracy: 1.0\n",
      "loss: 0.00787257682532072, accuracy: 1.0\n",
      "loss: 0.00798376090824604, accuracy: 1.0\n",
      "loss: 0.007609210908412933, accuracy: 1.0\n",
      "loss: 0.007548488210886717, accuracy: 1.0\n",
      "loss: 0.007758206222206354, accuracy: 1.0\n",
      "loss: 0.00835663452744484, accuracy: 1.0\n",
      "loss: 0.007644465193152428, accuracy: 1.0\n",
      "loss: 0.007590443827211857, accuracy: 1.0\n",
      "loss: 0.007656438741832972, accuracy: 1.0\n",
      "loss: 0.008101261220872402, accuracy: 1.0\n",
      "loss: 0.007466406095772982, accuracy: 1.0\n",
      "loss: 0.007858742959797382, accuracy: 1.0\n",
      "loss: 0.007742395158857107, accuracy: 1.0\n",
      "loss: 0.00787491723895073, accuracy: 1.0\n",
      "loss: 0.0076698861084878445, accuracy: 1.0\n",
      "loss: 0.007982496172189713, accuracy: 1.0\n",
      "loss: 0.007687666453421116, accuracy: 1.0\n",
      "loss: 0.007733349222689867, accuracy: 1.0\n",
      "loss: 0.007499628234654665, accuracy: 1.0\n",
      "loss: 0.0076081897132098675, accuracy: 1.0\n",
      "loss: 0.007910368032753468, accuracy: 1.0\n",
      "loss: 0.007622117176651955, accuracy: 1.0\n",
      "loss: 0.00754145672544837, accuracy: 1.0\n",
      "loss: 0.007558255922049284, accuracy: 1.0\n",
      "loss: 0.007732878439128399, accuracy: 1.0\n",
      "loss: 0.007627408020198345, accuracy: 1.0\n",
      "loss: 0.007528902031481266, accuracy: 1.0\n",
      "loss: 0.008131258189678192, accuracy: 1.0\n",
      "loss: 0.007887986488640308, accuracy: 1.0\n",
      "loss: 0.007513472810387611, accuracy: 1.0\n",
      "loss: 0.007548938039690256, accuracy: 1.0\n",
      "loss: 0.00794540997594595, accuracy: 1.0\n",
      "loss: 0.007918007671833038, accuracy: 1.0\n",
      "loss: 0.0073975734412670135, accuracy: 1.0\n",
      "loss: 0.007765071000903845, accuracy: 1.0\n",
      "loss: 0.007791373413056135, accuracy: 1.0\n",
      "loss: 0.007453086785972118, accuracy: 1.0\n",
      "loss: 0.007462588604539633, accuracy: 1.0\n",
      "loss: 0.007534348871558905, accuracy: 1.0\n",
      "loss: 0.007447078824043274, accuracy: 1.0\n",
      "loss: 0.007934494875371456, accuracy: 1.0\n",
      "loss: 0.007612571585923433, accuracy: 1.0\n",
      "loss: 0.008009189739823341, accuracy: 1.0\n",
      "loss: 0.007973953150212765, accuracy: 1.0\n",
      "loss: 0.0074127353727817535, accuracy: 1.0\n",
      "loss: 0.007444704882800579, accuracy: 1.0\n",
      "loss: 0.007579336408525705, accuracy: 1.0\n",
      "loss: 0.007571641355752945, accuracy: 1.0\n",
      "loss: 0.007582842838019133, accuracy: 1.0\n",
      "loss: 0.007426579482853413, accuracy: 1.0\n",
      "loss: 0.00773501256480813, accuracy: 1.0\n",
      "loss: 0.007578917779028416, accuracy: 1.0\n",
      "loss: 0.007435807026922703, accuracy: 1.0\n",
      "loss: 0.007617679890245199, accuracy: 1.0\n",
      "loss: 0.007929305545985699, accuracy: 1.0\n",
      "loss: 0.007400212809443474, accuracy: 1.0\n",
      "loss: 0.00763098755851388, accuracy: 1.0\n",
      "loss: 0.007569385226815939, accuracy: 1.0\n",
      "loss: 0.007480257656425238, accuracy: 1.0\n",
      "loss: 0.007831715978682041, accuracy: 1.0\n",
      "loss: 0.007616899907588959, accuracy: 1.0\n",
      "loss: 0.007439064793288708, accuracy: 1.0\n",
      "loss: 0.00746515579521656, accuracy: 1.0\n",
      "loss: 0.007724760100245476, accuracy: 1.0\n",
      "loss: 0.007543100975453854, accuracy: 1.0\n",
      "loss: 0.00753384456038475, accuracy: 1.0\n",
      "loss: 0.007355061359703541, accuracy: 1.0\n",
      "loss: 0.0076378099620342255, accuracy: 1.0\n",
      "loss: 0.00756048085168004, accuracy: 1.0\n",
      "loss: 0.0074052331037819386, accuracy: 1.0\n",
      "loss: 0.007769037503749132, accuracy: 1.0\n",
      "loss: 0.007319303695112467, accuracy: 1.0\n",
      "loss: 0.007799860090017319, accuracy: 1.0\n",
      "loss: 0.007571243215352297, accuracy: 1.0\n",
      "loss: 0.007691714912652969, accuracy: 1.0\n",
      "loss: 0.007544143125414848, accuracy: 1.0\n",
      "loss: 0.007455097511410713, accuracy: 1.0\n",
      "loss: 0.007308572065085173, accuracy: 1.0\n",
      "loss: 0.007518006023019552, accuracy: 1.0\n",
      "loss: 0.007329148706048727, accuracy: 1.0\n",
      "loss: 0.007520495448261499, accuracy: 1.0\n",
      "loss: 0.007711194455623627, accuracy: 1.0\n",
      "loss: 0.007517781108617783, accuracy: 1.0\n",
      "loss: 0.007372811436653137, accuracy: 1.0\n",
      "loss: 0.007816841825842857, accuracy: 1.0\n",
      "loss: 0.007554213050752878, accuracy: 1.0\n",
      "loss: 0.007394440472126007, accuracy: 1.0\n",
      "loss: 0.007352055516093969, accuracy: 1.0\n",
      "loss: 0.0074010794050991535, accuracy: 1.0\n",
      "loss: 0.007688785903155804, accuracy: 1.0\n",
      "loss: 0.007207577116787434, accuracy: 1.0\n",
      "loss: 0.007649095728993416, accuracy: 1.0\n",
      "loss: 0.007665449287742376, accuracy: 1.0\n",
      "loss: 0.007285438943654299, accuracy: 1.0\n",
      "loss: 0.007254625204950571, accuracy: 1.0\n",
      "loss: 0.007439988665282726, accuracy: 1.0\n",
      "loss: 0.0074767316691577435, accuracy: 1.0\n",
      "loss: 0.007520382758229971, accuracy: 1.0\n",
      "loss: 0.007470407988876104, accuracy: 1.0\n",
      "loss: 0.007454412989318371, accuracy: 1.0\n",
      "loss: 0.007743042428046465, accuracy: 1.0\n",
      "loss: 0.0075995102524757385, accuracy: 1.0\n",
      "loss: 0.0074348063208162785, accuracy: 1.0\n",
      "loss: 0.007307331543415785, accuracy: 1.0\n",
      "loss: 0.0074897268787026405, accuracy: 1.0\n",
      "loss: 0.007337729912251234, accuracy: 1.0\n",
      "loss: 0.007184350863099098, accuracy: 1.0\n",
      "loss: 0.007438152562826872, accuracy: 1.0\n",
      "loss: 0.0072317905724048615, accuracy: 1.0\n",
      "loss: 0.007590718567371368, accuracy: 1.0\n",
      "loss: 0.007340593263506889, accuracy: 1.0\n",
      "loss: 0.007367007900029421, accuracy: 1.0\n",
      "loss: 0.007296041119843721, accuracy: 1.0\n",
      "loss: 0.007354313973337412, accuracy: 1.0\n",
      "loss: 0.0070323939435184, accuracy: 1.0\n",
      "loss: 0.007193917874246836, accuracy: 1.0\n",
      "loss: 0.007206328213214874, accuracy: 1.0\n",
      "loss: 0.007350641768425703, accuracy: 1.0\n",
      "loss: 0.007275828626006842, accuracy: 1.0\n",
      "loss: 0.007335884030908346, accuracy: 1.0\n",
      "loss: 0.007253159768879414, accuracy: 1.0\n",
      "loss: 0.0072510503232479095, accuracy: 1.0\n",
      "loss: 0.007812104187905788, accuracy: 1.0\n",
      "loss: 0.007217977661639452, accuracy: 1.0\n",
      "loss: 0.00749939726665616, accuracy: 1.0\n",
      "loss: 0.007416834589093924, accuracy: 1.0\n",
      "loss: 0.007255321368575096, accuracy: 1.0\n",
      "loss: 0.007404261734336615, accuracy: 1.0\n",
      "loss: 0.007210422772914171, accuracy: 1.0\n",
      "loss: 0.007413643877953291, accuracy: 1.0\n",
      "loss: 0.007389108184725046, accuracy: 1.0\n",
      "loss: 0.00745333032682538, accuracy: 1.0\n",
      "loss: 0.007514316122978926, accuracy: 1.0\n",
      "loss: 0.007168649695813656, accuracy: 1.0\n",
      "loss: 0.0071241180412471294, accuracy: 1.0\n",
      "loss: 0.007136153988540173, accuracy: 1.0\n",
      "loss: 0.007300484459847212, accuracy: 1.0\n",
      "loss: 0.007532553281635046, accuracy: 1.0\n",
      "loss: 0.007031595334410667, accuracy: 1.0\n",
      "loss: 0.007474781479686499, accuracy: 1.0\n",
      "loss: 0.0072793904691934586, accuracy: 1.0\n",
      "loss: 0.0072966162115335464, accuracy: 1.0\n",
      "loss: 0.007530589587986469, accuracy: 1.0\n",
      "loss: 0.007200591266155243, accuracy: 1.0\n",
      "loss: 0.007238113321363926, accuracy: 1.0\n",
      "loss: 0.00773099297657609, accuracy: 1.0\n",
      "loss: 0.007200554478913546, accuracy: 1.0\n",
      "loss: 0.007617115043103695, accuracy: 1.0\n",
      "loss: 0.007194708567112684, accuracy: 1.0\n",
      "loss: 0.007034969050437212, accuracy: 1.0\n",
      "loss: 0.00719904899597168, accuracy: 1.0\n",
      "loss: 0.007220037281513214, accuracy: 1.0\n",
      "loss: 0.007465708069503307, accuracy: 1.0\n",
      "loss: 0.007275192067027092, accuracy: 1.0\n",
      "loss: 0.007158070802688599, accuracy: 1.0\n",
      "loss: 0.007108852732926607, accuracy: 1.0\n",
      "loss: 0.006838453467935324, accuracy: 1.0\n",
      "loss: 0.007457138504832983, accuracy: 1.0\n",
      "loss: 0.007235424593091011, accuracy: 1.0\n",
      "loss: 0.007327692117542028, accuracy: 1.0\n",
      "loss: 0.0072586024180054665, accuracy: 1.0\n",
      "loss: 0.007123019080609083, accuracy: 1.0\n",
      "loss: 0.007019763812422752, accuracy: 1.0\n",
      "loss: 0.007554149720817804, accuracy: 1.0\n",
      "loss: 0.0073564136400818825, accuracy: 1.0\n",
      "loss: 0.007254415657371283, accuracy: 1.0\n",
      "loss: 0.007110453210771084, accuracy: 1.0\n",
      "loss: 0.006887833122164011, accuracy: 1.0\n",
      "loss: 0.007124897558242083, accuracy: 1.0\n",
      "loss: 0.007070974446833134, accuracy: 1.0\n",
      "loss: 0.007180850952863693, accuracy: 1.0\n",
      "loss: 0.006924464832991362, accuracy: 1.0\n",
      "loss: 0.0074026212096214294, accuracy: 1.0\n",
      "loss: 0.007112191524356604, accuracy: 1.0\n",
      "loss: 0.007073287386447191, accuracy: 1.0\n",
      "loss: 0.0072058760561048985, accuracy: 1.0\n",
      "loss: 0.006916340906172991, accuracy: 1.0\n",
      "loss: 0.007112632505595684, accuracy: 1.0\n",
      "loss: 0.006995581556111574, accuracy: 1.0\n",
      "loss: 0.007200776599347591, accuracy: 1.0\n",
      "loss: 0.007202231325209141, accuracy: 1.0\n",
      "loss: 0.007487686350941658, accuracy: 1.0\n",
      "loss: 0.007174655329436064, accuracy: 1.0\n",
      "loss: 0.007172681856900454, accuracy: 1.0\n",
      "loss: 0.00715740816667676, accuracy: 1.0\n",
      "loss: 0.007210924755781889, accuracy: 1.0\n",
      "loss: 0.006993740331381559, accuracy: 1.0\n",
      "loss: 0.0071242256090044975, accuracy: 1.0\n",
      "loss: 0.0067711928859353065, accuracy: 1.0\n",
      "loss: 0.007448227144777775, accuracy: 1.0\n",
      "loss: 0.007492777891457081, accuracy: 1.0\n",
      "loss: 0.006992189213633537, accuracy: 1.0\n",
      "loss: 0.007145131938159466, accuracy: 1.0\n",
      "loss: 0.007180865854024887, accuracy: 1.0\n",
      "loss: 0.006977844517678022, accuracy: 1.0\n",
      "loss: 0.007097719237208366, accuracy: 1.0\n",
      "loss: 0.006922166328877211, accuracy: 1.0\n",
      "loss: 0.0071840775199234486, accuracy: 1.0\n",
      "loss: 0.007136479020118713, accuracy: 1.0\n",
      "loss: 0.007064823526889086, accuracy: 1.0\n",
      "loss: 0.007224044296890497, accuracy: 1.0\n",
      "loss: 0.007214854005724192, accuracy: 1.0\n",
      "loss: 0.007001001387834549, accuracy: 1.0\n",
      "loss: 0.006714724004268646, accuracy: 1.0\n",
      "loss: 0.007058975286781788, accuracy: 1.0\n",
      "loss: 0.006841748952865601, accuracy: 1.0\n",
      "loss: 0.006815375294536352, accuracy: 1.0\n",
      "loss: 0.007190565578639507, accuracy: 1.0\n",
      "loss: 0.0069115860387682915, accuracy: 1.0\n",
      "loss: 0.006846049800515175, accuracy: 1.0\n",
      "loss: 0.007067936006933451, accuracy: 1.0\n",
      "loss: 0.007337059359997511, accuracy: 1.0\n",
      "loss: 0.007103258278220892, accuracy: 1.0\n",
      "loss: 0.006789693143218756, accuracy: 1.0\n",
      "loss: 0.007127016317099333, accuracy: 1.0\n",
      "loss: 0.00695660337805748, accuracy: 1.0\n",
      "loss: 0.006907089613378048, accuracy: 1.0\n",
      "loss: 0.007112790364772081, accuracy: 1.0\n",
      "loss: 0.007096835412085056, accuracy: 1.0\n",
      "loss: 0.007043912541121244, accuracy: 1.0\n",
      "loss: 0.007024355232715607, accuracy: 1.0\n",
      "loss: 0.007350271567702293, accuracy: 1.0\n",
      "loss: 0.00753502594307065, accuracy: 1.0\n",
      "loss: 0.00699712336063385, accuracy: 1.0\n",
      "loss: 0.006916177459061146, accuracy: 1.0\n",
      "loss: 0.006924152374267578, accuracy: 1.0\n",
      "loss: 0.006906379945576191, accuracy: 1.0\n",
      "loss: 0.006932858843356371, accuracy: 1.0\n",
      "loss: 0.007037064526230097, accuracy: 1.0\n",
      "loss: 0.007114730775356293, accuracy: 1.0\n",
      "loss: 0.007106177043169737, accuracy: 1.0\n",
      "loss: 0.007004932034760714, accuracy: 1.0\n",
      "loss: 0.0070982822217047215, accuracy: 1.0\n",
      "loss: 0.0068112825974822044, accuracy: 1.0\n",
      "loss: 0.007132685277611017, accuracy: 1.0\n",
      "loss: 0.007451790384948254, accuracy: 1.0\n",
      "loss: 0.007040782831609249, accuracy: 1.0\n",
      "loss: 0.006769073661416769, accuracy: 1.0\n",
      "loss: 0.00703069195151329, accuracy: 1.0\n",
      "loss: 0.007178905885666609, accuracy: 1.0\n",
      "loss: 0.006673794239759445, accuracy: 1.0\n",
      "loss: 0.007338644936680794, accuracy: 1.0\n",
      "loss: 0.007008520420640707, accuracy: 1.0\n",
      "loss: 0.007184597197920084, accuracy: 1.0\n",
      "loss: 0.006998812779784203, accuracy: 1.0\n",
      "loss: 0.006896599195897579, accuracy: 1.0\n",
      "loss: 0.0069006322883069515, accuracy: 1.0\n",
      "loss: 0.006832850631326437, accuracy: 1.0\n",
      "loss: 0.007124385796487331, accuracy: 1.0\n",
      "loss: 0.007145258132368326, accuracy: 1.0\n",
      "loss: 0.0070971897803246975, accuracy: 1.0\n",
      "loss: 0.0069997492246329784, accuracy: 1.0\n",
      "loss: 0.006874335929751396, accuracy: 1.0\n",
      "loss: 0.006930628325790167, accuracy: 1.0\n",
      "loss: 0.006804357282817364, accuracy: 1.0\n",
      "loss: 0.006783518008887768, accuracy: 1.0\n",
      "loss: 0.007041878066956997, accuracy: 1.0\n",
      "loss: 0.006913312710821629, accuracy: 1.0\n",
      "loss: 0.006951114162802696, accuracy: 1.0\n",
      "loss: 0.007312519010156393, accuracy: 1.0\n",
      "loss: 0.006701834034174681, accuracy: 1.0\n",
      "loss: 0.006774637382477522, accuracy: 1.0\n",
      "loss: 0.006883390713483095, accuracy: 1.0\n",
      "loss: 0.006860247813165188, accuracy: 1.0\n",
      "loss: 0.007246601860970259, accuracy: 1.0\n",
      "loss: 0.006845223717391491, accuracy: 1.0\n",
      "loss: 0.006901468615978956, accuracy: 1.0\n",
      "loss: 0.006886277347803116, accuracy: 1.0\n",
      "loss: 0.006879319902509451, accuracy: 1.0\n",
      "loss: 0.007105420809239149, accuracy: 1.0\n",
      "loss: 0.0070743253454566, accuracy: 1.0\n",
      "loss: 0.006825703661888838, accuracy: 1.0\n",
      "loss: 0.006685739848762751, accuracy: 1.0\n",
      "loss: 0.006735984236001968, accuracy: 1.0\n",
      "loss: 0.006667717359960079, accuracy: 1.0\n",
      "loss: 0.006802800577133894, accuracy: 1.0\n",
      "loss: 0.006748145446181297, accuracy: 1.0\n",
      "loss: 0.007124507799744606, accuracy: 1.0\n",
      "loss: 0.006652528885751963, accuracy: 1.0\n",
      "loss: 0.00674057612195611, accuracy: 1.0\n",
      "loss: 0.006577007006853819, accuracy: 1.0\n",
      "loss: 0.007058083079755306, accuracy: 1.0\n",
      "loss: 0.006677973549813032, accuracy: 1.0\n",
      "loss: 0.006830156780779362, accuracy: 1.0\n",
      "loss: 0.006799507420510054, accuracy: 1.0\n",
      "loss: 0.006787661928683519, accuracy: 1.0\n",
      "loss: 0.006468002684414387, accuracy: 1.0\n",
      "loss: 0.006857548840343952, accuracy: 1.0\n",
      "loss: 0.006695127580314875, accuracy: 1.0\n",
      "loss: 0.006686484441161156, accuracy: 1.0\n",
      "loss: 0.006711976137012243, accuracy: 1.0\n",
      "loss: 0.006916672457009554, accuracy: 1.0\n",
      "loss: 0.006650666706264019, accuracy: 1.0\n",
      "loss: 0.006725747138261795, accuracy: 1.0\n",
      "loss: 0.0069968365132808685, accuracy: 1.0\n",
      "loss: 0.006621150765568018, accuracy: 1.0\n",
      "loss: 0.006988879758864641, accuracy: 1.0\n",
      "loss: 0.006986968219280243, accuracy: 1.0\n",
      "loss: 0.00693693570792675, accuracy: 1.0\n",
      "loss: 0.0067732492461800575, accuracy: 1.0\n",
      "loss: 0.006673901807516813, accuracy: 1.0\n",
      "loss: 0.00681157736107707, accuracy: 1.0\n",
      "loss: 0.006637871731072664, accuracy: 1.0\n",
      "loss: 0.006728596519678831, accuracy: 1.0\n",
      "loss: 0.006695001386106014, accuracy: 1.0\n",
      "loss: 0.006769378669559956, accuracy: 1.0\n",
      "loss: 0.006690389942377806, accuracy: 1.0\n",
      "loss: 0.006676917430013418, accuracy: 1.0\n",
      "loss: 0.006711584981530905, accuracy: 1.0\n",
      "loss: 0.00669935904443264, accuracy: 1.0\n",
      "loss: 0.006795465014874935, accuracy: 1.0\n",
      "loss: 0.006905844435095787, accuracy: 1.0\n",
      "loss: 0.006704169791191816, accuracy: 1.0\n",
      "loss: 0.006806455552577972, accuracy: 1.0\n",
      "loss: 0.006593065802007914, accuracy: 1.0\n",
      "loss: 0.006583972834050655, accuracy: 1.0\n",
      "loss: 0.006810696795582771, accuracy: 1.0\n",
      "loss: 0.006651931442320347, accuracy: 1.0\n",
      "loss: 0.006662758067250252, accuracy: 1.0\n",
      "loss: 0.006493082270026207, accuracy: 1.0\n",
      "loss: 0.006786864250898361, accuracy: 1.0\n",
      "loss: 0.006984435487538576, accuracy: 1.0\n",
      "loss: 0.006719398777931929, accuracy: 1.0\n",
      "loss: 0.006652018055319786, accuracy: 1.0\n",
      "loss: 0.006559797562658787, accuracy: 1.0\n",
      "loss: 0.006604968570172787, accuracy: 1.0\n",
      "loss: 0.0064706020057201385, accuracy: 1.0\n",
      "loss: 0.006770764011889696, accuracy: 1.0\n",
      "loss: 0.006591741926968098, accuracy: 1.0\n",
      "loss: 0.006862531881779432, accuracy: 1.0\n",
      "loss: 0.006754840724170208, accuracy: 1.0\n",
      "loss: 0.006510336417704821, accuracy: 1.0\n",
      "loss: 0.006597044877707958, accuracy: 1.0\n",
      "loss: 0.006859447807073593, accuracy: 1.0\n",
      "loss: 0.006849213968962431, accuracy: 1.0\n",
      "loss: 0.006723130587488413, accuracy: 1.0\n",
      "loss: 0.006557189393788576, accuracy: 1.0\n",
      "loss: 0.006584493909031153, accuracy: 1.0\n",
      "loss: 0.006493676919490099, accuracy: 1.0\n",
      "loss: 0.006675058044493198, accuracy: 1.0\n",
      "loss: 0.006420541554689407, accuracy: 1.0\n",
      "loss: 0.006516760680824518, accuracy: 1.0\n",
      "loss: 0.006556576583534479, accuracy: 1.0\n",
      "loss: 0.006575677543878555, accuracy: 1.0\n",
      "loss: 0.006767631508409977, accuracy: 1.0\n",
      "loss: 0.006757549941539764, accuracy: 1.0\n",
      "loss: 0.006630680989474058, accuracy: 1.0\n",
      "loss: 0.006760914344340563, accuracy: 1.0\n",
      "loss: 0.006601041182875633, accuracy: 1.0\n",
      "loss: 0.0068521685898303986, accuracy: 1.0\n",
      "loss: 0.006485003978013992, accuracy: 1.0\n",
      "loss: 0.006672498770058155, accuracy: 1.0\n",
      "loss: 0.006628801114857197, accuracy: 1.0\n",
      "loss: 0.006565142888575792, accuracy: 1.0\n",
      "loss: 0.00668879272416234, accuracy: 1.0\n",
      "loss: 0.006608426105231047, accuracy: 1.0\n",
      "loss: 0.006582286674529314, accuracy: 1.0\n",
      "loss: 0.0064558363519608974, accuracy: 1.0\n",
      "loss: 0.006591401062905788, accuracy: 1.0\n",
      "loss: 0.006538073532283306, accuracy: 1.0\n",
      "loss: 0.00661309901624918, accuracy: 1.0\n",
      "loss: 0.006437590345740318, accuracy: 1.0\n",
      "loss: 0.006580952554941177, accuracy: 1.0\n",
      "loss: 0.006583323236554861, accuracy: 1.0\n",
      "loss: 0.006979535333812237, accuracy: 1.0\n",
      "loss: 0.006492882966995239, accuracy: 1.0\n",
      "loss: 0.0067504351027309895, accuracy: 1.0\n",
      "loss: 0.006611814256757498, accuracy: 1.0\n",
      "loss: 0.0063677458092570305, accuracy: 1.0\n",
      "loss: 0.006440888624638319, accuracy: 1.0\n",
      "loss: 0.006655064877122641, accuracy: 1.0\n",
      "loss: 0.006715910509228706, accuracy: 1.0\n",
      "loss: 0.006544132251292467, accuracy: 1.0\n",
      "loss: 0.006588622462004423, accuracy: 1.0\n",
      "loss: 0.0064267562702298164, accuracy: 1.0\n",
      "loss: 0.006764890626072884, accuracy: 1.0\n",
      "loss: 0.006221518851816654, accuracy: 1.0\n",
      "loss: 0.00672883540391922, accuracy: 1.0\n",
      "loss: 0.006368928123265505, accuracy: 1.0\n",
      "loss: 0.006415112875401974, accuracy: 1.0\n",
      "loss: 0.006547871977090836, accuracy: 1.0\n",
      "loss: 0.006528170313686132, accuracy: 1.0\n",
      "loss: 0.0066096484661102295, accuracy: 1.0\n",
      "loss: 0.006485236342996359, accuracy: 1.0\n",
      "loss: 0.006479465402662754, accuracy: 1.0\n",
      "loss: 0.006545348092913628, accuracy: 1.0\n",
      "loss: 0.006377538200467825, accuracy: 1.0\n",
      "loss: 0.006464301608502865, accuracy: 1.0\n",
      "loss: 0.006634178571403027, accuracy: 1.0\n",
      "loss: 0.006340479012578726, accuracy: 1.0\n",
      "loss: 0.006529476493597031, accuracy: 1.0\n",
      "loss: 0.006379841826856136, accuracy: 1.0\n",
      "loss: 0.006345824804157019, accuracy: 1.0\n",
      "loss: 0.006298956926912069, accuracy: 1.0\n",
      "loss: 0.006739259697496891, accuracy: 1.0\n",
      "loss: 0.006647929549217224, accuracy: 1.0\n",
      "loss: 0.006384862121194601, accuracy: 1.0\n",
      "loss: 0.006477445363998413, accuracy: 1.0\n",
      "loss: 0.006506612058728933, accuracy: 1.0\n",
      "loss: 0.006427401676774025, accuracy: 1.0\n",
      "loss: 0.006661484483629465, accuracy: 1.0\n",
      "loss: 0.006576249375939369, accuracy: 1.0\n",
      "loss: 0.006335602141916752, accuracy: 1.0\n",
      "loss: 0.006661530118435621, accuracy: 1.0\n",
      "loss: 0.006231797393411398, accuracy: 1.0\n",
      "loss: 0.0066289715468883514, accuracy: 1.0\n",
      "loss: 0.006512384861707687, accuracy: 1.0\n",
      "loss: 0.006242061965167522, accuracy: 1.0\n",
      "loss: 0.006384185515344143, accuracy: 1.0\n",
      "loss: 0.006537472363561392, accuracy: 1.0\n",
      "loss: 0.006456352304667234, accuracy: 1.0\n",
      "loss: 0.006380626931786537, accuracy: 1.0\n",
      "loss: 0.006209339946508408, accuracy: 1.0\n",
      "loss: 0.0063268025405704975, accuracy: 1.0\n",
      "loss: 0.006401869002729654, accuracy: 1.0\n",
      "loss: 0.006571084260940552, accuracy: 1.0\n",
      "loss: 0.006517855916172266, accuracy: 1.0\n",
      "loss: 0.006395322736352682, accuracy: 1.0\n",
      "loss: 0.006319678854197264, accuracy: 1.0\n",
      "loss: 0.0063954987563192844, accuracy: 1.0\n",
      "loss: 0.006376936100423336, accuracy: 1.0\n",
      "loss: 0.006304312031716108, accuracy: 1.0\n",
      "loss: 0.006252820137888193, accuracy: 1.0\n",
      "loss: 0.006406398490071297, accuracy: 1.0\n",
      "loss: 0.006912271026521921, accuracy: 1.0\n",
      "loss: 0.006211154628545046, accuracy: 1.0\n",
      "loss: 0.006333795841783285, accuracy: 1.0\n",
      "loss: 0.006474874913692474, accuracy: 1.0\n",
      "loss: 0.006372846197336912, accuracy: 1.0\n",
      "loss: 0.006260094232857227, accuracy: 1.0\n",
      "loss: 0.006350286770612001, accuracy: 1.0\n",
      "loss: 0.006319754291325808, accuracy: 1.0\n",
      "loss: 0.006323910318315029, accuracy: 1.0\n",
      "loss: 0.006412378046661615, accuracy: 1.0\n",
      "loss: 0.006261907983571291, accuracy: 1.0\n",
      "loss: 0.006333807483315468, accuracy: 1.0\n",
      "loss: 0.006455336697399616, accuracy: 1.0\n",
      "loss: 0.006514552049338818, accuracy: 1.0\n",
      "loss: 0.006271350663155317, accuracy: 1.0\n",
      "loss: 0.006366918794810772, accuracy: 1.0\n",
      "loss: 0.00649136770516634, accuracy: 1.0\n",
      "loss: 0.006255411542952061, accuracy: 1.0\n",
      "loss: 0.00641883397474885, accuracy: 1.0\n",
      "loss: 0.006752306129783392, accuracy: 1.0\n",
      "loss: 0.006335839629173279, accuracy: 1.0\n",
      "loss: 0.006300457287579775, accuracy: 1.0\n",
      "loss: 0.006265782285481691, accuracy: 1.0\n",
      "loss: 0.006168603431433439, accuracy: 1.0\n",
      "loss: 0.006156639661639929, accuracy: 1.0\n",
      "loss: 0.006378223653882742, accuracy: 1.0\n",
      "loss: 0.006545373238623142, accuracy: 1.0\n",
      "loss: 0.006199490744620562, accuracy: 1.0\n",
      "loss: 0.006347790360450745, accuracy: 1.0\n",
      "loss: 0.006317454855889082, accuracy: 1.0\n",
      "loss: 0.006255751475691795, accuracy: 1.0\n",
      "loss: 0.006320944055914879, accuracy: 1.0\n",
      "loss: 0.006064866669476032, accuracy: 1.0\n",
      "loss: 0.006250771693885326, accuracy: 1.0\n",
      "loss: 0.006333952769637108, accuracy: 1.0\n",
      "loss: 0.006459678988903761, accuracy: 1.0\n",
      "loss: 0.00626981258392334, accuracy: 1.0\n",
      "loss: 0.0062909359112381935, accuracy: 1.0\n",
      "loss: 0.006091213785111904, accuracy: 1.0\n",
      "loss: 0.0065271430648863316, accuracy: 1.0\n",
      "loss: 0.006471620872616768, accuracy: 1.0\n",
      "loss: 0.006185468286275864, accuracy: 1.0\n",
      "loss: 0.00612183753401041, accuracy: 1.0\n",
      "loss: 0.006367396097630262, accuracy: 1.0\n",
      "loss: 0.006221878807991743, accuracy: 1.0\n",
      "loss: 0.0062250602059066296, accuracy: 1.0\n",
      "loss: 0.006239117588847876, accuracy: 1.0\n",
      "loss: 0.006228093523532152, accuracy: 1.0\n",
      "loss: 0.0063234358094632626, accuracy: 1.0\n",
      "loss: 0.006179032381623983, accuracy: 1.0\n",
      "loss: 0.0062987180426716805, accuracy: 1.0\n",
      "loss: 0.006238007452338934, accuracy: 1.0\n",
      "loss: 0.00630973419174552, accuracy: 1.0\n",
      "loss: 0.006349381525069475, accuracy: 1.0\n",
      "loss: 0.0064786202274262905, accuracy: 1.0\n",
      "loss: 0.006251192186027765, accuracy: 1.0\n",
      "loss: 0.006244593765586615, accuracy: 1.0\n",
      "loss: 0.0064041209407150745, accuracy: 1.0\n",
      "loss: 0.006084732711315155, accuracy: 1.0\n",
      "loss: 0.006331827957183123, accuracy: 1.0\n",
      "loss: 0.006237977650016546, accuracy: 1.0\n",
      "loss: 0.006282714661210775, accuracy: 1.0\n",
      "loss: 0.006690960377454758, accuracy: 1.0\n",
      "loss: 0.006225030869245529, accuracy: 1.0\n",
      "loss: 0.00624482287093997, accuracy: 1.0\n",
      "loss: 0.006483237259089947, accuracy: 1.0\n",
      "loss: 0.006264795083552599, accuracy: 1.0\n",
      "loss: 0.006092429626733065, accuracy: 1.0\n",
      "loss: 0.00627850741147995, accuracy: 1.0\n",
      "loss: 0.006055830512195826, accuracy: 1.0\n",
      "loss: 0.006046950351446867, accuracy: 1.0\n",
      "loss: 0.0062213013879954815, accuracy: 1.0\n",
      "loss: 0.006046462804079056, accuracy: 1.0\n",
      "loss: 0.006396285258233547, accuracy: 1.0\n",
      "loss: 0.006103847175836563, accuracy: 1.0\n",
      "loss: 0.006130743771791458, accuracy: 1.0\n",
      "loss: 0.006112687289714813, accuracy: 1.0\n",
      "loss: 0.006144373212009668, accuracy: 1.0\n",
      "loss: 0.006138806696981192, accuracy: 1.0\n",
      "loss: 0.006313283462077379, accuracy: 1.0\n",
      "loss: 0.006083116866648197, accuracy: 1.0\n",
      "loss: 0.006285205017775297, accuracy: 1.0\n",
      "loss: 0.00606246292591095, accuracy: 1.0\n",
      "loss: 0.006149045657366514, accuracy: 1.0\n",
      "loss: 0.0061795152723789215, accuracy: 1.0\n",
      "loss: 0.0061883702874183655, accuracy: 1.0\n",
      "loss: 0.005939204711467028, accuracy: 1.0\n",
      "loss: 0.006362986750900745, accuracy: 1.0\n",
      "loss: 0.006516605615615845, accuracy: 1.0\n",
      "loss: 0.006023882422596216, accuracy: 1.0\n",
      "loss: 0.006240642163902521, accuracy: 1.0\n",
      "loss: 0.006075988058000803, accuracy: 1.0\n",
      "loss: 0.006146568804979324, accuracy: 1.0\n",
      "loss: 0.006161462049931288, accuracy: 1.0\n",
      "loss: 0.006257997360080481, accuracy: 1.0\n",
      "loss: 0.0061303721740841866, accuracy: 1.0\n",
      "loss: 0.005973774939775467, accuracy: 1.0\n",
      "loss: 0.006114594172686338, accuracy: 1.0\n",
      "loss: 0.006109482143074274, accuracy: 1.0\n",
      "loss: 0.006058095023036003, accuracy: 1.0\n",
      "loss: 0.006111437454819679, accuracy: 1.0\n",
      "loss: 0.0062683881260454655, accuracy: 1.0\n",
      "loss: 0.006235452834516764, accuracy: 1.0\n",
      "loss: 0.005921190604567528, accuracy: 1.0\n",
      "loss: 0.006002787966281176, accuracy: 1.0\n",
      "loss: 0.0061654639430344105, accuracy: 1.0\n",
      "loss: 0.006109770853072405, accuracy: 1.0\n",
      "loss: 0.006064363289624453, accuracy: 1.0\n",
      "loss: 0.006176543887704611, accuracy: 1.0\n",
      "loss: 0.006051572971045971, accuracy: 1.0\n",
      "loss: 0.005945316981524229, accuracy: 1.0\n",
      "loss: 0.006175138056278229, accuracy: 1.0\n",
      "loss: 0.006225944496691227, accuracy: 1.0\n",
      "loss: 0.006262559909373522, accuracy: 1.0\n",
      "loss: 0.0061040716245770454, accuracy: 1.0\n",
      "loss: 0.006396777927875519, accuracy: 1.0\n",
      "loss: 0.006037539802491665, accuracy: 1.0\n",
      "loss: 0.00623759999871254, accuracy: 1.0\n",
      "loss: 0.006130625493824482, accuracy: 1.0\n",
      "loss: 0.006494611501693726, accuracy: 1.0\n",
      "loss: 0.006028716918081045, accuracy: 1.0\n",
      "loss: 0.006146480794996023, accuracy: 1.0\n",
      "loss: 0.0060900477692484856, accuracy: 1.0\n",
      "loss: 0.006044737994670868, accuracy: 1.0\n",
      "loss: 0.006006952840834856, accuracy: 1.0\n",
      "loss: 0.006032668519765139, accuracy: 1.0\n",
      "loss: 0.006261078175157309, accuracy: 1.0\n",
      "loss: 0.005920006427913904, accuracy: 1.0\n",
      "loss: 0.005969843361526728, accuracy: 1.0\n",
      "loss: 0.006127865985035896, accuracy: 1.0\n",
      "loss: 0.006301118992269039, accuracy: 1.0\n",
      "loss: 0.00590894091874361, accuracy: 1.0\n",
      "loss: 0.006193573120981455, accuracy: 1.0\n",
      "loss: 0.005996831227093935, accuracy: 1.0\n",
      "loss: 0.006071179173886776, accuracy: 1.0\n",
      "loss: 0.00606390880420804, accuracy: 1.0\n",
      "loss: 0.006178444717079401, accuracy: 1.0\n",
      "loss: 0.006194511894136667, accuracy: 1.0\n",
      "loss: 0.006120555568486452, accuracy: 1.0\n",
      "loss: 0.006378497462719679, accuracy: 1.0\n",
      "loss: 0.005974247585982084, accuracy: 1.0\n",
      "loss: 0.0058651757426559925, accuracy: 1.0\n",
      "loss: 0.006070978473871946, accuracy: 1.0\n",
      "loss: 0.005927179474383593, accuracy: 1.0\n",
      "loss: 0.005959983915090561, accuracy: 1.0\n",
      "loss: 0.006076341029256582, accuracy: 1.0\n",
      "loss: 0.005953078158199787, accuracy: 1.0\n",
      "loss: 0.005972071550786495, accuracy: 1.0\n",
      "loss: 0.005976270418614149, accuracy: 1.0\n",
      "loss: 0.0060467105358839035, accuracy: 1.0\n",
      "loss: 0.006027935538440943, accuracy: 1.0\n",
      "loss: 0.005920461844652891, accuracy: 1.0\n",
      "loss: 0.006012683734297752, accuracy: 1.0\n",
      "loss: 0.0062003047205507755, accuracy: 1.0\n",
      "loss: 0.006001847330480814, accuracy: 1.0\n",
      "loss: 0.005874055437743664, accuracy: 1.0\n",
      "loss: 0.005894212517887354, accuracy: 1.0\n",
      "loss: 0.005936047062277794, accuracy: 1.0\n",
      "loss: 0.005800932180136442, accuracy: 1.0\n",
      "loss: 0.005931275896728039, accuracy: 1.0\n",
      "loss: 0.005972920451313257, accuracy: 1.0\n",
      "loss: 0.005884237587451935, accuracy: 1.0\n",
      "loss: 0.005867485888302326, accuracy: 1.0\n",
      "loss: 0.006080548278987408, accuracy: 1.0\n",
      "loss: 0.0060048880986869335, accuracy: 1.0\n",
      "loss: 0.0060770632699131966, accuracy: 1.0\n",
      "loss: 0.005853744689375162, accuracy: 1.0\n",
      "loss: 0.005927983671426773, accuracy: 1.0\n",
      "loss: 0.005925620906054974, accuracy: 1.0\n",
      "loss: 0.005968074314296246, accuracy: 1.0\n",
      "loss: 0.005896472837775946, accuracy: 1.0\n",
      "loss: 0.0060410103760659695, accuracy: 1.0\n",
      "loss: 0.005997855681926012, accuracy: 1.0\n",
      "loss: 0.005971848499029875, accuracy: 1.0\n",
      "loss: 0.005938716698437929, accuracy: 1.0\n",
      "loss: 0.006001426838338375, accuracy: 1.0\n",
      "loss: 0.005898939445614815, accuracy: 1.0\n",
      "loss: 0.005853746552020311, accuracy: 1.0\n",
      "loss: 0.005837096832692623, accuracy: 1.0\n",
      "loss: 0.0057855285704135895, accuracy: 1.0\n",
      "loss: 0.006025419104844332, accuracy: 1.0\n",
      "loss: 0.005916608031839132, accuracy: 1.0\n",
      "loss: 0.005890785716474056, accuracy: 1.0\n",
      "loss: 0.005888347979635, accuracy: 1.0\n",
      "loss: 0.005892614834010601, accuracy: 1.0\n",
      "loss: 0.0059637767262756824, accuracy: 1.0\n",
      "loss: 0.0059122079983353615, accuracy: 1.0\n",
      "loss: 0.0057852622121572495, accuracy: 1.0\n",
      "loss: 0.005941375158727169, accuracy: 1.0\n",
      "loss: 0.0060029723681509495, accuracy: 1.0\n",
      "loss: 0.006058265920728445, accuracy: 1.0\n",
      "loss: 0.005836304277181625, accuracy: 1.0\n",
      "loss: 0.005865232553333044, accuracy: 1.0\n",
      "loss: 0.005923918914049864, accuracy: 1.0\n",
      "loss: 0.005892449989914894, accuracy: 1.0\n",
      "loss: 0.005935067310929298, accuracy: 1.0\n",
      "loss: 0.0057061039842665195, accuracy: 1.0\n",
      "loss: 0.0059560141526162624, accuracy: 1.0\n",
      "loss: 0.005851259920746088, accuracy: 1.0\n",
      "loss: 0.005915189627557993, accuracy: 1.0\n",
      "loss: 0.005930581130087376, accuracy: 1.0\n",
      "loss: 0.005826414562761784, accuracy: 1.0\n",
      "loss: 0.005726068280637264, accuracy: 1.0\n",
      "loss: 0.005786566529422998, accuracy: 1.0\n",
      "loss: 0.005824279971420765, accuracy: 1.0\n",
      "loss: 0.00596374087035656, accuracy: 1.0\n",
      "loss: 0.005854866001754999, accuracy: 1.0\n",
      "loss: 0.005771992728114128, accuracy: 1.0\n",
      "loss: 0.006027900148183107, accuracy: 1.0\n",
      "loss: 0.005858532153069973, accuracy: 1.0\n",
      "loss: 0.005911957006901503, accuracy: 1.0\n",
      "loss: 0.005807942245155573, accuracy: 1.0\n",
      "loss: 0.005823205690830946, accuracy: 1.0\n",
      "loss: 0.005730669014155865, accuracy: 1.0\n",
      "loss: 0.005949171259999275, accuracy: 1.0\n",
      "loss: 0.005756234284490347, accuracy: 1.0\n",
      "loss: 0.005840514320880175, accuracy: 1.0\n",
      "loss: 0.005862138234078884, accuracy: 1.0\n",
      "loss: 0.005739813204854727, accuracy: 1.0\n",
      "loss: 0.0057228803634643555, accuracy: 1.0\n",
      "loss: 0.005773012526333332, accuracy: 1.0\n",
      "loss: 0.005902858916670084, accuracy: 1.0\n",
      "loss: 0.005972269922494888, accuracy: 1.0\n",
      "loss: 0.005804985295981169, accuracy: 1.0\n",
      "loss: 0.00585898058488965, accuracy: 1.0\n",
      "loss: 0.0058653587475419044, accuracy: 1.0\n",
      "loss: 0.005803078878670931, accuracy: 1.0\n",
      "loss: 0.005703373812139034, accuracy: 1.0\n",
      "loss: 0.005790018010884523, accuracy: 1.0\n",
      "loss: 0.005825994536280632, accuracy: 1.0\n",
      "loss: 0.005892588756978512, accuracy: 1.0\n",
      "loss: 0.005731594283133745, accuracy: 1.0\n",
      "loss: 0.005632678046822548, accuracy: 1.0\n",
      "loss: 0.005818319972604513, accuracy: 1.0\n",
      "loss: 0.005735164042562246, accuracy: 1.0\n",
      "loss: 0.005720675457268953, accuracy: 1.0\n",
      "loss: 0.005796399898827076, accuracy: 1.0\n",
      "loss: 0.005938665941357613, accuracy: 1.0\n",
      "loss: 0.005845289211720228, accuracy: 1.0\n",
      "loss: 0.005810398142784834, accuracy: 1.0\n",
      "loss: 0.00569463474676013, accuracy: 1.0\n",
      "loss: 0.005670945625752211, accuracy: 1.0\n",
      "loss: 0.005743532907217741, accuracy: 1.0\n",
      "loss: 0.0058203451335430145, accuracy: 1.0\n",
      "loss: 0.0057618338614702225, accuracy: 1.0\n",
      "loss: 0.005785993300378323, accuracy: 1.0\n",
      "loss: 0.005821005906909704, accuracy: 1.0\n",
      "loss: 0.0057310424745082855, accuracy: 1.0\n",
      "loss: 0.005701134912669659, accuracy: 1.0\n",
      "loss: 0.005879230797290802, accuracy: 1.0\n",
      "loss: 0.005702023860067129, accuracy: 1.0\n",
      "loss: 0.005612395238131285, accuracy: 1.0\n",
      "loss: 0.006143035367131233, accuracy: 1.0\n",
      "loss: 0.005663187243044376, accuracy: 1.0\n",
      "loss: 0.005837650969624519, accuracy: 1.0\n",
      "loss: 0.00572793185710907, accuracy: 1.0\n",
      "loss: 0.005600884091109037, accuracy: 1.0\n",
      "loss: 0.005886151921004057, accuracy: 1.0\n",
      "loss: 0.0056657493114471436, accuracy: 1.0\n",
      "loss: 0.005850097630172968, accuracy: 1.0\n",
      "loss: 0.005823857616633177, accuracy: 1.0\n",
      "loss: 0.00593722565099597, accuracy: 1.0\n",
      "loss: 0.005904842633754015, accuracy: 1.0\n",
      "loss: 0.005828446708619595, accuracy: 1.0\n",
      "loss: 0.005850088782608509, accuracy: 1.0\n",
      "loss: 0.005587045103311539, accuracy: 1.0\n",
      "loss: 0.0056040058843791485, accuracy: 1.0\n",
      "loss: 0.005647654645144939, accuracy: 1.0\n",
      "loss: 0.005785257089883089, accuracy: 1.0\n",
      "loss: 0.00558138033375144, accuracy: 1.0\n",
      "loss: 0.0057172514498233795, accuracy: 1.0\n",
      "loss: 0.0057514738291502, accuracy: 1.0\n",
      "loss: 0.005653537809848785, accuracy: 1.0\n",
      "loss: 0.005713404156267643, accuracy: 1.0\n",
      "loss: 0.0055488524958491325, accuracy: 1.0\n",
      "loss: 0.005595522467046976, accuracy: 1.0\n",
      "loss: 0.005751038435846567, accuracy: 1.0\n",
      "loss: 0.005656003020703793, accuracy: 1.0\n",
      "loss: 0.005781671032309532, accuracy: 1.0\n",
      "loss: 0.005666060838848352, accuracy: 1.0\n",
      "loss: 0.005669130478054285, accuracy: 1.0\n",
      "loss: 0.005555142182856798, accuracy: 1.0\n",
      "loss: 0.005614120047539473, accuracy: 1.0\n",
      "loss: 0.005747450515627861, accuracy: 1.0\n",
      "loss: 0.005668425466865301, accuracy: 1.0\n",
      "loss: 0.00580243393778801, accuracy: 1.0\n",
      "loss: 0.005656661000102758, accuracy: 1.0\n",
      "loss: 0.005707730073481798, accuracy: 1.0\n",
      "loss: 0.005847804248332977, accuracy: 1.0\n",
      "loss: 0.005652331747114658, accuracy: 1.0\n",
      "loss: 0.005578429438173771, accuracy: 1.0\n",
      "loss: 0.005585305858403444, accuracy: 1.0\n",
      "loss: 0.005761816166341305, accuracy: 1.0\n",
      "loss: 0.005631200037896633, accuracy: 1.0\n",
      "loss: 0.005587183404713869, accuracy: 1.0\n",
      "loss: 0.005609221290796995, accuracy: 1.0\n",
      "loss: 0.005684645846486092, accuracy: 1.0\n",
      "loss: 0.00568210007622838, accuracy: 1.0\n",
      "loss: 0.005655481480062008, accuracy: 1.0\n",
      "loss: 0.005626871716231108, accuracy: 1.0\n",
      "loss: 0.0056333281099796295, accuracy: 1.0\n",
      "loss: 0.005502977874130011, accuracy: 1.0\n",
      "loss: 0.0056335232220590115, accuracy: 1.0\n",
      "loss: 0.005857779644429684, accuracy: 1.0\n",
      "loss: 0.005542203783988953, accuracy: 1.0\n",
      "loss: 0.0057007018476724625, accuracy: 1.0\n",
      "loss: 0.005546906031668186, accuracy: 1.0\n",
      "loss: 0.005717550404369831, accuracy: 1.0\n",
      "loss: 0.00567633518949151, accuracy: 1.0\n",
      "loss: 0.005833754315972328, accuracy: 1.0\n",
      "loss: 0.005680054426193237, accuracy: 1.0\n",
      "loss: 0.00553370313718915, accuracy: 1.0\n",
      "loss: 0.005597833544015884, accuracy: 1.0\n",
      "loss: 0.005489410366863012, accuracy: 1.0\n",
      "loss: 0.005732676014304161, accuracy: 1.0\n",
      "loss: 0.005752139259129763, accuracy: 1.0\n",
      "loss: 0.005492131691426039, accuracy: 1.0\n",
      "loss: 0.0057626995258033276, accuracy: 1.0\n",
      "loss: 0.005506706424057484, accuracy: 1.0\n",
      "loss: 0.005734364502131939, accuracy: 1.0\n",
      "loss: 0.005534409079700708, accuracy: 1.0\n",
      "loss: 0.005723781883716583, accuracy: 1.0\n",
      "loss: 0.005590222310274839, accuracy: 1.0\n",
      "loss: 0.005613734945654869, accuracy: 1.0\n",
      "loss: 0.005541887134313583, accuracy: 1.0\n",
      "loss: 0.00578233040869236, accuracy: 1.0\n",
      "loss: 0.005536966025829315, accuracy: 1.0\n",
      "loss: 0.005687912926077843, accuracy: 1.0\n",
      "loss: 0.005499627906829119, accuracy: 1.0\n",
      "loss: 0.005649460479617119, accuracy: 1.0\n",
      "loss: 0.005462292581796646, accuracy: 1.0\n",
      "loss: 0.0057671102695167065, accuracy: 1.0\n",
      "loss: 0.0054301731288433075, accuracy: 1.0\n",
      "loss: 0.005479967221617699, accuracy: 1.0\n",
      "loss: 0.005510801915079355, accuracy: 1.0\n",
      "loss: 0.005573029164224863, accuracy: 1.0\n",
      "loss: 0.00551626505330205, accuracy: 1.0\n",
      "loss: 0.005694092251360416, accuracy: 1.0\n",
      "loss: 0.005564443301409483, accuracy: 1.0\n",
      "loss: 0.005452368408441544, accuracy: 1.0\n",
      "loss: 0.005523092579096556, accuracy: 1.0\n",
      "loss: 0.005499938037246466, accuracy: 1.0\n",
      "loss: 0.005547466222196817, accuracy: 1.0\n",
      "loss: 0.005508581176400185, accuracy: 1.0\n",
      "loss: 0.005506580229848623, accuracy: 1.0\n",
      "loss: 0.005416121333837509, accuracy: 1.0\n",
      "loss: 0.005456538405269384, accuracy: 1.0\n",
      "loss: 0.005544744897633791, accuracy: 1.0\n",
      "loss: 0.005515638738870621, accuracy: 1.0\n",
      "loss: 0.00550413504242897, accuracy: 1.0\n",
      "loss: 0.00545392744243145, accuracy: 1.0\n",
      "loss: 0.005493684206157923, accuracy: 1.0\n",
      "loss: 0.005502489861100912, accuracy: 1.0\n",
      "loss: 0.005590812768787146, accuracy: 1.0\n",
      "loss: 0.0055031427182257175, accuracy: 1.0\n",
      "loss: 0.005566511768847704, accuracy: 1.0\n",
      "loss: 0.0054971096105873585, accuracy: 1.0\n",
      "loss: 0.005443989764899015, accuracy: 1.0\n",
      "loss: 0.0054031433537602425, accuracy: 1.0\n",
      "loss: 0.005538336466997862, accuracy: 1.0\n",
      "loss: 0.005442663561552763, accuracy: 1.0\n",
      "loss: 0.005579213611781597, accuracy: 1.0\n",
      "loss: 0.005427266005426645, accuracy: 1.0\n",
      "loss: 0.005555373150855303, accuracy: 1.0\n",
      "loss: 0.005589574109762907, accuracy: 1.0\n",
      "loss: 0.005468871910125017, accuracy: 1.0\n",
      "loss: 0.005430178251117468, accuracy: 1.0\n",
      "loss: 0.005527365021407604, accuracy: 1.0\n",
      "loss: 0.0054807341657578945, accuracy: 1.0\n",
      "loss: 0.005458707455545664, accuracy: 1.0\n",
      "loss: 0.0053567285649478436, accuracy: 1.0\n",
      "loss: 0.005530215799808502, accuracy: 1.0\n",
      "loss: 0.005411780904978514, accuracy: 1.0\n",
      "loss: 0.005408787168562412, accuracy: 1.0\n",
      "loss: 0.005440155975520611, accuracy: 1.0\n",
      "loss: 0.0054900748655200005, accuracy: 1.0\n",
      "loss: 0.005510810762643814, accuracy: 1.0\n",
      "loss: 0.005697957240045071, accuracy: 1.0\n",
      "loss: 0.00550356088206172, accuracy: 1.0\n",
      "loss: 0.005655936896800995, accuracy: 1.0\n",
      "loss: 0.005477950442582369, accuracy: 1.0\n",
      "loss: 0.005708233918994665, accuracy: 1.0\n",
      "loss: 0.0055374265648424625, accuracy: 1.0\n",
      "loss: 0.005497821141034365, accuracy: 1.0\n",
      "loss: 0.005442564841359854, accuracy: 1.0\n",
      "loss: 0.005546445958316326, accuracy: 1.0\n",
      "loss: 0.005369397345930338, accuracy: 1.0\n",
      "loss: 0.005661071743816137, accuracy: 1.0\n",
      "loss: 0.005414142739027739, accuracy: 1.0\n",
      "loss: 0.005409677512943745, accuracy: 1.0\n",
      "loss: 0.0055323317646980286, accuracy: 1.0\n",
      "loss: 0.005568698979914188, accuracy: 1.0\n",
      "loss: 0.005387736950069666, accuracy: 1.0\n",
      "loss: 0.00526537885889411, accuracy: 1.0\n",
      "loss: 0.005382698029279709, accuracy: 1.0\n",
      "loss: 0.005452257115393877, accuracy: 1.0\n",
      "loss: 0.005545943044126034, accuracy: 1.0\n",
      "loss: 0.005348782520741224, accuracy: 1.0\n",
      "loss: 0.005547855515033007, accuracy: 1.0\n",
      "loss: 0.005402572453022003, accuracy: 1.0\n",
      "loss: 0.005383360665291548, accuracy: 1.0\n",
      "loss: 0.005314473062753677, accuracy: 1.0\n",
      "loss: 0.005493077449500561, accuracy: 1.0\n",
      "loss: 0.0054690046235919, accuracy: 1.0\n",
      "loss: 0.005316928960382938, accuracy: 1.0\n",
      "loss: 0.005449746735394001, accuracy: 1.0\n",
      "loss: 0.005483473651111126, accuracy: 1.0\n",
      "loss: 0.005354755092412233, accuracy: 1.0\n",
      "loss: 0.00541995232924819, accuracy: 1.0\n",
      "loss: 0.005557538941502571, accuracy: 1.0\n",
      "loss: 0.005323079414665699, accuracy: 1.0\n",
      "loss: 0.005354023538529873, accuracy: 1.0\n",
      "loss: 0.005435761995613575, accuracy: 1.0\n",
      "loss: 0.0054682097397744656, accuracy: 1.0\n",
      "loss: 0.005391736514866352, accuracy: 1.0\n",
      "loss: 0.0053487373515963554, accuracy: 1.0\n",
      "loss: 0.005434914957731962, accuracy: 1.0\n",
      "loss: 0.005378326401114464, accuracy: 1.0\n",
      "loss: 0.005367476958781481, accuracy: 1.0\n",
      "loss: 0.005412597209215164, accuracy: 1.0\n",
      "loss: 0.005524537526071072, accuracy: 1.0\n",
      "loss: 0.0053552621975541115, accuracy: 1.0\n",
      "loss: 0.005404615309089422, accuracy: 1.0\n",
      "loss: 0.005327017977833748, accuracy: 1.0\n",
      "loss: 0.005347933154553175, accuracy: 1.0\n",
      "loss: 0.005391849670559168, accuracy: 1.0\n",
      "loss: 0.005340369418263435, accuracy: 1.0\n",
      "loss: 0.00539778359234333, accuracy: 1.0\n",
      "loss: 0.0056063514202833176, accuracy: 1.0\n",
      "loss: 0.005374500993639231, accuracy: 1.0\n",
      "loss: 0.005347434431314468, accuracy: 1.0\n",
      "loss: 0.005297535564750433, accuracy: 1.0\n",
      "loss: 0.005438921973109245, accuracy: 1.0\n",
      "loss: 0.0052678887732326984, accuracy: 1.0\n",
      "loss: 0.005412240047007799, accuracy: 1.0\n",
      "loss: 0.005273133050650358, accuracy: 1.0\n",
      "loss: 0.005340408533811569, accuracy: 1.0\n",
      "loss: 0.005566236563026905, accuracy: 1.0\n",
      "loss: 0.005365040618926287, accuracy: 1.0\n",
      "loss: 0.00547226844355464, accuracy: 1.0\n",
      "loss: 0.005440886132419109, accuracy: 1.0\n",
      "loss: 0.005336260888725519, accuracy: 1.0\n",
      "loss: 0.005299911834299564, accuracy: 1.0\n",
      "loss: 0.005388711579144001, accuracy: 1.0\n",
      "loss: 0.005295346025377512, accuracy: 1.0\n",
      "loss: 0.005284668877720833, accuracy: 1.0\n",
      "loss: 0.005383619572967291, accuracy: 1.0\n",
      "loss: 0.005372911691665649, accuracy: 1.0\n",
      "loss: 0.005351499654352665, accuracy: 1.0\n",
      "loss: 0.005270238965749741, accuracy: 1.0\n",
      "loss: 0.005300653167068958, accuracy: 1.0\n",
      "loss: 0.005241709295660257, accuracy: 1.0\n",
      "loss: 0.005348615348339081, accuracy: 1.0\n",
      "loss: 0.005310486536473036, accuracy: 1.0\n",
      "loss: 0.005291646346449852, accuracy: 1.0\n",
      "loss: 0.005153866019099951, accuracy: 1.0\n",
      "loss: 0.00554079981520772, accuracy: 1.0\n",
      "loss: 0.005281375255435705, accuracy: 1.0\n",
      "loss: 0.005450155586004257, accuracy: 1.0\n",
      "loss: 0.0051484489813447, accuracy: 1.0\n",
      "loss: 0.005403235089033842, accuracy: 1.0\n",
      "loss: 0.00529282633215189, accuracy: 1.0\n",
      "loss: 0.00535906245931983, accuracy: 1.0\n",
      "loss: 0.005259115248918533, accuracy: 1.0\n",
      "loss: 0.005344961304217577, accuracy: 1.0\n",
      "loss: 0.005452014040201902, accuracy: 1.0\n",
      "loss: 0.005212692078202963, accuracy: 1.0\n",
      "loss: 0.005127965938299894, accuracy: 1.0\n",
      "loss: 0.00527888024225831, accuracy: 1.0\n",
      "loss: 0.005238072015345097, accuracy: 1.0\n",
      "loss: 0.005166538059711456, accuracy: 1.0\n",
      "loss: 0.005381439812481403, accuracy: 1.0\n",
      "loss: 0.005319453310221434, accuracy: 1.0\n",
      "loss: 0.005328022874891758, accuracy: 1.0\n",
      "loss: 0.005345544312149286, accuracy: 1.0\n",
      "loss: 0.00539175933226943, accuracy: 1.0\n",
      "loss: 0.005269837565720081, accuracy: 1.0\n",
      "loss: 0.005186938680708408, accuracy: 1.0\n",
      "loss: 0.005421651527285576, accuracy: 1.0\n",
      "loss: 0.005386362783610821, accuracy: 1.0\n",
      "loss: 0.005185489077121019, accuracy: 1.0\n",
      "loss: 0.005305764265358448, accuracy: 1.0\n",
      "loss: 0.0052406140603125095, accuracy: 1.0\n",
      "loss: 0.0052589778788387775, accuracy: 1.0\n",
      "loss: 0.005410442128777504, accuracy: 1.0\n",
      "loss: 0.005238675978034735, accuracy: 1.0\n",
      "loss: 0.005201828666031361, accuracy: 1.0\n",
      "loss: 0.005307843908667564, accuracy: 1.0\n",
      "loss: 0.0053201718255877495, accuracy: 1.0\n",
      "loss: 0.005167891271412373, accuracy: 1.0\n",
      "loss: 0.005208027083426714, accuracy: 1.0\n",
      "loss: 0.00521432189270854, accuracy: 1.0\n",
      "loss: 0.005237239878624678, accuracy: 1.0\n",
      "loss: 0.005281710531562567, accuracy: 1.0\n",
      "loss: 0.005383279640227556, accuracy: 1.0\n",
      "loss: 0.005368542391806841, accuracy: 1.0\n",
      "loss: 0.005152377299964428, accuracy: 1.0\n",
      "loss: 0.0053596799261868, accuracy: 1.0\n",
      "loss: 0.005320443771779537, accuracy: 1.0\n",
      "loss: 0.005175608675926924, accuracy: 1.0\n",
      "loss: 0.005323987454175949, accuracy: 1.0\n",
      "loss: 0.005368647165596485, accuracy: 1.0\n",
      "loss: 0.005144285969436169, accuracy: 1.0\n",
      "loss: 0.005224785301834345, accuracy: 1.0\n",
      "loss: 0.005238229408860207, accuracy: 1.0\n",
      "loss: 0.005148101132363081, accuracy: 1.0\n",
      "loss: 0.00535176508128643, accuracy: 1.0\n",
      "loss: 0.005163817200809717, accuracy: 1.0\n",
      "loss: 0.00547084491699934, accuracy: 1.0\n",
      "loss: 0.005300605203956366, accuracy: 1.0\n",
      "loss: 0.005255649797618389, accuracy: 1.0\n",
      "loss: 0.005292896647006273, accuracy: 1.0\n",
      "loss: 0.005198656115680933, accuracy: 1.0\n",
      "loss: 0.005126131232827902, accuracy: 1.0\n",
      "loss: 0.0052632964216172695, accuracy: 1.0\n",
      "loss: 0.0050307041965425014, accuracy: 1.0\n",
      "loss: 0.005302837118506432, accuracy: 1.0\n",
      "loss: 0.005255741998553276, accuracy: 1.0\n",
      "loss: 0.005222087725996971, accuracy: 1.0\n",
      "loss: 0.005247422028332949, accuracy: 1.0\n",
      "loss: 0.005129841156303883, accuracy: 1.0\n",
      "loss: 0.005222000181674957, accuracy: 1.0\n",
      "loss: 0.005231388844549656, accuracy: 1.0\n",
      "loss: 0.005027749575674534, accuracy: 1.0\n",
      "loss: 0.005155717022716999, accuracy: 1.0\n",
      "loss: 0.005086581222712994, accuracy: 1.0\n",
      "loss: 0.0052060214802622795, accuracy: 1.0\n",
      "loss: 0.005090254824608564, accuracy: 1.0\n",
      "loss: 0.005138570908457041, accuracy: 1.0\n",
      "loss: 0.005234306212514639, accuracy: 1.0\n",
      "loss: 0.005336141679435968, accuracy: 1.0\n",
      "loss: 0.005215905141085386, accuracy: 1.0\n",
      "loss: 0.005180234555155039, accuracy: 1.0\n",
      "loss: 0.005141764879226685, accuracy: 1.0\n",
      "loss: 0.005135906860232353, accuracy: 1.0\n",
      "loss: 0.00511309877038002, accuracy: 1.0\n",
      "loss: 0.005149052478373051, accuracy: 1.0\n",
      "loss: 0.005060124676674604, accuracy: 1.0\n",
      "loss: 0.0050400905311107635, accuracy: 1.0\n",
      "loss: 0.0051605431362986565, accuracy: 1.0\n",
      "loss: 0.005137206520885229, accuracy: 1.0\n",
      "loss: 0.0050804223865270615, accuracy: 1.0\n",
      "loss: 0.005193993449211121, accuracy: 1.0\n",
      "loss: 0.005162476561963558, accuracy: 1.0\n",
      "loss: 0.0051615010015666485, accuracy: 1.0\n",
      "loss: 0.005328433122485876, accuracy: 1.0\n",
      "loss: 0.005087046418339014, accuracy: 1.0\n",
      "loss: 0.005203988403081894, accuracy: 1.0\n",
      "loss: 0.005091423634439707, accuracy: 1.0\n",
      "loss: 0.004986059386283159, accuracy: 1.0\n",
      "loss: 0.005157807841897011, accuracy: 1.0\n",
      "loss: 0.005116255953907967, accuracy: 1.0\n",
      "loss: 0.005113283172249794, accuracy: 1.0\n",
      "loss: 0.005160770379006863, accuracy: 1.0\n",
      "loss: 0.005187719129025936, accuracy: 1.0\n",
      "loss: 0.005093635991215706, accuracy: 1.0\n",
      "loss: 0.005037646275013685, accuracy: 1.0\n",
      "loss: 0.0051189386285841465, accuracy: 1.0\n",
      "loss: 0.005147173535078764, accuracy: 1.0\n",
      "loss: 0.005040057469159365, accuracy: 1.0\n",
      "loss: 0.00507281394675374, accuracy: 1.0\n",
      "loss: 0.005178662016987801, accuracy: 1.0\n",
      "loss: 0.005110223311930895, accuracy: 1.0\n",
      "loss: 0.005045524798333645, accuracy: 1.0\n",
      "loss: 0.005125242751091719, accuracy: 1.0\n",
      "loss: 0.005065775942057371, accuracy: 1.0\n",
      "loss: 0.005107847973704338, accuracy: 1.0\n",
      "loss: 0.00521099753677845, accuracy: 1.0\n",
      "loss: 0.005035415291786194, accuracy: 1.0\n",
      "loss: 0.0050044069066643715, accuracy: 1.0\n",
      "loss: 0.0050763823091983795, accuracy: 1.0\n",
      "loss: 0.005090781487524509, accuracy: 1.0\n",
      "loss: 0.005057374946773052, accuracy: 1.0\n",
      "loss: 0.0050935749895870686, accuracy: 1.0\n",
      "loss: 0.005015720147639513, accuracy: 1.0\n",
      "loss: 0.005052631720900536, accuracy: 1.0\n",
      "loss: 0.004997897893190384, accuracy: 1.0\n",
      "loss: 0.005108901299536228, accuracy: 1.0\n",
      "loss: 0.0050700693391263485, accuracy: 1.0\n",
      "loss: 0.005040580872446299, accuracy: 1.0\n",
      "loss: 0.005022826604545116, accuracy: 1.0\n",
      "loss: 0.005179805215448141, accuracy: 1.0\n",
      "loss: 0.005261709447950125, accuracy: 1.0\n",
      "loss: 0.00496438005939126, accuracy: 1.0\n",
      "loss: 0.004999087657779455, accuracy: 1.0\n",
      "loss: 0.005203896667808294, accuracy: 1.0\n",
      "loss: 0.005063510034233332, accuracy: 1.0\n",
      "loss: 0.005123102571815252, accuracy: 1.0\n",
      "loss: 0.005196115467697382, accuracy: 1.0\n",
      "loss: 0.005070733837783337, accuracy: 1.0\n",
      "loss: 0.005042322911322117, accuracy: 1.0\n",
      "loss: 0.0052569108083844185, accuracy: 1.0\n",
      "loss: 0.005014433059841394, accuracy: 1.0\n",
      "loss: 0.0050017619505524635, accuracy: 1.0\n",
      "loss: 0.00505828345194459, accuracy: 1.0\n",
      "loss: 0.005037336144596338, accuracy: 1.0\n",
      "loss: 0.0050535439513623714, accuracy: 1.0\n",
      "loss: 0.005220076069235802, accuracy: 1.0\n",
      "loss: 0.005048059858381748, accuracy: 1.0\n",
      "loss: 0.005014820955693722, accuracy: 1.0\n",
      "loss: 0.005142366513609886, accuracy: 1.0\n",
      "loss: 0.005034041125327349, accuracy: 1.0\n",
      "loss: 0.0049533536657691, accuracy: 1.0\n",
      "loss: 0.005054840352386236, accuracy: 1.0\n",
      "loss: 0.004830443300306797, accuracy: 1.0\n",
      "loss: 0.005023550242185593, accuracy: 1.0\n",
      "loss: 0.005022288300096989, accuracy: 1.0\n",
      "loss: 0.004973447881639004, accuracy: 1.0\n",
      "loss: 0.00494161294773221, accuracy: 1.0\n",
      "loss: 0.005049923434853554, accuracy: 1.0\n",
      "loss: 0.0050191693007946014, accuracy: 1.0\n",
      "loss: 0.005015688017010689, accuracy: 1.0\n",
      "loss: 0.005043432582169771, accuracy: 1.0\n",
      "loss: 0.005027457606047392, accuracy: 1.0\n",
      "loss: 0.005065159872174263, accuracy: 1.0\n",
      "loss: 0.005017601419240236, accuracy: 1.0\n",
      "loss: 0.004980640951544046, accuracy: 1.0\n",
      "loss: 0.004889616277068853, accuracy: 1.0\n",
      "loss: 0.005020489916205406, accuracy: 1.0\n",
      "loss: 0.004891067743301392, accuracy: 1.0\n",
      "loss: 0.005142063368111849, accuracy: 1.0\n",
      "loss: 0.0049934000708162785, accuracy: 1.0\n",
      "loss: 0.004997164476662874, accuracy: 1.0\n",
      "loss: 0.004978687968105078, accuracy: 1.0\n",
      "loss: 0.005041241180151701, accuracy: 1.0\n",
      "loss: 0.005029527470469475, accuracy: 1.0\n",
      "loss: 0.005010833032429218, accuracy: 1.0\n",
      "loss: 0.005080508999526501, accuracy: 1.0\n",
      "loss: 0.00497190747410059, accuracy: 1.0\n",
      "loss: 0.0048586647026240826, accuracy: 1.0\n",
      "loss: 0.004999525845050812, accuracy: 1.0\n",
      "loss: 0.0049898745492100716, accuracy: 1.0\n",
      "loss: 0.004838321357965469, accuracy: 1.0\n",
      "loss: 0.005060610361397266, accuracy: 1.0\n",
      "loss: 0.004928561858832836, accuracy: 1.0\n",
      "loss: 0.004967546556144953, accuracy: 1.0\n",
      "loss: 0.005199725739657879, accuracy: 1.0\n",
      "loss: 0.005068175494670868, accuracy: 1.0\n",
      "loss: 0.004967160057276487, accuracy: 1.0\n",
      "loss: 0.005086932331323624, accuracy: 1.0\n",
      "loss: 0.004933710675686598, accuracy: 1.0\n",
      "loss: 0.004911966156214476, accuracy: 1.0\n",
      "loss: 0.0051007685251533985, accuracy: 1.0\n",
      "loss: 0.005014246795326471, accuracy: 1.0\n",
      "loss: 0.004947238601744175, accuracy: 1.0\n",
      "loss: 0.004843736533075571, accuracy: 1.0\n",
      "loss: 0.005018047522753477, accuracy: 1.0\n",
      "loss: 0.0048662954941391945, accuracy: 1.0\n",
      "loss: 0.005053934175521135, accuracy: 1.0\n",
      "loss: 0.005009170155972242, accuracy: 1.0\n",
      "loss: 0.0050098951905965805, accuracy: 1.0\n",
      "loss: 0.004977161530405283, accuracy: 1.0\n",
      "loss: 0.005060202907770872, accuracy: 1.0\n",
      "loss: 0.004904412664473057, accuracy: 1.0\n",
      "loss: 0.00502392603084445, accuracy: 1.0\n",
      "loss: 0.004853862337768078, accuracy: 1.0\n",
      "loss: 0.0048635839484632015, accuracy: 1.0\n",
      "loss: 0.0049345241859555244, accuracy: 1.0\n",
      "loss: 0.0050399210304021835, accuracy: 1.0\n",
      "loss: 0.00492155272513628, accuracy: 1.0\n",
      "loss: 0.004946027882397175, accuracy: 1.0\n",
      "loss: 0.0048963832668960094, accuracy: 1.0\n",
      "loss: 0.004787264857441187, accuracy: 1.0\n",
      "loss: 0.0049895052798092365, accuracy: 1.0\n",
      "loss: 0.005147806368768215, accuracy: 1.0\n",
      "loss: 0.0048987907357513905, accuracy: 1.0\n",
      "loss: 0.005026713013648987, accuracy: 1.0\n",
      "loss: 0.004891130141913891, accuracy: 1.0\n",
      "loss: 0.004893486853688955, accuracy: 1.0\n",
      "loss: 0.004810603801161051, accuracy: 1.0\n",
      "loss: 0.004936045967042446, accuracy: 1.0\n",
      "loss: 0.004862227942794561, accuracy: 1.0\n",
      "loss: 0.004902259446680546, accuracy: 1.0\n",
      "loss: 0.004873066674917936, accuracy: 1.0\n",
      "loss: 0.004962330684065819, accuracy: 1.0\n",
      "loss: 0.004884433001279831, accuracy: 1.0\n",
      "loss: 0.004815608728677034, accuracy: 1.0\n",
      "loss: 0.004880561493337154, accuracy: 1.0\n",
      "loss: 0.004902363289147615, accuracy: 1.0\n",
      "loss: 0.004846550524234772, accuracy: 1.0\n",
      "loss: 0.004999101627618074, accuracy: 1.0\n",
      "loss: 0.004897156730294228, accuracy: 1.0\n",
      "loss: 0.004943764768540859, accuracy: 1.0\n",
      "loss: 0.004910971503704786, accuracy: 1.0\n",
      "loss: 0.004908611532300711, accuracy: 1.0\n",
      "loss: 0.004844797775149345, accuracy: 1.0\n",
      "loss: 0.004863847512751818, accuracy: 1.0\n",
      "loss: 0.004829408600926399, accuracy: 1.0\n",
      "loss: 0.004867130424827337, accuracy: 1.0\n",
      "loss: 0.00476563535630703, accuracy: 1.0\n",
      "loss: 0.00497015193104744, accuracy: 1.0\n",
      "loss: 0.004944308195263147, accuracy: 1.0\n",
      "loss: 0.004802382085472345, accuracy: 1.0\n",
      "loss: 0.004994117189198732, accuracy: 1.0\n",
      "loss: 0.004879871848970652, accuracy: 1.0\n",
      "loss: 0.004896520636975765, accuracy: 1.0\n",
      "loss: 0.004807728808373213, accuracy: 1.0\n",
      "loss: 0.004884013440459967, accuracy: 1.0\n",
      "loss: 0.0047922562807798386, accuracy: 1.0\n",
      "loss: 0.005070078186690807, accuracy: 1.0\n",
      "loss: 0.004826060496270657, accuracy: 1.0\n",
      "loss: 0.004807096440345049, accuracy: 1.0\n",
      "loss: 0.004905576817691326, accuracy: 1.0\n",
      "loss: 0.004795954562723637, accuracy: 1.0\n",
      "loss: 0.004872811026871204, accuracy: 1.0\n",
      "loss: 0.004806320182979107, accuracy: 1.0\n",
      "loss: 0.004886347334831953, accuracy: 1.0\n",
      "loss: 0.004723120015114546, accuracy: 1.0\n",
      "loss: 0.004789021797478199, accuracy: 1.0\n",
      "loss: 0.004957240540534258, accuracy: 1.0\n",
      "loss: 0.004814655985683203, accuracy: 1.0\n",
      "loss: 0.0047551607713103294, accuracy: 1.0\n",
      "loss: 0.004955627024173737, accuracy: 1.0\n",
      "loss: 0.0047404030337929726, accuracy: 1.0\n",
      "loss: 0.004743584431707859, accuracy: 1.0\n",
      "loss: 0.004967136774212122, accuracy: 1.0\n",
      "loss: 0.004862779285758734, accuracy: 1.0\n",
      "loss: 0.005098178517073393, accuracy: 1.0\n",
      "loss: 0.004879245534539223, accuracy: 1.0\n",
      "loss: 0.0048075346276164055, accuracy: 1.0\n",
      "loss: 0.0048255715519189835, accuracy: 1.0\n",
      "loss: 0.0048498790711164474, accuracy: 1.0\n",
      "loss: 0.004863078705966473, accuracy: 1.0\n",
      "loss: 0.00480135390534997, accuracy: 1.0\n",
      "loss: 0.004861473571509123, accuracy: 1.0\n",
      "loss: 0.004833525978028774, accuracy: 1.0\n",
      "loss: 0.0048459735698997974, accuracy: 1.0\n",
      "loss: 0.0048836637288331985, accuracy: 1.0\n",
      "loss: 0.004899783991277218, accuracy: 1.0\n",
      "loss: 0.004838091321289539, accuracy: 1.0\n",
      "loss: 0.0048459372483193874, accuracy: 1.0\n",
      "loss: 0.004792643245309591, accuracy: 1.0\n",
      "loss: 0.004737955518066883, accuracy: 1.0\n",
      "loss: 0.004758250899612904, accuracy: 1.0\n",
      "loss: 0.004765217658132315, accuracy: 1.0\n",
      "loss: 0.004726544953882694, accuracy: 1.0\n",
      "loss: 0.004808744415640831, accuracy: 1.0\n",
      "loss: 0.005020591896027327, accuracy: 1.0\n",
      "loss: 0.004738070536404848, accuracy: 1.0\n",
      "loss: 0.00477601820603013, accuracy: 1.0\n",
      "loss: 0.004818861372768879, accuracy: 1.0\n",
      "loss: 0.004830611869692802, accuracy: 1.0\n",
      "loss: 0.004873443860560656, accuracy: 1.0\n",
      "loss: 0.004818575456738472, accuracy: 1.0\n",
      "loss: 0.004811467137187719, accuracy: 1.0\n",
      "loss: 0.004722987301647663, accuracy: 1.0\n",
      "loss: 0.004774011671543121, accuracy: 1.0\n",
      "loss: 0.004698577336966991, accuracy: 1.0\n",
      "loss: 0.004799103830009699, accuracy: 1.0\n",
      "loss: 0.004775281995534897, accuracy: 1.0\n",
      "loss: 0.0047228834591805935, accuracy: 1.0\n",
      "loss: 0.004682903178036213, accuracy: 1.0\n",
      "loss: 0.004866363015025854, accuracy: 1.0\n",
      "loss: 0.004833768587559462, accuracy: 1.0\n",
      "loss: 0.004763982258737087, accuracy: 1.0\n",
      "loss: 0.004721681121736765, accuracy: 1.0\n",
      "loss: 0.004866074305027723, accuracy: 1.0\n",
      "loss: 0.004786624573171139, accuracy: 1.0\n",
      "loss: 0.004689034540206194, accuracy: 1.0\n",
      "loss: 0.004743883851915598, accuracy: 1.0\n",
      "loss: 0.004789511673152447, accuracy: 1.0\n",
      "loss: 0.004807723220437765, accuracy: 1.0\n",
      "loss: 0.004712293855845928, accuracy: 1.0\n",
      "loss: 0.004714700393378735, accuracy: 1.0\n",
      "loss: 0.004946963395923376, accuracy: 1.0\n",
      "loss: 0.004768994636833668, accuracy: 1.0\n",
      "loss: 0.0047627040185034275, accuracy: 1.0\n",
      "loss: 0.004678434692323208, accuracy: 1.0\n",
      "loss: 0.004641321487724781, accuracy: 1.0\n",
      "loss: 0.004911573603749275, accuracy: 1.0\n",
      "loss: 0.004721514414995909, accuracy: 1.0\n",
      "loss: 0.004800139460712671, accuracy: 1.0\n",
      "loss: 0.004736121743917465, accuracy: 1.0\n",
      "loss: 0.004764439072459936, accuracy: 1.0\n",
      "loss: 0.004648079629987478, accuracy: 1.0\n",
      "loss: 0.004639010410755873, accuracy: 1.0\n",
      "loss: 0.004759276285767555, accuracy: 1.0\n",
      "loss: 0.004794402979314327, accuracy: 1.0\n",
      "loss: 0.004786137491464615, accuracy: 1.0\n",
      "loss: 0.004788134712725878, accuracy: 1.0\n",
      "loss: 0.004801375325769186, accuracy: 1.0\n",
      "loss: 0.00466568348929286, accuracy: 1.0\n",
      "loss: 0.004674423951655626, accuracy: 1.0\n",
      "loss: 0.004733797162771225, accuracy: 1.0\n",
      "loss: 0.004698917735368013, accuracy: 1.0\n",
      "loss: 0.004707813262939453, accuracy: 1.0\n",
      "loss: 0.004620508756488562, accuracy: 1.0\n",
      "loss: 0.004678295459598303, accuracy: 1.0\n",
      "loss: 0.004729819484055042, accuracy: 1.0\n",
      "loss: 0.004754251800477505, accuracy: 1.0\n",
      "loss: 0.004710037726908922, accuracy: 1.0\n",
      "loss: 0.004603272303938866, accuracy: 1.0\n",
      "loss: 0.004710030741989613, accuracy: 1.0\n",
      "loss: 0.004722143989056349, accuracy: 1.0\n",
      "loss: 0.00468828622251749, accuracy: 1.0\n",
      "loss: 0.004658971913158894, accuracy: 1.0\n",
      "loss: 0.0046561346389353275, accuracy: 1.0\n",
      "loss: 0.004738811869174242, accuracy: 1.0\n",
      "loss: 0.004713498987257481, accuracy: 1.0\n",
      "loss: 0.004602995701134205, accuracy: 1.0\n",
      "loss: 0.004669596441090107, accuracy: 1.0\n",
      "loss: 0.004685009364038706, accuracy: 1.0\n",
      "loss: 0.004714112263172865, accuracy: 1.0\n",
      "loss: 0.004596611950546503, accuracy: 1.0\n",
      "loss: 0.004708518739789724, accuracy: 1.0\n",
      "loss: 0.004756518639624119, accuracy: 1.0\n",
      "loss: 0.004651017487049103, accuracy: 1.0\n",
      "loss: 0.0046784235164523125, accuracy: 1.0\n",
      "loss: 0.004708712454885244, accuracy: 1.0\n",
      "loss: 0.0046612173318862915, accuracy: 1.0\n",
      "loss: 0.004646757151931524, accuracy: 1.0\n",
      "loss: 0.004683222156018019, accuracy: 1.0\n",
      "loss: 0.004790073726326227, accuracy: 1.0\n",
      "loss: 0.004600386135280132, accuracy: 1.0\n",
      "loss: 0.004666431341320276, accuracy: 1.0\n",
      "loss: 0.004679378587752581, accuracy: 1.0\n",
      "loss: 0.004675549920648336, accuracy: 1.0\n",
      "loss: 0.0046852086670696735, accuracy: 1.0\n",
      "loss: 0.004648544825613499, accuracy: 1.0\n",
      "loss: 0.0047586048021912575, accuracy: 1.0\n",
      "loss: 0.00476108817383647, accuracy: 1.0\n",
      "loss: 0.0046645840629935265, accuracy: 1.0\n",
      "loss: 0.004604343790560961, accuracy: 1.0\n",
      "loss: 0.004627941641956568, accuracy: 1.0\n",
      "loss: 0.004599873907864094, accuracy: 1.0\n",
      "loss: 0.004646491724997759, accuracy: 1.0\n",
      "loss: 0.0045997933484613895, accuracy: 1.0\n",
      "loss: 0.004729664418846369, accuracy: 1.0\n",
      "loss: 0.004629761911928654, accuracy: 1.0\n",
      "loss: 0.004592352081090212, accuracy: 1.0\n",
      "loss: 0.004624632187187672, accuracy: 1.0\n",
      "loss: 0.004767772741615772, accuracy: 1.0\n",
      "loss: 0.004613813478499651, accuracy: 1.0\n",
      "loss: 0.0046197171323001385, accuracy: 1.0\n",
      "loss: 0.0046728248707950115, accuracy: 1.0\n",
      "loss: 0.004681284073740244, accuracy: 1.0\n",
      "loss: 0.004634151700884104, accuracy: 1.0\n",
      "loss: 0.00463008601218462, accuracy: 1.0\n",
      "loss: 0.004646736662834883, accuracy: 1.0\n",
      "loss: 0.004595995415002108, accuracy: 1.0\n",
      "loss: 0.004697358701378107, accuracy: 1.0\n",
      "loss: 0.004581269808113575, accuracy: 1.0\n",
      "loss: 0.004706469364464283, accuracy: 1.0\n",
      "loss: 0.00476420484483242, accuracy: 1.0\n",
      "loss: 0.0045320759527385235, accuracy: 1.0\n",
      "loss: 0.004759699571877718, accuracy: 1.0\n",
      "loss: 0.0047136954963207245, accuracy: 1.0\n",
      "loss: 0.004599462728947401, accuracy: 1.0\n",
      "loss: 0.004569720011204481, accuracy: 1.0\n",
      "loss: 0.004647715017199516, accuracy: 1.0\n",
      "loss: 0.004497638437896967, accuracy: 1.0\n",
      "loss: 0.004599983338266611, accuracy: 1.0\n",
      "loss: 0.0046172672882676125, accuracy: 1.0\n",
      "loss: 0.004623939283192158, accuracy: 1.0\n",
      "loss: 0.004486570134758949, accuracy: 1.0\n",
      "loss: 0.004626245703548193, accuracy: 1.0\n",
      "loss: 0.004569266922771931, accuracy: 1.0\n",
      "loss: 0.004593558609485626, accuracy: 1.0\n",
      "loss: 0.004633130040019751, accuracy: 1.0\n",
      "loss: 0.004534946754574776, accuracy: 1.0\n",
      "loss: 0.0045175193808972836, accuracy: 1.0\n",
      "loss: 0.0046238973736763, accuracy: 1.0\n",
      "loss: 0.004670183639973402, accuracy: 1.0\n",
      "loss: 0.004552058409899473, accuracy: 1.0\n",
      "loss: 0.004634031094610691, accuracy: 1.0\n",
      "loss: 0.004603223875164986, accuracy: 1.0\n",
      "loss: 0.004564336501061916, accuracy: 1.0\n",
      "loss: 0.004599618725478649, accuracy: 1.0\n",
      "loss: 0.004562427755445242, accuracy: 1.0\n",
      "loss: 0.004588449373841286, accuracy: 1.0\n",
      "loss: 0.004662028048187494, accuracy: 1.0\n",
      "loss: 0.004930555354803801, accuracy: 1.0\n",
      "loss: 0.004637519363313913, accuracy: 1.0\n",
      "loss: 0.004551801830530167, accuracy: 1.0\n",
      "loss: 0.00456658098846674, accuracy: 1.0\n",
      "loss: 0.004553553648293018, accuracy: 1.0\n",
      "loss: 0.004705388564616442, accuracy: 1.0\n",
      "loss: 0.0045957015827298164, accuracy: 1.0\n",
      "loss: 0.004647056106477976, accuracy: 1.0\n",
      "loss: 0.004563530907034874, accuracy: 1.0\n",
      "loss: 0.0045808544382452965, accuracy: 1.0\n",
      "loss: 0.004608100280165672, accuracy: 1.0\n",
      "loss: 0.004652174189686775, accuracy: 1.0\n",
      "loss: 0.00448168208822608, accuracy: 1.0\n",
      "loss: 0.004602678585797548, accuracy: 1.0\n",
      "loss: 0.004580711480230093, accuracy: 1.0\n",
      "loss: 0.004469282925128937, accuracy: 1.0\n",
      "loss: 0.004657690413296223, accuracy: 1.0\n",
      "loss: 0.0046156602911651134, accuracy: 1.0\n",
      "loss: 0.004577930085361004, accuracy: 1.0\n",
      "loss: 0.004568828735500574, accuracy: 1.0\n",
      "loss: 0.004493421874940395, accuracy: 1.0\n",
      "loss: 0.004550183657556772, accuracy: 1.0\n",
      "loss: 0.004598593804985285, accuracy: 1.0\n",
      "loss: 0.0045748804695904255, accuracy: 1.0\n",
      "loss: 0.0045334408059716225, accuracy: 1.0\n",
      "loss: 0.004590872675180435, accuracy: 1.0\n",
      "loss: 0.004535660147666931, accuracy: 1.0\n",
      "loss: 0.004606397356837988, accuracy: 1.0\n",
      "loss: 0.00449036993086338, accuracy: 1.0\n",
      "loss: 0.004543601535260677, accuracy: 1.0\n",
      "loss: 0.004569363314658403, accuracy: 1.0\n",
      "loss: 0.004553189035505056, accuracy: 1.0\n",
      "loss: 0.004469295032322407, accuracy: 1.0\n",
      "loss: 0.004512738902121782, accuracy: 1.0\n",
      "loss: 0.004579978063702583, accuracy: 1.0\n",
      "loss: 0.004512353800237179, accuracy: 1.0\n",
      "loss: 0.0044831992127001286, accuracy: 1.0\n",
      "loss: 0.004593330901116133, accuracy: 1.0\n",
      "loss: 0.004457512870430946, accuracy: 1.0\n",
      "loss: 0.004694224335253239, accuracy: 1.0\n",
      "loss: 0.004502114839851856, accuracy: 1.0\n",
      "loss: 0.004431621637195349, accuracy: 1.0\n",
      "loss: 0.004471136722713709, accuracy: 1.0\n",
      "loss: 0.004480365198105574, accuracy: 1.0\n",
      "loss: 0.004478953313082457, accuracy: 1.0\n",
      "loss: 0.004645054694265127, accuracy: 1.0\n",
      "loss: 0.004458904266357422, accuracy: 1.0\n",
      "loss: 0.004682343918830156, accuracy: 1.0\n",
      "loss: 0.004482687916606665, accuracy: 1.0\n",
      "loss: 0.004555272404104471, accuracy: 1.0\n",
      "loss: 0.004464883357286453, accuracy: 1.0\n",
      "loss: 0.00446662213653326, accuracy: 1.0\n",
      "loss: 0.004543396644294262, accuracy: 1.0\n",
      "loss: 0.004440913442522287, accuracy: 1.0\n",
      "loss: 0.0044882046058773994, accuracy: 1.0\n",
      "loss: 0.004539485089480877, accuracy: 1.0\n",
      "loss: 0.004479818046092987, accuracy: 1.0\n",
      "loss: 0.004489946644753218, accuracy: 1.0\n",
      "loss: 0.004515337757766247, accuracy: 1.0\n",
      "loss: 0.004484281875193119, accuracy: 1.0\n",
      "loss: 0.00446239672601223, accuracy: 1.0\n",
      "loss: 0.0045649223029613495, accuracy: 1.0\n",
      "loss: 0.004416665993630886, accuracy: 1.0\n",
      "loss: 0.004505714867264032, accuracy: 1.0\n",
      "loss: 0.004425420425832272, accuracy: 1.0\n",
      "loss: 0.004435278475284576, accuracy: 1.0\n",
      "loss: 0.004524830728769302, accuracy: 1.0\n",
      "loss: 0.004521630704402924, accuracy: 1.0\n",
      "loss: 0.004462088458240032, accuracy: 1.0\n",
      "loss: 0.004530283156782389, accuracy: 1.0\n",
      "loss: 0.004424427170306444, accuracy: 1.0\n",
      "loss: 0.0045603918842971325, accuracy: 1.0\n",
      "loss: 0.004408623557537794, accuracy: 1.0\n",
      "loss: 0.00445000734180212, accuracy: 1.0\n",
      "loss: 0.004481911659240723, accuracy: 1.0\n",
      "loss: 0.004567696247249842, accuracy: 1.0\n",
      "loss: 0.004499282222241163, accuracy: 1.0\n",
      "loss: 0.004426715895533562, accuracy: 1.0\n",
      "loss: 0.004485100042074919, accuracy: 1.0\n",
      "loss: 0.0045094178058207035, accuracy: 1.0\n",
      "loss: 0.0044153365306556225, accuracy: 1.0\n",
      "loss: 0.0044746301136910915, accuracy: 1.0\n",
      "loss: 0.004467668477445841, accuracy: 1.0\n",
      "loss: 0.004466323647648096, accuracy: 1.0\n",
      "loss: 0.004595080390572548, accuracy: 1.0\n",
      "loss: 0.0044087739661335945, accuracy: 1.0\n",
      "loss: 0.004518845118582249, accuracy: 1.0\n",
      "loss: 0.004440326243638992, accuracy: 1.0\n",
      "loss: 0.004371321760118008, accuracy: 1.0\n",
      "loss: 0.004382286686450243, accuracy: 1.0\n",
      "loss: 0.004405944142490625, accuracy: 1.0\n",
      "loss: 0.004432471934705973, accuracy: 1.0\n",
      "loss: 0.00442082853987813, accuracy: 1.0\n",
      "loss: 0.0045462301932275295, accuracy: 1.0\n",
      "loss: 0.004421773366630077, accuracy: 1.0\n",
      "loss: 0.004390205256640911, accuracy: 1.0\n",
      "loss: 0.0045151435770094395, accuracy: 1.0\n",
      "loss: 0.004518226254731417, accuracy: 1.0\n",
      "loss: 0.004520142916589975, accuracy: 1.0\n",
      "loss: 0.0044343313202261925, accuracy: 1.0\n",
      "loss: 0.004374094773083925, accuracy: 1.0\n",
      "loss: 0.00438825786113739, accuracy: 1.0\n",
      "loss: 0.004421210382133722, accuracy: 1.0\n",
      "loss: 0.004303649999201298, accuracy: 1.0\n",
      "loss: 0.004481711890548468, accuracy: 1.0\n",
      "loss: 0.004460166674107313, accuracy: 1.0\n",
      "loss: 0.004441764205694199, accuracy: 1.0\n",
      "loss: 0.004445139318704605, accuracy: 1.0\n",
      "loss: 0.00433808658272028, accuracy: 1.0\n",
      "loss: 0.00439609307795763, accuracy: 1.0\n",
      "loss: 0.00439872732385993, accuracy: 1.0\n",
      "loss: 0.00440274877473712, accuracy: 1.0\n",
      "loss: 0.004463593941181898, accuracy: 1.0\n",
      "loss: 0.0043930113315582275, accuracy: 1.0\n",
      "loss: 0.004389022011309862, accuracy: 1.0\n",
      "loss: 0.004457454662770033, accuracy: 1.0\n",
      "loss: 0.004412687383592129, accuracy: 1.0\n",
      "loss: 0.004431113600730896, accuracy: 1.0\n",
      "loss: 0.004444581922143698, accuracy: 1.0\n",
      "loss: 0.0044110617600381374, accuracy: 1.0\n",
      "loss: 0.004382745828479528, accuracy: 1.0\n",
      "loss: 0.004338270518928766, accuracy: 1.0\n",
      "loss: 0.004398410674184561, accuracy: 1.0\n",
      "loss: 0.00442331051453948, accuracy: 1.0\n",
      "loss: 0.0044692265801131725, accuracy: 1.0\n",
      "loss: 0.004353985656052828, accuracy: 1.0\n",
      "loss: 0.004406256601214409, accuracy: 1.0\n",
      "loss: 0.004379529505968094, accuracy: 1.0\n",
      "loss: 0.004371456801891327, accuracy: 1.0\n",
      "loss: 0.004334408789873123, accuracy: 1.0\n",
      "loss: 0.0043808394111692905, accuracy: 1.0\n",
      "loss: 0.0043420447036623955, accuracy: 1.0\n",
      "loss: 0.004430048167705536, accuracy: 1.0\n",
      "loss: 0.004377420991659164, accuracy: 1.0\n",
      "loss: 0.00436029490083456, accuracy: 1.0\n",
      "loss: 0.004380953963845968, accuracy: 1.0\n",
      "loss: 0.004364560358226299, accuracy: 1.0\n",
      "loss: 0.004299673717468977, accuracy: 1.0\n",
      "loss: 0.004541582893580198, accuracy: 1.0\n",
      "loss: 0.004327821545302868, accuracy: 1.0\n",
      "loss: 0.004343288019299507, accuracy: 1.0\n",
      "loss: 0.004361772444099188, accuracy: 1.0\n",
      "loss: 0.004331088159233332, accuracy: 1.0\n",
      "loss: 0.004329496994614601, accuracy: 1.0\n",
      "loss: 0.00433053495362401, accuracy: 1.0\n",
      "loss: 0.0044021084904670715, accuracy: 1.0\n",
      "loss: 0.004316957201808691, accuracy: 1.0\n",
      "loss: 0.004336734302341938, accuracy: 1.0\n",
      "loss: 0.004350301809608936, accuracy: 1.0\n",
      "loss: 0.0043186163529753685, accuracy: 1.0\n",
      "loss: 0.004387937486171722, accuracy: 1.0\n",
      "loss: 0.004338910337537527, accuracy: 1.0\n",
      "loss: 0.004446852020919323, accuracy: 1.0\n",
      "loss: 0.004328351002186537, accuracy: 1.0\n",
      "loss: 0.004370758309960365, accuracy: 1.0\n",
      "loss: 0.0043075429275631905, accuracy: 1.0\n",
      "loss: 0.004337311256676912, accuracy: 1.0\n",
      "loss: 0.004398449324071407, accuracy: 1.0\n",
      "loss: 0.004339463077485561, accuracy: 1.0\n",
      "loss: 0.004401072859764099, accuracy: 1.0\n",
      "loss: 0.004316938109695911, accuracy: 1.0\n",
      "loss: 0.0043649859726428986, accuracy: 1.0\n",
      "loss: 0.004405250772833824, accuracy: 1.0\n",
      "loss: 0.004395205061882734, accuracy: 1.0\n",
      "loss: 0.004293367266654968, accuracy: 1.0\n",
      "loss: 0.004358626436442137, accuracy: 1.0\n",
      "loss: 0.004334185738116503, accuracy: 1.0\n",
      "loss: 0.004374830052256584, accuracy: 1.0\n",
      "loss: 0.004280767869204283, accuracy: 1.0\n",
      "loss: 0.004341246094554663, accuracy: 1.0\n",
      "loss: 0.004342713393270969, accuracy: 1.0\n",
      "loss: 0.004322603810578585, accuracy: 1.0\n",
      "loss: 0.004407353699207306, accuracy: 1.0\n",
      "loss: 0.004286454524844885, accuracy: 1.0\n",
      "loss: 0.004303167574107647, accuracy: 1.0\n",
      "loss: 0.004411171190440655, accuracy: 1.0\n",
      "loss: 0.0043128132820129395, accuracy: 1.0\n",
      "loss: 0.004253496415913105, accuracy: 1.0\n",
      "loss: 0.0043359678238630295, accuracy: 1.0\n",
      "loss: 0.004314723890274763, accuracy: 1.0\n",
      "loss: 0.004305221140384674, accuracy: 1.0\n",
      "loss: 0.004336773883551359, accuracy: 1.0\n",
      "loss: 0.004330752417445183, accuracy: 1.0\n",
      "loss: 0.00438646599650383, accuracy: 1.0\n",
      "loss: 0.004356321878731251, accuracy: 1.0\n",
      "loss: 0.004342594183981419, accuracy: 1.0\n",
      "loss: 0.004378145094960928, accuracy: 1.0\n",
      "loss: 0.004395585041493177, accuracy: 1.0\n",
      "loss: 0.0042916857637465, accuracy: 1.0\n",
      "loss: 0.004246581345796585, accuracy: 1.0\n",
      "loss: 0.004413374699652195, accuracy: 1.0\n",
      "loss: 0.004286999348551035, accuracy: 1.0\n",
      "loss: 0.004294565878808498, accuracy: 1.0\n",
      "loss: 0.0044278353452682495, accuracy: 1.0\n",
      "loss: 0.004313134588301182, accuracy: 1.0\n",
      "loss: 0.004326203837990761, accuracy: 1.0\n",
      "loss: 0.004326288588345051, accuracy: 1.0\n",
      "loss: 0.004286816343665123, accuracy: 1.0\n",
      "loss: 0.004255037754774094, accuracy: 1.0\n",
      "loss: 0.004257575608789921, accuracy: 1.0\n",
      "loss: 0.004246830940246582, accuracy: 1.0\n",
      "loss: 0.004265474621206522, accuracy: 1.0\n",
      "loss: 0.0042981719598174095, accuracy: 1.0\n",
      "loss: 0.004323743749409914, accuracy: 1.0\n",
      "loss: 0.004265454597771168, accuracy: 1.0\n",
      "loss: 0.004333617631345987, accuracy: 1.0\n",
      "loss: 0.0042206658981740475, accuracy: 1.0\n",
      "loss: 0.004254956729710102, accuracy: 1.0\n",
      "loss: 0.004291136749088764, accuracy: 1.0\n",
      "loss: 0.0042726993560791016, accuracy: 1.0\n",
      "loss: 0.004184470046311617, accuracy: 1.0\n",
      "loss: 0.004255722742527723, accuracy: 1.0\n",
      "loss: 0.004328677896410227, accuracy: 1.0\n",
      "loss: 0.004251499194651842, accuracy: 1.0\n",
      "loss: 0.004270707722753286, accuracy: 1.0\n",
      "loss: 0.004269605036824942, accuracy: 1.0\n",
      "loss: 0.0042731682769954205, accuracy: 1.0\n",
      "loss: 0.004302754998207092, accuracy: 1.0\n",
      "loss: 0.004307642579078674, accuracy: 1.0\n",
      "loss: 0.004287309944629669, accuracy: 1.0\n",
      "loss: 0.0041813901625573635, accuracy: 1.0\n",
      "loss: 0.004476107656955719, accuracy: 1.0\n",
      "loss: 0.0042554764077067375, accuracy: 1.0\n",
      "loss: 0.00428441958501935, accuracy: 1.0\n",
      "loss: 0.004324970301240683, accuracy: 1.0\n",
      "loss: 0.0042853220365941525, accuracy: 1.0\n",
      "loss: 0.004313856828957796, accuracy: 1.0\n",
      "loss: 0.004239564761519432, accuracy: 1.0\n",
      "loss: 0.004339555278420448, accuracy: 1.0\n",
      "loss: 0.004240298178046942, accuracy: 1.0\n",
      "loss: 0.004253079649060965, accuracy: 1.0\n",
      "loss: 0.004193837754428387, accuracy: 1.0\n",
      "loss: 0.0042358273640275, accuracy: 1.0\n",
      "loss: 0.004257327411323786, accuracy: 1.0\n",
      "loss: 0.004231083206832409, accuracy: 1.0\n",
      "loss: 0.004197972360998392, accuracy: 1.0\n",
      "loss: 0.004279088694602251, accuracy: 1.0\n",
      "loss: 0.004215695895254612, accuracy: 1.0\n",
      "loss: 0.004219229333102703, accuracy: 1.0\n",
      "loss: 0.004147075582295656, accuracy: 1.0\n",
      "loss: 0.004204697906970978, accuracy: 1.0\n",
      "loss: 0.004287392366677523, accuracy: 1.0\n",
      "loss: 0.004165135324001312, accuracy: 1.0\n",
      "loss: 0.004200865514576435, accuracy: 1.0\n",
      "loss: 0.004182812757790089, accuracy: 1.0\n",
      "loss: 0.004311046097427607, accuracy: 1.0\n",
      "loss: 0.004257869906723499, accuracy: 1.0\n",
      "loss: 0.00421080831438303, accuracy: 1.0\n",
      "loss: 0.0041352626867592335, accuracy: 1.0\n",
      "loss: 0.004208724480122328, accuracy: 1.0\n",
      "loss: 0.004298205021768808, accuracy: 1.0\n",
      "loss: 0.0042366040870547295, accuracy: 1.0\n",
      "loss: 0.004230322781950235, accuracy: 1.0\n",
      "loss: 0.004249264486134052, accuracy: 1.0\n",
      "loss: 0.004221822135150433, accuracy: 1.0\n",
      "loss: 0.004207874182611704, accuracy: 1.0\n",
      "loss: 0.004235272761434317, accuracy: 1.0\n",
      "loss: 0.004177308641374111, accuracy: 1.0\n",
      "loss: 0.004214712418615818, accuracy: 1.0\n",
      "loss: 0.004148885607719421, accuracy: 1.0\n",
      "loss: 0.004133068956434727, accuracy: 1.0\n",
      "loss: 0.004324316047132015, accuracy: 1.0\n",
      "loss: 0.004173757508397102, accuracy: 1.0\n",
      "loss: 0.00417492026463151, accuracy: 1.0\n",
      "loss: 0.004196430090814829, accuracy: 1.0\n",
      "loss: 0.004305221606045961, accuracy: 1.0\n",
      "loss: 0.004177574068307877, accuracy: 1.0\n",
      "loss: 0.004267287440598011, accuracy: 1.0\n",
      "loss: 0.004191107116639614, accuracy: 1.0\n",
      "loss: 0.0042033507488667965, accuracy: 1.0\n",
      "loss: 0.004184290301054716, accuracy: 1.0\n",
      "loss: 0.004204153548926115, accuracy: 1.0\n",
      "loss: 0.004237561021000147, accuracy: 1.0\n",
      "loss: 0.004158402793109417, accuracy: 1.0\n",
      "loss: 0.004169722553342581, accuracy: 1.0\n",
      "loss: 0.004269339144229889, accuracy: 1.0\n",
      "loss: 0.00420605530962348, accuracy: 1.0\n",
      "loss: 0.004183770157396793, accuracy: 1.0\n",
      "loss: 0.004178619012236595, accuracy: 1.0\n",
      "loss: 0.004147002473473549, accuracy: 1.0\n",
      "loss: 0.0042187487706542015, accuracy: 1.0\n",
      "loss: 0.004187941551208496, accuracy: 1.0\n",
      "loss: 0.004203249700367451, accuracy: 1.0\n",
      "loss: 0.004180411342531443, accuracy: 1.0\n",
      "loss: 0.004168837796896696, accuracy: 1.0\n",
      "loss: 0.004204357974231243, accuracy: 1.0\n",
      "loss: 0.004148117266595364, accuracy: 1.0\n",
      "loss: 0.004180390387773514, accuracy: 1.0\n",
      "loss: 0.004225603304803371, accuracy: 1.0\n",
      "loss: 0.004209325183182955, accuracy: 1.0\n",
      "loss: 0.004208119120448828, accuracy: 1.0\n",
      "loss: 0.004152120091021061, accuracy: 1.0\n",
      "loss: 0.004172604531049728, accuracy: 1.0\n",
      "loss: 0.004153640475124121, accuracy: 1.0\n",
      "loss: 0.004193978384137154, accuracy: 1.0\n",
      "loss: 0.004145230632275343, accuracy: 1.0\n",
      "loss: 0.0041757249273359776, accuracy: 1.0\n",
      "loss: 0.004222066607326269, accuracy: 1.0\n",
      "loss: 0.004082237835973501, accuracy: 1.0\n",
      "loss: 0.004181424621492624, accuracy: 1.0\n",
      "loss: 0.004163548815995455, accuracy: 1.0\n",
      "loss: 0.0041268132627010345, accuracy: 1.0\n",
      "loss: 0.004212992265820503, accuracy: 1.0\n",
      "loss: 0.004143193829804659, accuracy: 1.0\n",
      "loss: 0.0041978550143539906, accuracy: 1.0\n",
      "loss: 0.004050895571708679, accuracy: 1.0\n",
      "loss: 0.004170651081949472, accuracy: 1.0\n",
      "loss: 0.004122187849134207, accuracy: 1.0\n",
      "loss: 0.004150605294853449, accuracy: 1.0\n",
      "loss: 0.0041296472772955894, accuracy: 1.0\n",
      "loss: 0.00412906426936388, accuracy: 1.0\n",
      "loss: 0.004188059363514185, accuracy: 1.0\n",
      "loss: 0.004140343517065048, accuracy: 1.0\n",
      "loss: 0.00413641519844532, accuracy: 1.0\n",
      "loss: 0.0041443645022809505, accuracy: 1.0\n",
      "loss: 0.0042174868285655975, accuracy: 1.0\n",
      "loss: 0.004227363970130682, accuracy: 1.0\n",
      "loss: 0.004175088368356228, accuracy: 1.0\n",
      "loss: 0.004128810949623585, accuracy: 1.0\n",
      "loss: 0.004169553052634001, accuracy: 1.0\n",
      "loss: 0.004089790396392345, accuracy: 1.0\n",
      "loss: 0.004155051428824663, accuracy: 1.0\n",
      "loss: 0.004106956068426371, accuracy: 1.0\n",
      "loss: 0.004153228830546141, accuracy: 1.0\n",
      "loss: 0.004170842934399843, accuracy: 1.0\n",
      "loss: 0.004146452061831951, accuracy: 1.0\n",
      "loss: 0.004111014772206545, accuracy: 1.0\n",
      "loss: 0.0041550579480826855, accuracy: 1.0\n",
      "loss: 0.00415271008387208, accuracy: 1.0\n",
      "loss: 0.0040743094868958, accuracy: 1.0\n",
      "loss: 0.004105995874851942, accuracy: 1.0\n",
      "loss: 0.0040579731576144695, accuracy: 1.0\n",
      "loss: 0.0041863988153636456, accuracy: 1.0\n",
      "loss: 0.004171335604041815, accuracy: 1.0\n",
      "loss: 0.004090412985533476, accuracy: 1.0\n",
      "loss: 0.004164035432040691, accuracy: 1.0\n",
      "loss: 0.004091054666787386, accuracy: 1.0\n",
      "loss: 0.004207013174891472, accuracy: 1.0\n",
      "loss: 0.004183268640190363, accuracy: 1.0\n",
      "loss: 0.004152723588049412, accuracy: 1.0\n",
      "loss: 0.00405582832172513, accuracy: 1.0\n",
      "loss: 0.004020439460873604, accuracy: 1.0\n",
      "loss: 0.004148202482610941, accuracy: 1.0\n",
      "loss: 0.004184348974376917, accuracy: 1.0\n",
      "loss: 0.004147825762629509, accuracy: 1.0\n",
      "loss: 0.004019377753138542, accuracy: 1.0\n",
      "loss: 0.004157012328505516, accuracy: 1.0\n",
      "loss: 0.004247823730111122, accuracy: 1.0\n",
      "loss: 0.004063308238983154, accuracy: 1.0\n",
      "loss: 0.004079833626747131, accuracy: 1.0\n",
      "loss: 0.0041545843705534935, accuracy: 1.0\n",
      "loss: 0.004115063697099686, accuracy: 1.0\n",
      "loss: 0.004086922388523817, accuracy: 1.0\n",
      "loss: 0.004117072094231844, accuracy: 1.0\n",
      "loss: 0.00405942602083087, accuracy: 1.0\n",
      "loss: 0.004089146852493286, accuracy: 1.0\n",
      "loss: 0.004145294893532991, accuracy: 1.0\n",
      "loss: 0.004048076458275318, accuracy: 1.0\n",
      "loss: 0.004114852752536535, accuracy: 1.0\n",
      "loss: 0.004028270486742258, accuracy: 1.0\n",
      "loss: 0.004133572336286306, accuracy: 1.0\n",
      "loss: 0.004076039884239435, accuracy: 1.0\n",
      "loss: 0.004088250920176506, accuracy: 1.0\n",
      "loss: 0.004092795308679342, accuracy: 1.0\n",
      "loss: 0.004094812087714672, accuracy: 1.0\n",
      "loss: 0.004058583173900843, accuracy: 1.0\n",
      "loss: 0.004074716009199619, accuracy: 1.0\n",
      "loss: 0.004022236913442612, accuracy: 1.0\n",
      "loss: 0.004085847642272711, accuracy: 1.0\n",
      "loss: 0.0040443153120577335, accuracy: 1.0\n",
      "loss: 0.004030315671116114, accuracy: 1.0\n",
      "loss: 0.004058598540723324, accuracy: 1.0\n",
      "loss: 0.0040929485112428665, accuracy: 1.0\n",
      "loss: 0.004068039357662201, accuracy: 1.0\n",
      "loss: 0.004071534611284733, accuracy: 1.0\n",
      "loss: 0.004007579758763313, accuracy: 1.0\n",
      "loss: 0.004005240276455879, accuracy: 1.0\n",
      "loss: 0.004067984409630299, accuracy: 1.0\n",
      "loss: 0.004025869537144899, accuracy: 1.0\n",
      "loss: 0.003995060920715332, accuracy: 1.0\n",
      "loss: 0.004134742543101311, accuracy: 1.0\n",
      "loss: 0.004017367027699947, accuracy: 1.0\n",
      "loss: 0.004133152775466442, accuracy: 1.0\n",
      "loss: 0.004094055853784084, accuracy: 1.0\n",
      "loss: 0.004053792450577021, accuracy: 1.0\n",
      "loss: 0.004089971072971821, accuracy: 1.0\n",
      "loss: 0.004017366096377373, accuracy: 1.0\n",
      "loss: 0.004142944701015949, accuracy: 1.0\n",
      "loss: 0.004026998765766621, accuracy: 1.0\n",
      "loss: 0.0041833799332380295, accuracy: 1.0\n",
      "loss: 0.004024190362542868, accuracy: 1.0\n",
      "loss: 0.004004404880106449, accuracy: 1.0\n",
      "loss: 0.004078701604157686, accuracy: 1.0\n",
      "loss: 0.0039697918109595776, accuracy: 1.0\n",
      "loss: 0.004139425698667765, accuracy: 1.0\n",
      "loss: 0.0040406957268714905, accuracy: 1.0\n",
      "loss: 0.004181531723588705, accuracy: 1.0\n",
      "loss: 0.004012187011539936, accuracy: 1.0\n",
      "loss: 0.004058651626110077, accuracy: 1.0\n",
      "loss: 0.004081427119672298, accuracy: 1.0\n",
      "loss: 0.003990613855421543, accuracy: 1.0\n",
      "loss: 0.004008370451629162, accuracy: 1.0\n",
      "loss: 0.0040414114482700825, accuracy: 1.0\n",
      "loss: 0.004030948504805565, accuracy: 1.0\n",
      "loss: 0.004058814141899347, accuracy: 1.0\n",
      "loss: 0.0040147313848137856, accuracy: 1.0\n",
      "loss: 0.003998495638370514, accuracy: 1.0\n",
      "loss: 0.003992884419858456, accuracy: 1.0\n",
      "loss: 0.003989405930042267, accuracy: 1.0\n",
      "loss: 0.004037840291857719, accuracy: 1.0\n",
      "loss: 0.0041123609989881516, accuracy: 1.0\n",
      "loss: 0.004005711525678635, accuracy: 1.0\n",
      "loss: 0.00400648545473814, accuracy: 1.0\n",
      "loss: 0.004111046437174082, accuracy: 1.0\n",
      "loss: 0.004013529047369957, accuracy: 1.0\n",
      "loss: 0.003954384941607714, accuracy: 1.0\n",
      "loss: 0.004068275447934866, accuracy: 1.0\n",
      "loss: 0.004078058525919914, accuracy: 1.0\n",
      "loss: 0.004014965612441301, accuracy: 1.0\n",
      "loss: 0.003965387120842934, accuracy: 1.0\n",
      "loss: 0.004066179972141981, accuracy: 1.0\n",
      "loss: 0.0039568739011883736, accuracy: 1.0\n",
      "loss: 0.004066611174494028, accuracy: 1.0\n",
      "loss: 0.004028020426630974, accuracy: 1.0\n",
      "loss: 0.004030007869005203, accuracy: 1.0\n",
      "loss: 0.004076422657817602, accuracy: 1.0\n",
      "loss: 0.003980870358645916, accuracy: 1.0\n",
      "loss: 0.004093904513865709, accuracy: 1.0\n",
      "loss: 0.004009786527603865, accuracy: 1.0\n",
      "loss: 0.003964443225413561, accuracy: 1.0\n",
      "loss: 0.003932481165975332, accuracy: 1.0\n",
      "loss: 0.003949217032641172, accuracy: 1.0\n",
      "loss: 0.004076012410223484, accuracy: 1.0\n",
      "loss: 0.0039398642256855965, accuracy: 1.0\n",
      "loss: 0.004014830105006695, accuracy: 1.0\n",
      "loss: 0.004001063760370016, accuracy: 1.0\n",
      "loss: 0.003942002542316914, accuracy: 1.0\n",
      "loss: 0.00394693436101079, accuracy: 1.0\n",
      "loss: 0.0040346235036849976, accuracy: 1.0\n",
      "loss: 0.003941003233194351, accuracy: 1.0\n",
      "loss: 0.0039556678384542465, accuracy: 1.0\n",
      "loss: 0.003931837622076273, accuracy: 1.0\n",
      "loss: 0.004035810474306345, accuracy: 1.0\n",
      "loss: 0.003948668949306011, accuracy: 1.0\n",
      "loss: 0.00400015851482749, accuracy: 1.0\n",
      "loss: 0.003996260464191437, accuracy: 1.0\n",
      "loss: 0.003969810903072357, accuracy: 1.0\n",
      "loss: 0.003990215715020895, accuracy: 1.0\n",
      "loss: 0.003983273636549711, accuracy: 1.0\n",
      "loss: 0.003995852079242468, accuracy: 1.0\n",
      "loss: 0.004068483132869005, accuracy: 1.0\n",
      "loss: 0.004096022807061672, accuracy: 1.0\n",
      "loss: 0.003965348936617374, accuracy: 1.0\n",
      "loss: 0.0039518182165920734, accuracy: 1.0\n",
      "loss: 0.003909965045750141, accuracy: 1.0\n",
      "loss: 0.003965051844716072, accuracy: 1.0\n",
      "loss: 0.003931755665689707, accuracy: 1.0\n",
      "loss: 0.003932289779186249, accuracy: 1.0\n",
      "loss: 0.004032990895211697, accuracy: 1.0\n",
      "loss: 0.004023918882012367, accuracy: 1.0\n",
      "loss: 0.003957650158554316, accuracy: 1.0\n",
      "loss: 0.003960811998695135, accuracy: 1.0\n",
      "loss: 0.003991228528320789, accuracy: 1.0\n",
      "loss: 0.004005678929388523, accuracy: 1.0\n",
      "loss: 0.003971092868596315, accuracy: 1.0\n",
      "loss: 0.003919902723282576, accuracy: 1.0\n",
      "loss: 0.003909017890691757, accuracy: 1.0\n",
      "loss: 0.003991111181676388, accuracy: 1.0\n",
      "loss: 0.00398244196549058, accuracy: 1.0\n",
      "loss: 0.003942601382732391, accuracy: 1.0\n",
      "loss: 0.004036086145788431, accuracy: 1.0\n",
      "loss: 0.004009719006717205, accuracy: 1.0\n",
      "loss: 0.003929163794964552, accuracy: 1.0\n",
      "loss: 0.003985527437180281, accuracy: 1.0\n",
      "loss: 0.003944411873817444, accuracy: 1.0\n",
      "loss: 0.003928227350115776, accuracy: 1.0\n",
      "loss: 0.003941049799323082, accuracy: 1.0\n",
      "loss: 0.003965626936405897, accuracy: 1.0\n",
      "loss: 0.003960275091230869, accuracy: 1.0\n",
      "loss: 0.003934803884476423, accuracy: 1.0\n",
      "loss: 0.004002103582024574, accuracy: 1.0\n",
      "loss: 0.003981508780270815, accuracy: 1.0\n",
      "loss: 0.003916157875210047, accuracy: 1.0\n",
      "loss: 0.003943652380257845, accuracy: 1.0\n",
      "loss: 0.0039229197427630424, accuracy: 1.0\n",
      "loss: 0.003954746760427952, accuracy: 1.0\n",
      "loss: 0.003918008878827095, accuracy: 1.0\n",
      "loss: 0.003940570168197155, accuracy: 1.0\n",
      "loss: 0.003877009730786085, accuracy: 1.0\n",
      "loss: 0.004030941519886255, accuracy: 1.0\n",
      "loss: 0.003916545771062374, accuracy: 1.0\n",
      "loss: 0.003992595709860325, accuracy: 1.0\n",
      "loss: 0.003920683637261391, accuracy: 1.0\n",
      "loss: 0.003893609857186675, accuracy: 1.0\n",
      "loss: 0.003908518701791763, accuracy: 1.0\n",
      "loss: 0.003953364212065935, accuracy: 1.0\n",
      "loss: 0.0038973072078078985, accuracy: 1.0\n",
      "loss: 0.003941197879612446, accuracy: 1.0\n",
      "loss: 0.003946366254240274, accuracy: 1.0\n",
      "loss: 0.003967111464589834, accuracy: 1.0\n",
      "loss: 0.003913087770342827, accuracy: 1.0\n",
      "loss: 0.003910532221198082, accuracy: 1.0\n",
      "loss: 0.003967637196183205, accuracy: 1.0\n",
      "loss: 0.0038605439476668835, accuracy: 1.0\n",
      "loss: 0.003906257450580597, accuracy: 1.0\n",
      "loss: 0.003983032424002886, accuracy: 1.0\n",
      "loss: 0.003891710191965103, accuracy: 1.0\n",
      "loss: 0.0038415526505559683, accuracy: 1.0\n",
      "loss: 0.003972397185862064, accuracy: 1.0\n",
      "loss: 0.003966665826737881, accuracy: 1.0\n",
      "loss: 0.003946117591112852, accuracy: 1.0\n",
      "loss: 0.003896525828167796, accuracy: 1.0\n",
      "loss: 0.003861862001940608, accuracy: 1.0\n",
      "loss: 0.0038471799343824387, accuracy: 1.0\n",
      "loss: 0.003924574237316847, accuracy: 1.0\n",
      "loss: 0.0040075103752315044, accuracy: 1.0\n",
      "loss: 0.00388258951716125, accuracy: 1.0\n",
      "loss: 0.0038913374301046133, accuracy: 1.0\n",
      "loss: 0.003852845635265112, accuracy: 1.0\n",
      "loss: 0.0038983793929219246, accuracy: 1.0\n",
      "loss: 0.0038961477112025023, accuracy: 1.0\n",
      "loss: 0.003834166331216693, accuracy: 1.0\n",
      "loss: 0.003919300157576799, accuracy: 1.0\n",
      "loss: 0.0038802926428616047, accuracy: 1.0\n",
      "loss: 0.003875384107232094, accuracy: 1.0\n",
      "loss: 0.003930105362087488, accuracy: 1.0\n",
      "loss: 0.0038930552545934916, accuracy: 1.0\n",
      "loss: 0.003854533191770315, accuracy: 1.0\n",
      "loss: 0.003958250395953655, accuracy: 1.0\n",
      "loss: 0.0038995782379060984, accuracy: 1.0\n",
      "loss: 0.003872812492772937, accuracy: 1.0\n",
      "loss: 0.003907466772943735, accuracy: 1.0\n",
      "loss: 0.003839086042717099, accuracy: 1.0\n",
      "loss: 0.0038686778862029314, accuracy: 1.0\n",
      "loss: 0.0038434509187936783, accuracy: 1.0\n",
      "loss: 0.004009477328509092, accuracy: 1.0\n",
      "loss: 0.0039030220359563828, accuracy: 1.0\n",
      "loss: 0.0038699510041624308, accuracy: 1.0\n",
      "loss: 0.0038776062428951263, accuracy: 1.0\n",
      "loss: 0.0040059336461126804, accuracy: 1.0\n",
      "loss: 0.0038739771116524935, accuracy: 1.0\n",
      "loss: 0.003852105000987649, accuracy: 1.0\n",
      "loss: 0.0038340359460562468, accuracy: 1.0\n",
      "loss: 0.003896346315741539, accuracy: 1.0\n",
      "loss: 0.003907076548784971, accuracy: 1.0\n",
      "loss: 0.003915697336196899, accuracy: 1.0\n",
      "loss: 0.003956510219722986, accuracy: 1.0\n",
      "loss: 0.003907414618879557, accuracy: 1.0\n",
      "loss: 0.00387498433701694, accuracy: 1.0\n",
      "loss: 0.0038839683402329683, accuracy: 1.0\n",
      "loss: 0.003923929296433926, accuracy: 1.0\n",
      "loss: 0.0038230435457080603, accuracy: 1.0\n",
      "loss: 0.003862332785502076, accuracy: 1.0\n",
      "loss: 0.003838128875941038, accuracy: 1.0\n",
      "loss: 0.0038741715252399445, accuracy: 1.0\n",
      "loss: 0.003901776159182191, accuracy: 1.0\n",
      "loss: 0.003907188773155212, accuracy: 1.0\n",
      "loss: 0.003887810045853257, accuracy: 1.0\n",
      "loss: 0.003865309525281191, accuracy: 1.0\n",
      "loss: 0.0038807045202702284, accuracy: 1.0\n",
      "loss: 0.0038687342312186956, accuracy: 1.0\n",
      "loss: 0.003863679012283683, accuracy: 1.0\n",
      "loss: 0.0037855489645153284, accuracy: 1.0\n",
      "loss: 0.003837084863334894, accuracy: 1.0\n",
      "loss: 0.003822830505669117, accuracy: 1.0\n",
      "loss: 0.0038962059188634157, accuracy: 1.0\n",
      "loss: 0.003869402687996626, accuracy: 1.0\n",
      "loss: 0.0038499629590660334, accuracy: 1.0\n",
      "loss: 0.0038257662672549486, accuracy: 1.0\n",
      "loss: 0.004000193439424038, accuracy: 1.0\n",
      "loss: 0.0038343542255461216, accuracy: 1.0\n",
      "loss: 0.00380391301587224, accuracy: 1.0\n",
      "loss: 0.003819822333753109, accuracy: 1.0\n",
      "loss: 0.003815653035417199, accuracy: 1.0\n",
      "loss: 0.0038105007261037827, accuracy: 1.0\n",
      "loss: 0.0038464150857180357, accuracy: 1.0\n",
      "loss: 0.0037997921463102102, accuracy: 1.0\n",
      "loss: 0.003916729241609573, accuracy: 1.0\n",
      "loss: 0.0038170861080288887, accuracy: 1.0\n",
      "loss: 0.003844306105747819, accuracy: 1.0\n",
      "loss: 0.0038480430375784636, accuracy: 1.0\n",
      "loss: 0.003814049530774355, accuracy: 1.0\n",
      "loss: 0.003800289938226342, accuracy: 1.0\n",
      "loss: 0.0038226277101784945, accuracy: 1.0\n",
      "loss: 0.00379000511020422, accuracy: 1.0\n",
      "loss: 0.0038250337820500135, accuracy: 1.0\n",
      "loss: 0.00386175443418324, accuracy: 1.0\n",
      "loss: 0.0038422609213739634, accuracy: 1.0\n",
      "loss: 0.003808526322245598, accuracy: 1.0\n",
      "loss: 0.0037689434830099344, accuracy: 1.0\n",
      "loss: 0.0037948719691485167, accuracy: 1.0\n",
      "loss: 0.0037945646326988935, accuracy: 1.0\n",
      "loss: 0.00379195436835289, accuracy: 1.0\n",
      "loss: 0.0037641427479684353, accuracy: 1.0\n",
      "loss: 0.003779883962124586, accuracy: 1.0\n",
      "loss: 0.0037947578821331263, accuracy: 1.0\n",
      "loss: 0.003788539208471775, accuracy: 1.0\n",
      "loss: 0.003761616302654147, accuracy: 1.0\n",
      "loss: 0.003788894973695278, accuracy: 1.0\n",
      "loss: 0.00382170919328928, accuracy: 1.0\n",
      "loss: 0.0037759896367788315, accuracy: 1.0\n",
      "loss: 0.003813107730820775, accuracy: 1.0\n",
      "loss: 0.003994337748736143, accuracy: 1.0\n",
      "loss: 0.003782653482630849, accuracy: 1.0\n",
      "loss: 0.0037973562721163034, accuracy: 1.0\n",
      "loss: 0.0037931278347969055, accuracy: 1.0\n",
      "loss: 0.003839269047603011, accuracy: 1.0\n",
      "loss: 0.0037838902790099382, accuracy: 1.0\n",
      "loss: 0.003913639113306999, accuracy: 1.0\n",
      "loss: 0.0037414811085909605, accuracy: 1.0\n",
      "loss: 0.0039005696307867765, accuracy: 1.0\n",
      "loss: 0.003732823533937335, accuracy: 1.0\n",
      "loss: 0.003755874466150999, accuracy: 1.0\n",
      "loss: 0.0038299381267279387, accuracy: 1.0\n",
      "loss: 0.0037388436030596495, accuracy: 1.0\n",
      "loss: 0.0037995451129972935, accuracy: 1.0\n",
      "loss: 0.003739794949069619, accuracy: 1.0\n",
      "loss: 0.0038546209689229727, accuracy: 1.0\n",
      "loss: 0.0037502837367355824, accuracy: 1.0\n",
      "loss: 0.0037475754506886005, accuracy: 1.0\n",
      "loss: 0.0037561652716249228, accuracy: 1.0\n",
      "loss: 0.0038026366382837296, accuracy: 1.0\n",
      "loss: 0.0037306472659111023, accuracy: 1.0\n",
      "loss: 0.0037483656778931618, accuracy: 1.0\n",
      "loss: 0.0038186651654541492, accuracy: 1.0\n",
      "loss: 0.003819037927314639, accuracy: 1.0\n",
      "loss: 0.0037279820535331964, accuracy: 1.0\n",
      "loss: 0.0038950603920966387, accuracy: 1.0\n",
      "loss: 0.003811425296589732, accuracy: 1.0\n",
      "loss: 0.00375352893024683, accuracy: 1.0\n",
      "loss: 0.00377772212959826, accuracy: 1.0\n",
      "loss: 0.003750323783606291, accuracy: 1.0\n",
      "loss: 0.0038169631734490395, accuracy: 1.0\n",
      "loss: 0.0036998011637479067, accuracy: 1.0\n",
      "loss: 0.00377473933622241, accuracy: 1.0\n",
      "loss: 0.003737475723028183, accuracy: 1.0\n",
      "loss: 0.00378085277043283, accuracy: 1.0\n",
      "loss: 0.0037734636571258307, accuracy: 1.0\n",
      "loss: 0.003732413984835148, accuracy: 1.0\n",
      "loss: 0.0038240854628384113, accuracy: 1.0\n",
      "loss: 0.003761052619665861, accuracy: 1.0\n",
      "loss: 0.0038007348775863647, accuracy: 1.0\n",
      "loss: 0.003824658691883087, accuracy: 1.0\n",
      "loss: 0.0037516842130571604, accuracy: 1.0\n",
      "loss: 0.0037166594993323088, accuracy: 1.0\n",
      "loss: 0.0038865189999341965, accuracy: 1.0\n",
      "loss: 0.003755302168428898, accuracy: 1.0\n",
      "loss: 0.0037311946507543325, accuracy: 1.0\n",
      "loss: 0.00374633283354342, accuracy: 1.0\n",
      "loss: 0.003705034265294671, accuracy: 1.0\n",
      "loss: 0.003763097571209073, accuracy: 1.0\n",
      "loss: 0.003787861205637455, accuracy: 1.0\n",
      "loss: 0.003816611599177122, accuracy: 1.0\n",
      "loss: 0.003801877610385418, accuracy: 1.0\n",
      "loss: 0.003705700859427452, accuracy: 1.0\n",
      "loss: 0.0037670868914574385, accuracy: 1.0\n",
      "loss: 0.0037289049942046404, accuracy: 1.0\n",
      "loss: 0.0037654598709195852, accuracy: 1.0\n",
      "loss: 0.0038375838194042444, accuracy: 1.0\n",
      "loss: 0.003721166169270873, accuracy: 1.0\n",
      "loss: 0.003756625112146139, accuracy: 1.0\n",
      "loss: 0.0036808664444833994, accuracy: 1.0\n",
      "loss: 0.0038351786788553, accuracy: 1.0\n",
      "loss: 0.003765454515814781, accuracy: 1.0\n",
      "loss: 0.003671518759801984, accuracy: 1.0\n",
      "loss: 0.0037376354448497295, accuracy: 1.0\n",
      "loss: 0.003746370319277048, accuracy: 1.0\n",
      "loss: 0.003908130805939436, accuracy: 1.0\n",
      "loss: 0.0037192523013800383, accuracy: 1.0\n",
      "loss: 0.0037526485975831747, accuracy: 1.0\n",
      "loss: 0.003694992046803236, accuracy: 1.0\n",
      "loss: 0.003664672141894698, accuracy: 1.0\n",
      "loss: 0.003712773323059082, accuracy: 1.0\n",
      "loss: 0.003725574817508459, accuracy: 1.0\n",
      "loss: 0.0037501007318496704, accuracy: 1.0\n",
      "loss: 0.0036898169200867414, accuracy: 1.0\n",
      "loss: 0.0037959301844239235, accuracy: 1.0\n",
      "loss: 0.0037260428071022034, accuracy: 1.0\n",
      "loss: 0.003800999838858843, accuracy: 1.0\n",
      "loss: 0.003667751792818308, accuracy: 1.0\n",
      "loss: 0.0037061653565615416, accuracy: 1.0\n",
      "loss: 0.003750859061256051, accuracy: 1.0\n",
      "loss: 0.0037614996545016766, accuracy: 1.0\n",
      "loss: 0.0036707408726215363, accuracy: 1.0\n",
      "loss: 0.003673667786642909, accuracy: 1.0\n",
      "loss: 0.003818409750238061, accuracy: 1.0\n",
      "loss: 0.0036803109105676413, accuracy: 1.0\n",
      "loss: 0.003745102556422353, accuracy: 1.0\n",
      "loss: 0.0037291727494448423, accuracy: 1.0\n",
      "loss: 0.003740791231393814, accuracy: 1.0\n",
      "loss: 0.003684653900563717, accuracy: 1.0\n",
      "loss: 0.0037442310713231564, accuracy: 1.0\n",
      "loss: 0.003657210385426879, accuracy: 1.0\n",
      "loss: 0.003748097224161029, accuracy: 1.0\n",
      "loss: 0.0037322198040783405, accuracy: 1.0\n",
      "loss: 0.003702246118336916, accuracy: 1.0\n",
      "loss: 0.003700614208355546, accuracy: 1.0\n",
      "loss: 0.0036898700054734945, accuracy: 1.0\n",
      "loss: 0.0037194706965237856, accuracy: 1.0\n",
      "loss: 0.0036853691563010216, accuracy: 1.0\n",
      "loss: 0.00372691685333848, accuracy: 1.0\n",
      "loss: 0.0037080496549606323, accuracy: 1.0\n",
      "loss: 0.003737680148333311, accuracy: 1.0\n",
      "loss: 0.0036731171421706676, accuracy: 1.0\n",
      "loss: 0.003633704036474228, accuracy: 1.0\n",
      "loss: 0.003698718501254916, accuracy: 1.0\n",
      "loss: 0.003662784118205309, accuracy: 1.0\n",
      "loss: 0.003693132195621729, accuracy: 1.0\n",
      "loss: 0.003705450799316168, accuracy: 1.0\n",
      "loss: 0.003693749662488699, accuracy: 1.0\n",
      "loss: 0.0036972116213291883, accuracy: 1.0\n",
      "loss: 0.0036223391070961952, accuracy: 1.0\n",
      "loss: 0.0036568522918969393, accuracy: 1.0\n",
      "loss: 0.0036643934436142445, accuracy: 1.0\n",
      "loss: 0.003685597563162446, accuracy: 1.0\n",
      "loss: 0.0036843125708401203, accuracy: 1.0\n",
      "loss: 0.003652600571513176, accuracy: 1.0\n",
      "loss: 0.003710054326802492, accuracy: 1.0\n",
      "loss: 0.003674149978905916, accuracy: 1.0\n",
      "loss: 0.003719571279361844, accuracy: 1.0\n",
      "loss: 0.0036382160615175962, accuracy: 1.0\n",
      "loss: 0.0036969888024032116, accuracy: 1.0\n",
      "loss: 0.003634451888501644, accuracy: 1.0\n",
      "loss: 0.0037024812772870064, accuracy: 1.0\n",
      "loss: 0.0037454315461218357, accuracy: 1.0\n",
      "loss: 0.0036376193165779114, accuracy: 1.0\n",
      "loss: 0.003666475648060441, accuracy: 1.0\n",
      "loss: 0.0037704301066696644, accuracy: 1.0\n",
      "loss: 0.0036645762156695127, accuracy: 1.0\n",
      "loss: 0.003653674852102995, accuracy: 1.0\n",
      "loss: 0.003736532758921385, accuracy: 1.0\n",
      "loss: 0.0036350395530462265, accuracy: 1.0\n",
      "loss: 0.0036419194657355547, accuracy: 1.0\n",
      "loss: 0.0037643425166606903, accuracy: 1.0\n",
      "loss: 0.003673516446724534, accuracy: 1.0\n",
      "loss: 0.0036293871235102415, accuracy: 1.0\n",
      "loss: 0.0036684495862573385, accuracy: 1.0\n",
      "loss: 0.003659095847979188, accuracy: 1.0\n",
      "loss: 0.003696424886584282, accuracy: 1.0\n",
      "loss: 0.003650156781077385, accuracy: 1.0\n",
      "loss: 0.003687535645440221, accuracy: 1.0\n",
      "loss: 0.003657632041722536, accuracy: 1.0\n",
      "loss: 0.003625381039455533, accuracy: 1.0\n",
      "loss: 0.0036847887095063925, accuracy: 1.0\n",
      "loss: 0.003661817405372858, accuracy: 1.0\n",
      "loss: 0.003668182995170355, accuracy: 1.0\n",
      "loss: 0.003638921771198511, accuracy: 1.0\n",
      "loss: 0.003662172704935074, accuracy: 1.0\n",
      "loss: 0.003597568953409791, accuracy: 1.0\n",
      "loss: 0.003615620546042919, accuracy: 1.0\n",
      "loss: 0.003624140517786145, accuracy: 1.0\n",
      "loss: 0.0036479406990110874, accuracy: 1.0\n",
      "loss: 0.0036511633079499006, accuracy: 1.0\n",
      "loss: 0.0036387669388204813, accuracy: 1.0\n",
      "loss: 0.0036407625302672386, accuracy: 1.0\n",
      "loss: 0.0036606979556381702, accuracy: 1.0\n",
      "loss: 0.003702695481479168, accuracy: 1.0\n",
      "loss: 0.00363971758633852, accuracy: 1.0\n",
      "loss: 0.003636243287473917, accuracy: 1.0\n",
      "loss: 0.003648786572739482, accuracy: 1.0\n",
      "loss: 0.003671928308904171, accuracy: 1.0\n",
      "loss: 0.003637639107182622, accuracy: 1.0\n",
      "loss: 0.0036459637340158224, accuracy: 1.0\n",
      "loss: 0.0036344192922115326, accuracy: 1.0\n",
      "loss: 0.0036751129664480686, accuracy: 1.0\n",
      "loss: 0.0036465979646891356, accuracy: 1.0\n",
      "loss: 0.003646544413641095, accuracy: 1.0\n",
      "loss: 0.003604212775826454, accuracy: 1.0\n",
      "loss: 0.003674814011901617, accuracy: 1.0\n",
      "loss: 0.003702059155330062, accuracy: 1.0\n",
      "loss: 0.003670433769002557, accuracy: 1.0\n",
      "loss: 0.0036251326091587543, accuracy: 1.0\n",
      "loss: 0.003686017356812954, accuracy: 1.0\n",
      "loss: 0.0036570588126778603, accuracy: 1.0\n",
      "loss: 0.0035938783548772335, accuracy: 1.0\n",
      "loss: 0.0036292707081884146, accuracy: 1.0\n",
      "loss: 0.00365259125828743, accuracy: 1.0\n",
      "loss: 0.003583621233701706, accuracy: 1.0\n",
      "loss: 0.0035876815672963858, accuracy: 1.0\n",
      "loss: 0.0036542415618896484, accuracy: 1.0\n",
      "loss: 0.0035898438654839993, accuracy: 1.0\n",
      "loss: 0.0035999559331685305, accuracy: 1.0\n",
      "loss: 0.0035878324415534735, accuracy: 1.0\n",
      "loss: 0.00357981794513762, accuracy: 1.0\n",
      "loss: 0.003641200950369239, accuracy: 1.0\n",
      "loss: 0.003582889912649989, accuracy: 1.0\n",
      "loss: 0.0035975025966763496, accuracy: 1.0\n",
      "loss: 0.0036376581992954016, accuracy: 1.0\n",
      "loss: 0.0036490471102297306, accuracy: 1.0\n",
      "loss: 0.0035867695696651936, accuracy: 1.0\n",
      "loss: 0.0036024716682732105, accuracy: 1.0\n",
      "loss: 0.003567212726920843, accuracy: 1.0\n",
      "loss: 0.0036586634814739227, accuracy: 1.0\n",
      "loss: 0.0035688544157892466, accuracy: 1.0\n",
      "loss: 0.0035577677190303802, accuracy: 1.0\n",
      "loss: 0.0035802472848445177, accuracy: 1.0\n",
      "loss: 0.00366475828923285, accuracy: 1.0\n",
      "loss: 0.0036857298109680414, accuracy: 1.0\n",
      "loss: 0.0035793667193502188, accuracy: 1.0\n",
      "loss: 0.0036337508354336023, accuracy: 1.0\n",
      "loss: 0.0035975296050310135, accuracy: 1.0\n",
      "loss: 0.0036052248906344175, accuracy: 1.0\n",
      "loss: 0.003619533497840166, accuracy: 1.0\n",
      "loss: 0.0036380791570991278, accuracy: 1.0\n",
      "loss: 0.0036096747498959303, accuracy: 1.0\n",
      "loss: 0.0036426226142793894, accuracy: 1.0\n",
      "loss: 0.003526868997141719, accuracy: 1.0\n",
      "loss: 0.0036056311801075935, accuracy: 1.0\n",
      "loss: 0.003590879961848259, accuracy: 1.0\n",
      "loss: 0.0036068912595510483, accuracy: 1.0\n",
      "loss: 0.003682246897369623, accuracy: 1.0\n",
      "loss: 0.0035944541450589895, accuracy: 1.0\n",
      "loss: 0.003540544770658016, accuracy: 1.0\n",
      "loss: 0.0035863916855305433, accuracy: 1.0\n",
      "loss: 0.0036111704539507627, accuracy: 1.0\n",
      "loss: 0.0035727268550544977, accuracy: 1.0\n",
      "loss: 0.0035070187877863646, accuracy: 1.0\n",
      "loss: 0.003583908546715975, accuracy: 1.0\n",
      "loss: 0.0035898713394999504, accuracy: 1.0\n",
      "loss: 0.003542935010045767, accuracy: 1.0\n",
      "loss: 0.003582084085792303, accuracy: 1.0\n",
      "loss: 0.003590579144656658, accuracy: 1.0\n",
      "loss: 0.0035650981590151787, accuracy: 1.0\n",
      "loss: 0.0035584100987762213, accuracy: 1.0\n",
      "loss: 0.0035631642676889896, accuracy: 1.0\n",
      "loss: 0.0035703578032553196, accuracy: 1.0\n",
      "loss: 0.00362893077544868, accuracy: 1.0\n",
      "loss: 0.003565369173884392, accuracy: 1.0\n",
      "loss: 0.0035554077476263046, accuracy: 1.0\n",
      "loss: 0.003529682056978345, accuracy: 1.0\n",
      "loss: 0.003570660948753357, accuracy: 1.0\n",
      "loss: 0.00357195851393044, accuracy: 1.0\n",
      "loss: 0.003502806182950735, accuracy: 1.0\n",
      "loss: 0.0035643195733428, accuracy: 1.0\n",
      "loss: 0.0035867088008672, accuracy: 1.0\n",
      "loss: 0.003627630416303873, accuracy: 1.0\n",
      "loss: 0.0035558624658733606, accuracy: 1.0\n",
      "loss: 0.0035486372653394938, accuracy: 1.0\n",
      "loss: 0.003536073723807931, accuracy: 1.0\n",
      "loss: 0.0035223138984292746, accuracy: 1.0\n",
      "loss: 0.0035711342934519053, accuracy: 1.0\n",
      "loss: 0.003534932155162096, accuracy: 1.0\n",
      "loss: 0.0035496996715664864, accuracy: 1.0\n",
      "loss: 0.0035266431514173746, accuracy: 1.0\n",
      "loss: 0.0036525344476103783, accuracy: 1.0\n",
      "loss: 0.0035576585214585066, accuracy: 1.0\n",
      "loss: 0.0035881600342690945, accuracy: 1.0\n",
      "loss: 0.0035798633471131325, accuracy: 1.0\n",
      "loss: 0.003563984762877226, accuracy: 1.0\n",
      "loss: 0.0035366895608603954, accuracy: 1.0\n",
      "loss: 0.0035935132764279842, accuracy: 1.0\n",
      "loss: 0.0035817278549075127, accuracy: 1.0\n",
      "loss: 0.0036023592110723257, accuracy: 1.0\n",
      "loss: 0.0035695936530828476, accuracy: 1.0\n",
      "loss: 0.0035415245220065117, accuracy: 1.0\n",
      "loss: 0.00348329683765769, accuracy: 1.0\n",
      "loss: 0.0035237178672105074, accuracy: 1.0\n",
      "loss: 0.0035592401400208473, accuracy: 1.0\n",
      "loss: 0.0035085591953247786, accuracy: 1.0\n",
      "loss: 0.0035094257909804583, accuracy: 1.0\n",
      "loss: 0.003557792864739895, accuracy: 1.0\n",
      "loss: 0.0035611719358712435, accuracy: 1.0\n",
      "loss: 0.0035350762773305178, accuracy: 1.0\n",
      "loss: 0.00351622118614614, accuracy: 1.0\n",
      "loss: 0.003517806064337492, accuracy: 1.0\n",
      "loss: 0.003562128869816661, accuracy: 1.0\n",
      "loss: 0.003548460314050317, accuracy: 1.0\n",
      "loss: 0.003512864699587226, accuracy: 1.0\n",
      "loss: 0.0035228074993938208, accuracy: 1.0\n",
      "loss: 0.00354411150328815, accuracy: 1.0\n",
      "loss: 0.0035751452669501305, accuracy: 1.0\n",
      "loss: 0.0035431478172540665, accuracy: 1.0\n",
      "loss: 0.0035277376882731915, accuracy: 1.0\n",
      "loss: 0.0035652760416269302, accuracy: 1.0\n",
      "loss: 0.003608107566833496, accuracy: 1.0\n",
      "loss: 0.003584670601412654, accuracy: 1.0\n",
      "loss: 0.00354207307100296, accuracy: 1.0\n",
      "loss: 0.0034927630331367254, accuracy: 1.0\n",
      "loss: 0.003549163695424795, accuracy: 1.0\n",
      "loss: 0.0034825794864445925, accuracy: 1.0\n",
      "loss: 0.0036230420228093863, accuracy: 1.0\n",
      "loss: 0.003544738981872797, accuracy: 1.0\n",
      "loss: 0.0034847550559788942, accuracy: 1.0\n",
      "loss: 0.003565140999853611, accuracy: 1.0\n",
      "loss: 0.0035463529638946056, accuracy: 1.0\n",
      "loss: 0.003516433062031865, accuracy: 1.0\n",
      "loss: 0.003447873517870903, accuracy: 1.0\n",
      "loss: 0.003488272661343217, accuracy: 1.0\n",
      "loss: 0.0035148542374372482, accuracy: 1.0\n",
      "loss: 0.0034996718168258667, accuracy: 1.0\n",
      "loss: 0.003455502213910222, accuracy: 1.0\n",
      "loss: 0.003536023199558258, accuracy: 1.0\n",
      "loss: 0.003506164997816086, accuracy: 1.0\n",
      "loss: 0.003538202028721571, accuracy: 1.0\n",
      "loss: 0.0035944031551480293, accuracy: 1.0\n",
      "loss: 0.003502514911815524, accuracy: 1.0\n",
      "loss: 0.003547810483723879, accuracy: 1.0\n",
      "loss: 0.003496639896184206, accuracy: 1.0\n",
      "loss: 0.003448384813964367, accuracy: 1.0\n",
      "loss: 0.0034997316543012857, accuracy: 1.0\n",
      "loss: 0.0035074905026704073, accuracy: 1.0\n",
      "loss: 0.003489696653559804, accuracy: 1.0\n",
      "loss: 0.003524097381159663, accuracy: 1.0\n",
      "loss: 0.0035039356444031, accuracy: 1.0\n",
      "loss: 0.0035489678848534822, accuracy: 1.0\n",
      "loss: 0.0034727933816611767, accuracy: 1.0\n",
      "loss: 0.0034896337892860174, accuracy: 1.0\n",
      "loss: 0.0035507569555193186, accuracy: 1.0\n",
      "loss: 0.0035229651257395744, accuracy: 1.0\n",
      "loss: 0.003440063912421465, accuracy: 1.0\n",
      "loss: 0.003459908999502659, accuracy: 1.0\n",
      "loss: 0.0035638015251606703, accuracy: 1.0\n",
      "loss: 0.0035021102521568537, accuracy: 1.0\n",
      "loss: 0.0035304652992635965, accuracy: 1.0\n",
      "loss: 0.0034868402872234583, accuracy: 1.0\n",
      "loss: 0.003497618017718196, accuracy: 1.0\n",
      "loss: 0.003523662919178605, accuracy: 1.0\n",
      "loss: 0.0034876251593232155, accuracy: 1.0\n",
      "loss: 0.003440320026129484, accuracy: 1.0\n",
      "loss: 0.0034891439136117697, accuracy: 1.0\n",
      "loss: 0.003469054587185383, accuracy: 1.0\n",
      "loss: 0.0034562014043331146, accuracy: 1.0\n",
      "loss: 0.003503052517771721, accuracy: 1.0\n",
      "loss: 0.003473833203315735, accuracy: 1.0\n",
      "loss: 0.003476934740319848, accuracy: 1.0\n",
      "loss: 0.0034733109641820192, accuracy: 1.0\n",
      "loss: 0.0034636161290109158, accuracy: 1.0\n",
      "loss: 0.003472115844488144, accuracy: 1.0\n",
      "loss: 0.0035068371798843145, accuracy: 1.0\n",
      "loss: 0.0034687805455178022, accuracy: 1.0\n",
      "loss: 0.0034591746516525745, accuracy: 1.0\n",
      "loss: 0.0034632289316505194, accuracy: 1.0\n",
      "loss: 0.003442638786509633, accuracy: 1.0\n",
      "loss: 0.0035440807696431875, accuracy: 1.0\n",
      "loss: 0.0034266100265085697, accuracy: 1.0\n",
      "loss: 0.0034440651070326567, accuracy: 1.0\n",
      "loss: 0.0034589890856295824, accuracy: 1.0\n",
      "loss: 0.003451636293902993, accuracy: 1.0\n",
      "loss: 0.003446883289143443, accuracy: 1.0\n",
      "loss: 0.0034816928673535585, accuracy: 1.0\n",
      "the training time is: 77.30968284606934\n",
      "accuracy: 0.7933333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_out_0.train_evaluate(q3_out_0_iter, data_test_iter, q3_out_0.u1, q3_out_0.v1, q3_out_0.b1, q3_out_0.u2, q3_out_0.v2, q3_out_0.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cadf527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_out_0.decision_boundary(q3_out_0.u1, q3_out_0.v1, q3_out_0.b1, q3_out_0.u2, q3_out_0.v2, q3_out_0.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4362a9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.6408163265306123\n",
      "loss: 2.1890904903411865, accuracy: 0.6408163265306123\n",
      "loss: 1.6035627126693726, accuracy: 0.5510204081632653\n",
      "loss: 1.2505862712860107, accuracy: 0.5836734693877551\n",
      "max accuracy: 0.6857142857142857\n",
      "loss: 0.6219936013221741, accuracy: 0.6857142857142857\n",
      "loss: 0.5586686730384827, accuracy: 0.6857142857142857\n",
      "loss: 0.6065605878829956, accuracy: 0.6653061224489796\n",
      "max accuracy: 0.6938775510204082\n",
      "loss: 0.6452327966690063, accuracy: 0.6938775510204082\n",
      "loss: 0.5179097652435303, accuracy: 0.6571428571428571\n",
      "loss: 0.6396886706352234, accuracy: 0.6653061224489796\n",
      "loss: 0.5907300710678101, accuracy: 0.6775510204081633\n",
      "loss: 0.6348327398300171, accuracy: 0.6571428571428571\n",
      "loss: 0.5220153331756592, accuracy: 0.6653061224489796\n",
      "loss: 0.5351672768592834, accuracy: 0.6816326530612244\n",
      "loss: 0.5227401852607727, accuracy: 0.6816326530612244\n",
      "loss: 0.5127193331718445, accuracy: 0.673469387755102\n",
      "loss: 0.5170689225196838, accuracy: 0.6775510204081633\n",
      "loss: 0.4973594844341278, accuracy: 0.6326530612244898\n",
      "loss: 0.5360269546508789, accuracy: 0.6816326530612244\n",
      "loss: 0.5045682787895203, accuracy: 0.6816326530612244\n",
      "loss: 0.4929686486721039, accuracy: 0.6653061224489796\n",
      "loss: 0.49468350410461426, accuracy: 0.6938775510204082\n",
      "max accuracy: 0.710204081632653\n",
      "loss: 0.5034910440444946, accuracy: 0.710204081632653\n",
      "loss: 0.5030114054679871, accuracy: 0.6530612244897959\n",
      "loss: 0.5251043438911438, accuracy: 0.6857142857142857\n",
      "max accuracy: 0.7428571428571429\n",
      "loss: 0.480192631483078, accuracy: 0.7428571428571429\n",
      "loss: 0.5382717847824097, accuracy: 0.6693877551020408\n",
      "loss: 0.5442785024642944, accuracy: 0.6571428571428571\n",
      "loss: 0.5600587725639343, accuracy: 0.7020408163265306\n",
      "loss: 0.4959481656551361, accuracy: 0.6612244897959184\n",
      "loss: 0.48885080218315125, accuracy: 0.6653061224489796\n",
      "loss: 0.48456811904907227, accuracy: 0.689795918367347\n",
      "loss: 0.5081093907356262, accuracy: 0.6979591836734694\n",
      "loss: 0.48927611112594604, accuracy: 0.6693877551020408\n",
      "loss: 0.4966578483581543, accuracy: 0.689795918367347\n",
      "loss: 0.5110123157501221, accuracy: 0.6979591836734694\n",
      "loss: 0.5020380020141602, accuracy: 0.6693877551020408\n",
      "loss: 0.48088324069976807, accuracy: 0.6530612244897959\n",
      "loss: 0.48455843329429626, accuracy: 0.673469387755102\n",
      "loss: 0.47925516963005066, accuracy: 0.6979591836734694\n",
      "loss: 0.49742981791496277, accuracy: 0.6938775510204082\n",
      "loss: 0.4892506003379822, accuracy: 0.6979591836734694\n",
      "loss: 0.47837033867836, accuracy: 0.689795918367347\n",
      "loss: 0.4963928461074829, accuracy: 0.6979591836734694\n",
      "loss: 0.4785294532775879, accuracy: 0.7061224489795919\n",
      "loss: 0.4962915778160095, accuracy: 0.7020408163265306\n",
      "loss: 0.4772397577762604, accuracy: 0.6775510204081633\n",
      "loss: 0.512605607509613, accuracy: 0.7061224489795919\n",
      "loss: 0.49924224615097046, accuracy: 0.7224489795918367\n",
      "loss: 0.4843582808971405, accuracy: 0.7020408163265306\n",
      "loss: 0.48426076769828796, accuracy: 0.6857142857142857\n",
      "loss: 0.4922470450401306, accuracy: 0.689795918367347\n",
      "loss: 0.4876879155635834, accuracy: 0.689795918367347\n",
      "loss: 0.47305622696876526, accuracy: 0.7142857142857143\n",
      "loss: 0.49228644371032715, accuracy: 0.6979591836734694\n",
      "loss: 0.47452735900878906, accuracy: 0.7142857142857143\n",
      "loss: 0.47956782579421997, accuracy: 0.710204081632653\n",
      "loss: 0.47992169857025146, accuracy: 0.710204081632653\n",
      "loss: 0.46922579407691956, accuracy: 0.7306122448979592\n",
      "loss: 0.4719378650188446, accuracy: 0.726530612244898\n",
      "loss: 0.4799076020717621, accuracy: 0.7061224489795919\n",
      "loss: 0.484928160905838, accuracy: 0.7183673469387755\n",
      "loss: 0.47241950035095215, accuracy: 0.710204081632653\n",
      "loss: 0.4690473973751068, accuracy: 0.7346938775510204\n",
      "loss: 0.47773367166519165, accuracy: 0.7142857142857143\n",
      "loss: 0.47127875685691833, accuracy: 0.7020408163265306\n",
      "loss: 0.4688625633716583, accuracy: 0.7428571428571429\n",
      "loss: 0.46889418363571167, accuracy: 0.7142857142857143\n",
      "loss: 0.46581214666366577, accuracy: 0.7428571428571429\n",
      "loss: 0.4672397971153259, accuracy: 0.7346938775510204\n",
      "loss: 0.46734896302223206, accuracy: 0.710204081632653\n",
      "loss: 0.4710387885570526, accuracy: 0.7142857142857143\n",
      "loss: 0.464129239320755, accuracy: 0.7346938775510204\n",
      "max accuracy: 0.7673469387755102\n",
      "loss: 0.47328948974609375, accuracy: 0.7673469387755102\n",
      "loss: 0.4673630893230438, accuracy: 0.7183673469387755\n",
      "loss: 0.47473230957984924, accuracy: 0.7142857142857143\n",
      "loss: 0.4609782099723816, accuracy: 0.726530612244898\n",
      "loss: 0.4916975796222687, accuracy: 0.689795918367347\n",
      "loss: 0.4722115993499756, accuracy: 0.7142857142857143\n",
      "loss: 0.46784618496894836, accuracy: 0.689795918367347\n",
      "loss: 0.462815523147583, accuracy: 0.726530612244898\n",
      "loss: 0.45992332696914673, accuracy: 0.7346938775510204\n",
      "loss: 0.45857831835746765, accuracy: 0.7224489795918367\n",
      "loss: 0.46765580773353577, accuracy: 0.7387755102040816\n",
      "loss: 0.467720627784729, accuracy: 0.7020408163265306\n",
      "loss: 0.4646400511264801, accuracy: 0.7306122448979592\n",
      "loss: 0.4693414270877838, accuracy: 0.7428571428571429\n",
      "loss: 0.4661446213722229, accuracy: 0.7224489795918367\n",
      "loss: 0.46462124586105347, accuracy: 0.710204081632653\n",
      "loss: 0.46406883001327515, accuracy: 0.7387755102040816\n",
      "loss: 0.4565068185329437, accuracy: 0.7346938775510204\n",
      "loss: 0.4590047597885132, accuracy: 0.7306122448979592\n",
      "loss: 0.47059422731399536, accuracy: 0.7224489795918367\n",
      "loss: 0.45946598052978516, accuracy: 0.726530612244898\n",
      "loss: 0.4587015211582184, accuracy: 0.7387755102040816\n",
      "loss: 0.46057161688804626, accuracy: 0.710204081632653\n",
      "loss: 0.45821571350097656, accuracy: 0.726530612244898\n",
      "loss: 0.45531949400901794, accuracy: 0.746938775510204\n",
      "loss: 0.45765361189842224, accuracy: 0.726530612244898\n",
      "loss: 0.454188734292984, accuracy: 0.7346938775510204\n",
      "loss: 0.46962833404541016, accuracy: 0.7142857142857143\n",
      "loss: 0.4672442376613617, accuracy: 0.7224489795918367\n",
      "loss: 0.44821539521217346, accuracy: 0.746938775510204\n",
      "loss: 0.4520587921142578, accuracy: 0.710204081632653\n",
      "loss: 0.4642598032951355, accuracy: 0.726530612244898\n",
      "loss: 0.4540868103504181, accuracy: 0.726530612244898\n",
      "loss: 0.4535066783428192, accuracy: 0.7224489795918367\n",
      "loss: 0.4621743857860565, accuracy: 0.7183673469387755\n",
      "loss: 0.4529343843460083, accuracy: 0.7346938775510204\n",
      "loss: 0.44727906584739685, accuracy: 0.7673469387755102\n",
      "loss: 0.4520501494407654, accuracy: 0.7428571428571429\n",
      "loss: 0.4552368223667145, accuracy: 0.7224489795918367\n",
      "loss: 0.4490523338317871, accuracy: 0.7224489795918367\n",
      "loss: 0.455894410610199, accuracy: 0.7387755102040816\n",
      "loss: 0.4628543257713318, accuracy: 0.7346938775510204\n",
      "loss: 0.44982534646987915, accuracy: 0.7428571428571429\n",
      "loss: 0.4497961401939392, accuracy: 0.7510204081632653\n",
      "loss: 0.45363423228263855, accuracy: 0.7387755102040816\n",
      "loss: 0.449799507856369, accuracy: 0.726530612244898\n",
      "loss: 0.4530605673789978, accuracy: 0.7551020408163265\n",
      "loss: 0.44799578189849854, accuracy: 0.7346938775510204\n",
      "loss: 0.4555261433124542, accuracy: 0.7346938775510204\n",
      "loss: 0.4651474058628082, accuracy: 0.7020408163265306\n",
      "loss: 0.4506882131099701, accuracy: 0.7142857142857143\n",
      "loss: 0.4494638741016388, accuracy: 0.7346938775510204\n",
      "max accuracy: 0.7755102040816326\n",
      "loss: 0.4547739326953888, accuracy: 0.7755102040816326\n",
      "loss: 0.45086008310317993, accuracy: 0.7591836734693878\n",
      "loss: 0.4525459408760071, accuracy: 0.7551020408163265\n",
      "loss: 0.45981401205062866, accuracy: 0.7306122448979592\n",
      "loss: 0.4563782215118408, accuracy: 0.7428571428571429\n",
      "loss: 0.4571257531642914, accuracy: 0.7428571428571429\n",
      "loss: 0.45116767287254333, accuracy: 0.7387755102040816\n",
      "loss: 0.44196322560310364, accuracy: 0.7510204081632653\n",
      "loss: 0.4486807882785797, accuracy: 0.763265306122449\n",
      "loss: 0.44912007451057434, accuracy: 0.7306122448979592\n",
      "loss: 0.4482986330986023, accuracy: 0.7306122448979592\n",
      "loss: 0.4449128210544586, accuracy: 0.763265306122449\n",
      "loss: 0.44588929414749146, accuracy: 0.7346938775510204\n",
      "loss: 0.4518768787384033, accuracy: 0.7346938775510204\n",
      "loss: 0.44829311966896057, accuracy: 0.7510204081632653\n",
      "loss: 0.44500645995140076, accuracy: 0.746938775510204\n",
      "loss: 0.44259893894195557, accuracy: 0.7346938775510204\n",
      "loss: 0.46640172600746155, accuracy: 0.7142857142857143\n",
      "loss: 0.449777215719223, accuracy: 0.7428571428571429\n",
      "loss: 0.44099491834640503, accuracy: 0.7510204081632653\n",
      "loss: 0.45342838764190674, accuracy: 0.7306122448979592\n",
      "loss: 0.44153088331222534, accuracy: 0.7346938775510204\n",
      "loss: 0.4483998715877533, accuracy: 0.7551020408163265\n",
      "loss: 0.43918856978416443, accuracy: 0.746938775510204\n",
      "loss: 0.4422930181026459, accuracy: 0.7346938775510204\n",
      "loss: 0.44035080075263977, accuracy: 0.7510204081632653\n",
      "loss: 0.441163569688797, accuracy: 0.7510204081632653\n",
      "loss: 0.45095816254615784, accuracy: 0.726530612244898\n",
      "loss: 0.4447600245475769, accuracy: 0.7551020408163265\n",
      "loss: 0.4396047592163086, accuracy: 0.746938775510204\n",
      "loss: 0.43561384081840515, accuracy: 0.763265306122449\n",
      "loss: 0.44133254885673523, accuracy: 0.746938775510204\n",
      "max accuracy: 0.7795918367346939\n",
      "loss: 0.4357606768608093, accuracy: 0.7795918367346939\n",
      "loss: 0.44295454025268555, accuracy: 0.7551020408163265\n",
      "loss: 0.43106454610824585, accuracy: 0.7755102040816326\n",
      "loss: 0.44269493222236633, accuracy: 0.726530612244898\n",
      "loss: 0.4414573609828949, accuracy: 0.7673469387755102\n",
      "loss: 0.4454573094844818, accuracy: 0.7387755102040816\n",
      "loss: 0.43765681982040405, accuracy: 0.7510204081632653\n",
      "loss: 0.44109514355659485, accuracy: 0.7183673469387755\n",
      "loss: 0.44399380683898926, accuracy: 0.7387755102040816\n",
      "loss: 0.43775391578674316, accuracy: 0.7551020408163265\n",
      "loss: 0.4427797198295593, accuracy: 0.7387755102040816\n",
      "loss: 0.44010624289512634, accuracy: 0.7306122448979592\n",
      "loss: 0.43874308466911316, accuracy: 0.7591836734693878\n",
      "loss: 0.44735684990882874, accuracy: 0.7387755102040816\n",
      "loss: 0.4436371326446533, accuracy: 0.7755102040816326\n",
      "loss: 0.44598153233528137, accuracy: 0.726530612244898\n",
      "loss: 0.43842077255249023, accuracy: 0.7591836734693878\n",
      "loss: 0.43518468737602234, accuracy: 0.7591836734693878\n",
      "loss: 0.447086900472641, accuracy: 0.7306122448979592\n",
      "loss: 0.43604525923728943, accuracy: 0.726530612244898\n",
      "loss: 0.4436034560203552, accuracy: 0.7551020408163265\n",
      "loss: 0.4372158944606781, accuracy: 0.7591836734693878\n",
      "loss: 0.43953248858451843, accuracy: 0.7591836734693878\n",
      "loss: 0.4368496835231781, accuracy: 0.7591836734693878\n",
      "loss: 0.4324685037136078, accuracy: 0.7714285714285715\n",
      "loss: 0.4406891465187073, accuracy: 0.7591836734693878\n",
      "loss: 0.4363311529159546, accuracy: 0.7591836734693878\n",
      "loss: 0.4330675005912781, accuracy: 0.763265306122449\n",
      "loss: 0.43741118907928467, accuracy: 0.7591836734693878\n",
      "loss: 0.43618205189704895, accuracy: 0.763265306122449\n",
      "loss: 0.4388672411441803, accuracy: 0.746938775510204\n",
      "loss: 0.4347737431526184, accuracy: 0.7428571428571429\n",
      "loss: 0.4333886504173279, accuracy: 0.7673469387755102\n",
      "loss: 0.43812909722328186, accuracy: 0.746938775510204\n",
      "loss: 0.4345555901527405, accuracy: 0.7551020408163265\n",
      "loss: 0.43351858854293823, accuracy: 0.7591836734693878\n",
      "loss: 0.4329115152359009, accuracy: 0.763265306122449\n",
      "loss: 0.44593507051467896, accuracy: 0.7551020408163265\n",
      "loss: 0.4305417835712433, accuracy: 0.7510204081632653\n",
      "loss: 0.43079590797424316, accuracy: 0.7714285714285715\n",
      "loss: 0.44183552265167236, accuracy: 0.763265306122449\n",
      "loss: 0.43868812918663025, accuracy: 0.7510204081632653\n",
      "loss: 0.4329012930393219, accuracy: 0.7551020408163265\n",
      "loss: 0.4323258399963379, accuracy: 0.7510204081632653\n",
      "loss: 0.43616944551467896, accuracy: 0.7591836734693878\n",
      "loss: 0.4290880560874939, accuracy: 0.7714285714285715\n",
      "loss: 0.4381820559501648, accuracy: 0.7591836734693878\n",
      "loss: 0.43134599924087524, accuracy: 0.7428571428571429\n",
      "loss: 0.4401099979877472, accuracy: 0.7387755102040816\n",
      "loss: 0.42816361784935, accuracy: 0.7673469387755102\n",
      "loss: 0.4297553598880768, accuracy: 0.7673469387755102\n",
      "loss: 0.4337920844554901, accuracy: 0.7591836734693878\n",
      "loss: 0.4331844747066498, accuracy: 0.7510204081632653\n",
      "loss: 0.43031951785087585, accuracy: 0.763265306122449\n",
      "loss: 0.43449416756629944, accuracy: 0.746938775510204\n",
      "loss: 0.4301966726779938, accuracy: 0.7673469387755102\n",
      "loss: 0.42898598313331604, accuracy: 0.7714285714285715\n",
      "loss: 0.4276958405971527, accuracy: 0.7591836734693878\n",
      "loss: 0.42874741554260254, accuracy: 0.7795918367346939\n",
      "loss: 0.4397030472755432, accuracy: 0.7551020408163265\n",
      "loss: 0.424737811088562, accuracy: 0.7714285714285715\n",
      "loss: 0.4357965588569641, accuracy: 0.7673469387755102\n",
      "loss: 0.429366797208786, accuracy: 0.7755102040816326\n",
      "loss: 0.43156898021698, accuracy: 0.7551020408163265\n",
      "loss: 0.4282021224498749, accuracy: 0.7591836734693878\n",
      "loss: 0.43292027711868286, accuracy: 0.7510204081632653\n",
      "max accuracy: 0.7877551020408163\n",
      "loss: 0.4223634600639343, accuracy: 0.7877551020408163\n",
      "loss: 0.42697814106941223, accuracy: 0.7673469387755102\n",
      "loss: 0.42674878239631653, accuracy: 0.7551020408163265\n",
      "loss: 0.42614462971687317, accuracy: 0.7795918367346939\n",
      "max accuracy: 0.8\n",
      "loss: 0.4239785671234131, accuracy: 0.8\n",
      "loss: 0.4311011731624603, accuracy: 0.7306122448979592\n",
      "loss: 0.42710191011428833, accuracy: 0.7714285714285715\n",
      "loss: 0.4302197992801666, accuracy: 0.7673469387755102\n",
      "loss: 0.4289686679840088, accuracy: 0.7755102040816326\n",
      "loss: 0.4288959503173828, accuracy: 0.763265306122449\n",
      "loss: 0.4261822998523712, accuracy: 0.7755102040816326\n",
      "loss: 0.423239141702652, accuracy: 0.7714285714285715\n",
      "loss: 0.42155900597572327, accuracy: 0.7673469387755102\n",
      "loss: 0.4222327768802643, accuracy: 0.7755102040816326\n",
      "loss: 0.4260824918746948, accuracy: 0.7591836734693878\n",
      "loss: 0.4256731867790222, accuracy: 0.7551020408163265\n",
      "loss: 0.42465853691101074, accuracy: 0.7714285714285715\n",
      "loss: 0.4237610101699829, accuracy: 0.7714285714285715\n",
      "loss: 0.43130671977996826, accuracy: 0.7714285714285715\n",
      "loss: 0.42916959524154663, accuracy: 0.7755102040816326\n",
      "loss: 0.42518070340156555, accuracy: 0.7591836734693878\n",
      "loss: 0.4272078573703766, accuracy: 0.763265306122449\n",
      "loss: 0.42646220326423645, accuracy: 0.7836734693877551\n",
      "loss: 0.4262389838695526, accuracy: 0.7673469387755102\n",
      "loss: 0.42823952436447144, accuracy: 0.763265306122449\n",
      "loss: 0.4259242117404938, accuracy: 0.7714285714285715\n",
      "loss: 0.42337262630462646, accuracy: 0.7551020408163265\n",
      "loss: 0.42188671231269836, accuracy: 0.7673469387755102\n",
      "loss: 0.4211784601211548, accuracy: 0.7510204081632653\n",
      "loss: 0.4269731342792511, accuracy: 0.763265306122449\n",
      "loss: 0.4224773943424225, accuracy: 0.7836734693877551\n",
      "loss: 0.4238947033882141, accuracy: 0.7673469387755102\n",
      "loss: 0.4239022433757782, accuracy: 0.7673469387755102\n",
      "loss: 0.42542675137519836, accuracy: 0.7795918367346939\n",
      "loss: 0.4332268238067627, accuracy: 0.7551020408163265\n",
      "loss: 0.42061784863471985, accuracy: 0.763265306122449\n",
      "loss: 0.41719695925712585, accuracy: 0.7714285714285715\n",
      "loss: 0.42470645904541016, accuracy: 0.7714285714285715\n",
      "loss: 0.4232935607433319, accuracy: 0.7714285714285715\n",
      "loss: 0.4173346161842346, accuracy: 0.7714285714285715\n",
      "loss: 0.41714558005332947, accuracy: 0.7795918367346939\n",
      "loss: 0.41899943351745605, accuracy: 0.763265306122449\n",
      "loss: 0.41868600249290466, accuracy: 0.7673469387755102\n",
      "loss: 0.4178548753261566, accuracy: 0.7714285714285715\n",
      "loss: 0.42909055948257446, accuracy: 0.763265306122449\n",
      "loss: 0.4265700876712799, accuracy: 0.763265306122449\n",
      "loss: 0.414166122674942, accuracy: 0.7591836734693878\n",
      "loss: 0.419524610042572, accuracy: 0.7714285714285715\n",
      "loss: 0.4173654019832611, accuracy: 0.7673469387755102\n",
      "loss: 0.42105957865715027, accuracy: 0.7918367346938775\n",
      "loss: 0.42129868268966675, accuracy: 0.7510204081632653\n",
      "loss: 0.4226332902908325, accuracy: 0.7795918367346939\n",
      "loss: 0.42023879289627075, accuracy: 0.7836734693877551\n",
      "loss: 0.4178820252418518, accuracy: 0.7755102040816326\n",
      "loss: 0.4169061779975891, accuracy: 0.7755102040816326\n",
      "loss: 0.4219314455986023, accuracy: 0.7510204081632653\n",
      "loss: 0.4169282615184784, accuracy: 0.7959183673469388\n",
      "loss: 0.4197542369365692, accuracy: 0.7510204081632653\n",
      "loss: 0.4195607900619507, accuracy: 0.8\n",
      "loss: 0.4199381470680237, accuracy: 0.7551020408163265\n",
      "loss: 0.4182102084159851, accuracy: 0.7755102040816326\n",
      "loss: 0.4213612675666809, accuracy: 0.7918367346938775\n",
      "loss: 0.42120009660720825, accuracy: 0.763265306122449\n",
      "loss: 0.41612571477890015, accuracy: 0.7673469387755102\n",
      "loss: 0.41862550377845764, accuracy: 0.7836734693877551\n",
      "loss: 0.425291508436203, accuracy: 0.7510204081632653\n",
      "loss: 0.42235568165779114, accuracy: 0.763265306122449\n",
      "loss: 0.415678471326828, accuracy: 0.7836734693877551\n",
      "loss: 0.41670235991477966, accuracy: 0.7755102040816326\n",
      "loss: 0.4116365909576416, accuracy: 0.7836734693877551\n",
      "loss: 0.40918248891830444, accuracy: 0.7795918367346939\n",
      "loss: 0.41861847043037415, accuracy: 0.7673469387755102\n",
      "loss: 0.413337379693985, accuracy: 0.7673469387755102\n",
      "loss: 0.41801854968070984, accuracy: 0.7836734693877551\n",
      "loss: 0.4229569733142853, accuracy: 0.7673469387755102\n",
      "loss: 0.415467232465744, accuracy: 0.7836734693877551\n",
      "loss: 0.41371989250183105, accuracy: 0.7836734693877551\n",
      "loss: 0.408135324716568, accuracy: 0.7836734693877551\n",
      "loss: 0.4195345640182495, accuracy: 0.7877551020408163\n",
      "loss: 0.4123160243034363, accuracy: 0.7591836734693878\n",
      "loss: 0.41016581654548645, accuracy: 0.7795918367346939\n",
      "loss: 0.413928359746933, accuracy: 0.7877551020408163\n",
      "loss: 0.41424238681793213, accuracy: 0.7877551020408163\n",
      "loss: 0.4149671196937561, accuracy: 0.7755102040816326\n",
      "loss: 0.4138992130756378, accuracy: 0.7795918367346939\n",
      "loss: 0.40813159942626953, accuracy: 0.7918367346938775\n",
      "loss: 0.41341790556907654, accuracy: 0.7755102040816326\n",
      "loss: 0.41345012187957764, accuracy: 0.7836734693877551\n",
      "loss: 0.4138390123844147, accuracy: 0.7795918367346939\n",
      "loss: 0.4114862084388733, accuracy: 0.7795918367346939\n",
      "loss: 0.41104811429977417, accuracy: 0.7755102040816326\n",
      "loss: 0.42107683420181274, accuracy: 0.7673469387755102\n",
      "loss: 0.4151349663734436, accuracy: 0.7755102040816326\n",
      "loss: 0.40912291407585144, accuracy: 0.7755102040816326\n",
      "max accuracy: 0.8122448979591836\n",
      "loss: 0.40631166100502014, accuracy: 0.8122448979591836\n",
      "loss: 0.4131821393966675, accuracy: 0.7755102040816326\n",
      "loss: 0.410300076007843, accuracy: 0.7795918367346939\n",
      "loss: 0.40459150075912476, accuracy: 0.7836734693877551\n",
      "loss: 0.41079267859458923, accuracy: 0.7836734693877551\n",
      "loss: 0.41138556599617004, accuracy: 0.7918367346938775\n",
      "loss: 0.42029887437820435, accuracy: 0.7591836734693878\n",
      "loss: 0.4037690758705139, accuracy: 0.8040816326530612\n",
      "loss: 0.4094908535480499, accuracy: 0.8081632653061225\n",
      "loss: 0.40953534841537476, accuracy: 0.7673469387755102\n",
      "loss: 0.4082607626914978, accuracy: 0.7673469387755102\n",
      "loss: 0.4118679165840149, accuracy: 0.7918367346938775\n",
      "loss: 0.41402482986450195, accuracy: 0.7591836734693878\n",
      "loss: 0.416719526052475, accuracy: 0.7755102040816326\n",
      "loss: 0.40408509969711304, accuracy: 0.8\n",
      "loss: 0.40902283787727356, accuracy: 0.8\n",
      "loss: 0.4065799415111542, accuracy: 0.7959183673469388\n",
      "loss: 0.4135339558124542, accuracy: 0.7877551020408163\n",
      "loss: 0.40172845125198364, accuracy: 0.8\n",
      "loss: 0.40687063336372375, accuracy: 0.8040816326530612\n",
      "loss: 0.4065932631492615, accuracy: 0.7918367346938775\n",
      "loss: 0.4065445065498352, accuracy: 0.8\n",
      "loss: 0.4037396013736725, accuracy: 0.7795918367346939\n",
      "loss: 0.4127585291862488, accuracy: 0.7877551020408163\n",
      "loss: 0.41173800826072693, accuracy: 0.7918367346938775\n",
      "loss: 0.41999560594558716, accuracy: 0.7918367346938775\n",
      "loss: 0.4002276360988617, accuracy: 0.8122448979591836\n",
      "loss: 0.4135492146015167, accuracy: 0.7918367346938775\n",
      "loss: 0.40625810623168945, accuracy: 0.7795918367346939\n",
      "loss: 0.4037894606590271, accuracy: 0.7877551020408163\n",
      "loss: 0.4078439474105835, accuracy: 0.7836734693877551\n",
      "loss: 0.41214990615844727, accuracy: 0.7959183673469388\n",
      "loss: 0.4036172330379486, accuracy: 0.7918367346938775\n",
      "loss: 0.41017165780067444, accuracy: 0.7918367346938775\n",
      "loss: 0.40633270144462585, accuracy: 0.7877551020408163\n",
      "loss: 0.4136500060558319, accuracy: 0.7673469387755102\n",
      "max accuracy: 0.8163265306122449\n",
      "loss: 0.40457814931869507, accuracy: 0.8163265306122449\n",
      "loss: 0.40612322092056274, accuracy: 0.7755102040816326\n",
      "loss: 0.40186795592308044, accuracy: 0.7959183673469388\n",
      "loss: 0.40151166915893555, accuracy: 0.8\n",
      "loss: 0.3960517346858978, accuracy: 0.7795918367346939\n",
      "loss: 0.40415313839912415, accuracy: 0.7673469387755102\n",
      "loss: 0.40588852763175964, accuracy: 0.8163265306122449\n",
      "loss: 0.40298140048980713, accuracy: 0.7918367346938775\n",
      "loss: 0.4106087386608124, accuracy: 0.7918367346938775\n",
      "loss: 0.4029749929904938, accuracy: 0.7918367346938775\n",
      "loss: 0.40128615498542786, accuracy: 0.8081632653061225\n",
      "loss: 0.3963626027107239, accuracy: 0.8\n",
      "loss: 0.40028688311576843, accuracy: 0.7959183673469388\n",
      "loss: 0.39750581979751587, accuracy: 0.7959183673469388\n",
      "loss: 0.40529951453208923, accuracy: 0.8040816326530612\n",
      "loss: 0.4082002639770508, accuracy: 0.7755102040816326\n",
      "loss: 0.40446603298187256, accuracy: 0.7877551020408163\n",
      "loss: 0.4014241397380829, accuracy: 0.8122448979591836\n",
      "loss: 0.3984510898590088, accuracy: 0.7918367346938775\n",
      "loss: 0.39697763323783875, accuracy: 0.8081632653061225\n",
      "loss: 0.399375855922699, accuracy: 0.7959183673469388\n",
      "loss: 0.400901734828949, accuracy: 0.7714285714285715\n",
      "loss: 0.3982168734073639, accuracy: 0.8040816326530612\n",
      "loss: 0.40237197279930115, accuracy: 0.7755102040816326\n",
      "loss: 0.40893295407295227, accuracy: 0.8081632653061225\n",
      "loss: 0.39940619468688965, accuracy: 0.7959183673469388\n",
      "max accuracy: 0.8204081632653061\n",
      "loss: 0.3946410119533539, accuracy: 0.8204081632653061\n",
      "loss: 0.3963063359260559, accuracy: 0.7959183673469388\n",
      "loss: 0.3970414996147156, accuracy: 0.8040816326530612\n",
      "loss: 0.39887523651123047, accuracy: 0.8\n",
      "loss: 0.39656680822372437, accuracy: 0.8081632653061225\n",
      "loss: 0.3969167470932007, accuracy: 0.8204081632653061\n",
      "loss: 0.3914734423160553, accuracy: 0.8040816326530612\n",
      "loss: 0.3950192630290985, accuracy: 0.8122448979591836\n",
      "loss: 0.4004147946834564, accuracy: 0.8\n",
      "loss: 0.4036611020565033, accuracy: 0.7877551020408163\n",
      "loss: 0.3956820070743561, accuracy: 0.8081632653061225\n",
      "max accuracy: 0.8244897959183674\n",
      "loss: 0.403970867395401, accuracy: 0.8244897959183674\n",
      "loss: 0.392106294631958, accuracy: 0.8081632653061225\n",
      "loss: 0.4022667109966278, accuracy: 0.8081632653061225\n",
      "loss: 0.39453667402267456, accuracy: 0.8163265306122449\n",
      "loss: 0.3938148021697998, accuracy: 0.8040816326530612\n",
      "loss: 0.3921072483062744, accuracy: 0.8\n",
      "loss: 0.40745213627815247, accuracy: 0.7959183673469388\n",
      "loss: 0.39148667454719543, accuracy: 0.8122448979591836\n",
      "loss: 0.39044123888015747, accuracy: 0.8204081632653061\n",
      "loss: 0.3882698118686676, accuracy: 0.8081632653061225\n",
      "loss: 0.39446282386779785, accuracy: 0.8\n",
      "max accuracy: 0.8285714285714286\n",
      "loss: 0.3910285234451294, accuracy: 0.8285714285714286\n",
      "loss: 0.38415172696113586, accuracy: 0.8\n",
      "loss: 0.38872939348220825, accuracy: 0.8081632653061225\n",
      "loss: 0.39041081070899963, accuracy: 0.7877551020408163\n",
      "loss: 0.4118136763572693, accuracy: 0.7755102040816326\n",
      "loss: 0.39557430148124695, accuracy: 0.8122448979591836\n",
      "max accuracy: 0.8326530612244898\n",
      "loss: 0.3857560157775879, accuracy: 0.8326530612244898\n",
      "loss: 0.394337922334671, accuracy: 0.8204081632653061\n",
      "loss: 0.38505300879478455, accuracy: 0.7755102040816326\n",
      "loss: 0.39723268151283264, accuracy: 0.7918367346938775\n",
      "loss: 0.3932936191558838, accuracy: 0.8040816326530612\n",
      "loss: 0.3947133421897888, accuracy: 0.7795918367346939\n",
      "loss: 0.38555708527565, accuracy: 0.8081632653061225\n",
      "loss: 0.38685697317123413, accuracy: 0.8081632653061225\n",
      "loss: 0.3960180878639221, accuracy: 0.8122448979591836\n",
      "loss: 0.3885706067085266, accuracy: 0.8\n",
      "loss: 0.3949427306652069, accuracy: 0.8040816326530612\n",
      "loss: 0.38835397362709045, accuracy: 0.7755102040816326\n",
      "loss: 0.39309343695640564, accuracy: 0.8040816326530612\n",
      "loss: 0.39080166816711426, accuracy: 0.8204081632653061\n",
      "loss: 0.3917054831981659, accuracy: 0.7959183673469388\n",
      "loss: 0.3851432502269745, accuracy: 0.8\n",
      "loss: 0.3943718671798706, accuracy: 0.7877551020408163\n",
      "loss: 0.3872796595096588, accuracy: 0.8163265306122449\n",
      "loss: 0.3873960077762604, accuracy: 0.8040816326530612\n",
      "loss: 0.3835037052631378, accuracy: 0.8040816326530612\n",
      "loss: 0.3877497911453247, accuracy: 0.7959183673469388\n",
      "loss: 0.3972768783569336, accuracy: 0.8\n",
      "loss: 0.3869233727455139, accuracy: 0.8\n",
      "loss: 0.39419299364089966, accuracy: 0.7918367346938775\n",
      "loss: 0.38810640573501587, accuracy: 0.8081632653061225\n",
      "loss: 0.3868775963783264, accuracy: 0.8081632653061225\n",
      "loss: 0.39380428194999695, accuracy: 0.8040816326530612\n",
      "loss: 0.384520947933197, accuracy: 0.8040816326530612\n",
      "loss: 0.3925420343875885, accuracy: 0.8040816326530612\n",
      "loss: 0.3865603804588318, accuracy: 0.8204081632653061\n",
      "max accuracy: 0.8367346938775511\n",
      "loss: 0.3777199983596802, accuracy: 0.8367346938775511\n",
      "loss: 0.3896810710430145, accuracy: 0.8040816326530612\n",
      "loss: 0.39748603105545044, accuracy: 0.7959183673469388\n",
      "loss: 0.38043731451034546, accuracy: 0.8081632653061225\n",
      "loss: 0.3927212357521057, accuracy: 0.8081632653061225\n",
      "loss: 0.39537250995635986, accuracy: 0.8040816326530612\n",
      "loss: 0.3839929699897766, accuracy: 0.8040816326530612\n",
      "loss: 0.387795090675354, accuracy: 0.8040816326530612\n",
      "loss: 0.3859837055206299, accuracy: 0.7877551020408163\n",
      "loss: 0.38484689593315125, accuracy: 0.8163265306122449\n",
      "loss: 0.38799571990966797, accuracy: 0.7959183673469388\n",
      "loss: 0.3776702880859375, accuracy: 0.8244897959183674\n",
      "loss: 0.3902413845062256, accuracy: 0.8040816326530612\n",
      "loss: 0.3917776048183441, accuracy: 0.8163265306122449\n",
      "loss: 0.38985714316368103, accuracy: 0.8163265306122449\n",
      "loss: 0.3887835741043091, accuracy: 0.7918367346938775\n",
      "loss: 0.3799130320549011, accuracy: 0.8204081632653061\n",
      "loss: 0.3736169934272766, accuracy: 0.8163265306122449\n",
      "loss: 0.38583841919898987, accuracy: 0.8122448979591836\n",
      "loss: 0.3813900649547577, accuracy: 0.8\n",
      "loss: 0.3931560814380646, accuracy: 0.7795918367346939\n",
      "loss: 0.3858564496040344, accuracy: 0.8244897959183674\n",
      "loss: 0.38540899753570557, accuracy: 0.8\n",
      "loss: 0.3787746727466583, accuracy: 0.8\n",
      "loss: 0.38184791803359985, accuracy: 0.8204081632653061\n",
      "loss: 0.3829802870750427, accuracy: 0.8040816326530612\n",
      "loss: 0.3763938248157501, accuracy: 0.8122448979591836\n",
      "loss: 0.3921407163143158, accuracy: 0.7836734693877551\n",
      "loss: 0.3795969784259796, accuracy: 0.8163265306122449\n",
      "loss: 0.3802110552787781, accuracy: 0.8122448979591836\n",
      "loss: 0.3859318494796753, accuracy: 0.8122448979591836\n",
      "loss: 0.38757380843162537, accuracy: 0.7795918367346939\n",
      "loss: 0.3876393735408783, accuracy: 0.7918367346938775\n",
      "loss: 0.37877270579338074, accuracy: 0.8204081632653061\n",
      "loss: 0.3805324137210846, accuracy: 0.8\n",
      "loss: 0.37903669476509094, accuracy: 0.8163265306122449\n",
      "loss: 0.3849581182003021, accuracy: 0.8040816326530612\n",
      "loss: 0.38138824701309204, accuracy: 0.8204081632653061\n",
      "loss: 0.3897407352924347, accuracy: 0.7918367346938775\n",
      "loss: 0.3761683702468872, accuracy: 0.8163265306122449\n",
      "loss: 0.3743884563446045, accuracy: 0.8204081632653061\n",
      "loss: 0.37133610248565674, accuracy: 0.8081632653061225\n",
      "loss: 0.38482898473739624, accuracy: 0.8081632653061225\n",
      "loss: 0.37380602955818176, accuracy: 0.8204081632653061\n",
      "loss: 0.37676694989204407, accuracy: 0.8\n",
      "loss: 0.37525713443756104, accuracy: 0.8040816326530612\n",
      "loss: 0.3970949351787567, accuracy: 0.8040816326530612\n",
      "loss: 0.38076066970825195, accuracy: 0.8244897959183674\n",
      "loss: 0.3832276463508606, accuracy: 0.8122448979591836\n",
      "loss: 0.38227853178977966, accuracy: 0.7959183673469388\n",
      "loss: 0.3798615336418152, accuracy: 0.8122448979591836\n",
      "loss: 0.37458181381225586, accuracy: 0.8040816326530612\n",
      "loss: 0.3823564350605011, accuracy: 0.7959183673469388\n",
      "loss: 0.3756985366344452, accuracy: 0.8285714285714286\n",
      "loss: 0.37413933873176575, accuracy: 0.8285714285714286\n",
      "loss: 0.374203085899353, accuracy: 0.8122448979591836\n",
      "loss: 0.38269296288490295, accuracy: 0.8081632653061225\n",
      "loss: 0.36915120482444763, accuracy: 0.8122448979591836\n",
      "loss: 0.3956643044948578, accuracy: 0.8081632653061225\n",
      "loss: 0.3790830671787262, accuracy: 0.8040816326530612\n",
      "loss: 0.3710765540599823, accuracy: 0.8163265306122449\n",
      "loss: 0.37824851274490356, accuracy: 0.8\n",
      "loss: 0.37703055143356323, accuracy: 0.8040816326530612\n",
      "loss: 0.3823152780532837, accuracy: 0.8122448979591836\n",
      "loss: 0.3695567548274994, accuracy: 0.8244897959183674\n",
      "loss: 0.3807559311389923, accuracy: 0.8122448979591836\n",
      "loss: 0.3792913854122162, accuracy: 0.8081632653061225\n",
      "loss: 0.3750142753124237, accuracy: 0.8122448979591836\n",
      "loss: 0.37852004170417786, accuracy: 0.8081632653061225\n",
      "loss: 0.3707447052001953, accuracy: 0.8122448979591836\n",
      "loss: 0.3705809414386749, accuracy: 0.8081632653061225\n",
      "loss: 0.3747469186782837, accuracy: 0.8244897959183674\n",
      "loss: 0.36166998744010925, accuracy: 0.8204081632653061\n",
      "loss: 0.37193775177001953, accuracy: 0.7836734693877551\n",
      "loss: 0.3759153187274933, accuracy: 0.8163265306122449\n",
      "loss: 0.36828792095184326, accuracy: 0.8163265306122449\n",
      "loss: 0.3666302561759949, accuracy: 0.8081632653061225\n",
      "loss: 0.37784603238105774, accuracy: 0.8\n",
      "loss: 0.3778010904788971, accuracy: 0.8081632653061225\n",
      "loss: 0.38005223870277405, accuracy: 0.8040816326530612\n",
      "loss: 0.36831745505332947, accuracy: 0.8163265306122449\n",
      "loss: 0.3705471456050873, accuracy: 0.8204081632653061\n",
      "loss: 0.36957424879074097, accuracy: 0.8285714285714286\n",
      "loss: 0.3704695999622345, accuracy: 0.8204081632653061\n",
      "loss: 0.37655192613601685, accuracy: 0.8122448979591836\n",
      "loss: 0.3716299831867218, accuracy: 0.7877551020408163\n",
      "loss: 0.3665434718132019, accuracy: 0.8244897959183674\n",
      "loss: 0.370108425617218, accuracy: 0.8163265306122449\n",
      "loss: 0.3657178580760956, accuracy: 0.8163265306122449\n",
      "loss: 0.37689492106437683, accuracy: 0.8081632653061225\n",
      "loss: 0.3716493248939514, accuracy: 0.8122448979591836\n",
      "loss: 0.3703565001487732, accuracy: 0.8204081632653061\n",
      "loss: 0.3717101812362671, accuracy: 0.8040816326530612\n",
      "loss: 0.3658547103404999, accuracy: 0.8367346938775511\n",
      "loss: 0.360306978225708, accuracy: 0.8367346938775511\n",
      "loss: 0.3787716329097748, accuracy: 0.8040816326530612\n",
      "loss: 0.3667885363101959, accuracy: 0.8163265306122449\n",
      "loss: 0.3810003399848938, accuracy: 0.8163265306122449\n",
      "loss: 0.36822888255119324, accuracy: 0.8163265306122449\n",
      "loss: 0.36998334527015686, accuracy: 0.8204081632653061\n",
      "loss: 0.3721638023853302, accuracy: 0.8081632653061225\n",
      "loss: 0.3623458743095398, accuracy: 0.8244897959183674\n",
      "loss: 0.3730766177177429, accuracy: 0.8204081632653061\n",
      "loss: 0.3647153377532959, accuracy: 0.8326530612244898\n",
      "loss: 0.37356123328208923, accuracy: 0.8244897959183674\n",
      "loss: 0.3666193187236786, accuracy: 0.8244897959183674\n",
      "loss: 0.3614247739315033, accuracy: 0.8244897959183674\n",
      "loss: 0.3669321537017822, accuracy: 0.8204081632653061\n",
      "loss: 0.3661938011646271, accuracy: 0.8204081632653061\n",
      "loss: 0.36522042751312256, accuracy: 0.8326530612244898\n",
      "loss: 0.3655582666397095, accuracy: 0.8244897959183674\n",
      "loss: 0.3722519874572754, accuracy: 0.8\n",
      "loss: 0.3657280504703522, accuracy: 0.8326530612244898\n",
      "loss: 0.3659636080265045, accuracy: 0.7918367346938775\n",
      "loss: 0.37542498111724854, accuracy: 0.8244897959183674\n",
      "loss: 0.3722590506076813, accuracy: 0.8081632653061225\n",
      "loss: 0.3700612187385559, accuracy: 0.8122448979591836\n",
      "loss: 0.38149142265319824, accuracy: 0.8122448979591836\n",
      "loss: 0.36333510279655457, accuracy: 0.8163265306122449\n",
      "loss: 0.360355406999588, accuracy: 0.8163265306122449\n",
      "loss: 0.3705078661441803, accuracy: 0.7959183673469388\n",
      "loss: 0.37265631556510925, accuracy: 0.8122448979591836\n",
      "loss: 0.36038273572921753, accuracy: 0.8244897959183674\n",
      "loss: 0.365146666765213, accuracy: 0.8204081632653061\n",
      "loss: 0.3679504096508026, accuracy: 0.8\n",
      "loss: 0.35780662298202515, accuracy: 0.8367346938775511\n",
      "loss: 0.37862130999565125, accuracy: 0.7959183673469388\n",
      "loss: 0.3620498478412628, accuracy: 0.8285714285714286\n",
      "loss: 0.37476110458374023, accuracy: 0.8081632653061225\n",
      "loss: 0.3672727942466736, accuracy: 0.8040816326530612\n",
      "loss: 0.3584817051887512, accuracy: 0.8081632653061225\n",
      "loss: 0.3635249733924866, accuracy: 0.8122448979591836\n",
      "loss: 0.36515623331069946, accuracy: 0.8244897959183674\n",
      "max accuracy: 0.8489795918367347\n",
      "loss: 0.36782434582710266, accuracy: 0.8489795918367347\n",
      "loss: 0.3619084358215332, accuracy: 0.8244897959183674\n",
      "loss: 0.3575575053691864, accuracy: 0.8367346938775511\n",
      "loss: 0.3663492500782013, accuracy: 0.8122448979591836\n",
      "loss: 0.36202719807624817, accuracy: 0.8326530612244898\n",
      "loss: 0.35719117522239685, accuracy: 0.8204081632653061\n",
      "loss: 0.3615945875644684, accuracy: 0.8244897959183674\n",
      "loss: 0.36183497309684753, accuracy: 0.8285714285714286\n",
      "loss: 0.36441218852996826, accuracy: 0.8448979591836735\n",
      "loss: 0.37222912907600403, accuracy: 0.8\n",
      "loss: 0.3702791929244995, accuracy: 0.8244897959183674\n",
      "loss: 0.3596491515636444, accuracy: 0.8163265306122449\n",
      "loss: 0.35734447836875916, accuracy: 0.8244897959183674\n",
      "loss: 0.3584415912628174, accuracy: 0.8204081632653061\n",
      "loss: 0.36368176341056824, accuracy: 0.8163265306122449\n",
      "loss: 0.36467432975769043, accuracy: 0.8204081632653061\n",
      "loss: 0.35981252789497375, accuracy: 0.8204081632653061\n",
      "loss: 0.3565767705440521, accuracy: 0.8244897959183674\n",
      "loss: 0.36651256680488586, accuracy: 0.8326530612244898\n",
      "loss: 0.3683087229728699, accuracy: 0.8122448979591836\n",
      "loss: 0.36174309253692627, accuracy: 0.8204081632653061\n",
      "loss: 0.36901143193244934, accuracy: 0.7959183673469388\n",
      "loss: 0.35998162627220154, accuracy: 0.8244897959183674\n",
      "loss: 0.3588472902774811, accuracy: 0.8244897959183674\n",
      "loss: 0.36473432183265686, accuracy: 0.8204081632653061\n",
      "loss: 0.3649183213710785, accuracy: 0.8204081632653061\n",
      "loss: 0.3615994453430176, accuracy: 0.8204081632653061\n",
      "loss: 0.3639267683029175, accuracy: 0.8122448979591836\n",
      "loss: 0.35973629355430603, accuracy: 0.8244897959183674\n",
      "loss: 0.35922208428382874, accuracy: 0.8081632653061225\n",
      "loss: 0.3585735261440277, accuracy: 0.8285714285714286\n",
      "loss: 0.35703155398368835, accuracy: 0.8204081632653061\n",
      "loss: 0.3536948263645172, accuracy: 0.8163265306122449\n",
      "loss: 0.3578922152519226, accuracy: 0.8244897959183674\n",
      "loss: 0.36004993319511414, accuracy: 0.8326530612244898\n",
      "loss: 0.36279296875, accuracy: 0.7959183673469388\n",
      "loss: 0.36656343936920166, accuracy: 0.8204081632653061\n",
      "loss: 0.3534103333950043, accuracy: 0.8244897959183674\n",
      "loss: 0.34864145517349243, accuracy: 0.8326530612244898\n",
      "loss: 0.3529389798641205, accuracy: 0.8163265306122449\n",
      "loss: 0.3586410582065582, accuracy: 0.8285714285714286\n",
      "loss: 0.34959977865219116, accuracy: 0.8244897959183674\n",
      "loss: 0.3555474281311035, accuracy: 0.8448979591836735\n",
      "loss: 0.34823259711265564, accuracy: 0.8285714285714286\n",
      "loss: 0.354033499956131, accuracy: 0.8244897959183674\n",
      "loss: 0.36740854382514954, accuracy: 0.8040816326530612\n",
      "loss: 0.3499111831188202, accuracy: 0.8285714285714286\n",
      "loss: 0.3520794212818146, accuracy: 0.8204081632653061\n",
      "loss: 0.35638266801834106, accuracy: 0.8367346938775511\n",
      "loss: 0.3546254634857178, accuracy: 0.8326530612244898\n",
      "loss: 0.3529702425003052, accuracy: 0.8367346938775511\n",
      "loss: 0.34596410393714905, accuracy: 0.8244897959183674\n",
      "loss: 0.3546617031097412, accuracy: 0.8204081632653061\n",
      "loss: 0.34957224130630493, accuracy: 0.8285714285714286\n",
      "loss: 0.35791245102882385, accuracy: 0.8122448979591836\n",
      "loss: 0.36203819513320923, accuracy: 0.8\n",
      "loss: 0.35519832372665405, accuracy: 0.8081632653061225\n",
      "loss: 0.34798240661621094, accuracy: 0.8367346938775511\n",
      "loss: 0.352863073348999, accuracy: 0.8367346938775511\n",
      "loss: 0.3653976023197174, accuracy: 0.8163265306122449\n",
      "loss: 0.35556578636169434, accuracy: 0.8122448979591836\n",
      "loss: 0.35617706179618835, accuracy: 0.8244897959183674\n",
      "loss: 0.36222341656684875, accuracy: 0.8040816326530612\n",
      "loss: 0.35149499773979187, accuracy: 0.8408163265306122\n",
      "loss: 0.3460758328437805, accuracy: 0.8122448979591836\n",
      "loss: 0.35731270909309387, accuracy: 0.8163265306122449\n",
      "loss: 0.3533771336078644, accuracy: 0.8326530612244898\n",
      "loss: 0.35856539011001587, accuracy: 0.8244897959183674\n",
      "loss: 0.35326719284057617, accuracy: 0.8326530612244898\n",
      "loss: 0.3523300290107727, accuracy: 0.8408163265306122\n",
      "loss: 0.36075636744499207, accuracy: 0.8163265306122449\n",
      "loss: 0.35178694128990173, accuracy: 0.8081632653061225\n",
      "loss: 0.3447836935520172, accuracy: 0.8244897959183674\n",
      "loss: 0.3507828414440155, accuracy: 0.8367346938775511\n",
      "loss: 0.3629864752292633, accuracy: 0.8163265306122449\n",
      "loss: 0.3496587574481964, accuracy: 0.8367346938775511\n",
      "loss: 0.36581674218177795, accuracy: 0.8040816326530612\n",
      "loss: 0.3534879982471466, accuracy: 0.8122448979591836\n",
      "loss: 0.35490256547927856, accuracy: 0.8122448979591836\n",
      "loss: 0.35452911257743835, accuracy: 0.8040816326530612\n",
      "loss: 0.3479551374912262, accuracy: 0.8285714285714286\n",
      "loss: 0.3420829176902771, accuracy: 0.8285714285714286\n",
      "loss: 0.34354501962661743, accuracy: 0.8244897959183674\n",
      "loss: 0.34976911544799805, accuracy: 0.8122448979591836\n",
      "loss: 0.3564925789833069, accuracy: 0.8163265306122449\n",
      "loss: 0.3415142893791199, accuracy: 0.8408163265306122\n",
      "loss: 0.36889615654945374, accuracy: 0.8122448979591836\n",
      "loss: 0.35492223501205444, accuracy: 0.8408163265306122\n",
      "loss: 0.3474283516407013, accuracy: 0.8163265306122449\n",
      "loss: 0.3471057415008545, accuracy: 0.8081632653061225\n",
      "loss: 0.3500228822231293, accuracy: 0.8163265306122449\n",
      "loss: 0.3512577712535858, accuracy: 0.8244897959183674\n",
      "loss: 0.3388190269470215, accuracy: 0.8285714285714286\n",
      "loss: 0.3446258306503296, accuracy: 0.8326530612244898\n",
      "loss: 0.3498036563396454, accuracy: 0.8204081632653061\n",
      "loss: 0.3536023199558258, accuracy: 0.8122448979591836\n",
      "loss: 0.34585902094841003, accuracy: 0.8163265306122449\n",
      "loss: 0.34788012504577637, accuracy: 0.8244897959183674\n",
      "loss: 0.3519502282142639, accuracy: 0.8326530612244898\n",
      "loss: 0.33645591139793396, accuracy: 0.8326530612244898\n",
      "loss: 0.3417127728462219, accuracy: 0.8204081632653061\n",
      "loss: 0.3499422073364258, accuracy: 0.8204081632653061\n",
      "loss: 0.3392395079135895, accuracy: 0.8204081632653061\n",
      "loss: 0.34916046261787415, accuracy: 0.8081632653061225\n",
      "loss: 0.33858776092529297, accuracy: 0.8326530612244898\n",
      "loss: 0.35758981108665466, accuracy: 0.8081632653061225\n",
      "loss: 0.3466418385505676, accuracy: 0.8244897959183674\n",
      "loss: 0.35101714730262756, accuracy: 0.8163265306122449\n",
      "loss: 0.3539176285266876, accuracy: 0.8204081632653061\n",
      "loss: 0.347444623708725, accuracy: 0.8163265306122449\n",
      "loss: 0.33900490403175354, accuracy: 0.8244897959183674\n",
      "loss: 0.35154303908348083, accuracy: 0.8244897959183674\n",
      "loss: 0.3467971682548523, accuracy: 0.8204081632653061\n",
      "loss: 0.3410300612449646, accuracy: 0.8448979591836735\n",
      "loss: 0.3426431119441986, accuracy: 0.8204081632653061\n",
      "loss: 0.3478429317474365, accuracy: 0.8244897959183674\n",
      "max accuracy: 0.8612244897959184\n",
      "loss: 0.33081087470054626, accuracy: 0.8612244897959184\n",
      "loss: 0.3439476191997528, accuracy: 0.8408163265306122\n",
      "loss: 0.34174132347106934, accuracy: 0.8244897959183674\n",
      "loss: 0.3570992350578308, accuracy: 0.8408163265306122\n",
      "loss: 0.3542318046092987, accuracy: 0.8163265306122449\n",
      "loss: 0.33922234177589417, accuracy: 0.8367346938775511\n",
      "loss: 0.34494680166244507, accuracy: 0.8163265306122449\n",
      "loss: 0.3450876772403717, accuracy: 0.8163265306122449\n",
      "loss: 0.34437376260757446, accuracy: 0.8489795918367347\n",
      "loss: 0.3456439673900604, accuracy: 0.8285714285714286\n",
      "loss: 0.3488004803657532, accuracy: 0.8204081632653061\n",
      "loss: 0.35152265429496765, accuracy: 0.8204081632653061\n",
      "loss: 0.3463738262653351, accuracy: 0.8326530612244898\n",
      "loss: 0.3337993621826172, accuracy: 0.8448979591836735\n",
      "loss: 0.348542183637619, accuracy: 0.8326530612244898\n",
      "loss: 0.34142258763313293, accuracy: 0.8367346938775511\n",
      "loss: 0.335903525352478, accuracy: 0.8285714285714286\n",
      "loss: 0.33563515543937683, accuracy: 0.8367346938775511\n",
      "loss: 0.3430986702442169, accuracy: 0.8571428571428571\n",
      "loss: 0.3428116738796234, accuracy: 0.8285714285714286\n",
      "loss: 0.34825846552848816, accuracy: 0.8163265306122449\n",
      "loss: 0.3604643940925598, accuracy: 0.7836734693877551\n",
      "loss: 0.335233598947525, accuracy: 0.8530612244897959\n",
      "loss: 0.34307682514190674, accuracy: 0.8285714285714286\n",
      "loss: 0.34878379106521606, accuracy: 0.8326530612244898\n",
      "loss: 0.3504422605037689, accuracy: 0.8244897959183674\n",
      "loss: 0.33404985070228577, accuracy: 0.8367346938775511\n",
      "loss: 0.33108067512512207, accuracy: 0.8408163265306122\n",
      "loss: 0.3380100727081299, accuracy: 0.8326530612244898\n",
      "loss: 0.33842477202415466, accuracy: 0.8204081632653061\n",
      "loss: 0.3400711715221405, accuracy: 0.8408163265306122\n",
      "loss: 0.3429972529411316, accuracy: 0.8204081632653061\n",
      "loss: 0.3406164348125458, accuracy: 0.8489795918367347\n",
      "loss: 0.3396380841732025, accuracy: 0.8244897959183674\n",
      "loss: 0.3372913897037506, accuracy: 0.8489795918367347\n",
      "loss: 0.3386918306350708, accuracy: 0.8326530612244898\n",
      "loss: 0.35994744300842285, accuracy: 0.8204081632653061\n",
      "loss: 0.3301409184932709, accuracy: 0.8244897959183674\n",
      "loss: 0.33646997809410095, accuracy: 0.8367346938775511\n",
      "loss: 0.3410075306892395, accuracy: 0.8448979591836735\n",
      "loss: 0.3304271399974823, accuracy: 0.8489795918367347\n",
      "loss: 0.3351643681526184, accuracy: 0.8204081632653061\n",
      "loss: 0.3359416127204895, accuracy: 0.8408163265306122\n",
      "loss: 0.35410407185554504, accuracy: 0.8285714285714286\n",
      "loss: 0.33832064270973206, accuracy: 0.8204081632653061\n",
      "loss: 0.33937937021255493, accuracy: 0.8367346938775511\n",
      "loss: 0.3350822925567627, accuracy: 0.8448979591836735\n",
      "loss: 0.34467750787734985, accuracy: 0.8163265306122449\n",
      "loss: 0.33412232995033264, accuracy: 0.8530612244897959\n",
      "loss: 0.33929941058158875, accuracy: 0.8163265306122449\n",
      "loss: 0.3315945565700531, accuracy: 0.8489795918367347\n",
      "loss: 0.3490716218948364, accuracy: 0.8244897959183674\n",
      "loss: 0.3356834650039673, accuracy: 0.8204081632653061\n",
      "loss: 0.3319299519062042, accuracy: 0.8244897959183674\n",
      "loss: 0.3293391466140747, accuracy: 0.8367346938775511\n",
      "loss: 0.34385448694229126, accuracy: 0.8367346938775511\n",
      "loss: 0.3300744593143463, accuracy: 0.8367346938775511\n",
      "loss: 0.32891908288002014, accuracy: 0.8408163265306122\n",
      "loss: 0.3486919403076172, accuracy: 0.8244897959183674\n",
      "loss: 0.34359416365623474, accuracy: 0.8285714285714286\n",
      "loss: 0.35226839780807495, accuracy: 0.7959183673469388\n",
      "loss: 0.33637353777885437, accuracy: 0.8204081632653061\n",
      "loss: 0.3316083252429962, accuracy: 0.8489795918367347\n",
      "loss: 0.33776798844337463, accuracy: 0.8367346938775511\n",
      "loss: 0.3321155607700348, accuracy: 0.8571428571428571\n",
      "loss: 0.3395855724811554, accuracy: 0.8530612244897959\n",
      "loss: 0.3363547921180725, accuracy: 0.8326530612244898\n",
      "loss: 0.33605894446372986, accuracy: 0.8489795918367347\n",
      "loss: 0.3338833749294281, accuracy: 0.8489795918367347\n",
      "loss: 0.34167298674583435, accuracy: 0.8489795918367347\n",
      "loss: 0.3304123282432556, accuracy: 0.8285714285714286\n",
      "loss: 0.3300352990627289, accuracy: 0.8163265306122449\n",
      "loss: 0.3312433362007141, accuracy: 0.8244897959183674\n",
      "loss: 0.3280188739299774, accuracy: 0.8489795918367347\n",
      "loss: 0.3362838327884674, accuracy: 0.8285714285714286\n",
      "loss: 0.3246850073337555, accuracy: 0.8244897959183674\n",
      "loss: 0.34109923243522644, accuracy: 0.8367346938775511\n",
      "loss: 0.34592345356941223, accuracy: 0.8367346938775511\n",
      "loss: 0.3283784091472626, accuracy: 0.8408163265306122\n",
      "loss: 0.32156017422676086, accuracy: 0.8448979591836735\n",
      "loss: 0.33480632305145264, accuracy: 0.8326530612244898\n",
      "loss: 0.33411213755607605, accuracy: 0.8326530612244898\n",
      "loss: 0.33303526043891907, accuracy: 0.8448979591836735\n",
      "loss: 0.32722973823547363, accuracy: 0.8326530612244898\n",
      "loss: 0.33369922637939453, accuracy: 0.8326530612244898\n",
      "loss: 0.3301476240158081, accuracy: 0.8244897959183674\n",
      "loss: 0.340110182762146, accuracy: 0.8408163265306122\n",
      "loss: 0.32632002234458923, accuracy: 0.8448979591836735\n",
      "loss: 0.33891135454177856, accuracy: 0.8489795918367347\n",
      "loss: 0.3449472188949585, accuracy: 0.8326530612244898\n",
      "loss: 0.3434005081653595, accuracy: 0.8326530612244898\n",
      "loss: 0.3310948610305786, accuracy: 0.8448979591836735\n",
      "loss: 0.3272066116333008, accuracy: 0.8367346938775511\n",
      "loss: 0.3257421851158142, accuracy: 0.8285714285714286\n",
      "loss: 0.3253066837787628, accuracy: 0.8204081632653061\n",
      "loss: 0.32935476303100586, accuracy: 0.8326530612244898\n",
      "loss: 0.32560065388679504, accuracy: 0.8489795918367347\n",
      "loss: 0.33002549409866333, accuracy: 0.8489795918367347\n",
      "loss: 0.33141255378723145, accuracy: 0.8448979591836735\n",
      "loss: 0.3274516761302948, accuracy: 0.8244897959183674\n",
      "loss: 0.32373034954071045, accuracy: 0.8612244897959184\n",
      "loss: 0.3196628987789154, accuracy: 0.8448979591836735\n",
      "loss: 0.33706751465797424, accuracy: 0.8448979591836735\n",
      "loss: 0.3224360942840576, accuracy: 0.8448979591836735\n",
      "loss: 0.3294539451599121, accuracy: 0.8367346938775511\n",
      "loss: 0.32211393117904663, accuracy: 0.8367346938775511\n",
      "loss: 0.33954039216041565, accuracy: 0.8285714285714286\n",
      "loss: 0.3294813334941864, accuracy: 0.8367346938775511\n",
      "loss: 0.33217987418174744, accuracy: 0.8367346938775511\n",
      "loss: 0.33250880241394043, accuracy: 0.8489795918367347\n",
      "loss: 0.34468311071395874, accuracy: 0.8081632653061225\n",
      "loss: 0.33733606338500977, accuracy: 0.8244897959183674\n",
      "loss: 0.3276103734970093, accuracy: 0.8244897959183674\n",
      "loss: 0.33020585775375366, accuracy: 0.8571428571428571\n",
      "loss: 0.3215135633945465, accuracy: 0.8367346938775511\n",
      "loss: 0.3261115252971649, accuracy: 0.8448979591836735\n",
      "loss: 0.33123958110809326, accuracy: 0.8326530612244898\n",
      "loss: 0.3198607563972473, accuracy: 0.8489795918367347\n",
      "loss: 0.32085147500038147, accuracy: 0.8326530612244898\n",
      "loss: 0.31942811608314514, accuracy: 0.8571428571428571\n",
      "loss: 0.32955899834632874, accuracy: 0.8448979591836735\n",
      "loss: 0.333982914686203, accuracy: 0.8448979591836735\n",
      "loss: 0.32273316383361816, accuracy: 0.8408163265306122\n",
      "loss: 0.32959726452827454, accuracy: 0.8448979591836735\n",
      "loss: 0.3214363753795624, accuracy: 0.8448979591836735\n",
      "max accuracy: 0.8653061224489796\n",
      "loss: 0.3095479905605316, accuracy: 0.8653061224489796\n",
      "loss: 0.32742246985435486, accuracy: 0.8367346938775511\n",
      "loss: 0.3328985571861267, accuracy: 0.8326530612244898\n",
      "loss: 0.32185280323028564, accuracy: 0.8408163265306122\n",
      "loss: 0.34111228585243225, accuracy: 0.8244897959183674\n",
      "loss: 0.3243401050567627, accuracy: 0.8448979591836735\n",
      "loss: 0.323498010635376, accuracy: 0.8489795918367347\n",
      "loss: 0.32496723532676697, accuracy: 0.8408163265306122\n",
      "loss: 0.3186246454715729, accuracy: 0.8367346938775511\n",
      "loss: 0.3320024907588959, accuracy: 0.8326530612244898\n",
      "loss: 0.3290494978427887, accuracy: 0.8326530612244898\n",
      "loss: 0.32218682765960693, accuracy: 0.8244897959183674\n",
      "loss: 0.3185000717639923, accuracy: 0.8530612244897959\n",
      "loss: 0.32353901863098145, accuracy: 0.8448979591836735\n",
      "loss: 0.3136589825153351, accuracy: 0.8448979591836735\n",
      "loss: 0.33245301246643066, accuracy: 0.8408163265306122\n",
      "loss: 0.3179769814014435, accuracy: 0.8408163265306122\n",
      "loss: 0.3439873456954956, accuracy: 0.8285714285714286\n",
      "loss: 0.3301413953304291, accuracy: 0.8489795918367347\n",
      "loss: 0.33886367082595825, accuracy: 0.8326530612244898\n",
      "loss: 0.3334466516971588, accuracy: 0.8244897959183674\n",
      "loss: 0.31829747557640076, accuracy: 0.8489795918367347\n",
      "loss: 0.3176732659339905, accuracy: 0.8408163265306122\n",
      "loss: 0.3263871967792511, accuracy: 0.8448979591836735\n",
      "loss: 0.3338305950164795, accuracy: 0.8448979591836735\n",
      "loss: 0.31702345609664917, accuracy: 0.8448979591836735\n",
      "loss: 0.31836917996406555, accuracy: 0.8448979591836735\n",
      "loss: 0.3178757429122925, accuracy: 0.8489795918367347\n",
      "loss: 0.3258603513240814, accuracy: 0.8489795918367347\n",
      "loss: 0.3156287968158722, accuracy: 0.8448979591836735\n",
      "loss: 0.31622177362442017, accuracy: 0.8367346938775511\n",
      "loss: 0.3236876428127289, accuracy: 0.8367346938775511\n",
      "loss: 0.32577627897262573, accuracy: 0.8612244897959184\n",
      "loss: 0.33345574140548706, accuracy: 0.8530612244897959\n",
      "loss: 0.33719757199287415, accuracy: 0.8244897959183674\n",
      "loss: 0.32497212290763855, accuracy: 0.8489795918367347\n",
      "loss: 0.3312697112560272, accuracy: 0.8489795918367347\n",
      "loss: 0.32547494769096375, accuracy: 0.8367346938775511\n",
      "loss: 0.32093924283981323, accuracy: 0.8367346938775511\n",
      "loss: 0.32644349336624146, accuracy: 0.8285714285714286\n",
      "loss: 0.3248448371887207, accuracy: 0.8571428571428571\n",
      "loss: 0.32511523365974426, accuracy: 0.8408163265306122\n",
      "loss: 0.33893847465515137, accuracy: 0.8163265306122449\n",
      "loss: 0.31559130549430847, accuracy: 0.8530612244897959\n",
      "loss: 0.3270764648914337, accuracy: 0.8408163265306122\n",
      "loss: 0.3097493052482605, accuracy: 0.8653061224489796\n",
      "loss: 0.3203371465206146, accuracy: 0.8612244897959184\n",
      "loss: 0.3327699601650238, accuracy: 0.8408163265306122\n",
      "loss: 0.3098226487636566, accuracy: 0.8489795918367347\n",
      "loss: 0.31140878796577454, accuracy: 0.8612244897959184\n",
      "loss: 0.31554269790649414, accuracy: 0.8530612244897959\n",
      "max accuracy: 0.8693877551020408\n",
      "loss: 0.30788886547088623, accuracy: 0.8693877551020408\n",
      "loss: 0.3338189721107483, accuracy: 0.8367346938775511\n",
      "loss: 0.31847110390663147, accuracy: 0.8326530612244898\n",
      "max accuracy: 0.8734693877551021\n",
      "loss: 0.3060331642627716, accuracy: 0.8734693877551021\n",
      "loss: 0.3276912271976471, accuracy: 0.8530612244897959\n",
      "loss: 0.30696624517440796, accuracy: 0.8612244897959184\n",
      "loss: 0.3199494779109955, accuracy: 0.8326530612244898\n",
      "loss: 0.3315719664096832, accuracy: 0.8204081632653061\n",
      "loss: 0.3117660880088806, accuracy: 0.8408163265306122\n",
      "loss: 0.31299230456352234, accuracy: 0.8612244897959184\n",
      "loss: 0.30891773104667664, accuracy: 0.8489795918367347\n",
      "loss: 0.3195439875125885, accuracy: 0.8367346938775511\n",
      "loss: 0.3294147849082947, accuracy: 0.8448979591836735\n",
      "loss: 0.328116774559021, accuracy: 0.8448979591836735\n",
      "loss: 0.32523059844970703, accuracy: 0.8204081632653061\n",
      "loss: 0.3133751451969147, accuracy: 0.8408163265306122\n",
      "loss: 0.31097611784935, accuracy: 0.8612244897959184\n",
      "loss: 0.3074391484260559, accuracy: 0.8489795918367347\n",
      "loss: 0.3266465663909912, accuracy: 0.8367346938775511\n",
      "loss: 0.3272615075111389, accuracy: 0.8408163265306122\n",
      "loss: 0.3119058907032013, accuracy: 0.8408163265306122\n",
      "loss: 0.31391704082489014, accuracy: 0.8408163265306122\n",
      "loss: 0.29915592074394226, accuracy: 0.8693877551020408\n",
      "loss: 0.32677051424980164, accuracy: 0.8244897959183674\n",
      "loss: 0.3037392795085907, accuracy: 0.8571428571428571\n",
      "loss: 0.3144039511680603, accuracy: 0.8367346938775511\n",
      "loss: 0.3071860373020172, accuracy: 0.8489795918367347\n",
      "loss: 0.30740347504615784, accuracy: 0.8530612244897959\n",
      "loss: 0.3089869022369385, accuracy: 0.8571428571428571\n",
      "loss: 0.32011428475379944, accuracy: 0.8367346938775511\n",
      "loss: 0.30586838722229004, accuracy: 0.8612244897959184\n",
      "loss: 0.32737812399864197, accuracy: 0.8571428571428571\n",
      "loss: 0.32080066204071045, accuracy: 0.8489795918367347\n",
      "loss: 0.3199433386325836, accuracy: 0.8285714285714286\n",
      "loss: 0.3157729506492615, accuracy: 0.8489795918367347\n",
      "loss: 0.3126969337463379, accuracy: 0.8530612244897959\n",
      "loss: 0.3017105758190155, accuracy: 0.8693877551020408\n",
      "loss: 0.3066173195838928, accuracy: 0.8612244897959184\n",
      "loss: 0.30874910950660706, accuracy: 0.8612244897959184\n",
      "loss: 0.31105902791023254, accuracy: 0.8448979591836735\n",
      "loss: 0.30854418873786926, accuracy: 0.8571428571428571\n",
      "max accuracy: 0.8816326530612245\n",
      "loss: 0.29995572566986084, accuracy: 0.8816326530612245\n",
      "loss: 0.30402228236198425, accuracy: 0.8489795918367347\n",
      "loss: 0.3023739755153656, accuracy: 0.8612244897959184\n",
      "loss: 0.31758564710617065, accuracy: 0.8530612244897959\n",
      "loss: 0.3095386326313019, accuracy: 0.8489795918367347\n",
      "loss: 0.30296868085861206, accuracy: 0.8693877551020408\n",
      "loss: 0.30505529046058655, accuracy: 0.8408163265306122\n",
      "loss: 0.306689977645874, accuracy: 0.8448979591836735\n",
      "loss: 0.3078557848930359, accuracy: 0.8489795918367347\n",
      "loss: 0.3264036178588867, accuracy: 0.8408163265306122\n",
      "loss: 0.3028978705406189, accuracy: 0.8734693877551021\n",
      "loss: 0.3058044910430908, accuracy: 0.8571428571428571\n",
      "loss: 0.3027905821800232, accuracy: 0.8489795918367347\n",
      "loss: 0.3098565340042114, accuracy: 0.8326530612244898\n",
      "loss: 0.31478407979011536, accuracy: 0.8653061224489796\n",
      "loss: 0.30220505595207214, accuracy: 0.8489795918367347\n",
      "loss: 0.3287346661090851, accuracy: 0.8040816326530612\n",
      "loss: 0.316245436668396, accuracy: 0.8530612244897959\n",
      "loss: 0.3200018107891083, accuracy: 0.8571428571428571\n",
      "loss: 0.3091166317462921, accuracy: 0.8734693877551021\n",
      "loss: 0.29866862297058105, accuracy: 0.8693877551020408\n",
      "loss: 0.3130856454372406, accuracy: 0.8571428571428571\n",
      "loss: 0.305600106716156, accuracy: 0.8653061224489796\n",
      "loss: 0.30647513270378113, accuracy: 0.8448979591836735\n",
      "loss: 0.30338066816329956, accuracy: 0.8571428571428571\n",
      "loss: 0.30251309275627136, accuracy: 0.8571428571428571\n",
      "loss: 0.30876967310905457, accuracy: 0.8693877551020408\n",
      "loss: 0.30759337544441223, accuracy: 0.8816326530612245\n",
      "loss: 0.3048749268054962, accuracy: 0.8530612244897959\n",
      "loss: 0.31542956829071045, accuracy: 0.8530612244897959\n",
      "loss: 0.3134845197200775, accuracy: 0.8612244897959184\n",
      "loss: 0.2992265522480011, accuracy: 0.8816326530612245\n",
      "loss: 0.3109811246395111, accuracy: 0.8653061224489796\n",
      "loss: 0.3238205313682556, accuracy: 0.8489795918367347\n",
      "loss: 0.30663543939590454, accuracy: 0.8612244897959184\n",
      "loss: 0.30128368735313416, accuracy: 0.8612244897959184\n",
      "loss: 0.2980669438838959, accuracy: 0.8693877551020408\n",
      "loss: 0.3035351634025574, accuracy: 0.8816326530612245\n",
      "max accuracy: 0.8979591836734694\n",
      "loss: 0.29872363805770874, accuracy: 0.8979591836734694\n",
      "loss: 0.3019542992115021, accuracy: 0.8653061224489796\n",
      "loss: 0.3016955554485321, accuracy: 0.8489795918367347\n",
      "loss: 0.3120078146457672, accuracy: 0.8326530612244898\n",
      "loss: 0.3045909106731415, accuracy: 0.8530612244897959\n",
      "loss: 0.30546146631240845, accuracy: 0.8734693877551021\n",
      "loss: 0.30301812291145325, accuracy: 0.8734693877551021\n",
      "loss: 0.30087411403656006, accuracy: 0.8448979591836735\n",
      "loss: 0.29707297682762146, accuracy: 0.8775510204081632\n",
      "loss: 0.30352672934532166, accuracy: 0.8530612244897959\n",
      "loss: 0.3000044524669647, accuracy: 0.8693877551020408\n",
      "loss: 0.31015652418136597, accuracy: 0.8489795918367347\n",
      "loss: 0.297105073928833, accuracy: 0.8612244897959184\n",
      "loss: 0.30795466899871826, accuracy: 0.8653061224489796\n",
      "loss: 0.3174980580806732, accuracy: 0.8653061224489796\n",
      "loss: 0.30652695894241333, accuracy: 0.8571428571428571\n",
      "loss: 0.30389833450317383, accuracy: 0.8571428571428571\n",
      "loss: 0.30290842056274414, accuracy: 0.8489795918367347\n",
      "loss: 0.30443915724754333, accuracy: 0.8530612244897959\n",
      "loss: 0.30492717027664185, accuracy: 0.8571428571428571\n",
      "loss: 0.2956382930278778, accuracy: 0.8571428571428571\n",
      "loss: 0.3081037700176239, accuracy: 0.8530612244897959\n",
      "loss: 0.2958270013332367, accuracy: 0.8530612244897959\n",
      "loss: 0.29470351338386536, accuracy: 0.8571428571428571\n",
      "loss: 0.3161824345588684, accuracy: 0.8571428571428571\n",
      "loss: 0.3172420263290405, accuracy: 0.8408163265306122\n",
      "loss: 0.2951030135154724, accuracy: 0.8489795918367347\n",
      "loss: 0.3152720630168915, accuracy: 0.8448979591836735\n",
      "loss: 0.28830403089523315, accuracy: 0.8816326530612245\n",
      "loss: 0.31093478202819824, accuracy: 0.8571428571428571\n",
      "loss: 0.29987862706184387, accuracy: 0.8489795918367347\n",
      "loss: 0.29086485505104065, accuracy: 0.8857142857142857\n",
      "loss: 0.323221355676651, accuracy: 0.8489795918367347\n",
      "loss: 0.2970659136772156, accuracy: 0.8775510204081632\n",
      "loss: 0.30316707491874695, accuracy: 0.8653061224489796\n",
      "loss: 0.3091067969799042, accuracy: 0.8530612244897959\n",
      "loss: 0.28967949748039246, accuracy: 0.8653061224489796\n",
      "loss: 0.3076837360858917, accuracy: 0.8571428571428571\n",
      "loss: 0.30474743247032166, accuracy: 0.8408163265306122\n",
      "loss: 0.29306986927986145, accuracy: 0.8653061224489796\n",
      "loss: 0.29169395565986633, accuracy: 0.8653061224489796\n",
      "loss: 0.31374457478523254, accuracy: 0.8408163265306122\n",
      "loss: 0.30871424078941345, accuracy: 0.8408163265306122\n",
      "loss: 0.29391899704933167, accuracy: 0.8653061224489796\n",
      "loss: 0.3105713725090027, accuracy: 0.8612244897959184\n",
      "loss: 0.2972582280635834, accuracy: 0.8693877551020408\n",
      "loss: 0.2927141487598419, accuracy: 0.8693877551020408\n",
      "loss: 0.300881952047348, accuracy: 0.8571428571428571\n",
      "loss: 0.3093641400337219, accuracy: 0.8612244897959184\n",
      "loss: 0.2935926914215088, accuracy: 0.8571428571428571\n",
      "loss: 0.29732343554496765, accuracy: 0.8775510204081632\n",
      "loss: 0.3043910264968872, accuracy: 0.8734693877551021\n",
      "loss: 0.2878846824169159, accuracy: 0.8816326530612245\n",
      "loss: 0.2941950857639313, accuracy: 0.8938775510204081\n",
      "loss: 0.29904401302337646, accuracy: 0.8612244897959184\n",
      "loss: 0.28794369101524353, accuracy: 0.8816326530612245\n",
      "loss: 0.301097571849823, accuracy: 0.8571428571428571\n",
      "loss: 0.3153737485408783, accuracy: 0.8408163265306122\n",
      "loss: 0.31376075744628906, accuracy: 0.8326530612244898\n",
      "loss: 0.3004488945007324, accuracy: 0.8653061224489796\n",
      "loss: 0.2895093560218811, accuracy: 0.8653061224489796\n",
      "loss: 0.309770792722702, accuracy: 0.8693877551020408\n",
      "loss: 0.30609625577926636, accuracy: 0.8530612244897959\n",
      "loss: 0.28968870639801025, accuracy: 0.8571428571428571\n",
      "loss: 0.2971510589122772, accuracy: 0.8612244897959184\n",
      "loss: 0.2918087840080261, accuracy: 0.8775510204081632\n",
      "loss: 0.28623849153518677, accuracy: 0.8653061224489796\n",
      "loss: 0.30318477749824524, accuracy: 0.8734693877551021\n",
      "loss: 0.2858266532421112, accuracy: 0.8693877551020408\n",
      "loss: 0.29852017760276794, accuracy: 0.8653061224489796\n",
      "loss: 0.28596067428588867, accuracy: 0.8653061224489796\n",
      "loss: 0.29641786217689514, accuracy: 0.8530612244897959\n",
      "loss: 0.2893993556499481, accuracy: 0.889795918367347\n",
      "loss: 0.3004187047481537, accuracy: 0.8734693877551021\n",
      "loss: 0.29155707359313965, accuracy: 0.8775510204081632\n",
      "loss: 0.29096126556396484, accuracy: 0.8653061224489796\n",
      "loss: 0.29807668924331665, accuracy: 0.8571428571428571\n",
      "loss: 0.29089197516441345, accuracy: 0.8734693877551021\n",
      "loss: 0.2960159480571747, accuracy: 0.8734693877551021\n",
      "loss: 0.2999265789985657, accuracy: 0.8571428571428571\n",
      "loss: 0.2958265244960785, accuracy: 0.8653061224489796\n",
      "loss: 0.2825618386268616, accuracy: 0.8693877551020408\n",
      "loss: 0.281836599111557, accuracy: 0.8734693877551021\n",
      "loss: 0.2857184410095215, accuracy: 0.8857142857142857\n",
      "loss: 0.284543514251709, accuracy: 0.8857142857142857\n",
      "loss: 0.2922128140926361, accuracy: 0.8734693877551021\n",
      "loss: 0.2915295958518982, accuracy: 0.8571428571428571\n",
      "loss: 0.2861687242984772, accuracy: 0.889795918367347\n",
      "loss: 0.28519853949546814, accuracy: 0.8775510204081632\n",
      "loss: 0.2800122797489166, accuracy: 0.889795918367347\n",
      "loss: 0.3058633506298065, accuracy: 0.8612244897959184\n",
      "loss: 0.2911454439163208, accuracy: 0.8530612244897959\n",
      "loss: 0.29146909713745117, accuracy: 0.8816326530612245\n",
      "loss: 0.28297773003578186, accuracy: 0.8693877551020408\n",
      "loss: 0.2804194688796997, accuracy: 0.8775510204081632\n",
      "loss: 0.2812720537185669, accuracy: 0.889795918367347\n",
      "loss: 0.28610387444496155, accuracy: 0.8816326530612245\n",
      "loss: 0.2890053391456604, accuracy: 0.8734693877551021\n",
      "loss: 0.299891859292984, accuracy: 0.8571428571428571\n",
      "loss: 0.2804863452911377, accuracy: 0.8857142857142857\n",
      "loss: 0.2867775857448578, accuracy: 0.8775510204081632\n",
      "loss: 0.27665528655052185, accuracy: 0.8816326530612245\n",
      "loss: 0.28267619013786316, accuracy: 0.8857142857142857\n",
      "loss: 0.27595430612564087, accuracy: 0.889795918367347\n",
      "loss: 0.27984893321990967, accuracy: 0.8775510204081632\n",
      "loss: 0.2961060106754303, accuracy: 0.8571428571428571\n",
      "loss: 0.2866348326206207, accuracy: 0.8571428571428571\n",
      "max accuracy: 0.9020408163265307\n",
      "loss: 0.2732912302017212, accuracy: 0.9020408163265307\n",
      "loss: 0.27330535650253296, accuracy: 0.8857142857142857\n",
      "loss: 0.3037723898887634, accuracy: 0.8653061224489796\n",
      "loss: 0.2950385510921478, accuracy: 0.8653061224489796\n",
      "loss: 0.28239062428474426, accuracy: 0.8816326530612245\n",
      "loss: 0.29397574067115784, accuracy: 0.8571428571428571\n",
      "loss: 0.2879789173603058, accuracy: 0.8857142857142857\n",
      "loss: 0.2806931734085083, accuracy: 0.8693877551020408\n",
      "loss: 0.2941417992115021, accuracy: 0.8571428571428571\n",
      "loss: 0.3130462169647217, accuracy: 0.8612244897959184\n",
      "loss: 0.29186585545539856, accuracy: 0.8775510204081632\n",
      "loss: 0.29183146357536316, accuracy: 0.8775510204081632\n",
      "loss: 0.28900331258773804, accuracy: 0.8775510204081632\n",
      "loss: 0.29056599736213684, accuracy: 0.8734693877551021\n",
      "loss: 0.2958206236362457, accuracy: 0.8938775510204081\n",
      "loss: 0.28377020359039307, accuracy: 0.8857142857142857\n",
      "loss: 0.273440957069397, accuracy: 0.8857142857142857\n",
      "loss: 0.28671640157699585, accuracy: 0.8693877551020408\n",
      "loss: 0.28165653347969055, accuracy: 0.8693877551020408\n",
      "loss: 0.2896486520767212, accuracy: 0.8612244897959184\n",
      "loss: 0.2806002199649811, accuracy: 0.8857142857142857\n",
      "loss: 0.2872704565525055, accuracy: 0.8775510204081632\n",
      "loss: 0.27642008662223816, accuracy: 0.8653061224489796\n",
      "loss: 0.27605560421943665, accuracy: 0.8775510204081632\n",
      "loss: 0.2761777639389038, accuracy: 0.8816326530612245\n",
      "loss: 0.30108556151390076, accuracy: 0.8285714285714286\n",
      "loss: 0.28105220198631287, accuracy: 0.8775510204081632\n",
      "loss: 0.29033568501472473, accuracy: 0.8530612244897959\n",
      "loss: 0.2780930995941162, accuracy: 0.8938775510204081\n",
      "loss: 0.2712475657463074, accuracy: 0.8775510204081632\n",
      "max accuracy: 0.9061224489795918\n",
      "loss: 0.2708677649497986, accuracy: 0.9061224489795918\n",
      "loss: 0.28098562359809875, accuracy: 0.8816326530612245\n",
      "loss: 0.27672356367111206, accuracy: 0.8938775510204081\n",
      "loss: 0.28551527857780457, accuracy: 0.8693877551020408\n",
      "loss: 0.28233814239501953, accuracy: 0.8775510204081632\n",
      "loss: 0.30356937646865845, accuracy: 0.8734693877551021\n",
      "loss: 0.2822297215461731, accuracy: 0.8816326530612245\n",
      "loss: 0.2734846770763397, accuracy: 0.889795918367347\n",
      "loss: 0.2846023440361023, accuracy: 0.8816326530612245\n",
      "loss: 0.28156524896621704, accuracy: 0.8693877551020408\n",
      "loss: 0.28052061796188354, accuracy: 0.8734693877551021\n",
      "loss: 0.2761414647102356, accuracy: 0.8979591836734694\n",
      "loss: 0.27981552481651306, accuracy: 0.889795918367347\n",
      "loss: 0.2735176384449005, accuracy: 0.8979591836734694\n",
      "loss: 0.2805637717247009, accuracy: 0.8938775510204081\n",
      "max accuracy: 0.9102040816326531\n",
      "loss: 0.27723705768585205, accuracy: 0.9102040816326531\n",
      "loss: 0.27269434928894043, accuracy: 0.8816326530612245\n",
      "loss: 0.26982855796813965, accuracy: 0.9102040816326531\n",
      "loss: 0.2703787684440613, accuracy: 0.8857142857142857\n",
      "loss: 0.272727370262146, accuracy: 0.8857142857142857\n",
      "loss: 0.2799294888973236, accuracy: 0.8693877551020408\n",
      "loss: 0.2693760097026825, accuracy: 0.8857142857142857\n",
      "loss: 0.27350160479545593, accuracy: 0.889795918367347\n",
      "loss: 0.2791542410850525, accuracy: 0.8571428571428571\n",
      "loss: 0.2946111559867859, accuracy: 0.8938775510204081\n",
      "loss: 0.27586111426353455, accuracy: 0.9061224489795918\n",
      "loss: 0.26500532031059265, accuracy: 0.8938775510204081\n",
      "loss: 0.3009818196296692, accuracy: 0.8653061224489796\n",
      "loss: 0.296072781085968, accuracy: 0.8857142857142857\n",
      "loss: 0.2698260247707367, accuracy: 0.9061224489795918\n",
      "loss: 0.2730073630809784, accuracy: 0.9020408163265307\n",
      "loss: 0.27529245615005493, accuracy: 0.8857142857142857\n",
      "loss: 0.2780470550060272, accuracy: 0.8938775510204081\n",
      "loss: 0.2649890184402466, accuracy: 0.8979591836734694\n",
      "loss: 0.2715992331504822, accuracy: 0.8775510204081632\n",
      "loss: 0.26889583468437195, accuracy: 0.889795918367347\n",
      "loss: 0.27389010787010193, accuracy: 0.889795918367347\n",
      "loss: 0.2653254270553589, accuracy: 0.9020408163265307\n",
      "max accuracy: 0.9142857142857143\n",
      "loss: 0.2578212022781372, accuracy: 0.9142857142857143\n",
      "loss: 0.2697490453720093, accuracy: 0.8857142857142857\n",
      "loss: 0.2921544313430786, accuracy: 0.8775510204081632\n",
      "loss: 0.26848655939102173, accuracy: 0.889795918367347\n",
      "loss: 0.268627405166626, accuracy: 0.9102040816326531\n",
      "loss: 0.2786436080932617, accuracy: 0.889795918367347\n",
      "loss: 0.27557989954948425, accuracy: 0.889795918367347\n",
      "loss: 0.29635563492774963, accuracy: 0.8734693877551021\n",
      "loss: 0.2919614315032959, accuracy: 0.8693877551020408\n",
      "loss: 0.26772990822792053, accuracy: 0.8938775510204081\n",
      "loss: 0.27289658784866333, accuracy: 0.8775510204081632\n",
      "loss: 0.28757086396217346, accuracy: 0.8775510204081632\n",
      "loss: 0.2916087210178375, accuracy: 0.8530612244897959\n",
      "loss: 0.2667064368724823, accuracy: 0.8775510204081632\n",
      "loss: 0.25862404704093933, accuracy: 0.9142857142857143\n",
      "loss: 0.2612972855567932, accuracy: 0.8938775510204081\n",
      "loss: 0.27760472893714905, accuracy: 0.8816326530612245\n",
      "loss: 0.2842117249965668, accuracy: 0.8938775510204081\n",
      "loss: 0.27375340461730957, accuracy: 0.8775510204081632\n",
      "loss: 0.2706928253173828, accuracy: 0.8857142857142857\n",
      "loss: 0.2773117125034332, accuracy: 0.8653061224489796\n",
      "loss: 0.27404549717903137, accuracy: 0.8857142857142857\n",
      "loss: 0.31490981578826904, accuracy: 0.8244897959183674\n",
      "loss: 0.2803420424461365, accuracy: 0.8775510204081632\n",
      "loss: 0.3054693043231964, accuracy: 0.8489795918367347\n",
      "loss: 0.2641073167324066, accuracy: 0.8857142857142857\n",
      "loss: 0.26052236557006836, accuracy: 0.889795918367347\n",
      "loss: 0.27775293588638306, accuracy: 0.889795918367347\n",
      "loss: 0.27807292342185974, accuracy: 0.8979591836734694\n",
      "loss: 0.2805996239185333, accuracy: 0.8734693877551021\n",
      "loss: 0.26187071204185486, accuracy: 0.9102040816326531\n",
      "loss: 0.25955402851104736, accuracy: 0.889795918367347\n",
      "loss: 0.2590034604072571, accuracy: 0.9020408163265307\n",
      "loss: 0.27451416850090027, accuracy: 0.8775510204081632\n",
      "loss: 0.25789186358451843, accuracy: 0.889795918367347\n",
      "loss: 0.26337093114852905, accuracy: 0.8775510204081632\n",
      "loss: 0.26139581203460693, accuracy: 0.8857142857142857\n",
      "loss: 0.27801376581192017, accuracy: 0.8775510204081632\n",
      "loss: 0.26052239537239075, accuracy: 0.8857142857142857\n",
      "loss: 0.27656733989715576, accuracy: 0.8734693877551021\n",
      "loss: 0.2595587968826294, accuracy: 0.9142857142857143\n",
      "loss: 0.2557750344276428, accuracy: 0.9061224489795918\n",
      "loss: 0.2650972902774811, accuracy: 0.8857142857142857\n",
      "loss: 0.25625282526016235, accuracy: 0.8979591836734694\n",
      "loss: 0.25876322388648987, accuracy: 0.889795918367347\n",
      "loss: 0.2579526901245117, accuracy: 0.9142857142857143\n",
      "loss: 0.2586667537689209, accuracy: 0.9102040816326531\n",
      "loss: 0.26379019021987915, accuracy: 0.889795918367347\n",
      "loss: 0.25707435607910156, accuracy: 0.8979591836734694\n",
      "loss: 0.26781991124153137, accuracy: 0.9061224489795918\n",
      "loss: 0.2743578553199768, accuracy: 0.8693877551020408\n",
      "loss: 0.258463978767395, accuracy: 0.8979591836734694\n",
      "loss: 0.24937769770622253, accuracy: 0.9142857142857143\n",
      "loss: 0.26509812474250793, accuracy: 0.8693877551020408\n",
      "loss: 0.26937931776046753, accuracy: 0.889795918367347\n",
      "loss: 0.25182512402534485, accuracy: 0.8979591836734694\n",
      "loss: 0.2584797143936157, accuracy: 0.8938775510204081\n",
      "loss: 0.25852546095848083, accuracy: 0.9102040816326531\n",
      "loss: 0.2619142532348633, accuracy: 0.889795918367347\n",
      "loss: 0.2552678883075714, accuracy: 0.8857142857142857\n",
      "loss: 0.2557227909564972, accuracy: 0.9102040816326531\n",
      "loss: 0.25348979234695435, accuracy: 0.9061224489795918\n",
      "loss: 0.247280091047287, accuracy: 0.9020408163265307\n",
      "loss: 0.2606585621833801, accuracy: 0.9020408163265307\n",
      "loss: 0.2523777186870575, accuracy: 0.8979591836734694\n",
      "loss: 0.2571813762187958, accuracy: 0.9061224489795918\n",
      "loss: 0.2596423625946045, accuracy: 0.8979591836734694\n",
      "loss: 0.2821715772151947, accuracy: 0.8816326530612245\n",
      "loss: 0.26447442173957825, accuracy: 0.8775510204081632\n",
      "loss: 0.2747362554073334, accuracy: 0.8775510204081632\n",
      "loss: 0.28273648023605347, accuracy: 0.8734693877551021\n",
      "loss: 0.25007981061935425, accuracy: 0.9061224489795918\n",
      "loss: 0.2577466070652008, accuracy: 0.8979591836734694\n",
      "loss: 0.26928722858428955, accuracy: 0.8979591836734694\n",
      "loss: 0.2675441801548004, accuracy: 0.8938775510204081\n",
      "loss: 0.2588525116443634, accuracy: 0.8775510204081632\n",
      "loss: 0.2771414518356323, accuracy: 0.9061224489795918\n",
      "loss: 0.25913193821907043, accuracy: 0.889795918367347\n",
      "loss: 0.26087844371795654, accuracy: 0.8938775510204081\n",
      "loss: 0.2578800320625305, accuracy: 0.8979591836734694\n",
      "loss: 0.27562791109085083, accuracy: 0.8938775510204081\n",
      "max accuracy: 0.9224489795918367\n",
      "loss: 0.24993540346622467, accuracy: 0.9224489795918367\n",
      "loss: 0.25492623448371887, accuracy: 0.9142857142857143\n",
      "loss: 0.25162744522094727, accuracy: 0.9102040816326531\n",
      "loss: 0.28694069385528564, accuracy: 0.8938775510204081\n",
      "loss: 0.26584717631340027, accuracy: 0.8857142857142857\n",
      "loss: 0.2504786550998688, accuracy: 0.9020408163265307\n",
      "loss: 0.2454422414302826, accuracy: 0.8979591836734694\n",
      "loss: 0.25817757844924927, accuracy: 0.9061224489795918\n",
      "loss: 0.24642358720302582, accuracy: 0.9061224489795918\n",
      "loss: 0.2432900220155716, accuracy: 0.9224489795918367\n",
      "loss: 0.24438434839248657, accuracy: 0.9061224489795918\n",
      "loss: 0.2545952796936035, accuracy: 0.9020408163265307\n",
      "max accuracy: 0.926530612244898\n",
      "loss: 0.24252597987651825, accuracy: 0.926530612244898\n",
      "loss: 0.2932814359664917, accuracy: 0.8857142857142857\n",
      "loss: 0.24830056726932526, accuracy: 0.9142857142857143\n",
      "loss: 0.2458217740058899, accuracy: 0.9142857142857143\n",
      "loss: 0.2578609883785248, accuracy: 0.8938775510204081\n",
      "max accuracy: 0.9306122448979591\n",
      "loss: 0.24565213918685913, accuracy: 0.9306122448979591\n",
      "loss: 0.2425498515367508, accuracy: 0.8979591836734694\n",
      "loss: 0.2620876431465149, accuracy: 0.8734693877551021\n",
      "loss: 0.2595685124397278, accuracy: 0.889795918367347\n",
      "loss: 0.2628024220466614, accuracy: 0.8734693877551021\n",
      "loss: 0.2523004710674286, accuracy: 0.9102040816326531\n",
      "loss: 0.2437228560447693, accuracy: 0.9183673469387755\n",
      "loss: 0.25164976716041565, accuracy: 0.9183673469387755\n",
      "loss: 0.25219061970710754, accuracy: 0.9020408163265307\n",
      "loss: 0.25907132029533386, accuracy: 0.8938775510204081\n",
      "loss: 0.24403060972690582, accuracy: 0.9061224489795918\n",
      "loss: 0.2543904185295105, accuracy: 0.8734693877551021\n",
      "loss: 0.2689846456050873, accuracy: 0.9061224489795918\n",
      "loss: 0.2382924109697342, accuracy: 0.9142857142857143\n",
      "loss: 0.24415595829486847, accuracy: 0.9061224489795918\n",
      "loss: 0.2537946105003357, accuracy: 0.9142857142857143\n",
      "loss: 0.24350781738758087, accuracy: 0.9020408163265307\n",
      "loss: 0.2426852136850357, accuracy: 0.9183673469387755\n",
      "loss: 0.26759764552116394, accuracy: 0.9020408163265307\n",
      "loss: 0.24647736549377441, accuracy: 0.9020408163265307\n",
      "loss: 0.2389984279870987, accuracy: 0.9142857142857143\n",
      "loss: 0.267102986574173, accuracy: 0.8653061224489796\n",
      "loss: 0.25669172406196594, accuracy: 0.9020408163265307\n",
      "loss: 0.2510077953338623, accuracy: 0.9102040816326531\n",
      "loss: 0.24295741319656372, accuracy: 0.9224489795918367\n",
      "loss: 0.2526543140411377, accuracy: 0.8938775510204081\n",
      "loss: 0.2542797923088074, accuracy: 0.8775510204081632\n",
      "loss: 0.2669563889503479, accuracy: 0.8734693877551021\n",
      "loss: 0.24525238573551178, accuracy: 0.926530612244898\n",
      "loss: 0.24629338085651398, accuracy: 0.8816326530612245\n",
      "loss: 0.24977535009384155, accuracy: 0.9061224489795918\n",
      "loss: 0.24875493347644806, accuracy: 0.9142857142857143\n",
      "loss: 0.24700739979743958, accuracy: 0.8979591836734694\n",
      "loss: 0.2514335513114929, accuracy: 0.9020408163265307\n",
      "loss: 0.23202766478061676, accuracy: 0.926530612244898\n",
      "loss: 0.260469526052475, accuracy: 0.8816326530612245\n",
      "loss: 0.257804811000824, accuracy: 0.9102040816326531\n",
      "loss: 0.2413637489080429, accuracy: 0.9142857142857143\n",
      "loss: 0.2454431653022766, accuracy: 0.9142857142857143\n",
      "loss: 0.28242480754852295, accuracy: 0.8489795918367347\n",
      "loss: 0.24739694595336914, accuracy: 0.8857142857142857\n",
      "loss: 0.28352054953575134, accuracy: 0.8734693877551021\n",
      "loss: 0.26195138692855835, accuracy: 0.8816326530612245\n",
      "loss: 0.24384616315364838, accuracy: 0.889795918367347\n",
      "loss: 0.2910822629928589, accuracy: 0.8816326530612245\n",
      "loss: 0.24183955788612366, accuracy: 0.889795918367347\n",
      "loss: 0.23497509956359863, accuracy: 0.9061224489795918\n",
      "loss: 0.29182860255241394, accuracy: 0.8734693877551021\n",
      "loss: 0.24082347750663757, accuracy: 0.9061224489795918\n",
      "loss: 0.26857736706733704, accuracy: 0.889795918367347\n",
      "loss: 0.2381783425807953, accuracy: 0.926530612244898\n",
      "loss: 0.23915459215641022, accuracy: 0.9183673469387755\n",
      "loss: 0.23587794601917267, accuracy: 0.8979591836734694\n",
      "loss: 0.23993350565433502, accuracy: 0.9102040816326531\n",
      "loss: 0.23777149617671967, accuracy: 0.9102040816326531\n",
      "loss: 0.25080642104148865, accuracy: 0.8979591836734694\n",
      "loss: 0.2364286482334137, accuracy: 0.9306122448979591\n",
      "loss: 0.26333948969841003, accuracy: 0.8938775510204081\n",
      "loss: 0.2607305943965912, accuracy: 0.8816326530612245\n",
      "loss: 0.24793168902397156, accuracy: 0.9183673469387755\n",
      "loss: 0.22912347316741943, accuracy: 0.9102040816326531\n",
      "loss: 0.2495211362838745, accuracy: 0.9142857142857143\n",
      "loss: 0.26140180230140686, accuracy: 0.9102040816326531\n",
      "loss: 0.24361801147460938, accuracy: 0.9020408163265307\n",
      "loss: 0.24998889863491058, accuracy: 0.8979591836734694\n",
      "loss: 0.24596434831619263, accuracy: 0.8979591836734694\n",
      "loss: 0.23378382623195648, accuracy: 0.9102040816326531\n",
      "loss: 0.24591633677482605, accuracy: 0.8938775510204081\n",
      "max accuracy: 0.9469387755102041\n",
      "loss: 0.23255345225334167, accuracy: 0.9469387755102041\n",
      "loss: 0.23988892138004303, accuracy: 0.9224489795918367\n",
      "loss: 0.2361859828233719, accuracy: 0.9183673469387755\n",
      "loss: 0.2341492474079132, accuracy: 0.9306122448979591\n",
      "loss: 0.2405175119638443, accuracy: 0.889795918367347\n",
      "loss: 0.2605721354484558, accuracy: 0.8816326530612245\n",
      "loss: 0.2443571835756302, accuracy: 0.9061224489795918\n",
      "loss: 0.23183049261569977, accuracy: 0.9183673469387755\n",
      "loss: 0.2357356995344162, accuracy: 0.9061224489795918\n",
      "loss: 0.24959884583950043, accuracy: 0.8734693877551021\n",
      "loss: 0.2368147075176239, accuracy: 0.9346938775510204\n",
      "loss: 0.22335194051265717, accuracy: 0.9224489795918367\n",
      "loss: 0.24272774159908295, accuracy: 0.9061224489795918\n",
      "loss: 0.2347608357667923, accuracy: 0.9183673469387755\n",
      "loss: 0.2416030764579773, accuracy: 0.9183673469387755\n",
      "loss: 0.24345676600933075, accuracy: 0.9142857142857143\n",
      "loss: 0.22515012323856354, accuracy: 0.9346938775510204\n",
      "loss: 0.2397099882364273, accuracy: 0.9061224489795918\n",
      "loss: 0.2344636172056198, accuracy: 0.9306122448979591\n",
      "loss: 0.23918817937374115, accuracy: 0.9020408163265307\n",
      "loss: 0.2330206334590912, accuracy: 0.9224489795918367\n",
      "loss: 0.24183472990989685, accuracy: 0.9224489795918367\n",
      "loss: 0.22649163007736206, accuracy: 0.9224489795918367\n",
      "loss: 0.23520617187023163, accuracy: 0.9061224489795918\n",
      "loss: 0.24060669541358948, accuracy: 0.9102040816326531\n",
      "loss: 0.231852188706398, accuracy: 0.8979591836734694\n",
      "loss: 0.2308695763349533, accuracy: 0.9224489795918367\n",
      "loss: 0.23304767906665802, accuracy: 0.9224489795918367\n",
      "loss: 0.23078201711177826, accuracy: 0.926530612244898\n",
      "loss: 0.2335897982120514, accuracy: 0.9183673469387755\n",
      "loss: 0.26360225677490234, accuracy: 0.8734693877551021\n",
      "loss: 0.23263780772686005, accuracy: 0.9224489795918367\n",
      "loss: 0.25508832931518555, accuracy: 0.8857142857142857\n",
      "loss: 0.2562401592731476, accuracy: 0.8938775510204081\n",
      "loss: 0.22951442003250122, accuracy: 0.9306122448979591\n",
      "loss: 0.23393628001213074, accuracy: 0.9020408163265307\n",
      "loss: 0.24229511618614197, accuracy: 0.9102040816326531\n",
      "loss: 0.26364853978157043, accuracy: 0.9061224489795918\n",
      "loss: 0.23623886704444885, accuracy: 0.9020408163265307\n",
      "loss: 0.23031823337078094, accuracy: 0.9306122448979591\n",
      "loss: 0.24952346086502075, accuracy: 0.8938775510204081\n",
      "loss: 0.2308342456817627, accuracy: 0.9061224489795918\n",
      "loss: 0.23214156925678253, accuracy: 0.9183673469387755\n",
      "loss: 0.2405724972486496, accuracy: 0.9061224489795918\n",
      "loss: 0.23169489204883575, accuracy: 0.8979591836734694\n",
      "loss: 0.22693991661071777, accuracy: 0.9224489795918367\n",
      "loss: 0.2350403070449829, accuracy: 0.926530612244898\n",
      "loss: 0.22786414623260498, accuracy: 0.9142857142857143\n",
      "loss: 0.24179136753082275, accuracy: 0.9020408163265307\n",
      "loss: 0.2311943918466568, accuracy: 0.9142857142857143\n",
      "loss: 0.23086489737033844, accuracy: 0.9102040816326531\n",
      "loss: 0.22959770262241364, accuracy: 0.9142857142857143\n",
      "loss: 0.2304457724094391, accuracy: 0.926530612244898\n",
      "loss: 0.21766266226768494, accuracy: 0.9183673469387755\n",
      "loss: 0.23161381483078003, accuracy: 0.8979591836734694\n",
      "loss: 0.2170111984014511, accuracy: 0.9224489795918367\n",
      "loss: 0.2655707597732544, accuracy: 0.8816326530612245\n",
      "loss: 0.2438998520374298, accuracy: 0.9020408163265307\n",
      "loss: 0.23857715725898743, accuracy: 0.8979591836734694\n",
      "loss: 0.22633127868175507, accuracy: 0.9102040816326531\n",
      "loss: 0.23310846090316772, accuracy: 0.9224489795918367\n",
      "loss: 0.22420357167720795, accuracy: 0.9306122448979591\n",
      "loss: 0.22562047839164734, accuracy: 0.9142857142857143\n",
      "loss: 0.26010778546333313, accuracy: 0.9020408163265307\n",
      "loss: 0.2229509949684143, accuracy: 0.9183673469387755\n",
      "loss: 0.23768706619739532, accuracy: 0.9020408163265307\n",
      "loss: 0.2559128403663635, accuracy: 0.8938775510204081\n",
      "loss: 0.22314056754112244, accuracy: 0.9224489795918367\n",
      "loss: 0.23762676119804382, accuracy: 0.8857142857142857\n",
      "loss: 0.22590921819210052, accuracy: 0.9306122448979591\n",
      "loss: 0.22116518020629883, accuracy: 0.9102040816326531\n",
      "loss: 0.23189376294612885, accuracy: 0.8979591836734694\n",
      "loss: 0.22134126722812653, accuracy: 0.9142857142857143\n",
      "loss: 0.2146703600883484, accuracy: 0.9224489795918367\n",
      "loss: 0.22724179923534393, accuracy: 0.9061224489795918\n",
      "loss: 0.23810772597789764, accuracy: 0.9020408163265307\n",
      "loss: 0.22673381865024567, accuracy: 0.9224489795918367\n",
      "loss: 0.22228498756885529, accuracy: 0.9224489795918367\n",
      "loss: 0.22027575969696045, accuracy: 0.926530612244898\n",
      "loss: 0.2217026948928833, accuracy: 0.9142857142857143\n",
      "loss: 0.2506573498249054, accuracy: 0.8857142857142857\n",
      "loss: 0.22145608067512512, accuracy: 0.8979591836734694\n",
      "loss: 0.222600057721138, accuracy: 0.9387755102040817\n",
      "loss: 0.23121213912963867, accuracy: 0.9346938775510204\n",
      "loss: 0.22885043919086456, accuracy: 0.9224489795918367\n",
      "loss: 0.23037095367908478, accuracy: 0.9142857142857143\n",
      "loss: 0.21474216878414154, accuracy: 0.9428571428571428\n",
      "loss: 0.2157447338104248, accuracy: 0.9306122448979591\n",
      "loss: 0.2088269293308258, accuracy: 0.9346938775510204\n",
      "loss: 0.23017041385173798, accuracy: 0.9020408163265307\n",
      "loss: 0.22421219944953918, accuracy: 0.926530612244898\n",
      "loss: 0.23231469094753265, accuracy: 0.9020408163265307\n",
      "loss: 0.22046327590942383, accuracy: 0.9183673469387755\n",
      "loss: 0.22862187027931213, accuracy: 0.9142857142857143\n",
      "loss: 0.21716007590293884, accuracy: 0.9306122448979591\n",
      "loss: 0.2245609313249588, accuracy: 0.9142857142857143\n",
      "loss: 0.2088949829339981, accuracy: 0.926530612244898\n",
      "loss: 0.23728592693805695, accuracy: 0.9183673469387755\n",
      "loss: 0.21154998242855072, accuracy: 0.9224489795918367\n",
      "loss: 0.23211795091629028, accuracy: 0.9142857142857143\n",
      "loss: 0.22615458071231842, accuracy: 0.9102040816326531\n",
      "loss: 0.2200271040201187, accuracy: 0.9306122448979591\n",
      "loss: 0.2546640634536743, accuracy: 0.8653061224489796\n",
      "loss: 0.25073522329330444, accuracy: 0.9020408163265307\n",
      "loss: 0.2146199345588684, accuracy: 0.9387755102040817\n",
      "loss: 0.22009475529193878, accuracy: 0.9224489795918367\n",
      "loss: 0.2528071701526642, accuracy: 0.9061224489795918\n",
      "loss: 0.2166365087032318, accuracy: 0.9061224489795918\n",
      "loss: 0.21548032760620117, accuracy: 0.926530612244898\n",
      "loss: 0.2158862203359604, accuracy: 0.9183673469387755\n",
      "loss: 0.23521892726421356, accuracy: 0.9102040816326531\n",
      "loss: 0.21034803986549377, accuracy: 0.9183673469387755\n",
      "loss: 0.2170635461807251, accuracy: 0.926530612244898\n",
      "loss: 0.21242839097976685, accuracy: 0.9387755102040817\n",
      "loss: 0.21785058081150055, accuracy: 0.9224489795918367\n",
      "loss: 0.22303786873817444, accuracy: 0.9102040816326531\n",
      "loss: 0.2096603810787201, accuracy: 0.9387755102040817\n",
      "loss: 0.22195826470851898, accuracy: 0.9183673469387755\n",
      "loss: 0.2214963585138321, accuracy: 0.9142857142857143\n",
      "loss: 0.21294018626213074, accuracy: 0.9183673469387755\n",
      "loss: 0.21646150946617126, accuracy: 0.9224489795918367\n",
      "loss: 0.21951377391815186, accuracy: 0.9306122448979591\n",
      "loss: 0.2444753497838974, accuracy: 0.9142857142857143\n",
      "loss: 0.2151130735874176, accuracy: 0.9102040816326531\n",
      "loss: 0.20626361668109894, accuracy: 0.9428571428571428\n",
      "loss: 0.22809188067913055, accuracy: 0.9306122448979591\n",
      "loss: 0.22475571930408478, accuracy: 0.9020408163265307\n",
      "loss: 0.23496678471565247, accuracy: 0.9224489795918367\n",
      "loss: 0.21371856331825256, accuracy: 0.9061224489795918\n",
      "loss: 0.2279454469680786, accuracy: 0.9183673469387755\n",
      "loss: 0.21246659755706787, accuracy: 0.9224489795918367\n",
      "loss: 0.2082591950893402, accuracy: 0.926530612244898\n",
      "loss: 0.22352154552936554, accuracy: 0.9346938775510204\n",
      "loss: 0.22406792640686035, accuracy: 0.9061224489795918\n",
      "loss: 0.20627361536026, accuracy: 0.9469387755102041\n",
      "loss: 0.21341075003147125, accuracy: 0.9224489795918367\n",
      "loss: 0.2137162834405899, accuracy: 0.9102040816326531\n",
      "loss: 0.23080627620220184, accuracy: 0.9020408163265307\n",
      "loss: 0.20385752618312836, accuracy: 0.9306122448979591\n",
      "loss: 0.2163199931383133, accuracy: 0.926530612244898\n",
      "loss: 0.21111716330051422, accuracy: 0.926530612244898\n",
      "loss: 0.22154667973518372, accuracy: 0.926530612244898\n",
      "loss: 0.22612273693084717, accuracy: 0.9061224489795918\n",
      "loss: 0.21895723044872284, accuracy: 0.9142857142857143\n",
      "loss: 0.21464300155639648, accuracy: 0.9102040816326531\n",
      "loss: 0.22742486000061035, accuracy: 0.9142857142857143\n",
      "loss: 0.24004922807216644, accuracy: 0.8979591836734694\n",
      "loss: 0.20929251611232758, accuracy: 0.9224489795918367\n",
      "loss: 0.2146168202161789, accuracy: 0.9306122448979591\n",
      "loss: 0.20464874804019928, accuracy: 0.9183673469387755\n",
      "loss: 0.20533084869384766, accuracy: 0.9183673469387755\n",
      "loss: 0.2101917862892151, accuracy: 0.9142857142857143\n",
      "loss: 0.22918052971363068, accuracy: 0.9020408163265307\n",
      "loss: 0.20932962000370026, accuracy: 0.9346938775510204\n",
      "loss: 0.22260288894176483, accuracy: 0.9142857142857143\n",
      "loss: 0.20996204018592834, accuracy: 0.9428571428571428\n",
      "loss: 0.2115488201379776, accuracy: 0.9102040816326531\n",
      "loss: 0.21453773975372314, accuracy: 0.9224489795918367\n",
      "loss: 0.20937828719615936, accuracy: 0.9183673469387755\n",
      "loss: 0.21655070781707764, accuracy: 0.9142857142857143\n",
      "loss: 0.20575186610221863, accuracy: 0.9387755102040817\n",
      "loss: 0.2291271686553955, accuracy: 0.9306122448979591\n",
      "loss: 0.20921839773654938, accuracy: 0.9224489795918367\n",
      "loss: 0.21533098816871643, accuracy: 0.9387755102040817\n",
      "loss: 0.24290956556797028, accuracy: 0.889795918367347\n",
      "loss: 0.20759131014347076, accuracy: 0.9224489795918367\n",
      "loss: 0.1959819495677948, accuracy: 0.926530612244898\n",
      "loss: 0.2096187323331833, accuracy: 0.9224489795918367\n",
      "loss: 0.21008123457431793, accuracy: 0.9142857142857143\n",
      "loss: 0.22898903489112854, accuracy: 0.9142857142857143\n",
      "loss: 0.22305041551589966, accuracy: 0.9183673469387755\n",
      "loss: 0.20676501095294952, accuracy: 0.9183673469387755\n",
      "loss: 0.2082081437110901, accuracy: 0.9224489795918367\n",
      "loss: 0.22031810879707336, accuracy: 0.926530612244898\n",
      "loss: 0.20540688931941986, accuracy: 0.926530612244898\n",
      "loss: 0.22139891982078552, accuracy: 0.9183673469387755\n",
      "loss: 0.21305815875530243, accuracy: 0.9224489795918367\n",
      "loss: 0.23476797342300415, accuracy: 0.8857142857142857\n",
      "loss: 0.21998165547847748, accuracy: 0.9224489795918367\n",
      "loss: 0.19891197979450226, accuracy: 0.9346938775510204\n",
      "loss: 0.21988433599472046, accuracy: 0.9306122448979591\n",
      "loss: 0.212315633893013, accuracy: 0.9224489795918367\n",
      "loss: 0.20638740062713623, accuracy: 0.9428571428571428\n",
      "loss: 0.2027047723531723, accuracy: 0.9387755102040817\n",
      "loss: 0.21081410348415375, accuracy: 0.9306122448979591\n",
      "loss: 0.2315334975719452, accuracy: 0.9102040816326531\n",
      "loss: 0.2151578813791275, accuracy: 0.9306122448979591\n",
      "loss: 0.19977526366710663, accuracy: 0.9346938775510204\n",
      "loss: 0.203963503241539, accuracy: 0.9306122448979591\n",
      "loss: 0.1983027458190918, accuracy: 0.9306122448979591\n",
      "loss: 0.210474893450737, accuracy: 0.9061224489795918\n",
      "loss: 0.21547551453113556, accuracy: 0.9142857142857143\n",
      "loss: 0.2211950272321701, accuracy: 0.889795918367347\n",
      "loss: 0.23537877202033997, accuracy: 0.8979591836734694\n",
      "loss: 0.20505376160144806, accuracy: 0.9224489795918367\n",
      "loss: 0.20089441537857056, accuracy: 0.9387755102040817\n",
      "loss: 0.19760259985923767, accuracy: 0.9306122448979591\n",
      "loss: 0.22102920711040497, accuracy: 0.9061224489795918\n",
      "max accuracy: 0.9551020408163265\n",
      "loss: 0.1929107904434204, accuracy: 0.9551020408163265\n",
      "loss: 0.21921099722385406, accuracy: 0.9020408163265307\n",
      "loss: 0.20829413831233978, accuracy: 0.9346938775510204\n",
      "loss: 0.2089124619960785, accuracy: 0.926530612244898\n",
      "loss: 0.20881709456443787, accuracy: 0.9102040816326531\n",
      "loss: 0.19899046421051025, accuracy: 0.9387755102040817\n",
      "loss: 0.19496598839759827, accuracy: 0.9306122448979591\n",
      "loss: 0.23376710712909698, accuracy: 0.9020408163265307\n",
      "loss: 0.2222195416688919, accuracy: 0.9142857142857143\n",
      "loss: 0.19398202002048492, accuracy: 0.9224489795918367\n",
      "loss: 0.2002132534980774, accuracy: 0.9306122448979591\n",
      "loss: 0.2222171425819397, accuracy: 0.9142857142857143\n",
      "loss: 0.21552975475788116, accuracy: 0.9224489795918367\n",
      "loss: 0.21795642375946045, accuracy: 0.9142857142857143\n",
      "loss: 0.20682990550994873, accuracy: 0.926530612244898\n",
      "loss: 0.20205998420715332, accuracy: 0.926530612244898\n",
      "loss: 0.19486956298351288, accuracy: 0.9346938775510204\n",
      "loss: 0.20842653512954712, accuracy: 0.9306122448979591\n",
      "loss: 0.20178428292274475, accuracy: 0.9346938775510204\n",
      "loss: 0.19717125594615936, accuracy: 0.926530612244898\n",
      "loss: 0.1917247623205185, accuracy: 0.926530612244898\n",
      "loss: 0.20410868525505066, accuracy: 0.9142857142857143\n",
      "loss: 0.19623830914497375, accuracy: 0.9469387755102041\n",
      "loss: 0.20056280493736267, accuracy: 0.9306122448979591\n",
      "loss: 0.22640448808670044, accuracy: 0.9183673469387755\n",
      "loss: 0.21898365020751953, accuracy: 0.8979591836734694\n",
      "loss: 0.1896720975637436, accuracy: 0.9551020408163265\n",
      "loss: 0.19324891269207, accuracy: 0.9428571428571428\n",
      "loss: 0.18595756590366364, accuracy: 0.9306122448979591\n",
      "loss: 0.19025683403015137, accuracy: 0.9428571428571428\n",
      "loss: 0.1934775561094284, accuracy: 0.9346938775510204\n",
      "loss: 0.19394074380397797, accuracy: 0.9387755102040817\n",
      "loss: 0.2030727118253708, accuracy: 0.9183673469387755\n",
      "loss: 0.24778218567371368, accuracy: 0.8938775510204081\n",
      "loss: 0.20095506310462952, accuracy: 0.9387755102040817\n",
      "loss: 0.1854637861251831, accuracy: 0.9510204081632653\n",
      "loss: 0.19457103312015533, accuracy: 0.9306122448979591\n",
      "loss: 0.2153664231300354, accuracy: 0.9061224489795918\n",
      "loss: 0.20672348141670227, accuracy: 0.9224489795918367\n",
      "loss: 0.18812240660190582, accuracy: 0.9469387755102041\n",
      "loss: 0.19324590265750885, accuracy: 0.9183673469387755\n",
      "loss: 0.20380979776382446, accuracy: 0.9224489795918367\n",
      "loss: 0.20361541211605072, accuracy: 0.9428571428571428\n",
      "loss: 0.20003288984298706, accuracy: 0.9183673469387755\n",
      "loss: 0.1979304999113083, accuracy: 0.9346938775510204\n",
      "loss: 0.19206549227237701, accuracy: 0.9469387755102041\n",
      "loss: 0.19805414974689484, accuracy: 0.9346938775510204\n",
      "loss: 0.18390198051929474, accuracy: 0.9510204081632653\n",
      "loss: 0.20784439146518707, accuracy: 0.9142857142857143\n",
      "loss: 0.19107364118099213, accuracy: 0.9510204081632653\n",
      "loss: 0.20698343217372894, accuracy: 0.9346938775510204\n",
      "loss: 0.20447108149528503, accuracy: 0.9183673469387755\n",
      "loss: 0.18886129558086395, accuracy: 0.9306122448979591\n",
      "loss: 0.19389601051807404, accuracy: 0.926530612244898\n",
      "loss: 0.18988840281963348, accuracy: 0.9387755102040817\n",
      "loss: 0.19011709094047546, accuracy: 0.9428571428571428\n",
      "loss: 0.1927279233932495, accuracy: 0.9428571428571428\n",
      "loss: 0.20977675914764404, accuracy: 0.9142857142857143\n",
      "loss: 0.19737602770328522, accuracy: 0.926530612244898\n",
      "loss: 0.19257493317127228, accuracy: 0.9306122448979591\n",
      "loss: 0.1868310570716858, accuracy: 0.9387755102040817\n",
      "loss: 0.20238636434078217, accuracy: 0.926530612244898\n",
      "loss: 0.19471894204616547, accuracy: 0.9224489795918367\n",
      "loss: 0.19740751385688782, accuracy: 0.9428571428571428\n",
      "loss: 0.20235341787338257, accuracy: 0.9306122448979591\n",
      "loss: 0.1964552402496338, accuracy: 0.9224489795918367\n",
      "loss: 0.2006339281797409, accuracy: 0.9306122448979591\n",
      "loss: 0.19277149438858032, accuracy: 0.9306122448979591\n",
      "loss: 0.22036845982074738, accuracy: 0.9224489795918367\n",
      "loss: 0.19620585441589355, accuracy: 0.9306122448979591\n",
      "loss: 0.20599183440208435, accuracy: 0.9346938775510204\n",
      "loss: 0.19949208199977875, accuracy: 0.9428571428571428\n",
      "loss: 0.19946295022964478, accuracy: 0.926530612244898\n",
      "loss: 0.1914920061826706, accuracy: 0.9469387755102041\n",
      "loss: 0.18554961681365967, accuracy: 0.9510204081632653\n",
      "loss: 0.1834411323070526, accuracy: 0.9469387755102041\n",
      "loss: 0.18403230607509613, accuracy: 0.9469387755102041\n",
      "loss: 0.20675130188465118, accuracy: 0.9224489795918367\n",
      "loss: 0.18651333451271057, accuracy: 0.9346938775510204\n",
      "loss: 0.19303642213344574, accuracy: 0.9306122448979591\n",
      "loss: 0.20870116353034973, accuracy: 0.9224489795918367\n",
      "loss: 0.20073829591274261, accuracy: 0.9306122448979591\n",
      "loss: 0.19488625228405, accuracy: 0.9306122448979591\n",
      "loss: 0.19164708256721497, accuracy: 0.9346938775510204\n",
      "loss: 0.18479155004024506, accuracy: 0.9428571428571428\n",
      "loss: 0.1963132917881012, accuracy: 0.926530612244898\n",
      "loss: 0.20102982223033905, accuracy: 0.9102040816326531\n",
      "loss: 0.1930421143770218, accuracy: 0.9142857142857143\n",
      "loss: 0.2151954174041748, accuracy: 0.9102040816326531\n",
      "loss: 0.18177573382854462, accuracy: 0.9387755102040817\n",
      "loss: 0.18799230456352234, accuracy: 0.9346938775510204\n",
      "loss: 0.20893678069114685, accuracy: 0.9346938775510204\n",
      "loss: 0.1939881592988968, accuracy: 0.9346938775510204\n",
      "loss: 0.19425009191036224, accuracy: 0.9183673469387755\n",
      "loss: 0.18832318484783173, accuracy: 0.9346938775510204\n",
      "loss: 0.18272168934345245, accuracy: 0.9551020408163265\n",
      "loss: 0.1902989000082016, accuracy: 0.9469387755102041\n",
      "loss: 0.1971987783908844, accuracy: 0.9387755102040817\n",
      "loss: 0.19902591407299042, accuracy: 0.9183673469387755\n",
      "loss: 0.19266502559185028, accuracy: 0.926530612244898\n",
      "loss: 0.2051314264535904, accuracy: 0.9306122448979591\n",
      "loss: 0.18962697684764862, accuracy: 0.926530612244898\n",
      "loss: 0.19908244907855988, accuracy: 0.9387755102040817\n",
      "loss: 0.18789592385292053, accuracy: 0.9428571428571428\n",
      "loss: 0.18330880999565125, accuracy: 0.9510204081632653\n",
      "loss: 0.19805340468883514, accuracy: 0.9346938775510204\n",
      "loss: 0.18396170437335968, accuracy: 0.9346938775510204\n",
      "loss: 0.17818209528923035, accuracy: 0.9346938775510204\n",
      "loss: 0.191066712141037, accuracy: 0.9306122448979591\n",
      "loss: 0.193926602602005, accuracy: 0.9224489795918367\n",
      "loss: 0.18945935368537903, accuracy: 0.9387755102040817\n",
      "loss: 0.199535071849823, accuracy: 0.9306122448979591\n",
      "loss: 0.1886603683233261, accuracy: 0.9510204081632653\n",
      "loss: 0.18007899820804596, accuracy: 0.9510204081632653\n",
      "loss: 0.20885567367076874, accuracy: 0.9224489795918367\n",
      "loss: 0.19281333684921265, accuracy: 0.9306122448979591\n",
      "loss: 0.18586324155330658, accuracy: 0.9387755102040817\n",
      "loss: 0.1882459670305252, accuracy: 0.9306122448979591\n",
      "loss: 0.17632681131362915, accuracy: 0.9469387755102041\n",
      "loss: 0.20185253024101257, accuracy: 0.9224489795918367\n",
      "loss: 0.18849505484104156, accuracy: 0.9346938775510204\n",
      "loss: 0.18891486525535583, accuracy: 0.9469387755102041\n",
      "loss: 0.20555485785007477, accuracy: 0.9306122448979591\n",
      "loss: 0.19266673922538757, accuracy: 0.9387755102040817\n",
      "loss: 0.20468339323997498, accuracy: 0.926530612244898\n",
      "loss: 0.20124533772468567, accuracy: 0.9224489795918367\n",
      "loss: 0.18761500716209412, accuracy: 0.9346938775510204\n",
      "loss: 0.18842457234859467, accuracy: 0.9387755102040817\n",
      "loss: 0.1905534714460373, accuracy: 0.9387755102040817\n",
      "loss: 0.17504148185253143, accuracy: 0.9387755102040817\n",
      "loss: 0.18398381769657135, accuracy: 0.926530612244898\n",
      "loss: 0.1907782107591629, accuracy: 0.9428571428571428\n",
      "loss: 0.18047289550304413, accuracy: 0.9551020408163265\n",
      "loss: 0.1835227906703949, accuracy: 0.9346938775510204\n",
      "loss: 0.1811513751745224, accuracy: 0.9346938775510204\n",
      "loss: 0.1793518215417862, accuracy: 0.9469387755102041\n",
      "loss: 0.1894950419664383, accuracy: 0.926530612244898\n",
      "loss: 0.19407004117965698, accuracy: 0.9306122448979591\n",
      "loss: 0.2110557109117508, accuracy: 0.9224489795918367\n",
      "loss: 0.18324318528175354, accuracy: 0.9510204081632653\n",
      "loss: 0.19346924126148224, accuracy: 0.9183673469387755\n",
      "loss: 0.18345652520656586, accuracy: 0.9387755102040817\n",
      "loss: 0.17451611161231995, accuracy: 0.9510204081632653\n",
      "loss: 0.17840413749217987, accuracy: 0.9428571428571428\n",
      "loss: 0.18984322249889374, accuracy: 0.9306122448979591\n",
      "loss: 0.1813925951719284, accuracy: 0.9306122448979591\n",
      "loss: 0.20049041509628296, accuracy: 0.9306122448979591\n",
      "loss: 0.17817522585391998, accuracy: 0.9346938775510204\n",
      "loss: 0.19708435237407684, accuracy: 0.9102040816326531\n",
      "loss: 0.18391701579093933, accuracy: 0.9428571428571428\n",
      "loss: 0.19440890848636627, accuracy: 0.926530612244898\n",
      "loss: 0.18758222460746765, accuracy: 0.9306122448979591\n",
      "max accuracy: 0.9591836734693877\n",
      "loss: 0.1761419177055359, accuracy: 0.9591836734693877\n",
      "loss: 0.1939641684293747, accuracy: 0.9142857142857143\n",
      "loss: 0.1832738220691681, accuracy: 0.926530612244898\n",
      "loss: 0.16951128840446472, accuracy: 0.9551020408163265\n",
      "loss: 0.19397255778312683, accuracy: 0.9346938775510204\n",
      "loss: 0.18216419219970703, accuracy: 0.9591836734693877\n",
      "loss: 0.1796582043170929, accuracy: 0.9346938775510204\n",
      "loss: 0.17620357871055603, accuracy: 0.9428571428571428\n",
      "loss: 0.20882143080234528, accuracy: 0.9346938775510204\n",
      "loss: 0.17511749267578125, accuracy: 0.9306122448979591\n",
      "loss: 0.20926208794116974, accuracy: 0.926530612244898\n",
      "loss: 0.1902039647102356, accuracy: 0.9346938775510204\n",
      "loss: 0.17957794666290283, accuracy: 0.9469387755102041\n",
      "loss: 0.1805429756641388, accuracy: 0.9469387755102041\n",
      "loss: 0.17915809154510498, accuracy: 0.9428571428571428\n",
      "loss: 0.17147980630397797, accuracy: 0.9428571428571428\n",
      "loss: 0.18104763329029083, accuracy: 0.9387755102040817\n",
      "loss: 0.20699714124202728, accuracy: 0.9102040816326531\n",
      "loss: 0.19339130818843842, accuracy: 0.9224489795918367\n",
      "loss: 0.19181740283966064, accuracy: 0.926530612244898\n",
      "loss: 0.170284703373909, accuracy: 0.9510204081632653\n",
      "loss: 0.1695336103439331, accuracy: 0.9469387755102041\n",
      "loss: 0.1983007937669754, accuracy: 0.9224489795918367\n",
      "loss: 0.19329951703548431, accuracy: 0.9428571428571428\n",
      "loss: 0.17917078733444214, accuracy: 0.9428571428571428\n",
      "loss: 0.18828356266021729, accuracy: 0.926530612244898\n",
      "loss: 0.1809244453907013, accuracy: 0.9183673469387755\n",
      "loss: 0.18328694999217987, accuracy: 0.9551020408163265\n",
      "loss: 0.16792161762714386, accuracy: 0.9591836734693877\n",
      "loss: 0.1767134666442871, accuracy: 0.9469387755102041\n",
      "loss: 0.18043926358222961, accuracy: 0.9306122448979591\n",
      "max accuracy: 0.9673469387755103\n",
      "loss: 0.167856365442276, accuracy: 0.9673469387755103\n",
      "loss: 0.1856086552143097, accuracy: 0.9224489795918367\n",
      "loss: 0.1944107711315155, accuracy: 0.926530612244898\n",
      "loss: 0.18120332062244415, accuracy: 0.9428571428571428\n",
      "loss: 0.18040403723716736, accuracy: 0.9346938775510204\n",
      "loss: 0.17744332551956177, accuracy: 0.9510204081632653\n",
      "loss: 0.1711941510438919, accuracy: 0.9469387755102041\n",
      "loss: 0.20973056554794312, accuracy: 0.9224489795918367\n",
      "loss: 0.17472350597381592, accuracy: 0.9346938775510204\n",
      "loss: 0.1866513043642044, accuracy: 0.9306122448979591\n",
      "loss: 0.16421301662921906, accuracy: 0.9510204081632653\n",
      "loss: 0.1741664558649063, accuracy: 0.9428571428571428\n",
      "loss: 0.1782383918762207, accuracy: 0.9428571428571428\n",
      "loss: 0.17420516908168793, accuracy: 0.9591836734693877\n",
      "loss: 0.16761772334575653, accuracy: 0.9510204081632653\n",
      "loss: 0.1619899719953537, accuracy: 0.9306122448979591\n",
      "loss: 0.17597809433937073, accuracy: 0.9510204081632653\n",
      "loss: 0.17315633594989777, accuracy: 0.9510204081632653\n",
      "loss: 0.1764346957206726, accuracy: 0.9469387755102041\n",
      "loss: 0.16701367497444153, accuracy: 0.9510204081632653\n",
      "loss: 0.2021411657333374, accuracy: 0.9183673469387755\n",
      "loss: 0.18268230557441711, accuracy: 0.9306122448979591\n",
      "loss: 0.17084957659244537, accuracy: 0.9469387755102041\n",
      "loss: 0.17502816021442413, accuracy: 0.9387755102040817\n",
      "loss: 0.18417049944400787, accuracy: 0.9387755102040817\n",
      "loss: 0.1684287190437317, accuracy: 0.9510204081632653\n",
      "loss: 0.16257460415363312, accuracy: 0.9469387755102041\n",
      "loss: 0.1742618978023529, accuracy: 0.9428571428571428\n",
      "loss: 0.17461150884628296, accuracy: 0.9387755102040817\n",
      "loss: 0.1837742179632187, accuracy: 0.9306122448979591\n",
      "loss: 0.16574928164482117, accuracy: 0.9469387755102041\n",
      "loss: 0.17540869116783142, accuracy: 0.9428571428571428\n",
      "loss: 0.16379226744174957, accuracy: 0.9346938775510204\n",
      "loss: 0.17469896376132965, accuracy: 0.9469387755102041\n",
      "loss: 0.1719791293144226, accuracy: 0.9469387755102041\n",
      "loss: 0.1689281314611435, accuracy: 0.9469387755102041\n",
      "loss: 0.1914137303829193, accuracy: 0.926530612244898\n",
      "loss: 0.1873505562543869, accuracy: 0.9183673469387755\n",
      "loss: 0.17195627093315125, accuracy: 0.9469387755102041\n",
      "loss: 0.16647307574748993, accuracy: 0.9428571428571428\n",
      "loss: 0.17715029418468475, accuracy: 0.9387755102040817\n",
      "loss: 0.18400895595550537, accuracy: 0.9428571428571428\n",
      "loss: 0.18561886250972748, accuracy: 0.9183673469387755\n",
      "loss: 0.18443554639816284, accuracy: 0.926530612244898\n",
      "loss: 0.16914427280426025, accuracy: 0.9469387755102041\n",
      "loss: 0.16322533786296844, accuracy: 0.9591836734693877\n",
      "loss: 0.16540518403053284, accuracy: 0.9469387755102041\n",
      "loss: 0.17962633073329926, accuracy: 0.9428571428571428\n",
      "loss: 0.20179714262485504, accuracy: 0.926530612244898\n",
      "loss: 0.16341009736061096, accuracy: 0.9510204081632653\n",
      "loss: 0.17967543005943298, accuracy: 0.9387755102040817\n",
      "loss: 0.18865197896957397, accuracy: 0.9142857142857143\n",
      "loss: 0.19670289754867554, accuracy: 0.9346938775510204\n",
      "loss: 0.1846356987953186, accuracy: 0.9306122448979591\n",
      "loss: 0.16116450726985931, accuracy: 0.9428571428571428\n",
      "loss: 0.16354568302631378, accuracy: 0.9387755102040817\n",
      "loss: 0.17376936972141266, accuracy: 0.9510204081632653\n",
      "loss: 0.18477033078670502, accuracy: 0.9224489795918367\n",
      "loss: 0.18319998681545258, accuracy: 0.9306122448979591\n",
      "loss: 0.1593315154314041, accuracy: 0.9551020408163265\n",
      "loss: 0.1698860228061676, accuracy: 0.9346938775510204\n",
      "loss: 0.15949712693691254, accuracy: 0.9387755102040817\n",
      "loss: 0.1563461720943451, accuracy: 0.9591836734693877\n",
      "loss: 0.17311105132102966, accuracy: 0.9428571428571428\n",
      "loss: 0.17908698320388794, accuracy: 0.9428571428571428\n",
      "loss: 0.1707020103931427, accuracy: 0.9387755102040817\n",
      "loss: 0.16364260017871857, accuracy: 0.9469387755102041\n",
      "loss: 0.15862701833248138, accuracy: 0.9387755102040817\n",
      "loss: 0.16944654285907745, accuracy: 0.926530612244898\n",
      "loss: 0.1705714613199234, accuracy: 0.9469387755102041\n",
      "loss: 0.1616278886795044, accuracy: 0.9469387755102041\n",
      "loss: 0.165411576628685, accuracy: 0.9469387755102041\n",
      "loss: 0.16607005894184113, accuracy: 0.9428571428571428\n",
      "loss: 0.16040128469467163, accuracy: 0.9469387755102041\n",
      "loss: 0.16605104506015778, accuracy: 0.9428571428571428\n",
      "loss: 0.1786467730998993, accuracy: 0.9469387755102041\n",
      "loss: 0.17461296916007996, accuracy: 0.9306122448979591\n",
      "loss: 0.1695907860994339, accuracy: 0.9387755102040817\n",
      "loss: 0.1711536943912506, accuracy: 0.9346938775510204\n",
      "loss: 0.18064171075820923, accuracy: 0.926530612244898\n",
      "loss: 0.16508403420448303, accuracy: 0.9510204081632653\n",
      "loss: 0.17629146575927734, accuracy: 0.926530612244898\n",
      "loss: 0.16641396284103394, accuracy: 0.9469387755102041\n",
      "loss: 0.1695166975259781, accuracy: 0.9306122448979591\n",
      "loss: 0.17201538383960724, accuracy: 0.9346938775510204\n",
      "loss: 0.16462963819503784, accuracy: 0.9469387755102041\n",
      "loss: 0.16774515807628632, accuracy: 0.9469387755102041\n",
      "loss: 0.21335291862487793, accuracy: 0.9102040816326531\n",
      "loss: 0.15434147417545319, accuracy: 0.9551020408163265\n",
      "loss: 0.1637599766254425, accuracy: 0.9428571428571428\n",
      "loss: 0.15584665536880493, accuracy: 0.9551020408163265\n",
      "loss: 0.1798635870218277, accuracy: 0.9306122448979591\n",
      "loss: 0.1698228418827057, accuracy: 0.9387755102040817\n",
      "loss: 0.15819469094276428, accuracy: 0.9428571428571428\n",
      "loss: 0.1685151308774948, accuracy: 0.9387755102040817\n",
      "loss: 0.18149983882904053, accuracy: 0.9224489795918367\n",
      "loss: 0.1943158060312271, accuracy: 0.9306122448979591\n",
      "loss: 0.16861926019191742, accuracy: 0.9346938775510204\n",
      "loss: 0.1605062186717987, accuracy: 0.9551020408163265\n",
      "loss: 0.16473667323589325, accuracy: 0.9469387755102041\n",
      "loss: 0.17140617966651917, accuracy: 0.9428571428571428\n",
      "loss: 0.15930579602718353, accuracy: 0.9510204081632653\n",
      "loss: 0.15586327016353607, accuracy: 0.9551020408163265\n",
      "loss: 0.17285719513893127, accuracy: 0.9428571428571428\n",
      "loss: 0.1773703694343567, accuracy: 0.9387755102040817\n",
      "loss: 0.17422907054424286, accuracy: 0.9387755102040817\n",
      "loss: 0.15557999908924103, accuracy: 0.9510204081632653\n",
      "loss: 0.18097610771656036, accuracy: 0.926530612244898\n",
      "loss: 0.15925706923007965, accuracy: 0.9510204081632653\n",
      "loss: 0.1807052195072174, accuracy: 0.926530612244898\n",
      "loss: 0.1658250242471695, accuracy: 0.9428571428571428\n",
      "loss: 0.15953916311264038, accuracy: 0.9428571428571428\n",
      "loss: 0.16321441531181335, accuracy: 0.9551020408163265\n",
      "loss: 0.1772232949733734, accuracy: 0.9306122448979591\n",
      "loss: 0.1597961187362671, accuracy: 0.9428571428571428\n",
      "loss: 0.1643463671207428, accuracy: 0.9387755102040817\n",
      "loss: 0.16433224081993103, accuracy: 0.9469387755102041\n",
      "loss: 0.1795680969953537, accuracy: 0.9224489795918367\n",
      "loss: 0.1773790419101715, accuracy: 0.9469387755102041\n",
      "loss: 0.1579604148864746, accuracy: 0.9469387755102041\n",
      "loss: 0.1579066514968872, accuracy: 0.9591836734693877\n",
      "loss: 0.1575586497783661, accuracy: 0.9469387755102041\n",
      "loss: 0.18664169311523438, accuracy: 0.9306122448979591\n",
      "loss: 0.16823652386665344, accuracy: 0.9346938775510204\n",
      "loss: 0.15558598935604095, accuracy: 0.9469387755102041\n",
      "loss: 0.17508941888809204, accuracy: 0.9428571428571428\n",
      "loss: 0.18304383754730225, accuracy: 0.926530612244898\n",
      "loss: 0.1662253886461258, accuracy: 0.9428571428571428\n",
      "loss: 0.16682633757591248, accuracy: 0.9387755102040817\n",
      "loss: 0.17092491686344147, accuracy: 0.9224489795918367\n",
      "loss: 0.1777203530073166, accuracy: 0.9346938775510204\n",
      "loss: 0.1679898053407669, accuracy: 0.9346938775510204\n",
      "loss: 0.163314551115036, accuracy: 0.9387755102040817\n",
      "loss: 0.15478314459323883, accuracy: 0.9510204081632653\n",
      "loss: 0.16337370872497559, accuracy: 0.9469387755102041\n",
      "loss: 0.15647710859775543, accuracy: 0.9387755102040817\n",
      "loss: 0.15228797495365143, accuracy: 0.9551020408163265\n",
      "loss: 0.1537410169839859, accuracy: 0.9469387755102041\n",
      "loss: 0.15077421069145203, accuracy: 0.9469387755102041\n",
      "loss: 0.1473914384841919, accuracy: 0.963265306122449\n",
      "loss: 0.15773823857307434, accuracy: 0.9428571428571428\n",
      "loss: 0.16094717383384705, accuracy: 0.9551020408163265\n",
      "loss: 0.16380831599235535, accuracy: 0.9591836734693877\n",
      "loss: 0.1555335968732834, accuracy: 0.9469387755102041\n",
      "loss: 0.15254980325698853, accuracy: 0.9591836734693877\n",
      "loss: 0.1529110074043274, accuracy: 0.9591836734693877\n",
      "loss: 0.15054869651794434, accuracy: 0.9510204081632653\n",
      "loss: 0.1650918573141098, accuracy: 0.9428571428571428\n",
      "loss: 0.1478664129972458, accuracy: 0.9551020408163265\n",
      "loss: 0.15553142130374908, accuracy: 0.9469387755102041\n",
      "loss: 0.16437259316444397, accuracy: 0.9428571428571428\n",
      "loss: 0.1487930715084076, accuracy: 0.9551020408163265\n",
      "loss: 0.160479336977005, accuracy: 0.9428571428571428\n",
      "loss: 0.17032979428768158, accuracy: 0.9469387755102041\n",
      "loss: 0.15185977518558502, accuracy: 0.963265306122449\n",
      "loss: 0.14817403256893158, accuracy: 0.9510204081632653\n",
      "loss: 0.15333884954452515, accuracy: 0.9510204081632653\n",
      "loss: 0.15701374411582947, accuracy: 0.9428571428571428\n",
      "loss: 0.14688605070114136, accuracy: 0.9591836734693877\n",
      "loss: 0.17038600146770477, accuracy: 0.9469387755102041\n",
      "loss: 0.16477540135383606, accuracy: 0.9387755102040817\n",
      "loss: 0.15104982256889343, accuracy: 0.963265306122449\n",
      "loss: 0.15045614540576935, accuracy: 0.9591836734693877\n",
      "loss: 0.16000092029571533, accuracy: 0.9306122448979591\n",
      "loss: 0.15265151858329773, accuracy: 0.9510204081632653\n",
      "loss: 0.14809559285640717, accuracy: 0.9551020408163265\n",
      "loss: 0.16541875898838043, accuracy: 0.9510204081632653\n",
      "loss: 0.17653127014636993, accuracy: 0.9346938775510204\n",
      "loss: 0.17238739132881165, accuracy: 0.9346938775510204\n",
      "loss: 0.1532510370016098, accuracy: 0.9387755102040817\n",
      "loss: 0.15351040661334991, accuracy: 0.9510204081632653\n",
      "loss: 0.15750819444656372, accuracy: 0.9428571428571428\n",
      "loss: 0.15781183540821075, accuracy: 0.9591836734693877\n",
      "loss: 0.15468066930770874, accuracy: 0.9551020408163265\n",
      "loss: 0.14945684373378754, accuracy: 0.9387755102040817\n",
      "loss: 0.1541823446750641, accuracy: 0.9428571428571428\n",
      "loss: 0.14799287915229797, accuracy: 0.9591836734693877\n",
      "loss: 0.1456991732120514, accuracy: 0.9673469387755103\n",
      "loss: 0.14746461808681488, accuracy: 0.963265306122449\n",
      "loss: 0.1444311887025833, accuracy: 0.9591836734693877\n",
      "loss: 0.14542429149150848, accuracy: 0.963265306122449\n",
      "loss: 0.1449567824602127, accuracy: 0.9510204081632653\n",
      "loss: 0.16330255568027496, accuracy: 0.9346938775510204\n",
      "loss: 0.14963273704051971, accuracy: 0.9469387755102041\n",
      "loss: 0.15623150765895844, accuracy: 0.9428571428571428\n",
      "loss: 0.1502288579940796, accuracy: 0.9428571428571428\n",
      "loss: 0.14725357294082642, accuracy: 0.9551020408163265\n",
      "loss: 0.16100677847862244, accuracy: 0.9510204081632653\n",
      "loss: 0.15407831966876984, accuracy: 0.9428571428571428\n",
      "loss: 0.1468551903963089, accuracy: 0.9469387755102041\n",
      "loss: 0.17158401012420654, accuracy: 0.9387755102040817\n",
      "loss: 0.14802023768424988, accuracy: 0.9591836734693877\n",
      "loss: 0.15002314746379852, accuracy: 0.9510204081632653\n",
      "loss: 0.14891964197158813, accuracy: 0.9510204081632653\n",
      "loss: 0.1484711766242981, accuracy: 0.9591836734693877\n",
      "loss: 0.14946559071540833, accuracy: 0.9551020408163265\n",
      "loss: 0.1523842215538025, accuracy: 0.9551020408163265\n",
      "loss: 0.1437394917011261, accuracy: 0.9551020408163265\n",
      "loss: 0.14942730963230133, accuracy: 0.9551020408163265\n",
      "loss: 0.14794784784317017, accuracy: 0.9551020408163265\n",
      "loss: 0.15609942376613617, accuracy: 0.9551020408163265\n",
      "loss: 0.14838740229606628, accuracy: 0.9591836734693877\n",
      "loss: 0.14204692840576172, accuracy: 0.9551020408163265\n",
      "loss: 0.1479598432779312, accuracy: 0.9510204081632653\n",
      "loss: 0.14159958064556122, accuracy: 0.9510204081632653\n",
      "loss: 0.15508897602558136, accuracy: 0.9346938775510204\n",
      "loss: 0.15233521163463593, accuracy: 0.9469387755102041\n",
      "loss: 0.1594352424144745, accuracy: 0.9551020408163265\n",
      "loss: 0.14577049016952515, accuracy: 0.9510204081632653\n",
      "loss: 0.14393118023872375, accuracy: 0.9591836734693877\n",
      "loss: 0.14804793894290924, accuracy: 0.9428571428571428\n",
      "loss: 0.1496676355600357, accuracy: 0.9591836734693877\n",
      "loss: 0.19197513163089752, accuracy: 0.926530612244898\n",
      "loss: 0.14817452430725098, accuracy: 0.963265306122449\n",
      "loss: 0.14496026933193207, accuracy: 0.9510204081632653\n",
      "loss: 0.14603273570537567, accuracy: 0.9551020408163265\n",
      "loss: 0.14648640155792236, accuracy: 0.9510204081632653\n",
      "loss: 0.14398138225078583, accuracy: 0.9591836734693877\n",
      "loss: 0.1501513421535492, accuracy: 0.963265306122449\n",
      "loss: 0.15853041410446167, accuracy: 0.9469387755102041\n",
      "loss: 0.15443675220012665, accuracy: 0.9387755102040817\n",
      "loss: 0.1670619249343872, accuracy: 0.9346938775510204\n",
      "loss: 0.1415233314037323, accuracy: 0.9591836734693877\n",
      "loss: 0.137578085064888, accuracy: 0.9673469387755103\n",
      "loss: 0.141288161277771, accuracy: 0.9591836734693877\n",
      "loss: 0.14266003668308258, accuracy: 0.9551020408163265\n",
      "loss: 0.14458872377872467, accuracy: 0.9591836734693877\n",
      "loss: 0.15624065697193146, accuracy: 0.9346938775510204\n",
      "loss: 0.14705926179885864, accuracy: 0.9591836734693877\n",
      "loss: 0.14190499484539032, accuracy: 0.9591836734693877\n",
      "loss: 0.14484572410583496, accuracy: 0.9591836734693877\n",
      "loss: 0.1420602798461914, accuracy: 0.9673469387755103\n",
      "loss: 0.14694271981716156, accuracy: 0.9387755102040817\n",
      "loss: 0.14562192559242249, accuracy: 0.9551020408163265\n",
      "loss: 0.1604546755552292, accuracy: 0.9346938775510204\n",
      "loss: 0.14832134544849396, accuracy: 0.9510204081632653\n",
      "loss: 0.14133943617343903, accuracy: 0.9551020408163265\n",
      "loss: 0.14271864295005798, accuracy: 0.9591836734693877\n",
      "loss: 0.13954992592334747, accuracy: 0.9551020408163265\n",
      "loss: 0.1433166265487671, accuracy: 0.9591836734693877\n",
      "loss: 0.14926698803901672, accuracy: 0.9510204081632653\n",
      "loss: 0.13820073008537292, accuracy: 0.9551020408163265\n",
      "loss: 0.15741153061389923, accuracy: 0.9469387755102041\n",
      "loss: 0.14483539760112762, accuracy: 0.9551020408163265\n",
      "loss: 0.13535043597221375, accuracy: 0.9591836734693877\n",
      "loss: 0.1383308321237564, accuracy: 0.9591836734693877\n",
      "loss: 0.13783831894397736, accuracy: 0.9591836734693877\n",
      "loss: 0.1811961829662323, accuracy: 0.9346938775510204\n",
      "loss: 0.15090586245059967, accuracy: 0.9551020408163265\n",
      "loss: 0.13834887742996216, accuracy: 0.9673469387755103\n",
      "loss: 0.137209951877594, accuracy: 0.9591836734693877\n",
      "loss: 0.14010922610759735, accuracy: 0.9591836734693877\n",
      "loss: 0.13875462114810944, accuracy: 0.9591836734693877\n",
      "loss: 0.13502225279808044, accuracy: 0.9591836734693877\n",
      "loss: 0.14590466022491455, accuracy: 0.9510204081632653\n",
      "loss: 0.13921156525611877, accuracy: 0.9591836734693877\n",
      "loss: 0.1854339838027954, accuracy: 0.9142857142857143\n",
      "loss: 0.13452467322349548, accuracy: 0.963265306122449\n",
      "loss: 0.15390029549598694, accuracy: 0.9469387755102041\n",
      "loss: 0.1424536257982254, accuracy: 0.9551020408163265\n",
      "loss: 0.13599812984466553, accuracy: 0.9551020408163265\n",
      "loss: 0.1464506983757019, accuracy: 0.9510204081632653\n",
      "loss: 0.1555195152759552, accuracy: 0.9469387755102041\n",
      "loss: 0.15813744068145752, accuracy: 0.9428571428571428\n",
      "loss: 0.13892030715942383, accuracy: 0.9551020408163265\n",
      "loss: 0.13478220999240875, accuracy: 0.9510204081632653\n",
      "loss: 0.15001600980758667, accuracy: 0.9591836734693877\n",
      "loss: 0.13613958656787872, accuracy: 0.9551020408163265\n",
      "loss: 0.15575286746025085, accuracy: 0.9306122448979591\n",
      "loss: 0.14411601424217224, accuracy: 0.9387755102040817\n",
      "loss: 0.1396637260913849, accuracy: 0.9551020408163265\n",
      "loss: 0.13217468559741974, accuracy: 0.9591836734693877\n",
      "loss: 0.14911888539791107, accuracy: 0.9469387755102041\n",
      "loss: 0.1417381912469864, accuracy: 0.9551020408163265\n",
      "loss: 0.14898933470249176, accuracy: 0.9591836734693877\n",
      "loss: 0.14659008383750916, accuracy: 0.963265306122449\n",
      "loss: 0.13143976032733917, accuracy: 0.9510204081632653\n",
      "loss: 0.1633272022008896, accuracy: 0.9469387755102041\n",
      "loss: 0.14320507645606995, accuracy: 0.9551020408163265\n",
      "loss: 0.15438151359558105, accuracy: 0.9428571428571428\n",
      "loss: 0.1355794072151184, accuracy: 0.9551020408163265\n",
      "loss: 0.13660801947116852, accuracy: 0.9428571428571428\n",
      "loss: 0.1400204598903656, accuracy: 0.9469387755102041\n",
      "loss: 0.1458895057439804, accuracy: 0.9469387755102041\n",
      "loss: 0.14275111258029938, accuracy: 0.9469387755102041\n",
      "loss: 0.13807961344718933, accuracy: 0.9551020408163265\n",
      "loss: 0.15758444368839264, accuracy: 0.9428571428571428\n",
      "loss: 0.13386142253875732, accuracy: 0.9591836734693877\n",
      "loss: 0.1596323549747467, accuracy: 0.9428571428571428\n",
      "loss: 0.13804620504379272, accuracy: 0.9551020408163265\n",
      "loss: 0.14065834879875183, accuracy: 0.9510204081632653\n",
      "loss: 0.1456225961446762, accuracy: 0.9510204081632653\n",
      "loss: 0.145530104637146, accuracy: 0.9510204081632653\n",
      "loss: 0.1336153894662857, accuracy: 0.9469387755102041\n",
      "loss: 0.13938482105731964, accuracy: 0.9428571428571428\n",
      "loss: 0.13624703884124756, accuracy: 0.9591836734693877\n",
      "loss: 0.14606177806854248, accuracy: 0.9510204081632653\n",
      "loss: 0.142888605594635, accuracy: 0.9510204081632653\n",
      "loss: 0.1355896294116974, accuracy: 0.9591836734693877\n",
      "loss: 0.16190743446350098, accuracy: 0.9428571428571428\n",
      "loss: 0.14925244450569153, accuracy: 0.9551020408163265\n",
      "loss: 0.13583891093730927, accuracy: 0.963265306122449\n",
      "loss: 0.13290293514728546, accuracy: 0.9469387755102041\n",
      "loss: 0.13856244087219238, accuracy: 0.9551020408163265\n",
      "max accuracy: 0.9714285714285714\n",
      "loss: 0.13154280185699463, accuracy: 0.9714285714285714\n",
      "loss: 0.15051503479480743, accuracy: 0.9346938775510204\n",
      "loss: 0.14255854487419128, accuracy: 0.9469387755102041\n",
      "loss: 0.1562141478061676, accuracy: 0.9346938775510204\n",
      "loss: 0.1316145360469818, accuracy: 0.9591836734693877\n",
      "loss: 0.14099858701229095, accuracy: 0.9551020408163265\n",
      "loss: 0.1396644562482834, accuracy: 0.9510204081632653\n",
      "loss: 0.1367613524198532, accuracy: 0.9551020408163265\n",
      "loss: 0.13245999813079834, accuracy: 0.9591836734693877\n",
      "loss: 0.15727198123931885, accuracy: 0.9469387755102041\n",
      "loss: 0.13575586676597595, accuracy: 0.9591836734693877\n",
      "loss: 0.14262822270393372, accuracy: 0.9510204081632653\n",
      "loss: 0.14931899309158325, accuracy: 0.9469387755102041\n",
      "loss: 0.1389877200126648, accuracy: 0.9591836734693877\n",
      "loss: 0.149129256606102, accuracy: 0.9551020408163265\n",
      "loss: 0.12466702610254288, accuracy: 0.9551020408163265\n",
      "loss: 0.1395498663187027, accuracy: 0.9551020408163265\n",
      "loss: 0.12938426434993744, accuracy: 0.9510204081632653\n",
      "loss: 0.14742092788219452, accuracy: 0.9387755102040817\n",
      "loss: 0.12529337406158447, accuracy: 0.9591836734693877\n",
      "loss: 0.1315765678882599, accuracy: 0.9591836734693877\n",
      "loss: 0.12348798662424088, accuracy: 0.9714285714285714\n",
      "loss: 0.14580364525318146, accuracy: 0.9428571428571428\n",
      "loss: 0.13239605724811554, accuracy: 0.9510204081632653\n",
      "loss: 0.13916179537773132, accuracy: 0.9591836734693877\n",
      "loss: 0.14266523718833923, accuracy: 0.9469387755102041\n",
      "loss: 0.13385067880153656, accuracy: 0.963265306122449\n",
      "loss: 0.1333712488412857, accuracy: 0.9510204081632653\n",
      "loss: 0.1342432051897049, accuracy: 0.963265306122449\n",
      "loss: 0.13500237464904785, accuracy: 0.9510204081632653\n",
      "loss: 0.13936752080917358, accuracy: 0.9428571428571428\n",
      "loss: 0.13542728126049042, accuracy: 0.9510204081632653\n",
      "loss: 0.12970560789108276, accuracy: 0.9551020408163265\n",
      "loss: 0.12682223320007324, accuracy: 0.9714285714285714\n",
      "loss: 0.1569080501794815, accuracy: 0.9469387755102041\n",
      "loss: 0.13274137675762177, accuracy: 0.9673469387755103\n",
      "loss: 0.14741620421409607, accuracy: 0.963265306122449\n",
      "loss: 0.12655457854270935, accuracy: 0.9673469387755103\n",
      "loss: 0.14122694730758667, accuracy: 0.9428571428571428\n",
      "loss: 0.14111873507499695, accuracy: 0.9428571428571428\n",
      "loss: 0.12458018213510513, accuracy: 0.963265306122449\n",
      "loss: 0.12221977114677429, accuracy: 0.963265306122449\n",
      "loss: 0.13943932950496674, accuracy: 0.9551020408163265\n",
      "loss: 0.13198156654834747, accuracy: 0.9510204081632653\n",
      "loss: 0.1335369199514389, accuracy: 0.9673469387755103\n",
      "loss: 0.129492849111557, accuracy: 0.9551020408163265\n",
      "loss: 0.12884119153022766, accuracy: 0.9551020408163265\n",
      "loss: 0.14243601262569427, accuracy: 0.9510204081632653\n",
      "loss: 0.14237326383590698, accuracy: 0.9510204081632653\n",
      "loss: 0.12109913676977158, accuracy: 0.9673469387755103\n",
      "loss: 0.14735399186611176, accuracy: 0.9428571428571428\n",
      "loss: 0.1317298412322998, accuracy: 0.9469387755102041\n",
      "loss: 0.1301332712173462, accuracy: 0.9551020408163265\n",
      "loss: 0.12377471476793289, accuracy: 0.9551020408163265\n",
      "loss: 0.12977562844753265, accuracy: 0.9673469387755103\n",
      "loss: 0.1414804458618164, accuracy: 0.9551020408163265\n",
      "loss: 0.12232305854558945, accuracy: 0.963265306122449\n",
      "loss: 0.13073542714118958, accuracy: 0.963265306122449\n",
      "loss: 0.12599614262580872, accuracy: 0.963265306122449\n",
      "loss: 0.13315382599830627, accuracy: 0.9428571428571428\n",
      "loss: 0.12332919239997864, accuracy: 0.963265306122449\n",
      "loss: 0.13244058191776276, accuracy: 0.9428571428571428\n",
      "loss: 0.12433842569589615, accuracy: 0.9591836734693877\n",
      "loss: 0.12193173915147781, accuracy: 0.9673469387755103\n",
      "loss: 0.12395048141479492, accuracy: 0.9673469387755103\n",
      "loss: 0.1244092732667923, accuracy: 0.9510204081632653\n",
      "loss: 0.13074076175689697, accuracy: 0.9551020408163265\n",
      "loss: 0.1296401470899582, accuracy: 0.9551020408163265\n",
      "loss: 0.12887854874134064, accuracy: 0.9551020408163265\n",
      "loss: 0.12754984200000763, accuracy: 0.9591836734693877\n",
      "loss: 0.13496294617652893, accuracy: 0.9469387755102041\n",
      "loss: 0.13685107231140137, accuracy: 0.9591836734693877\n",
      "loss: 0.16315189003944397, accuracy: 0.9428571428571428\n",
      "loss: 0.1284974366426468, accuracy: 0.9591836734693877\n",
      "loss: 0.12658317387104034, accuracy: 0.9551020408163265\n",
      "loss: 0.1256416290998459, accuracy: 0.9510204081632653\n",
      "loss: 0.12650738656520844, accuracy: 0.9510204081632653\n",
      "max accuracy: 0.9755102040816327\n",
      "loss: 0.1218029111623764, accuracy: 0.9755102040816327\n",
      "loss: 0.119423046708107, accuracy: 0.9551020408163265\n",
      "loss: 0.12775105237960815, accuracy: 0.9510204081632653\n",
      "loss: 0.13331912457942963, accuracy: 0.9469387755102041\n",
      "loss: 0.1233266070485115, accuracy: 0.9551020408163265\n",
      "loss: 0.13461968302726746, accuracy: 0.9510204081632653\n",
      "loss: 0.130649134516716, accuracy: 0.9591836734693877\n",
      "loss: 0.13180330395698547, accuracy: 0.9551020408163265\n",
      "loss: 0.12276332080364227, accuracy: 0.9551020408163265\n",
      "loss: 0.1281866431236267, accuracy: 0.9551020408163265\n",
      "loss: 0.18376074731349945, accuracy: 0.926530612244898\n",
      "loss: 0.13037024438381195, accuracy: 0.9469387755102041\n",
      "loss: 0.13262255489826202, accuracy: 0.9591836734693877\n",
      "loss: 0.12922221422195435, accuracy: 0.9591836734693877\n",
      "loss: 0.12534897029399872, accuracy: 0.963265306122449\n",
      "loss: 0.12165647000074387, accuracy: 0.9591836734693877\n",
      "loss: 0.13064056634902954, accuracy: 0.9510204081632653\n",
      "loss: 0.12833327054977417, accuracy: 0.9551020408163265\n",
      "loss: 0.12350716441869736, accuracy: 0.9551020408163265\n",
      "loss: 0.14020906388759613, accuracy: 0.9551020408163265\n",
      "loss: 0.13100369274616241, accuracy: 0.9591836734693877\n",
      "loss: 0.1383741796016693, accuracy: 0.963265306122449\n",
      "loss: 0.12542231380939484, accuracy: 0.9591836734693877\n",
      "loss: 0.12046604603528976, accuracy: 0.963265306122449\n",
      "loss: 0.11783555895090103, accuracy: 0.9714285714285714\n",
      "loss: 0.12092506885528564, accuracy: 0.9714285714285714\n",
      "loss: 0.12764297425746918, accuracy: 0.9591836734693877\n",
      "loss: 0.12652182579040527, accuracy: 0.963265306122449\n",
      "loss: 0.12946192920207977, accuracy: 0.9591836734693877\n",
      "loss: 0.13077761232852936, accuracy: 0.9673469387755103\n",
      "loss: 0.12208694219589233, accuracy: 0.9591836734693877\n",
      "loss: 0.15434741973876953, accuracy: 0.9428571428571428\n",
      "loss: 0.13578355312347412, accuracy: 0.9591836734693877\n",
      "loss: 0.13798867166042328, accuracy: 0.9469387755102041\n",
      "loss: 0.12469426542520523, accuracy: 0.9551020408163265\n",
      "loss: 0.12006677687168121, accuracy: 0.9469387755102041\n",
      "loss: 0.12684711813926697, accuracy: 0.9714285714285714\n",
      "loss: 0.11958498507738113, accuracy: 0.9510204081632653\n",
      "loss: 0.12339676916599274, accuracy: 0.9591836734693877\n",
      "loss: 0.12535540759563446, accuracy: 0.9591836734693877\n",
      "loss: 0.13150236010551453, accuracy: 0.9510204081632653\n",
      "loss: 0.12069402635097504, accuracy: 0.9673469387755103\n",
      "loss: 0.13457156717777252, accuracy: 0.9591836734693877\n",
      "loss: 0.1253286749124527, accuracy: 0.9469387755102041\n",
      "loss: 0.11657889187335968, accuracy: 0.9591836734693877\n",
      "loss: 0.1183660477399826, accuracy: 0.9673469387755103\n",
      "loss: 0.11974212527275085, accuracy: 0.9591836734693877\n",
      "loss: 0.12673640251159668, accuracy: 0.9591836734693877\n",
      "loss: 0.12236803770065308, accuracy: 0.9591836734693877\n",
      "loss: 0.11678897589445114, accuracy: 0.9551020408163265\n",
      "loss: 0.12865526974201202, accuracy: 0.9510204081632653\n",
      "loss: 0.12777112424373627, accuracy: 0.963265306122449\n",
      "loss: 0.1204601600766182, accuracy: 0.963265306122449\n",
      "loss: 0.11844497174024582, accuracy: 0.9510204081632653\n",
      "loss: 0.13493046164512634, accuracy: 0.963265306122449\n",
      "loss: 0.1576583981513977, accuracy: 0.9306122448979591\n",
      "loss: 0.13001291453838348, accuracy: 0.9469387755102041\n",
      "loss: 0.11861894279718399, accuracy: 0.9591836734693877\n",
      "loss: 0.11612038314342499, accuracy: 0.963265306122449\n",
      "loss: 0.11980406939983368, accuracy: 0.9591836734693877\n",
      "loss: 0.11791040748357773, accuracy: 0.9673469387755103\n",
      "loss: 0.14557646214962006, accuracy: 0.9346938775510204\n",
      "loss: 0.1196717694401741, accuracy: 0.9673469387755103\n",
      "loss: 0.12158267945051193, accuracy: 0.9551020408163265\n",
      "loss: 0.12010806053876877, accuracy: 0.9591836734693877\n",
      "loss: 0.11853683739900589, accuracy: 0.963265306122449\n",
      "loss: 0.1160014346241951, accuracy: 0.9591836734693877\n",
      "loss: 0.12480095028877258, accuracy: 0.9551020408163265\n",
      "loss: 0.14327938854694366, accuracy: 0.9469387755102041\n",
      "loss: 0.11881301552057266, accuracy: 0.9591836734693877\n",
      "loss: 0.1403011977672577, accuracy: 0.9510204081632653\n",
      "loss: 0.11112724244594574, accuracy: 0.9714285714285714\n",
      "loss: 0.12030299752950668, accuracy: 0.9673469387755103\n",
      "loss: 0.14307592809200287, accuracy: 0.9591836734693877\n",
      "loss: 0.11867973208427429, accuracy: 0.9673469387755103\n",
      "loss: 0.11834056675434113, accuracy: 0.9673469387755103\n",
      "loss: 0.12091666460037231, accuracy: 0.9510204081632653\n",
      "loss: 0.11724910885095596, accuracy: 0.9673469387755103\n",
      "loss: 0.12039034068584442, accuracy: 0.9510204081632653\n",
      "loss: 0.12407558411359787, accuracy: 0.9591836734693877\n",
      "loss: 0.13236311078071594, accuracy: 0.9591836734693877\n",
      "loss: 0.11638172715902328, accuracy: 0.963265306122449\n",
      "loss: 0.12535177171230316, accuracy: 0.9551020408163265\n",
      "loss: 0.1332281529903412, accuracy: 0.9551020408163265\n",
      "loss: 0.11403754353523254, accuracy: 0.963265306122449\n",
      "loss: 0.13206391036510468, accuracy: 0.9551020408163265\n",
      "loss: 0.12438718229532242, accuracy: 0.9673469387755103\n",
      "loss: 0.11842315644025803, accuracy: 0.9673469387755103\n",
      "loss: 0.11743873357772827, accuracy: 0.963265306122449\n",
      "loss: 0.11567391455173492, accuracy: 0.963265306122449\n",
      "loss: 0.11658509820699692, accuracy: 0.9510204081632653\n",
      "loss: 0.12469557672739029, accuracy: 0.9591836734693877\n",
      "loss: 0.11812654882669449, accuracy: 0.9551020408163265\n",
      "loss: 0.11257404834032059, accuracy: 0.9714285714285714\n",
      "loss: 0.13565459847450256, accuracy: 0.9428571428571428\n",
      "loss: 0.1160249263048172, accuracy: 0.9591836734693877\n",
      "loss: 0.11487843096256256, accuracy: 0.9591836734693877\n",
      "loss: 0.1197017952799797, accuracy: 0.9591836734693877\n",
      "loss: 0.11049877107143402, accuracy: 0.963265306122449\n",
      "loss: 0.1216781735420227, accuracy: 0.9673469387755103\n",
      "loss: 0.12648281455039978, accuracy: 0.9551020408163265\n",
      "loss: 0.13174119591712952, accuracy: 0.9673469387755103\n",
      "loss: 0.11392474174499512, accuracy: 0.9510204081632653\n",
      "loss: 0.11712420731782913, accuracy: 0.963265306122449\n",
      "loss: 0.11961796879768372, accuracy: 0.963265306122449\n",
      "loss: 0.12285710126161575, accuracy: 0.9591836734693877\n",
      "loss: 0.1139567494392395, accuracy: 0.9673469387755103\n",
      "loss: 0.11693134158849716, accuracy: 0.963265306122449\n",
      "loss: 0.1109924167394638, accuracy: 0.963265306122449\n",
      "loss: 0.11561404168605804, accuracy: 0.9591836734693877\n",
      "loss: 0.11466054618358612, accuracy: 0.9673469387755103\n",
      "loss: 0.13521143794059753, accuracy: 0.9551020408163265\n",
      "loss: 0.11149223893880844, accuracy: 0.9755102040816327\n",
      "loss: 0.11141334474086761, accuracy: 0.9755102040816327\n",
      "loss: 0.12279780209064484, accuracy: 0.9591836734693877\n",
      "loss: 0.10768736898899078, accuracy: 0.963265306122449\n",
      "loss: 0.12871579825878143, accuracy: 0.963265306122449\n",
      "loss: 0.13426266610622406, accuracy: 0.9306122448979591\n",
      "loss: 0.11813908070325851, accuracy: 0.963265306122449\n",
      "loss: 0.12077019363641739, accuracy: 0.9591836734693877\n",
      "loss: 0.12671682238578796, accuracy: 0.963265306122449\n",
      "loss: 0.11470946669578552, accuracy: 0.9591836734693877\n",
      "loss: 0.11781288683414459, accuracy: 0.9469387755102041\n",
      "loss: 0.1366136372089386, accuracy: 0.9387755102040817\n",
      "loss: 0.1127723976969719, accuracy: 0.963265306122449\n",
      "loss: 0.11302829533815384, accuracy: 0.963265306122449\n",
      "loss: 0.12015179544687271, accuracy: 0.9591836734693877\n",
      "loss: 0.11599349230527878, accuracy: 0.9591836734693877\n",
      "loss: 0.12375162541866302, accuracy: 0.9591836734693877\n",
      "loss: 0.1405220925807953, accuracy: 0.9387755102040817\n",
      "loss: 0.14843259751796722, accuracy: 0.9387755102040817\n",
      "loss: 0.14548645913600922, accuracy: 0.9387755102040817\n",
      "loss: 0.12322692573070526, accuracy: 0.9551020408163265\n",
      "loss: 0.11166363954544067, accuracy: 0.963265306122449\n",
      "loss: 0.11108221113681793, accuracy: 0.9591836734693877\n",
      "loss: 0.11057937145233154, accuracy: 0.963265306122449\n",
      "loss: 0.12138444185256958, accuracy: 0.9510204081632653\n",
      "loss: 0.11078230291604996, accuracy: 0.963265306122449\n",
      "loss: 0.11006800085306168, accuracy: 0.9673469387755103\n",
      "loss: 0.12023978680372238, accuracy: 0.9510204081632653\n",
      "loss: 0.12181231379508972, accuracy: 0.9591836734693877\n",
      "loss: 0.1118592843413353, accuracy: 0.9673469387755103\n",
      "loss: 0.11338573694229126, accuracy: 0.963265306122449\n",
      "loss: 0.10839837789535522, accuracy: 0.9673469387755103\n",
      "loss: 0.11483858525753021, accuracy: 0.963265306122449\n",
      "loss: 0.11090101301670074, accuracy: 0.9714285714285714\n",
      "loss: 0.11888173222541809, accuracy: 0.9551020408163265\n",
      "loss: 0.13322360813617706, accuracy: 0.9551020408163265\n",
      "loss: 0.11032836884260178, accuracy: 0.9673469387755103\n",
      "loss: 0.11292652040719986, accuracy: 0.963265306122449\n",
      "loss: 0.11014660447835922, accuracy: 0.9551020408163265\n",
      "loss: 0.11426833271980286, accuracy: 0.9673469387755103\n",
      "loss: 0.10913461446762085, accuracy: 0.9673469387755103\n",
      "loss: 0.10924628376960754, accuracy: 0.9673469387755103\n",
      "loss: 0.11019387096166611, accuracy: 0.9673469387755103\n",
      "loss: 0.10785673558712006, accuracy: 0.9714285714285714\n",
      "loss: 0.11022968590259552, accuracy: 0.963265306122449\n",
      "loss: 0.11185140907764435, accuracy: 0.9714285714285714\n",
      "loss: 0.11078258603811264, accuracy: 0.9591836734693877\n",
      "loss: 0.10731524229049683, accuracy: 0.9673469387755103\n",
      "loss: 0.12623527646064758, accuracy: 0.9428571428571428\n",
      "loss: 0.11692759394645691, accuracy: 0.963265306122449\n",
      "loss: 0.11928723752498627, accuracy: 0.9673469387755103\n",
      "loss: 0.12424550950527191, accuracy: 0.9387755102040817\n",
      "loss: 0.10924609005451202, accuracy: 0.9714285714285714\n",
      "loss: 0.11441351473331451, accuracy: 0.963265306122449\n",
      "loss: 0.11249351501464844, accuracy: 0.9591836734693877\n",
      "loss: 0.11703937500715256, accuracy: 0.9591836734693877\n",
      "loss: 0.11562339216470718, accuracy: 0.9551020408163265\n",
      "loss: 0.12302770465612411, accuracy: 0.9510204081632653\n",
      "loss: 0.11721336841583252, accuracy: 0.9510204081632653\n",
      "loss: 0.13092362880706787, accuracy: 0.9469387755102041\n",
      "loss: 0.11665567755699158, accuracy: 0.9591836734693877\n",
      "loss: 0.12160664796829224, accuracy: 0.9591836734693877\n",
      "loss: 0.11429165303707123, accuracy: 0.9714285714285714\n",
      "loss: 0.11567582190036774, accuracy: 0.9551020408163265\n",
      "loss: 0.10896605998277664, accuracy: 0.9714285714285714\n",
      "loss: 0.1120128333568573, accuracy: 0.9714285714285714\n",
      "loss: 0.1053009182214737, accuracy: 0.9673469387755103\n",
      "loss: 0.10731691122055054, accuracy: 0.9591836734693877\n",
      "loss: 0.10697667300701141, accuracy: 0.9673469387755103\n",
      "loss: 0.10529373586177826, accuracy: 0.9591836734693877\n",
      "loss: 0.10874217748641968, accuracy: 0.9591836734693877\n",
      "loss: 0.10677021741867065, accuracy: 0.963265306122449\n",
      "loss: 0.10793213546276093, accuracy: 0.963265306122449\n",
      "loss: 0.10392238944768906, accuracy: 0.963265306122449\n",
      "loss: 0.11106479912996292, accuracy: 0.963265306122449\n",
      "loss: 0.10864520817995071, accuracy: 0.9673469387755103\n",
      "loss: 0.10542681068181992, accuracy: 0.963265306122449\n",
      "loss: 0.10193441808223724, accuracy: 0.9714285714285714\n",
      "loss: 0.11320550739765167, accuracy: 0.9714285714285714\n",
      "loss: 0.10751605778932571, accuracy: 0.9673469387755103\n",
      "loss: 0.11302787810564041, accuracy: 0.9673469387755103\n",
      "loss: 0.10766066610813141, accuracy: 0.963265306122449\n",
      "loss: 0.12325602769851685, accuracy: 0.9551020408163265\n",
      "loss: 0.11209180951118469, accuracy: 0.963265306122449\n",
      "loss: 0.10536015778779984, accuracy: 0.963265306122449\n",
      "loss: 0.1069285124540329, accuracy: 0.963265306122449\n",
      "loss: 0.10597384721040726, accuracy: 0.9673469387755103\n",
      "loss: 0.11456431448459625, accuracy: 0.9551020408163265\n",
      "loss: 0.11176257580518723, accuracy: 0.9591836734693877\n",
      "loss: 0.11641127616167068, accuracy: 0.9551020408163265\n",
      "loss: 0.1109824851155281, accuracy: 0.9673469387755103\n",
      "loss: 0.11218015849590302, accuracy: 0.963265306122449\n",
      "loss: 0.10235515981912613, accuracy: 0.9673469387755103\n",
      "loss: 0.11130430549383163, accuracy: 0.9673469387755103\n",
      "loss: 0.1174379363656044, accuracy: 0.9591836734693877\n",
      "loss: 0.11068755388259888, accuracy: 0.9551020408163265\n",
      "loss: 0.1071004793047905, accuracy: 0.9673469387755103\n",
      "loss: 0.10597001016139984, accuracy: 0.9591836734693877\n",
      "loss: 0.11054064333438873, accuracy: 0.9551020408163265\n",
      "loss: 0.1138683557510376, accuracy: 0.963265306122449\n",
      "loss: 0.10710401087999344, accuracy: 0.963265306122449\n",
      "loss: 0.10883132368326187, accuracy: 0.963265306122449\n",
      "loss: 0.11064128577709198, accuracy: 0.9714285714285714\n",
      "loss: 0.10757406800985336, accuracy: 0.9551020408163265\n",
      "loss: 0.10407771915197372, accuracy: 0.963265306122449\n",
      "loss: 0.10780379176139832, accuracy: 0.963265306122449\n",
      "loss: 0.10649826377630234, accuracy: 0.9714285714285714\n",
      "loss: 0.10519618541002274, accuracy: 0.963265306122449\n",
      "loss: 0.11291629076004028, accuracy: 0.963265306122449\n",
      "loss: 0.10316529870033264, accuracy: 0.9755102040816327\n",
      "loss: 0.1006256565451622, accuracy: 0.9551020408163265\n",
      "loss: 0.10565941780805588, accuracy: 0.9714285714285714\n",
      "loss: 0.10938286036252975, accuracy: 0.963265306122449\n",
      "loss: 0.10227029770612717, accuracy: 0.9755102040816327\n",
      "loss: 0.10397565364837646, accuracy: 0.963265306122449\n",
      "loss: 0.10422515869140625, accuracy: 0.9673469387755103\n",
      "loss: 0.09808343648910522, accuracy: 0.9673469387755103\n",
      "loss: 0.10452824085950851, accuracy: 0.9591836734693877\n",
      "loss: 0.10524898022413254, accuracy: 0.9714285714285714\n",
      "loss: 0.10285619646310806, accuracy: 0.9755102040816327\n",
      "loss: 0.10278868675231934, accuracy: 0.963265306122449\n",
      "loss: 0.10304437577724457, accuracy: 0.963265306122449\n",
      "loss: 0.1104610413312912, accuracy: 0.9591836734693877\n",
      "loss: 0.1072581559419632, accuracy: 0.9469387755102041\n",
      "loss: 0.1023823618888855, accuracy: 0.9714285714285714\n",
      "loss: 0.10893799364566803, accuracy: 0.9551020408163265\n",
      "loss: 0.10824474692344666, accuracy: 0.9591836734693877\n",
      "loss: 0.09945959597826004, accuracy: 0.9755102040816327\n",
      "loss: 0.11429721117019653, accuracy: 0.9551020408163265\n",
      "loss: 0.09902230650186539, accuracy: 0.9714285714285714\n",
      "loss: 0.10947509855031967, accuracy: 0.9510204081632653\n",
      "loss: 0.10140372812747955, accuracy: 0.9714285714285714\n",
      "loss: 0.10228785127401352, accuracy: 0.9673469387755103\n",
      "loss: 0.10165272653102875, accuracy: 0.9673469387755103\n",
      "loss: 0.10200183093547821, accuracy: 0.9673469387755103\n",
      "loss: 0.10238152742385864, accuracy: 0.9591836734693877\n",
      "loss: 0.10407913476228714, accuracy: 0.9755102040816327\n",
      "loss: 0.1116761639714241, accuracy: 0.9510204081632653\n",
      "loss: 0.105869360268116, accuracy: 0.9551020408163265\n",
      "max accuracy: 0.9795918367346939\n",
      "loss: 0.09883309155702591, accuracy: 0.9795918367346939\n",
      "loss: 0.10532031208276749, accuracy: 0.9673469387755103\n",
      "loss: 0.10869172215461731, accuracy: 0.9673469387755103\n",
      "loss: 0.10602729022502899, accuracy: 0.9714285714285714\n",
      "loss: 0.09826989471912384, accuracy: 0.9714285714285714\n",
      "loss: 0.10653847455978394, accuracy: 0.963265306122449\n",
      "loss: 0.11118166893720627, accuracy: 0.9673469387755103\n",
      "loss: 0.11728955805301666, accuracy: 0.9469387755102041\n",
      "loss: 0.09798631817102432, accuracy: 0.9755102040816327\n",
      "loss: 0.1044304296374321, accuracy: 0.9591836734693877\n",
      "loss: 0.09751317650079727, accuracy: 0.9714285714285714\n",
      "loss: 0.10192625224590302, accuracy: 0.9673469387755103\n",
      "loss: 0.09928225725889206, accuracy: 0.9714285714285714\n",
      "loss: 0.10251481831073761, accuracy: 0.9714285714285714\n",
      "loss: 0.1093594953417778, accuracy: 0.963265306122449\n",
      "loss: 0.09843536466360092, accuracy: 0.9714285714285714\n",
      "loss: 0.11347921192646027, accuracy: 0.963265306122449\n",
      "loss: 0.09853563457727432, accuracy: 0.9714285714285714\n",
      "loss: 0.10142922401428223, accuracy: 0.9673469387755103\n",
      "loss: 0.10677042603492737, accuracy: 0.9714285714285714\n",
      "loss: 0.1017986536026001, accuracy: 0.9673469387755103\n",
      "loss: 0.10108673572540283, accuracy: 0.9510204081632653\n",
      "loss: 0.10452256351709366, accuracy: 0.9551020408163265\n",
      "loss: 0.10705668479204178, accuracy: 0.9591836734693877\n",
      "loss: 0.09910762310028076, accuracy: 0.9714285714285714\n",
      "loss: 0.10424038022756577, accuracy: 0.9673469387755103\n",
      "loss: 0.09934911876916885, accuracy: 0.9673469387755103\n",
      "loss: 0.0981314480304718, accuracy: 0.9673469387755103\n",
      "loss: 0.09782925248146057, accuracy: 0.963265306122449\n",
      "loss: 0.10188550502061844, accuracy: 0.9714285714285714\n",
      "loss: 0.10672372579574585, accuracy: 0.9551020408163265\n",
      "loss: 0.09783998876810074, accuracy: 0.9714285714285714\n",
      "loss: 0.0977531149983406, accuracy: 0.9714285714285714\n",
      "loss: 0.09541555494070053, accuracy: 0.9795918367346939\n",
      "loss: 0.09476643800735474, accuracy: 0.9755102040816327\n",
      "loss: 0.10170244425535202, accuracy: 0.9591836734693877\n",
      "loss: 0.09744004905223846, accuracy: 0.9755102040816327\n",
      "loss: 0.10522342473268509, accuracy: 0.9714285714285714\n",
      "loss: 0.10418382287025452, accuracy: 0.9673469387755103\n",
      "loss: 0.09630320221185684, accuracy: 0.963265306122449\n",
      "loss: 0.10398608446121216, accuracy: 0.963265306122449\n",
      "loss: 0.09446258842945099, accuracy: 0.9714285714285714\n",
      "loss: 0.10878337919712067, accuracy: 0.9551020408163265\n",
      "loss: 0.10074438154697418, accuracy: 0.9591836734693877\n",
      "loss: 0.10318408161401749, accuracy: 0.963265306122449\n",
      "loss: 0.09566394239664078, accuracy: 0.9714285714285714\n",
      "loss: 0.09649398922920227, accuracy: 0.9714285714285714\n",
      "loss: 0.100864477455616, accuracy: 0.9714285714285714\n",
      "loss: 0.09804696589708328, accuracy: 0.9714285714285714\n",
      "loss: 0.09776527434587479, accuracy: 0.9755102040816327\n",
      "loss: 0.10331398248672485, accuracy: 0.9673469387755103\n",
      "loss: 0.09579987078905106, accuracy: 0.9795918367346939\n",
      "loss: 0.10397177189588547, accuracy: 0.9673469387755103\n",
      "loss: 0.1061486005783081, accuracy: 0.9673469387755103\n",
      "loss: 0.10132835060358047, accuracy: 0.963265306122449\n",
      "loss: 0.09729315340518951, accuracy: 0.9714285714285714\n",
      "loss: 0.09607717394828796, accuracy: 0.9673469387755103\n",
      "loss: 0.09858442842960358, accuracy: 0.9673469387755103\n",
      "loss: 0.09590557962656021, accuracy: 0.9714285714285714\n",
      "loss: 0.09666990488767624, accuracy: 0.963265306122449\n",
      "loss: 0.10199254006147385, accuracy: 0.9714285714285714\n",
      "loss: 0.10152000188827515, accuracy: 0.9591836734693877\n",
      "loss: 0.09631741791963577, accuracy: 0.9755102040816327\n",
      "loss: 0.09784756600856781, accuracy: 0.963265306122449\n",
      "loss: 0.10312748700380325, accuracy: 0.963265306122449\n",
      "loss: 0.09652972966432571, accuracy: 0.963265306122449\n",
      "loss: 0.1020762175321579, accuracy: 0.9714285714285714\n",
      "loss: 0.09476818144321442, accuracy: 0.9795918367346939\n",
      "loss: 0.10359574854373932, accuracy: 0.963265306122449\n",
      "loss: 0.09862840920686722, accuracy: 0.9714285714285714\n",
      "loss: 0.09246872365474701, accuracy: 0.963265306122449\n",
      "loss: 0.09391424804925919, accuracy: 0.9714285714285714\n",
      "loss: 0.103767029941082, accuracy: 0.9714285714285714\n",
      "loss: 0.11603546142578125, accuracy: 0.9510204081632653\n",
      "loss: 0.10097142308950424, accuracy: 0.9795918367346939\n",
      "loss: 0.11241883784532547, accuracy: 0.9551020408163265\n",
      "loss: 0.10203828662633896, accuracy: 0.9591836734693877\n",
      "loss: 0.09759631007909775, accuracy: 0.9755102040816327\n",
      "loss: 0.09949006885290146, accuracy: 0.9673469387755103\n",
      "loss: 0.10545583814382553, accuracy: 0.963265306122449\n",
      "loss: 0.10876967757940292, accuracy: 0.9591836734693877\n",
      "loss: 0.11310029774904251, accuracy: 0.963265306122449\n",
      "loss: 0.11217935383319855, accuracy: 0.9714285714285714\n",
      "loss: 0.10130155086517334, accuracy: 0.963265306122449\n",
      "loss: 0.0982765480875969, accuracy: 0.963265306122449\n",
      "loss: 0.09641268104314804, accuracy: 0.963265306122449\n",
      "loss: 0.09647880494594574, accuracy: 0.9714285714285714\n",
      "loss: 0.10138478875160217, accuracy: 0.9673469387755103\n",
      "loss: 0.09488516300916672, accuracy: 0.9714285714285714\n",
      "loss: 0.09193937480449677, accuracy: 0.9714285714285714\n",
      "loss: 0.09460701793432236, accuracy: 0.963265306122449\n",
      "loss: 0.10518711805343628, accuracy: 0.9591836734693877\n",
      "loss: 0.10810661315917969, accuracy: 0.9673469387755103\n",
      "loss: 0.09409816563129425, accuracy: 0.9795918367346939\n",
      "loss: 0.09437649697065353, accuracy: 0.9755102040816327\n",
      "loss: 0.09310414642095566, accuracy: 0.9673469387755103\n",
      "loss: 0.09325569868087769, accuracy: 0.9714285714285714\n",
      "loss: 0.09345970302820206, accuracy: 0.963265306122449\n",
      "loss: 0.09617819637060165, accuracy: 0.9673469387755103\n",
      "loss: 0.09670063853263855, accuracy: 0.9714285714285714\n",
      "loss: 0.09387148171663284, accuracy: 0.963265306122449\n",
      "loss: 0.09294597059488297, accuracy: 0.9755102040816327\n",
      "loss: 0.09937839210033417, accuracy: 0.9714285714285714\n",
      "loss: 0.0967414602637291, accuracy: 0.9795918367346939\n",
      "loss: 0.10069960355758667, accuracy: 0.9673469387755103\n",
      "loss: 0.12854169309139252, accuracy: 0.9387755102040817\n",
      "loss: 0.09672263264656067, accuracy: 0.9673469387755103\n",
      "loss: 0.09193605184555054, accuracy: 0.9755102040816327\n",
      "loss: 0.09315263479948044, accuracy: 0.9755102040816327\n",
      "loss: 0.10052116960287094, accuracy: 0.9673469387755103\n",
      "loss: 0.0905648022890091, accuracy: 0.9755102040816327\n",
      "loss: 0.10074808448553085, accuracy: 0.9714285714285714\n",
      "loss: 0.09348814934492111, accuracy: 0.9714285714285714\n",
      "loss: 0.09520293772220612, accuracy: 0.9714285714285714\n",
      "loss: 0.09327374398708344, accuracy: 0.9714285714285714\n",
      "loss: 0.09174394607543945, accuracy: 0.9755102040816327\n",
      "loss: 0.1036151573061943, accuracy: 0.963265306122449\n",
      "loss: 0.1051439419388771, accuracy: 0.9591836734693877\n",
      "loss: 0.10509593784809113, accuracy: 0.9591836734693877\n",
      "loss: 0.10653940588235855, accuracy: 0.9714285714285714\n",
      "loss: 0.11411742120981216, accuracy: 0.9551020408163265\n",
      "loss: 0.09221229702234268, accuracy: 0.9673469387755103\n",
      "loss: 0.0968526229262352, accuracy: 0.9714285714285714\n",
      "loss: 0.09844149649143219, accuracy: 0.9673469387755103\n",
      "loss: 0.09388507902622223, accuracy: 0.9755102040816327\n",
      "loss: 0.09114817529916763, accuracy: 0.9755102040816327\n",
      "loss: 0.1019182950258255, accuracy: 0.9714285714285714\n",
      "loss: 0.0934787392616272, accuracy: 0.9673469387755103\n",
      "loss: 0.09702009707689285, accuracy: 0.9673469387755103\n",
      "loss: 0.09050662815570831, accuracy: 0.9755102040816327\n",
      "loss: 0.10432060807943344, accuracy: 0.9591836734693877\n",
      "loss: 0.09295089542865753, accuracy: 0.9755102040816327\n",
      "loss: 0.09163510054349899, accuracy: 0.9714285714285714\n",
      "loss: 0.09808262437582016, accuracy: 0.963265306122449\n",
      "loss: 0.09036476910114288, accuracy: 0.9795918367346939\n",
      "loss: 0.10145130008459091, accuracy: 0.9591836734693877\n",
      "loss: 0.09868305921554565, accuracy: 0.9714285714285714\n",
      "loss: 0.09475027769804001, accuracy: 0.9755102040816327\n",
      "loss: 0.09023049473762512, accuracy: 0.9795918367346939\n",
      "loss: 0.09650419652462006, accuracy: 0.9714285714285714\n",
      "loss: 0.09560071676969528, accuracy: 0.9714285714285714\n",
      "loss: 0.09027460962533951, accuracy: 0.9673469387755103\n",
      "loss: 0.09369195997714996, accuracy: 0.963265306122449\n",
      "loss: 0.09223221242427826, accuracy: 0.9673469387755103\n",
      "loss: 0.09754929691553116, accuracy: 0.963265306122449\n",
      "loss: 0.08673962205648422, accuracy: 0.9755102040816327\n",
      "loss: 0.08715026825666428, accuracy: 0.9714285714285714\n",
      "loss: 0.11041577905416489, accuracy: 0.9591836734693877\n",
      "loss: 0.11300326883792877, accuracy: 0.9673469387755103\n",
      "loss: 0.09167574346065521, accuracy: 0.963265306122449\n",
      "loss: 0.09233319759368896, accuracy: 0.9795918367346939\n",
      "loss: 0.09092702716588974, accuracy: 0.963265306122449\n",
      "loss: 0.09541840851306915, accuracy: 0.9714285714285714\n",
      "loss: 0.09202561527490616, accuracy: 0.9755102040816327\n",
      "loss: 0.09267652034759521, accuracy: 0.9591836734693877\n",
      "loss: 0.0869208425283432, accuracy: 0.9795918367346939\n",
      "loss: 0.08953078091144562, accuracy: 0.9673469387755103\n",
      "loss: 0.10421887785196304, accuracy: 0.9591836734693877\n",
      "loss: 0.09472551941871643, accuracy: 0.9673469387755103\n",
      "loss: 0.08874407410621643, accuracy: 0.9714285714285714\n",
      "loss: 0.09201225638389587, accuracy: 0.9714285714285714\n",
      "loss: 0.1015881672501564, accuracy: 0.963265306122449\n",
      "loss: 0.09156850725412369, accuracy: 0.9673469387755103\n",
      "loss: 0.09154466539621353, accuracy: 0.9673469387755103\n",
      "loss: 0.1051069051027298, accuracy: 0.9510204081632653\n",
      "loss: 0.09027673304080963, accuracy: 0.9714285714285714\n",
      "loss: 0.08783987909555435, accuracy: 0.9673469387755103\n",
      "loss: 0.09316311031579971, accuracy: 0.9673469387755103\n",
      "loss: 0.08952179551124573, accuracy: 0.9714285714285714\n",
      "loss: 0.08710048347711563, accuracy: 0.9795918367346939\n",
      "loss: 0.08879466354846954, accuracy: 0.9673469387755103\n",
      "loss: 0.08958124369382858, accuracy: 0.9714285714285714\n",
      "loss: 0.09329814463853836, accuracy: 0.9673469387755103\n",
      "loss: 0.08804663270711899, accuracy: 0.9755102040816327\n",
      "max accuracy: 0.9836734693877551\n",
      "loss: 0.0844234749674797, accuracy: 0.9836734693877551\n",
      "loss: 0.08823850750923157, accuracy: 0.9755102040816327\n",
      "loss: 0.08741132915019989, accuracy: 0.9836734693877551\n",
      "loss: 0.08959095925092697, accuracy: 0.9755102040816327\n",
      "loss: 0.10096288472414017, accuracy: 0.9591836734693877\n",
      "loss: 0.09193158894777298, accuracy: 0.9673469387755103\n",
      "loss: 0.09266182035207748, accuracy: 0.9673469387755103\n",
      "loss: 0.10390305519104004, accuracy: 0.9551020408163265\n",
      "loss: 0.09450326859951019, accuracy: 0.9591836734693877\n",
      "loss: 0.0893876850605011, accuracy: 0.963265306122449\n",
      "loss: 0.08697695285081863, accuracy: 0.9755102040816327\n",
      "loss: 0.12053070962429047, accuracy: 0.9551020408163265\n",
      "loss: 0.09041198343038559, accuracy: 0.9714285714285714\n",
      "loss: 0.0915834978222847, accuracy: 0.9714285714285714\n",
      "loss: 0.09305039793252945, accuracy: 0.9795918367346939\n",
      "loss: 0.08853809535503387, accuracy: 0.9755102040816327\n",
      "loss: 0.08718699961900711, accuracy: 0.963265306122449\n",
      "loss: 0.09020084887742996, accuracy: 0.9714285714285714\n",
      "loss: 0.09632930159568787, accuracy: 0.9673469387755103\n",
      "loss: 0.10059843957424164, accuracy: 0.9591836734693877\n",
      "loss: 0.0920393317937851, accuracy: 0.963265306122449\n",
      "loss: 0.09525898098945618, accuracy: 0.9673469387755103\n",
      "loss: 0.08671387284994125, accuracy: 0.9714285714285714\n",
      "loss: 0.09453164786100388, accuracy: 0.9673469387755103\n",
      "loss: 0.08743666857481003, accuracy: 0.9755102040816327\n",
      "loss: 0.09454163163900375, accuracy: 0.9673469387755103\n",
      "loss: 0.08351320773363113, accuracy: 0.9836734693877551\n",
      "loss: 0.0888100191950798, accuracy: 0.9673469387755103\n",
      "loss: 0.0938875749707222, accuracy: 0.9673469387755103\n",
      "loss: 0.09662030637264252, accuracy: 0.9673469387755103\n",
      "loss: 0.09163962304592133, accuracy: 0.9755102040816327\n",
      "loss: 0.10248925536870956, accuracy: 0.9510204081632653\n",
      "loss: 0.08441951870918274, accuracy: 0.9755102040816327\n",
      "loss: 0.09045848250389099, accuracy: 0.9673469387755103\n",
      "loss: 0.089420847594738, accuracy: 0.9673469387755103\n",
      "loss: 0.0918363630771637, accuracy: 0.9755102040816327\n",
      "loss: 0.08763224631547928, accuracy: 0.9673469387755103\n",
      "loss: 0.09482365101575851, accuracy: 0.9673469387755103\n",
      "loss: 0.09105025231838226, accuracy: 0.9714285714285714\n",
      "loss: 0.08805803209543228, accuracy: 0.9755102040816327\n",
      "loss: 0.09362003207206726, accuracy: 0.9673469387755103\n",
      "loss: 0.10557147115468979, accuracy: 0.9469387755102041\n",
      "loss: 0.08094701170921326, accuracy: 0.963265306122449\n",
      "loss: 0.08908630907535553, accuracy: 0.9714285714285714\n",
      "loss: 0.0870598778128624, accuracy: 0.963265306122449\n",
      "loss: 0.089753158390522, accuracy: 0.9755102040816327\n",
      "loss: 0.09116766601800919, accuracy: 0.963265306122449\n",
      "loss: 0.0881986990571022, accuracy: 0.9673469387755103\n",
      "loss: 0.08480005711317062, accuracy: 0.9755102040816327\n",
      "loss: 0.08488044887781143, accuracy: 0.9714285714285714\n",
      "loss: 0.08381436765193939, accuracy: 0.9714285714285714\n",
      "loss: 0.08180902898311615, accuracy: 0.9755102040816327\n",
      "loss: 0.10430954396724701, accuracy: 0.9551020408163265\n",
      "loss: 0.08603201061487198, accuracy: 0.9795918367346939\n",
      "loss: 0.08892551064491272, accuracy: 0.9714285714285714\n",
      "loss: 0.08538246899843216, accuracy: 0.9714285714285714\n",
      "loss: 0.09523901343345642, accuracy: 0.9591836734693877\n",
      "loss: 0.09127730131149292, accuracy: 0.9714285714285714\n",
      "loss: 0.08330240845680237, accuracy: 0.9673469387755103\n",
      "loss: 0.09274864196777344, accuracy: 0.9591836734693877\n",
      "loss: 0.08930209279060364, accuracy: 0.9714285714285714\n",
      "loss: 0.09050874412059784, accuracy: 0.963265306122449\n",
      "loss: 0.08683666586875916, accuracy: 0.9714285714285714\n",
      "loss: 0.098726287484169, accuracy: 0.9673469387755103\n",
      "loss: 0.08727379888296127, accuracy: 0.9795918367346939\n",
      "loss: 0.09565133601427078, accuracy: 0.963265306122449\n",
      "loss: 0.08472555875778198, accuracy: 0.9714285714285714\n",
      "loss: 0.0889887660741806, accuracy: 0.9755102040816327\n",
      "loss: 0.0904502421617508, accuracy: 0.9714285714285714\n",
      "loss: 0.10076570510864258, accuracy: 0.9673469387755103\n",
      "loss: 0.08538851886987686, accuracy: 0.9673469387755103\n",
      "loss: 0.08555938303470612, accuracy: 0.9714285714285714\n",
      "loss: 0.08523692935705185, accuracy: 0.9795918367346939\n",
      "loss: 0.08660583943128586, accuracy: 0.9755102040816327\n",
      "loss: 0.08864516764879227, accuracy: 0.963265306122449\n",
      "loss: 0.0872131958603859, accuracy: 0.9714285714285714\n",
      "loss: 0.0880054384469986, accuracy: 0.9714285714285714\n",
      "loss: 0.09019237756729126, accuracy: 0.963265306122449\n",
      "loss: 0.08644583076238632, accuracy: 0.9755102040816327\n",
      "loss: 0.08390532433986664, accuracy: 0.9836734693877551\n",
      "loss: 0.08754107356071472, accuracy: 0.9755102040816327\n",
      "loss: 0.10358873754739761, accuracy: 0.9591836734693877\n",
      "loss: 0.09473075717687607, accuracy: 0.963265306122449\n",
      "loss: 0.08251294493675232, accuracy: 0.9673469387755103\n",
      "loss: 0.1563735455274582, accuracy: 0.9510204081632653\n",
      "loss: 0.09798959642648697, accuracy: 0.9551020408163265\n",
      "loss: 0.08218535780906677, accuracy: 0.9795918367346939\n",
      "loss: 0.08386912941932678, accuracy: 0.9795918367346939\n",
      "loss: 0.08804246783256531, accuracy: 0.963265306122449\n",
      "loss: 0.08944669365882874, accuracy: 0.9714285714285714\n",
      "loss: 0.08678939193487167, accuracy: 0.9714285714285714\n",
      "loss: 0.08504566550254822, accuracy: 0.9836734693877551\n",
      "loss: 0.084820955991745, accuracy: 0.9591836734693877\n",
      "loss: 0.08356208354234695, accuracy: 0.9755102040816327\n",
      "loss: 0.09393717348575592, accuracy: 0.963265306122449\n",
      "loss: 0.08669936656951904, accuracy: 0.9714285714285714\n",
      "loss: 0.08214066922664642, accuracy: 0.9714285714285714\n",
      "loss: 0.08128707855939865, accuracy: 0.9755102040816327\n",
      "loss: 0.1007111519575119, accuracy: 0.9551020408163265\n",
      "loss: 0.1036394014954567, accuracy: 0.9551020408163265\n",
      "loss: 0.08329446613788605, accuracy: 0.9795918367346939\n",
      "loss: 0.08996816724538803, accuracy: 0.9755102040816327\n",
      "loss: 0.0861167386174202, accuracy: 0.9714285714285714\n",
      "loss: 0.10218273848295212, accuracy: 0.9551020408163265\n",
      "loss: 0.10770568996667862, accuracy: 0.9591836734693877\n",
      "loss: 0.08774683624505997, accuracy: 0.963265306122449\n",
      "loss: 0.08758698403835297, accuracy: 0.9755102040816327\n",
      "loss: 0.08168390393257141, accuracy: 0.9755102040816327\n",
      "loss: 0.07922624796628952, accuracy: 0.9795918367346939\n",
      "loss: 0.08651872724294662, accuracy: 0.9673469387755103\n",
      "loss: 0.08491994440555573, accuracy: 0.9673469387755103\n",
      "loss: 0.08358445018529892, accuracy: 0.9795918367346939\n",
      "loss: 0.07986577600240707, accuracy: 0.9836734693877551\n",
      "loss: 0.08780227601528168, accuracy: 0.9673469387755103\n",
      "loss: 0.08123218268156052, accuracy: 0.9795918367346939\n",
      "loss: 0.0815182700753212, accuracy: 0.9673469387755103\n",
      "loss: 0.08229745179414749, accuracy: 0.9795918367346939\n",
      "loss: 0.0864376500248909, accuracy: 0.9673469387755103\n",
      "loss: 0.0863746777176857, accuracy: 0.963265306122449\n",
      "loss: 0.08001615852117538, accuracy: 0.9755102040816327\n",
      "loss: 0.08172424137592316, accuracy: 0.9673469387755103\n",
      "loss: 0.0775826945900917, accuracy: 0.9795918367346939\n",
      "loss: 0.09108400344848633, accuracy: 0.9714285714285714\n",
      "loss: 0.08628293126821518, accuracy: 0.963265306122449\n",
      "loss: 0.08045610785484314, accuracy: 0.9755102040816327\n",
      "loss: 0.0863509327173233, accuracy: 0.9673469387755103\n",
      "loss: 0.0822141021490097, accuracy: 0.9673469387755103\n",
      "loss: 0.08865249156951904, accuracy: 0.9836734693877551\n",
      "loss: 0.08130327612161636, accuracy: 0.9673469387755103\n",
      "loss: 0.08953861147165298, accuracy: 0.963265306122449\n",
      "loss: 0.08565329760313034, accuracy: 0.9714285714285714\n",
      "loss: 0.08579612523317337, accuracy: 0.9714285714285714\n",
      "loss: 0.0804913267493248, accuracy: 0.9836734693877551\n",
      "loss: 0.07758022099733353, accuracy: 0.9836734693877551\n",
      "loss: 0.07969819754362106, accuracy: 0.9755102040816327\n",
      "loss: 0.08602190017700195, accuracy: 0.9755102040816327\n",
      "loss: 0.07784517109394073, accuracy: 0.9795918367346939\n",
      "loss: 0.09092794358730316, accuracy: 0.9755102040816327\n",
      "loss: 0.08096872270107269, accuracy: 0.9795918367346939\n",
      "loss: 0.08890058845281601, accuracy: 0.9673469387755103\n",
      "loss: 0.09127094596624374, accuracy: 0.963265306122449\n",
      "loss: 0.09535574167966843, accuracy: 0.9551020408163265\n",
      "loss: 0.08189822733402252, accuracy: 0.9714285714285714\n",
      "loss: 0.08705774694681168, accuracy: 0.9673469387755103\n",
      "loss: 0.08279544115066528, accuracy: 0.9755102040816327\n",
      "loss: 0.08457563072443008, accuracy: 0.9673469387755103\n",
      "loss: 0.08598092943429947, accuracy: 0.9795918367346939\n",
      "loss: 0.08206221461296082, accuracy: 0.9714285714285714\n",
      "loss: 0.09189435094594955, accuracy: 0.9551020408163265\n",
      "loss: 0.07427603751420975, accuracy: 0.9795918367346939\n",
      "loss: 0.08613515645265579, accuracy: 0.9714285714285714\n",
      "loss: 0.07995723932981491, accuracy: 0.963265306122449\n",
      "loss: 0.08407911658287048, accuracy: 0.9795918367346939\n",
      "loss: 0.07823542505502701, accuracy: 0.9714285714285714\n",
      "loss: 0.08378460258245468, accuracy: 0.9755102040816327\n",
      "loss: 0.0816216766834259, accuracy: 0.9755102040816327\n",
      "loss: 0.13101166486740112, accuracy: 0.9428571428571428\n",
      "loss: 0.08988727629184723, accuracy: 0.9591836734693877\n",
      "loss: 0.0845792293548584, accuracy: 0.9673469387755103\n",
      "loss: 0.07626783102750778, accuracy: 0.9795918367346939\n",
      "loss: 0.08822816610336304, accuracy: 0.9551020408163265\n",
      "loss: 0.08571194112300873, accuracy: 0.9755102040816327\n",
      "max accuracy: 0.9877551020408163\n",
      "loss: 0.07813666015863419, accuracy: 0.9877551020408163\n",
      "loss: 0.08666554093360901, accuracy: 0.9673469387755103\n",
      "loss: 0.0869927704334259, accuracy: 0.9673469387755103\n",
      "loss: 0.07789159566164017, accuracy: 0.9795918367346939\n",
      "loss: 0.08471868932247162, accuracy: 0.9795918367346939\n",
      "loss: 0.08265142142772675, accuracy: 0.9755102040816327\n",
      "loss: 0.08343905955553055, accuracy: 0.9714285714285714\n",
      "loss: 0.07762277126312256, accuracy: 0.9836734693877551\n",
      "loss: 0.07857370376586914, accuracy: 0.9795918367346939\n",
      "loss: 0.08453872799873352, accuracy: 0.9714285714285714\n",
      "loss: 0.08005278557538986, accuracy: 0.9755102040816327\n",
      "loss: 0.07891874015331268, accuracy: 0.9714285714285714\n",
      "loss: 0.08255244046449661, accuracy: 0.9795918367346939\n",
      "loss: 0.0809357613325119, accuracy: 0.9714285714285714\n",
      "loss: 0.07959504425525665, accuracy: 0.9714285714285714\n",
      "loss: 0.08290155977010727, accuracy: 0.9673469387755103\n",
      "loss: 0.08508940041065216, accuracy: 0.963265306122449\n",
      "loss: 0.07806918025016785, accuracy: 0.9755102040816327\n",
      "loss: 0.08312119543552399, accuracy: 0.9673469387755103\n",
      "loss: 0.08750686049461365, accuracy: 0.9591836734693877\n",
      "loss: 0.08121909946203232, accuracy: 0.9795918367346939\n",
      "loss: 0.07741228491067886, accuracy: 0.9795918367346939\n",
      "loss: 0.08243774622678757, accuracy: 0.9714285714285714\n",
      "loss: 0.08558572828769684, accuracy: 0.9673469387755103\n",
      "loss: 0.07815102487802505, accuracy: 0.9795918367346939\n",
      "loss: 0.07960032671689987, accuracy: 0.9755102040816327\n",
      "loss: 0.07628368586301804, accuracy: 0.9795918367346939\n",
      "loss: 0.07968278974294662, accuracy: 0.9714285714285714\n",
      "loss: 0.0777193158864975, accuracy: 0.9795918367346939\n",
      "loss: 0.09699828922748566, accuracy: 0.9673469387755103\n",
      "loss: 0.09134262800216675, accuracy: 0.963265306122449\n",
      "loss: 0.08156223595142365, accuracy: 0.9795918367346939\n",
      "loss: 0.0800827145576477, accuracy: 0.963265306122449\n",
      "loss: 0.07490206509828568, accuracy: 0.9836734693877551\n",
      "loss: 0.07440377026796341, accuracy: 0.9755102040816327\n",
      "loss: 0.07708269357681274, accuracy: 0.9755102040816327\n",
      "loss: 0.07696966081857681, accuracy: 0.9755102040816327\n",
      "loss: 0.0828312486410141, accuracy: 0.9714285714285714\n",
      "loss: 0.10843611508607864, accuracy: 0.9551020408163265\n",
      "loss: 0.09041254222393036, accuracy: 0.9591836734693877\n",
      "loss: 0.09024762362241745, accuracy: 0.9591836734693877\n",
      "loss: 0.08164318650960922, accuracy: 0.9755102040816327\n",
      "loss: 0.07943718135356903, accuracy: 0.9714285714285714\n",
      "loss: 0.07905127108097076, accuracy: 0.9795918367346939\n",
      "loss: 0.0841221809387207, accuracy: 0.963265306122449\n",
      "loss: 0.08430807292461395, accuracy: 0.9714285714285714\n",
      "loss: 0.07717398554086685, accuracy: 0.9836734693877551\n",
      "loss: 0.08243705332279205, accuracy: 0.9714285714285714\n",
      "loss: 0.08483262360095978, accuracy: 0.9755102040816327\n",
      "loss: 0.07826734334230423, accuracy: 0.963265306122449\n",
      "loss: 0.0851539820432663, accuracy: 0.9755102040816327\n",
      "loss: 0.07805952429771423, accuracy: 0.9714285714285714\n",
      "loss: 0.07801370322704315, accuracy: 0.9755102040816327\n",
      "loss: 0.07917454093694687, accuracy: 0.9673469387755103\n",
      "loss: 0.07568363100290298, accuracy: 0.9714285714285714\n",
      "loss: 0.07237504422664642, accuracy: 0.9836734693877551\n",
      "loss: 0.09367507696151733, accuracy: 0.963265306122449\n",
      "loss: 0.07680782675743103, accuracy: 0.9836734693877551\n",
      "loss: 0.07310255616903305, accuracy: 0.9836734693877551\n",
      "loss: 0.07716312259435654, accuracy: 0.9714285714285714\n",
      "loss: 0.07880201190710068, accuracy: 0.9673469387755103\n",
      "loss: 0.07405310124158859, accuracy: 0.9836734693877551\n",
      "loss: 0.07602368295192719, accuracy: 0.9673469387755103\n",
      "loss: 0.08499261736869812, accuracy: 0.9673469387755103\n",
      "loss: 0.07555651664733887, accuracy: 0.9836734693877551\n",
      "loss: 0.0835796669125557, accuracy: 0.9714285714285714\n",
      "loss: 0.08077553659677505, accuracy: 0.9795918367346939\n",
      "loss: 0.07646110653877258, accuracy: 0.9836734693877551\n",
      "loss: 0.07241971790790558, accuracy: 0.9755102040816327\n",
      "loss: 0.07730180025100708, accuracy: 0.9755102040816327\n",
      "loss: 0.08204151690006256, accuracy: 0.9755102040816327\n",
      "loss: 0.07559412717819214, accuracy: 0.9755102040816327\n",
      "loss: 0.07439936697483063, accuracy: 0.9836734693877551\n",
      "loss: 0.0796695351600647, accuracy: 0.9755102040816327\n",
      "loss: 0.08190420269966125, accuracy: 0.9714285714285714\n",
      "loss: 0.08252409100532532, accuracy: 0.9714285714285714\n",
      "loss: 0.0761101096868515, accuracy: 0.9836734693877551\n",
      "loss: 0.07449102401733398, accuracy: 0.9755102040816327\n",
      "loss: 0.07430656254291534, accuracy: 0.9755102040816327\n",
      "loss: 0.0737529993057251, accuracy: 0.9755102040816327\n",
      "loss: 0.0819379985332489, accuracy: 0.9755102040816327\n",
      "loss: 0.07613689452409744, accuracy: 0.9795918367346939\n",
      "loss: 0.07872647047042847, accuracy: 0.9714285714285714\n",
      "loss: 0.07497560977935791, accuracy: 0.9836734693877551\n",
      "loss: 0.07364428788423538, accuracy: 0.9714285714285714\n",
      "loss: 0.07564704120159149, accuracy: 0.9795918367346939\n",
      "loss: 0.07619557529687881, accuracy: 0.9795918367346939\n",
      "loss: 0.08693210035562515, accuracy: 0.9673469387755103\n",
      "loss: 0.07614429295063019, accuracy: 0.9714285714285714\n",
      "loss: 0.0715155377984047, accuracy: 0.9795918367346939\n",
      "loss: 0.07950831204652786, accuracy: 0.9795918367346939\n",
      "loss: 0.0780705064535141, accuracy: 0.9714285714285714\n",
      "loss: 0.07844149321317673, accuracy: 0.9755102040816327\n",
      "loss: 0.09278681874275208, accuracy: 0.9591836734693877\n",
      "loss: 0.09383663535118103, accuracy: 0.9673469387755103\n",
      "loss: 0.07747682183980942, accuracy: 0.9755102040816327\n",
      "loss: 0.0781049132347107, accuracy: 0.9795918367346939\n",
      "loss: 0.07494549453258514, accuracy: 0.9755102040816327\n",
      "loss: 0.07342130690813065, accuracy: 0.9755102040816327\n",
      "loss: 0.07629804313182831, accuracy: 0.9714285714285714\n",
      "loss: 0.08755204826593399, accuracy: 0.963265306122449\n",
      "loss: 0.07963801175355911, accuracy: 0.9755102040816327\n",
      "loss: 0.08051865547895432, accuracy: 0.9795918367346939\n",
      "loss: 0.07971661537885666, accuracy: 0.9714285714285714\n",
      "loss: 0.07628349214792252, accuracy: 0.9755102040816327\n",
      "loss: 0.07398653030395508, accuracy: 0.9795918367346939\n",
      "loss: 0.07809486985206604, accuracy: 0.9795918367346939\n",
      "loss: 0.07555685192346573, accuracy: 0.9795918367346939\n",
      "loss: 0.090442955493927, accuracy: 0.9714285714285714\n",
      "loss: 0.08524882048368454, accuracy: 0.9755102040816327\n",
      "loss: 0.07478538900613785, accuracy: 0.9755102040816327\n",
      "loss: 0.07067679613828659, accuracy: 0.9877551020408163\n",
      "loss: 0.07547704130411148, accuracy: 0.9714285714285714\n",
      "loss: 0.07632860541343689, accuracy: 0.9795918367346939\n",
      "loss: 0.07341260462999344, accuracy: 0.9836734693877551\n",
      "loss: 0.07987819612026215, accuracy: 0.9795918367346939\n",
      "loss: 0.07507967203855515, accuracy: 0.9836734693877551\n",
      "loss: 0.07135175913572311, accuracy: 0.9795918367346939\n",
      "loss: 0.07429806888103485, accuracy: 0.9836734693877551\n",
      "loss: 0.07509677857160568, accuracy: 0.9755102040816327\n",
      "loss: 0.07205383479595184, accuracy: 0.9877551020408163\n",
      "loss: 0.08254524320363998, accuracy: 0.9591836734693877\n",
      "loss: 0.0743650496006012, accuracy: 0.9714285714285714\n",
      "loss: 0.06863108277320862, accuracy: 0.9877551020408163\n",
      "loss: 0.06900550425052643, accuracy: 0.9755102040816327\n",
      "loss: 0.07591325044631958, accuracy: 0.9714285714285714\n",
      "loss: 0.06913816183805466, accuracy: 0.9755102040816327\n",
      "loss: 0.07229217141866684, accuracy: 0.9714285714285714\n",
      "loss: 0.07058831304311752, accuracy: 0.9795918367346939\n",
      "loss: 0.07479249686002731, accuracy: 0.9714285714285714\n",
      "loss: 0.07246455550193787, accuracy: 0.9755102040816327\n",
      "loss: 0.07607465982437134, accuracy: 0.9755102040816327\n",
      "loss: 0.07226084172725677, accuracy: 0.9795918367346939\n",
      "loss: 0.07372406125068665, accuracy: 0.9795918367346939\n",
      "loss: 0.07235468178987503, accuracy: 0.9755102040816327\n",
      "loss: 0.07132130116224289, accuracy: 0.9836734693877551\n",
      "loss: 0.07907749712467194, accuracy: 0.963265306122449\n",
      "loss: 0.07206910103559494, accuracy: 0.9795918367346939\n",
      "loss: 0.07638782262802124, accuracy: 0.9714285714285714\n",
      "loss: 0.07337276637554169, accuracy: 0.9714285714285714\n",
      "loss: 0.07642433047294617, accuracy: 0.9795918367346939\n",
      "loss: 0.07227058708667755, accuracy: 0.9795918367346939\n",
      "loss: 0.07383750379085541, accuracy: 0.9795918367346939\n",
      "loss: 0.07489355653524399, accuracy: 0.9795918367346939\n",
      "loss: 0.07757896184921265, accuracy: 0.9755102040816327\n",
      "loss: 0.0794251561164856, accuracy: 0.9755102040816327\n",
      "loss: 0.07376714795827866, accuracy: 0.9755102040816327\n",
      "loss: 0.07141134887933731, accuracy: 0.9836734693877551\n",
      "loss: 0.07789051532745361, accuracy: 0.9673469387755103\n",
      "loss: 0.06770528852939606, accuracy: 0.9836734693877551\n",
      "loss: 0.07113289833068848, accuracy: 0.9836734693877551\n",
      "loss: 0.06678961962461472, accuracy: 0.9836734693877551\n",
      "loss: 0.06970621645450592, accuracy: 0.9836734693877551\n",
      "loss: 0.07834665477275848, accuracy: 0.9836734693877551\n",
      "loss: 0.0760362297296524, accuracy: 0.9795918367346939\n",
      "loss: 0.06943325698375702, accuracy: 0.9836734693877551\n",
      "loss: 0.07434301823377609, accuracy: 0.9755102040816327\n",
      "loss: 0.07001132518053055, accuracy: 0.9877551020408163\n",
      "loss: 0.0785631611943245, accuracy: 0.9755102040816327\n",
      "loss: 0.11678837239742279, accuracy: 0.9551020408163265\n",
      "loss: 0.08316823840141296, accuracy: 0.9755102040816327\n",
      "loss: 0.07769618183374405, accuracy: 0.9795918367346939\n",
      "loss: 0.08770235627889633, accuracy: 0.9714285714285714\n",
      "loss: 0.07402992248535156, accuracy: 0.9673469387755103\n",
      "loss: 0.07495070993900299, accuracy: 0.9795918367346939\n",
      "loss: 0.07978608459234238, accuracy: 0.9795918367346939\n",
      "loss: 0.07036317139863968, accuracy: 0.9836734693877551\n",
      "loss: 0.07124442607164383, accuracy: 0.9795918367346939\n",
      "loss: 0.07035695761442184, accuracy: 0.9877551020408163\n",
      "loss: 0.07152705639600754, accuracy: 0.9836734693877551\n",
      "loss: 0.0763743668794632, accuracy: 0.9714285714285714\n",
      "loss: 0.0726998895406723, accuracy: 0.9755102040816327\n",
      "loss: 0.0688275396823883, accuracy: 0.9877551020408163\n",
      "loss: 0.06857737898826599, accuracy: 0.9795918367346939\n",
      "loss: 0.06703327596187592, accuracy: 0.9836734693877551\n",
      "loss: 0.0718328133225441, accuracy: 0.9795918367346939\n",
      "loss: 0.07255280017852783, accuracy: 0.9714285714285714\n",
      "loss: 0.06856760382652283, accuracy: 0.9795918367346939\n",
      "loss: 0.10292457789182663, accuracy: 0.9714285714285714\n",
      "loss: 0.07415196299552917, accuracy: 0.9755102040816327\n",
      "loss: 0.07147754728794098, accuracy: 0.9795918367346939\n",
      "loss: 0.07420682907104492, accuracy: 0.963265306122449\n",
      "loss: 0.0693194642663002, accuracy: 0.9836734693877551\n",
      "loss: 0.07023205608129501, accuracy: 0.9795918367346939\n",
      "loss: 0.0881330668926239, accuracy: 0.9673469387755103\n",
      "loss: 0.07102109491825104, accuracy: 0.9795918367346939\n",
      "loss: 0.07106129825115204, accuracy: 0.9836734693877551\n",
      "loss: 0.07161231338977814, accuracy: 0.9755102040816327\n",
      "loss: 0.06660707294940948, accuracy: 0.9836734693877551\n",
      "loss: 0.07023581117391586, accuracy: 0.9795918367346939\n",
      "loss: 0.06382980942726135, accuracy: 0.9836734693877551\n",
      "loss: 0.06762471795082092, accuracy: 0.9836734693877551\n",
      "loss: 0.06447585672140121, accuracy: 0.9795918367346939\n",
      "loss: 0.0741528645157814, accuracy: 0.9795918367346939\n",
      "loss: 0.06582938134670258, accuracy: 0.9836734693877551\n",
      "loss: 0.07353134453296661, accuracy: 0.9755102040816327\n",
      "loss: 0.0718199834227562, accuracy: 0.9795918367346939\n",
      "loss: 0.07036387920379639, accuracy: 0.9795918367346939\n",
      "loss: 0.07022007554769516, accuracy: 0.9795918367346939\n",
      "loss: 0.0729641243815422, accuracy: 0.9836734693877551\n",
      "loss: 0.07238484174013138, accuracy: 0.9755102040816327\n",
      "loss: 0.07105758786201477, accuracy: 0.9755102040816327\n",
      "loss: 0.07197020202875137, accuracy: 0.9795918367346939\n",
      "loss: 0.07637722790241241, accuracy: 0.9673469387755103\n",
      "loss: 0.06807377934455872, accuracy: 0.9836734693877551\n",
      "loss: 0.0766674280166626, accuracy: 0.9673469387755103\n",
      "loss: 0.06601090729236603, accuracy: 0.9836734693877551\n",
      "loss: 0.0722041055560112, accuracy: 0.9755102040816327\n",
      "loss: 0.06775461882352829, accuracy: 0.9795918367346939\n",
      "loss: 0.06890085339546204, accuracy: 0.9755102040816327\n",
      "loss: 0.06976880133152008, accuracy: 0.9755102040816327\n",
      "loss: 0.06792948395013809, accuracy: 0.9836734693877551\n",
      "loss: 0.07056457549333572, accuracy: 0.9795918367346939\n",
      "loss: 0.07376188039779663, accuracy: 0.9795918367346939\n",
      "loss: 0.06662802398204803, accuracy: 0.9795918367346939\n",
      "loss: 0.06551896035671234, accuracy: 0.9836734693877551\n",
      "loss: 0.06579823046922684, accuracy: 0.9836734693877551\n",
      "loss: 0.06880563497543335, accuracy: 0.9836734693877551\n",
      "loss: 0.06618186086416245, accuracy: 0.9836734693877551\n",
      "loss: 0.06839330494403839, accuracy: 0.9795918367346939\n",
      "loss: 0.07095831632614136, accuracy: 0.9755102040816327\n",
      "loss: 0.06537291407585144, accuracy: 0.9877551020408163\n",
      "loss: 0.07106215506792068, accuracy: 0.9836734693877551\n",
      "loss: 0.07764851301908493, accuracy: 0.9673469387755103\n",
      "loss: 0.07497268170118332, accuracy: 0.9714285714285714\n",
      "loss: 0.06919416785240173, accuracy: 0.9836734693877551\n",
      "loss: 0.07534357160329819, accuracy: 0.9755102040816327\n",
      "loss: 0.07103457301855087, accuracy: 0.9795918367346939\n",
      "loss: 0.07833575457334518, accuracy: 0.9755102040816327\n",
      "loss: 0.07136160880327225, accuracy: 0.9795918367346939\n",
      "loss: 0.07082278281450272, accuracy: 0.9795918367346939\n",
      "loss: 0.06594761461019516, accuracy: 0.9836734693877551\n",
      "loss: 0.06937847286462784, accuracy: 0.9755102040816327\n",
      "loss: 0.06612259149551392, accuracy: 0.9836734693877551\n",
      "loss: 0.07817014306783676, accuracy: 0.9755102040816327\n",
      "loss: 0.07751929759979248, accuracy: 0.9714285714285714\n",
      "loss: 0.06640030443668365, accuracy: 0.9795918367346939\n",
      "loss: 0.06749892979860306, accuracy: 0.9795918367346939\n",
      "loss: 0.06560293585062027, accuracy: 0.9877551020408163\n",
      "loss: 0.06666455417871475, accuracy: 0.9836734693877551\n",
      "loss: 0.06720685213804245, accuracy: 0.9836734693877551\n",
      "loss: 0.06797067821025848, accuracy: 0.9795918367346939\n",
      "loss: 0.07429298013448715, accuracy: 0.9673469387755103\n",
      "loss: 0.06947219371795654, accuracy: 0.9795918367346939\n",
      "loss: 0.06384757161140442, accuracy: 0.9877551020408163\n",
      "loss: 0.06513676792383194, accuracy: 0.9836734693877551\n",
      "loss: 0.0665028840303421, accuracy: 0.9877551020408163\n",
      "loss: 0.0641142949461937, accuracy: 0.9836734693877551\n",
      "loss: 0.0678669810295105, accuracy: 0.9795918367346939\n",
      "loss: 0.07507530599832535, accuracy: 0.9673469387755103\n",
      "loss: 0.0685681402683258, accuracy: 0.9836734693877551\n",
      "loss: 0.06587618589401245, accuracy: 0.9795918367346939\n",
      "loss: 0.07725457102060318, accuracy: 0.9795918367346939\n",
      "loss: 0.0749870091676712, accuracy: 0.9755102040816327\n",
      "loss: 0.06868159025907516, accuracy: 0.9795918367346939\n",
      "loss: 0.06454911828041077, accuracy: 0.9877551020408163\n",
      "loss: 0.06532581150531769, accuracy: 0.9795918367346939\n",
      "loss: 0.06708668917417526, accuracy: 0.9795918367346939\n",
      "loss: 0.06922270357608795, accuracy: 0.9795918367346939\n",
      "loss: 0.06470921635627747, accuracy: 0.9877551020408163\n",
      "loss: 0.06761530041694641, accuracy: 0.9795918367346939\n",
      "loss: 0.06371133029460907, accuracy: 0.9795918367346939\n",
      "loss: 0.068699911236763, accuracy: 0.9836734693877551\n",
      "loss: 0.0680655762553215, accuracy: 0.9795918367346939\n",
      "max accuracy: 0.9918367346938776\n",
      "loss: 0.0650557205080986, accuracy: 0.9918367346938776\n",
      "loss: 0.0644753947854042, accuracy: 0.9836734693877551\n",
      "loss: 0.06432279199361801, accuracy: 0.9877551020408163\n",
      "loss: 0.06436805427074432, accuracy: 0.9836734693877551\n",
      "loss: 0.06827228516340256, accuracy: 0.9877551020408163\n",
      "loss: 0.06583752483129501, accuracy: 0.9795918367346939\n",
      "loss: 0.06841888278722763, accuracy: 0.9836734693877551\n",
      "loss: 0.06165449321269989, accuracy: 0.9877551020408163\n",
      "loss: 0.06814419478178024, accuracy: 0.9755102040816327\n",
      "loss: 0.06300700455904007, accuracy: 0.9877551020408163\n",
      "loss: 0.10974276810884476, accuracy: 0.9428571428571428\n",
      "loss: 0.1005420908331871, accuracy: 0.9673469387755103\n",
      "loss: 0.0732930526137352, accuracy: 0.9795918367346939\n",
      "loss: 0.07631203532218933, accuracy: 0.9795918367346939\n",
      "loss: 0.06811735779047012, accuracy: 0.9755102040816327\n",
      "loss: 0.06729427725076675, accuracy: 0.9795918367346939\n",
      "loss: 0.06514153629541397, accuracy: 0.9877551020408163\n",
      "loss: 0.06814813613891602, accuracy: 0.9673469387755103\n",
      "loss: 0.061338186264038086, accuracy: 0.9836734693877551\n",
      "loss: 0.06702281534671783, accuracy: 0.9836734693877551\n",
      "loss: 0.06761990487575531, accuracy: 0.9836734693877551\n",
      "loss: 0.06580150872468948, accuracy: 0.9755102040816327\n",
      "loss: 0.0682198703289032, accuracy: 0.9714285714285714\n",
      "loss: 0.06472654640674591, accuracy: 0.9836734693877551\n",
      "loss: 0.06269165873527527, accuracy: 0.9755102040816327\n",
      "loss: 0.06290183961391449, accuracy: 0.9795918367346939\n",
      "loss: 0.06329000741243362, accuracy: 0.9836734693877551\n",
      "loss: 0.06287050247192383, accuracy: 0.9836734693877551\n",
      "loss: 0.06551817804574966, accuracy: 0.9755102040816327\n",
      "loss: 0.06143547222018242, accuracy: 0.9836734693877551\n",
      "loss: 0.06692776829004288, accuracy: 0.9795918367346939\n",
      "loss: 0.061142291873693466, accuracy: 0.9877551020408163\n",
      "loss: 0.06139009818434715, accuracy: 0.9877551020408163\n",
      "loss: 0.06229139864444733, accuracy: 0.9795918367346939\n",
      "loss: 0.07215018570423126, accuracy: 0.9877551020408163\n",
      "loss: 0.06365516781806946, accuracy: 0.9836734693877551\n",
      "loss: 0.06450154632329941, accuracy: 0.9795918367346939\n",
      "loss: 0.06715147942304611, accuracy: 0.9755102040816327\n",
      "loss: 0.07480501383543015, accuracy: 0.9755102040816327\n",
      "loss: 0.07696495950222015, accuracy: 0.9755102040816327\n",
      "loss: 0.06956684589385986, accuracy: 0.9755102040816327\n",
      "loss: 0.06554267555475235, accuracy: 0.9836734693877551\n",
      "loss: 0.07318934053182602, accuracy: 0.9795918367346939\n",
      "loss: 0.06721186637878418, accuracy: 0.9714285714285714\n",
      "loss: 0.06276809424161911, accuracy: 0.9877551020408163\n",
      "loss: 0.06276358664035797, accuracy: 0.9836734693877551\n",
      "loss: 0.06553124636411667, accuracy: 0.9755102040816327\n",
      "loss: 0.06426158547401428, accuracy: 0.9755102040816327\n",
      "loss: 0.05998498946428299, accuracy: 0.9836734693877551\n",
      "loss: 0.06344189494848251, accuracy: 0.9836734693877551\n",
      "loss: 0.06657995283603668, accuracy: 0.9795918367346939\n",
      "loss: 0.06351839005947113, accuracy: 0.9795918367346939\n",
      "loss: 0.06235441938042641, accuracy: 0.9795918367346939\n",
      "loss: 0.06338532269001007, accuracy: 0.9836734693877551\n",
      "loss: 0.08215313404798508, accuracy: 0.9714285714285714\n",
      "loss: 0.0653405711054802, accuracy: 0.9877551020408163\n",
      "loss: 0.061050303280353546, accuracy: 0.9795918367346939\n",
      "loss: 0.06036892160773277, accuracy: 0.9755102040816327\n",
      "loss: 0.06335292756557465, accuracy: 0.9836734693877551\n",
      "loss: 0.06407926231622696, accuracy: 0.9795918367346939\n",
      "loss: 0.061087969690561295, accuracy: 0.9795918367346939\n",
      "loss: 0.060224927961826324, accuracy: 0.9918367346938776\n",
      "loss: 0.0639057382941246, accuracy: 0.9795918367346939\n",
      "loss: 0.06299968808889389, accuracy: 0.9836734693877551\n",
      "loss: 0.07197629660367966, accuracy: 0.9714285714285714\n",
      "loss: 0.06873016059398651, accuracy: 0.9877551020408163\n",
      "loss: 0.06082005798816681, accuracy: 0.9836734693877551\n",
      "loss: 0.060193322598934174, accuracy: 0.9795918367346939\n",
      "loss: 0.06061825528740883, accuracy: 0.9795918367346939\n",
      "loss: 0.06099774315953255, accuracy: 0.9755102040816327\n",
      "loss: 0.0632481500506401, accuracy: 0.9836734693877551\n",
      "loss: 0.0615372359752655, accuracy: 0.9836734693877551\n",
      "loss: 0.06804443150758743, accuracy: 0.9836734693877551\n",
      "loss: 0.06326434016227722, accuracy: 0.9795918367346939\n",
      "loss: 0.06724714487791061, accuracy: 0.9836734693877551\n",
      "loss: 0.06713241338729858, accuracy: 0.9755102040816327\n",
      "loss: 0.062407225370407104, accuracy: 0.9795918367346939\n",
      "loss: 0.06112246587872505, accuracy: 0.9877551020408163\n",
      "loss: 0.05989403277635574, accuracy: 0.9836734693877551\n",
      "loss: 0.07036171108484268, accuracy: 0.9714285714285714\n",
      "loss: 0.06133704632520676, accuracy: 0.9795918367346939\n",
      "loss: 0.06045521795749664, accuracy: 0.9836734693877551\n",
      "loss: 0.06795604526996613, accuracy: 0.9795918367346939\n",
      "loss: 0.06582897901535034, accuracy: 0.9877551020408163\n",
      "loss: 0.0611099973320961, accuracy: 0.9795918367346939\n",
      "loss: 0.06032518297433853, accuracy: 0.9795918367346939\n",
      "loss: 0.06369448453187943, accuracy: 0.9795918367346939\n",
      "loss: 0.06144056096673012, accuracy: 0.9795918367346939\n",
      "loss: 0.06341344118118286, accuracy: 0.9836734693877551\n",
      "loss: 0.07004963606595993, accuracy: 0.9714285714285714\n",
      "loss: 0.06406176835298538, accuracy: 0.9755102040816327\n",
      "loss: 0.06746800243854523, accuracy: 0.9755102040816327\n",
      "loss: 0.06142791360616684, accuracy: 0.9836734693877551\n",
      "loss: 0.05681881308555603, accuracy: 0.9836734693877551\n",
      "loss: 0.0619015246629715, accuracy: 0.9795918367346939\n",
      "loss: 0.06188174709677696, accuracy: 0.9795918367346939\n",
      "loss: 0.059168506413698196, accuracy: 0.9877551020408163\n",
      "loss: 0.058758292347192764, accuracy: 0.9836734693877551\n",
      "loss: 0.06944703310728073, accuracy: 0.9714285714285714\n",
      "loss: 0.06082535535097122, accuracy: 0.9795918367346939\n",
      "loss: 0.062039878219366074, accuracy: 0.9795918367346939\n",
      "loss: 0.06478165835142136, accuracy: 0.9795918367346939\n",
      "loss: 0.06384849548339844, accuracy: 0.9795918367346939\n",
      "loss: 0.06178862601518631, accuracy: 0.9836734693877551\n",
      "loss: 0.0612371601164341, accuracy: 0.9795918367346939\n",
      "loss: 0.07145596295595169, accuracy: 0.9673469387755103\n",
      "loss: 0.06795571744441986, accuracy: 0.9836734693877551\n",
      "loss: 0.06450451165437698, accuracy: 0.9877551020408163\n",
      "loss: 0.05748753994703293, accuracy: 0.9836734693877551\n",
      "loss: 0.05779170244932175, accuracy: 0.9795918367346939\n",
      "loss: 0.06044495850801468, accuracy: 0.9795918367346939\n",
      "loss: 0.05749444290995598, accuracy: 0.9877551020408163\n",
      "loss: 0.05695849657058716, accuracy: 0.9877551020408163\n",
      "loss: 0.058473724871873856, accuracy: 0.9755102040816327\n",
      "loss: 0.06343019753694534, accuracy: 0.9755102040816327\n",
      "loss: 0.06781047582626343, accuracy: 0.9836734693877551\n",
      "loss: 0.06337215006351471, accuracy: 0.9877551020408163\n",
      "loss: 0.05586735159158707, accuracy: 0.9918367346938776\n",
      "loss: 0.05755726993083954, accuracy: 0.9877551020408163\n",
      "loss: 0.05527447909116745, accuracy: 0.9836734693877551\n",
      "loss: 0.062464743852615356, accuracy: 0.9755102040816327\n",
      "loss: 0.06538804620504379, accuracy: 0.9755102040816327\n",
      "loss: 0.05679512023925781, accuracy: 0.9877551020408163\n",
      "loss: 0.06151166558265686, accuracy: 0.9877551020408163\n",
      "loss: 0.060639671981334686, accuracy: 0.9877551020408163\n",
      "loss: 0.06549735367298126, accuracy: 0.9755102040816327\n",
      "loss: 0.06061401590704918, accuracy: 0.9836734693877551\n",
      "loss: 0.057614509016275406, accuracy: 0.9877551020408163\n",
      "loss: 0.06013883650302887, accuracy: 0.9836734693877551\n",
      "loss: 0.06117546558380127, accuracy: 0.9836734693877551\n",
      "loss: 0.0691811665892601, accuracy: 0.9755102040816327\n",
      "loss: 0.057610996067523956, accuracy: 0.9877551020408163\n",
      "loss: 0.057698413729667664, accuracy: 0.9877551020408163\n",
      "loss: 0.056887008249759674, accuracy: 0.9836734693877551\n",
      "loss: 0.05870210751891136, accuracy: 0.9836734693877551\n",
      "loss: 0.06035560369491577, accuracy: 0.9755102040816327\n",
      "loss: 0.06323021650314331, accuracy: 0.9877551020408163\n",
      "loss: 0.05604257434606552, accuracy: 0.9877551020408163\n",
      "loss: 0.060548316687345505, accuracy: 0.9795918367346939\n",
      "loss: 0.05905620753765106, accuracy: 0.9877551020408163\n",
      "loss: 0.06747832149267197, accuracy: 0.9795918367346939\n",
      "loss: 0.10083771497011185, accuracy: 0.9673469387755103\n",
      "loss: 0.059941016137599945, accuracy: 0.9836734693877551\n",
      "loss: 0.05816234275698662, accuracy: 0.9877551020408163\n",
      "loss: 0.05477292463183403, accuracy: 0.9877551020408163\n",
      "loss: 0.055157165974378586, accuracy: 0.9795918367346939\n",
      "loss: 0.06069888547062874, accuracy: 0.9795918367346939\n",
      "loss: 0.058863215148448944, accuracy: 0.9836734693877551\n",
      "loss: 0.06211910769343376, accuracy: 0.9877551020408163\n",
      "loss: 0.0567709244787693, accuracy: 0.9877551020408163\n",
      "loss: 0.061488423496484756, accuracy: 0.9755102040816327\n",
      "loss: 0.06301278620958328, accuracy: 0.9755102040816327\n",
      "loss: 0.05973716452717781, accuracy: 0.9877551020408163\n",
      "loss: 0.06250827014446259, accuracy: 0.9795918367346939\n",
      "loss: 0.05954885110259056, accuracy: 0.9877551020408163\n",
      "loss: 0.05739724636077881, accuracy: 0.9836734693877551\n",
      "loss: 0.05438120663166046, accuracy: 0.9877551020408163\n",
      "loss: 0.056429896503686905, accuracy: 0.9877551020408163\n",
      "loss: 0.05865505337715149, accuracy: 0.9877551020408163\n",
      "loss: 0.05779718607664108, accuracy: 0.9836734693877551\n",
      "loss: 0.06295321136713028, accuracy: 0.9755102040816327\n",
      "loss: 0.0577244870364666, accuracy: 0.9836734693877551\n",
      "loss: 0.06088282912969589, accuracy: 0.9836734693877551\n",
      "loss: 0.06195927411317825, accuracy: 0.9836734693877551\n",
      "loss: 0.06204472854733467, accuracy: 0.9714285714285714\n",
      "loss: 0.05581614375114441, accuracy: 0.9877551020408163\n",
      "loss: 0.05660656839609146, accuracy: 0.9836734693877551\n",
      "loss: 0.0540778748691082, accuracy: 0.9836734693877551\n",
      "loss: 0.055898308753967285, accuracy: 0.9795918367346939\n",
      "loss: 0.06236179918050766, accuracy: 0.9755102040816327\n",
      "loss: 0.057698603719472885, accuracy: 0.9877551020408163\n",
      "loss: 0.0601450651884079, accuracy: 0.9836734693877551\n",
      "loss: 0.059180937707424164, accuracy: 0.9755102040816327\n",
      "loss: 0.05888523906469345, accuracy: 0.9877551020408163\n",
      "loss: 0.0540311336517334, accuracy: 0.9877551020408163\n",
      "loss: 0.07394525408744812, accuracy: 0.9714285714285714\n",
      "loss: 0.05698617920279503, accuracy: 0.9836734693877551\n",
      "loss: 0.05648165941238403, accuracy: 0.9836734693877551\n",
      "loss: 0.05886338651180267, accuracy: 0.9836734693877551\n",
      "loss: 0.05491987243294716, accuracy: 0.9836734693877551\n",
      "loss: 0.054867930710315704, accuracy: 0.9877551020408163\n",
      "loss: 0.056146129965782166, accuracy: 0.9836734693877551\n",
      "loss: 0.05313548445701599, accuracy: 0.9877551020408163\n",
      "loss: 0.05603521689772606, accuracy: 0.9877551020408163\n",
      "loss: 0.05764349177479744, accuracy: 0.9877551020408163\n",
      "loss: 0.05718154087662697, accuracy: 0.9836734693877551\n",
      "loss: 0.056447833776474, accuracy: 0.9755102040816327\n",
      "loss: 0.05429084971547127, accuracy: 0.9836734693877551\n",
      "loss: 0.054986726492643356, accuracy: 0.9836734693877551\n",
      "loss: 0.05427701398730278, accuracy: 0.9918367346938776\n",
      "loss: 0.05179409310221672, accuracy: 0.9918367346938776\n",
      "loss: 0.05454636737704277, accuracy: 0.9877551020408163\n",
      "loss: 0.05548514425754547, accuracy: 0.9836734693877551\n",
      "loss: 0.05546749010682106, accuracy: 0.9836734693877551\n",
      "loss: 0.054000355303287506, accuracy: 0.9877551020408163\n",
      "loss: 0.05367810279130936, accuracy: 0.9877551020408163\n",
      "loss: 0.05581330880522728, accuracy: 0.9877551020408163\n",
      "loss: 0.05624513700604439, accuracy: 0.9877551020408163\n",
      "loss: 0.05644636228680611, accuracy: 0.9918367346938776\n",
      "loss: 0.05189378932118416, accuracy: 0.9877551020408163\n",
      "loss: 0.052980050444602966, accuracy: 0.9755102040816327\n",
      "loss: 0.059334881603717804, accuracy: 0.9877551020408163\n",
      "loss: 0.05607802793383598, accuracy: 0.9836734693877551\n",
      "loss: 0.05187603458762169, accuracy: 0.9877551020408163\n",
      "loss: 0.0549108125269413, accuracy: 0.9877551020408163\n",
      "loss: 0.05647239461541176, accuracy: 0.9918367346938776\n",
      "loss: 0.052451685070991516, accuracy: 0.9836734693877551\n",
      "loss: 0.05947917699813843, accuracy: 0.9795918367346939\n",
      "loss: 0.05398393049836159, accuracy: 0.9877551020408163\n",
      "loss: 0.055899478495121, accuracy: 0.9918367346938776\n",
      "loss: 0.05385756492614746, accuracy: 0.9795918367346939\n",
      "loss: 0.052653297781944275, accuracy: 0.9877551020408163\n",
      "loss: 0.08298910409212112, accuracy: 0.9673469387755103\n",
      "loss: 0.09377075731754303, accuracy: 0.963265306122449\n",
      "loss: 0.0625472366809845, accuracy: 0.9877551020408163\n",
      "loss: 0.0589301660656929, accuracy: 0.9877551020408163\n",
      "loss: 0.05072451010346413, accuracy: 0.9877551020408163\n",
      "loss: 0.05270079895853996, accuracy: 0.9836734693877551\n",
      "loss: 0.05506519228219986, accuracy: 0.9836734693877551\n",
      "loss: 0.05050421133637428, accuracy: 0.9877551020408163\n",
      "loss: 0.05308917909860611, accuracy: 0.9836734693877551\n",
      "loss: 0.06063804030418396, accuracy: 0.9795918367346939\n",
      "loss: 0.05866759270429611, accuracy: 0.9877551020408163\n",
      "loss: 0.0531555600464344, accuracy: 0.9877551020408163\n",
      "loss: 0.052809055894613266, accuracy: 0.9836734693877551\n",
      "loss: 0.050879739224910736, accuracy: 0.9918367346938776\n",
      "loss: 0.05369880050420761, accuracy: 0.9877551020408163\n",
      "loss: 0.051913753151893616, accuracy: 0.9836734693877551\n",
      "loss: 0.05291491746902466, accuracy: 0.9877551020408163\n",
      "loss: 0.06319573521614075, accuracy: 0.9755102040816327\n",
      "loss: 0.05973945930600166, accuracy: 0.9795918367346939\n",
      "loss: 0.05229151248931885, accuracy: 0.9918367346938776\n",
      "loss: 0.053114790469408035, accuracy: 0.9877551020408163\n",
      "loss: 0.058254361152648926, accuracy: 0.9836734693877551\n",
      "loss: 0.05155203863978386, accuracy: 0.9877551020408163\n",
      "loss: 0.05365751311182976, accuracy: 0.9877551020408163\n",
      "loss: 0.049678198993206024, accuracy: 0.9877551020408163\n",
      "loss: 0.05325458198785782, accuracy: 0.9877551020408163\n",
      "loss: 0.05862477049231529, accuracy: 0.9795918367346939\n",
      "loss: 0.060593247413635254, accuracy: 0.9795918367346939\n",
      "loss: 0.05694250389933586, accuracy: 0.9755102040816327\n",
      "loss: 0.06724327057600021, accuracy: 0.9714285714285714\n",
      "loss: 0.0512363575398922, accuracy: 0.9877551020408163\n",
      "loss: 0.05358436331152916, accuracy: 0.9877551020408163\n",
      "loss: 0.05031745508313179, accuracy: 0.9877551020408163\n",
      "loss: 0.05475262179970741, accuracy: 0.9836734693877551\n",
      "loss: 0.05739349126815796, accuracy: 0.9795918367346939\n",
      "loss: 0.06040668487548828, accuracy: 0.9795918367346939\n",
      "loss: 0.056659236550331116, accuracy: 0.9877551020408163\n",
      "loss: 0.05567086115479469, accuracy: 0.9877551020408163\n",
      "loss: 0.05386200547218323, accuracy: 0.9836734693877551\n",
      "loss: 0.05349813029170036, accuracy: 0.9836734693877551\n",
      "loss: 0.05352390930056572, accuracy: 0.9877551020408163\n",
      "loss: 0.06525252759456635, accuracy: 0.9755102040816327\n",
      "loss: 0.05228903517127037, accuracy: 0.9795918367346939\n",
      "loss: 0.053053177893161774, accuracy: 0.9795918367346939\n",
      "loss: 0.0555276982486248, accuracy: 0.9836734693877551\n",
      "loss: 0.04957020282745361, accuracy: 0.9918367346938776\n",
      "loss: 0.05032773315906525, accuracy: 0.9918367346938776\n",
      "loss: 0.05237530171871185, accuracy: 0.9836734693877551\n",
      "loss: 0.04959309473633766, accuracy: 0.9918367346938776\n",
      "loss: 0.05242615193128586, accuracy: 0.9836734693877551\n",
      "loss: 0.049502160400152206, accuracy: 0.9836734693877551\n",
      "loss: 0.048017390072345734, accuracy: 0.9918367346938776\n",
      "loss: 0.04861659184098244, accuracy: 0.9918367346938776\n",
      "loss: 0.05055852606892586, accuracy: 0.9877551020408163\n",
      "loss: 0.052218109369277954, accuracy: 0.9877551020408163\n",
      "loss: 0.05594396963715553, accuracy: 0.9836734693877551\n",
      "loss: 0.05410594865679741, accuracy: 0.9795918367346939\n",
      "loss: 0.05064310133457184, accuracy: 0.9918367346938776\n",
      "loss: 0.06191186606884003, accuracy: 0.9836734693877551\n",
      "loss: 0.048780277371406555, accuracy: 0.9918367346938776\n",
      "loss: 0.05421529337763786, accuracy: 0.9877551020408163\n",
      "loss: 0.05573277547955513, accuracy: 0.9836734693877551\n",
      "loss: 0.050387248396873474, accuracy: 0.9877551020408163\n",
      "loss: 0.05384540557861328, accuracy: 0.9836734693877551\n",
      "loss: 0.052178025245666504, accuracy: 0.9836734693877551\n",
      "loss: 0.0472932830452919, accuracy: 0.9877551020408163\n",
      "loss: 0.059556782245635986, accuracy: 0.9755102040816327\n",
      "loss: 0.05379408597946167, accuracy: 0.9836734693877551\n",
      "loss: 0.05443727970123291, accuracy: 0.9836734693877551\n",
      "loss: 0.05135009065270424, accuracy: 0.9877551020408163\n",
      "loss: 0.059122055768966675, accuracy: 0.9755102040816327\n",
      "loss: 0.04906630516052246, accuracy: 0.9836734693877551\n",
      "max accuracy: 0.9959183673469387\n",
      "loss: 0.049026429653167725, accuracy: 0.9959183673469387\n",
      "loss: 0.05698992311954498, accuracy: 0.9836734693877551\n",
      "loss: 0.05019662156701088, accuracy: 0.9877551020408163\n",
      "loss: 0.05507044121623039, accuracy: 0.9836734693877551\n",
      "loss: 0.048452869057655334, accuracy: 0.9836734693877551\n",
      "loss: 0.04866160452365875, accuracy: 0.9918367346938776\n",
      "loss: 0.05788548290729523, accuracy: 0.9795918367346939\n",
      "loss: 0.05016277730464935, accuracy: 0.9918367346938776\n",
      "loss: 0.0500682108104229, accuracy: 0.9877551020408163\n",
      "loss: 0.04781852290034294, accuracy: 0.9877551020408163\n",
      "loss: 0.05282384902238846, accuracy: 0.9918367346938776\n",
      "loss: 0.054875101894140244, accuracy: 0.9836734693877551\n",
      "loss: 0.05103442817926407, accuracy: 0.9795918367346939\n",
      "loss: 0.048528242856264114, accuracy: 0.9877551020408163\n",
      "loss: 0.0536419041454792, accuracy: 0.9836734693877551\n",
      "loss: 0.04695862531661987, accuracy: 0.9877551020408163\n",
      "loss: 0.048717159777879715, accuracy: 0.9795918367346939\n",
      "loss: 0.05093304067850113, accuracy: 0.9836734693877551\n",
      "loss: 0.05041932314634323, accuracy: 0.9918367346938776\n",
      "loss: 0.056180112063884735, accuracy: 0.9795918367346939\n",
      "loss: 0.05914534628391266, accuracy: 0.9755102040816327\n",
      "loss: 0.05235860496759415, accuracy: 0.9755102040816327\n",
      "loss: 0.052494458854198456, accuracy: 0.9918367346938776\n",
      "loss: 0.04683931544423103, accuracy: 0.9959183673469387\n",
      "loss: 0.04724488407373428, accuracy: 0.9877551020408163\n",
      "loss: 0.05046100169420242, accuracy: 0.9918367346938776\n",
      "loss: 0.050663240253925323, accuracy: 0.9877551020408163\n",
      "loss: 0.047170091420412064, accuracy: 0.9918367346938776\n",
      "loss: 0.04656340554356575, accuracy: 0.9918367346938776\n",
      "loss: 0.04781876504421234, accuracy: 0.9918367346938776\n",
      "loss: 0.045495398342609406, accuracy: 0.9918367346938776\n",
      "loss: 0.061596792191267014, accuracy: 0.9714285714285714\n",
      "loss: 0.05019927769899368, accuracy: 0.9918367346938776\n",
      "loss: 0.05140623077750206, accuracy: 0.9877551020408163\n",
      "loss: 0.06753542274236679, accuracy: 0.9714285714285714\n",
      "loss: 0.04987920820713043, accuracy: 0.9918367346938776\n",
      "loss: 0.04887434467673302, accuracy: 0.9918367346938776\n",
      "loss: 0.04681367799639702, accuracy: 0.9918367346938776\n",
      "loss: 0.04897017404437065, accuracy: 0.9836734693877551\n",
      "loss: 0.047677021473646164, accuracy: 0.9918367346938776\n",
      "loss: 0.04686485975980759, accuracy: 0.9877551020408163\n",
      "loss: 0.04437572881579399, accuracy: 0.9877551020408163\n",
      "loss: 0.049921564757823944, accuracy: 0.9877551020408163\n",
      "loss: 0.050525911152362823, accuracy: 0.9836734693877551\n",
      "loss: 0.04852966591715813, accuracy: 0.9877551020408163\n",
      "loss: 0.05017440393567085, accuracy: 0.9795918367346939\n",
      "loss: 0.04631040245294571, accuracy: 0.9877551020408163\n",
      "loss: 0.04629142954945564, accuracy: 0.9918367346938776\n",
      "loss: 0.04697933793067932, accuracy: 0.9877551020408163\n",
      "loss: 0.04710150882601738, accuracy: 0.9918367346938776\n",
      "loss: 0.04679800942540169, accuracy: 0.9836734693877551\n",
      "loss: 0.04554913938045502, accuracy: 0.9877551020408163\n",
      "loss: 0.04596840590238571, accuracy: 0.9877551020408163\n",
      "loss: 0.04791712760925293, accuracy: 0.9877551020408163\n",
      "loss: 0.058325979858636856, accuracy: 0.9795918367346939\n",
      "loss: 0.0496668666601181, accuracy: 0.9836734693877551\n",
      "loss: 0.04969305545091629, accuracy: 0.9877551020408163\n",
      "loss: 0.0475662462413311, accuracy: 0.9918367346938776\n",
      "loss: 0.04467073827981949, accuracy: 0.9877551020408163\n",
      "loss: 0.04618821293115616, accuracy: 0.9877551020408163\n",
      "loss: 0.04963956028223038, accuracy: 0.9877551020408163\n",
      "loss: 0.0469786711037159, accuracy: 0.9877551020408163\n",
      "loss: 0.0441107451915741, accuracy: 0.9877551020408163\n",
      "loss: 0.04461978003382683, accuracy: 0.9959183673469387\n",
      "loss: 0.050795309245586395, accuracy: 0.9877551020408163\n",
      "loss: 0.04789379611611366, accuracy: 0.9877551020408163\n",
      "loss: 0.04435056075453758, accuracy: 0.9877551020408163\n",
      "loss: 0.0521252378821373, accuracy: 0.9755102040816327\n",
      "loss: 0.04696739464998245, accuracy: 0.9877551020408163\n",
      "loss: 0.047902751713991165, accuracy: 0.9877551020408163\n",
      "loss: 0.052408572286367416, accuracy: 0.9836734693877551\n",
      "loss: 0.04728975147008896, accuracy: 0.9877551020408163\n",
      "loss: 0.04573206603527069, accuracy: 0.9918367346938776\n",
      "loss: 0.044648949056863785, accuracy: 0.9877551020408163\n",
      "loss: 0.042609598487615585, accuracy: 0.9918367346938776\n",
      "loss: 0.051176197826862335, accuracy: 0.9795918367346939\n",
      "loss: 0.04481244459748268, accuracy: 0.9918367346938776\n",
      "loss: 0.04488210007548332, accuracy: 0.9836734693877551\n",
      "loss: 0.05012029409408569, accuracy: 0.9836734693877551\n",
      "loss: 0.04956550523638725, accuracy: 0.9918367346938776\n",
      "loss: 0.043369658291339874, accuracy: 0.9918367346938776\n",
      "loss: 0.05106179788708687, accuracy: 0.9836734693877551\n",
      "loss: 0.043525759130716324, accuracy: 0.9918367346938776\n",
      "loss: 0.04747149348258972, accuracy: 0.9918367346938776\n",
      "loss: 0.050508465617895126, accuracy: 0.9795918367346939\n",
      "loss: 0.04624279960989952, accuracy: 0.9877551020408163\n",
      "loss: 0.05154552310705185, accuracy: 0.9877551020408163\n",
      "loss: 0.04658237099647522, accuracy: 0.9877551020408163\n",
      "loss: 0.04643672704696655, accuracy: 0.9877551020408163\n",
      "loss: 0.04632314667105675, accuracy: 0.9877551020408163\n",
      "loss: 0.04499498009681702, accuracy: 0.9959183673469387\n",
      "loss: 0.04503165930509567, accuracy: 0.9918367346938776\n",
      "loss: 0.04649507626891136, accuracy: 0.9877551020408163\n",
      "loss: 0.04624826833605766, accuracy: 0.9877551020408163\n",
      "loss: 0.04644261673092842, accuracy: 0.9877551020408163\n",
      "loss: 0.04442304000258446, accuracy: 0.9877551020408163\n",
      "loss: 0.04893498867750168, accuracy: 0.9836734693877551\n",
      "loss: 0.04538998007774353, accuracy: 0.9877551020408163\n",
      "loss: 0.04313801974058151, accuracy: 0.9918367346938776\n",
      "loss: 0.044999416917562485, accuracy: 0.9877551020408163\n",
      "loss: 0.0441180095076561, accuracy: 0.9918367346938776\n",
      "loss: 0.04412439465522766, accuracy: 0.9959183673469387\n",
      "loss: 0.0507255494594574, accuracy: 0.9877551020408163\n",
      "loss: 0.045889243483543396, accuracy: 0.9959183673469387\n",
      "loss: 0.049990225583314896, accuracy: 0.9795918367346939\n",
      "loss: 0.04302246868610382, accuracy: 0.9959183673469387\n",
      "loss: 0.04768211767077446, accuracy: 0.9877551020408163\n",
      "loss: 0.04531992971897125, accuracy: 0.9918367346938776\n",
      "loss: 0.04579587280750275, accuracy: 0.9877551020408163\n",
      "loss: 0.049790844321250916, accuracy: 0.9836734693877551\n",
      "loss: 0.04489267244935036, accuracy: 0.9918367346938776\n",
      "loss: 0.048147302120923996, accuracy: 0.9918367346938776\n",
      "loss: 0.04987940564751625, accuracy: 0.9836734693877551\n",
      "loss: 0.04242614284157753, accuracy: 0.9959183673469387\n",
      "loss: 0.043883878737688065, accuracy: 0.9959183673469387\n",
      "loss: 0.043931324034929276, accuracy: 0.9877551020408163\n",
      "loss: 0.04395691677927971, accuracy: 0.9877551020408163\n",
      "loss: 0.04786020889878273, accuracy: 0.9877551020408163\n",
      "loss: 0.04517728090286255, accuracy: 0.9918367346938776\n",
      "loss: 0.0425589345395565, accuracy: 0.9918367346938776\n",
      "loss: 0.04315605014562607, accuracy: 0.9918367346938776\n",
      "loss: 0.043432533740997314, accuracy: 0.9918367346938776\n",
      "loss: 0.04091883450746536, accuracy: 0.9918367346938776\n",
      "loss: 0.043592434376478195, accuracy: 0.9877551020408163\n",
      "loss: 0.04029696807265282, accuracy: 0.9918367346938776\n",
      "loss: 0.04867497831583023, accuracy: 0.9836734693877551\n",
      "loss: 0.0492367185652256, accuracy: 0.9836734693877551\n",
      "loss: 0.04158689081668854, accuracy: 0.9918367346938776\n",
      "loss: 0.043153613805770874, accuracy: 0.9877551020408163\n",
      "loss: 0.0401153601706028, accuracy: 0.9918367346938776\n",
      "loss: 0.04246320202946663, accuracy: 0.9959183673469387\n",
      "loss: 0.041161999106407166, accuracy: 0.9918367346938776\n",
      "loss: 0.04407738149166107, accuracy: 0.9959183673469387\n",
      "loss: 0.04197017103433609, accuracy: 0.9918367346938776\n",
      "loss: 0.04362243413925171, accuracy: 0.9918367346938776\n",
      "loss: 0.04432179033756256, accuracy: 0.9918367346938776\n",
      "loss: 0.04226826876401901, accuracy: 0.9918367346938776\n",
      "loss: 0.04342092573642731, accuracy: 0.9918367346938776\n",
      "loss: 0.03969878330826759, accuracy: 0.9877551020408163\n",
      "loss: 0.039510540664196014, accuracy: 0.9877551020408163\n",
      "max accuracy: 1.0\n",
      "loss: 0.04057513177394867, accuracy: 1.0\n",
      "loss: 0.04196888953447342, accuracy: 0.9877551020408163\n",
      "loss: 0.04308568686246872, accuracy: 0.9877551020408163\n",
      "loss: 0.04129437357187271, accuracy: 0.9959183673469387\n",
      "loss: 0.04175581410527229, accuracy: 0.9877551020408163\n",
      "loss: 0.041135165840387344, accuracy: 0.9918367346938776\n",
      "loss: 0.04766250401735306, accuracy: 0.9755102040816327\n",
      "loss: 0.04373956099152565, accuracy: 0.9877551020408163\n",
      "loss: 0.05197121575474739, accuracy: 0.9836734693877551\n",
      "loss: 0.040881089866161346, accuracy: 0.9959183673469387\n",
      "loss: 0.0420309454202652, accuracy: 0.9959183673469387\n",
      "loss: 0.04862380027770996, accuracy: 0.9836734693877551\n",
      "loss: 0.04594315588474274, accuracy: 0.9877551020408163\n",
      "loss: 0.0421893410384655, accuracy: 0.9959183673469387\n",
      "loss: 0.04280105233192444, accuracy: 0.9836734693877551\n",
      "loss: 0.04164160415530205, accuracy: 0.9918367346938776\n",
      "loss: 0.041356150060892105, accuracy: 0.9959183673469387\n",
      "loss: 0.04071950167417526, accuracy: 0.9918367346938776\n",
      "loss: 0.04082821309566498, accuracy: 1.0\n",
      "loss: 0.0445634126663208, accuracy: 0.9877551020408163\n",
      "loss: 0.042648795992136, accuracy: 0.9877551020408163\n",
      "loss: 0.039043694734573364, accuracy: 0.9959183673469387\n",
      "loss: 0.03987337276339531, accuracy: 0.9918367346938776\n",
      "loss: 0.042528316378593445, accuracy: 0.9918367346938776\n",
      "loss: 0.04498658701777458, accuracy: 0.9836734693877551\n",
      "loss: 0.038700491189956665, accuracy: 0.9918367346938776\n",
      "loss: 0.04237719997763634, accuracy: 0.9918367346938776\n",
      "loss: 0.0411035418510437, accuracy: 0.9918367346938776\n",
      "loss: 0.04116387292742729, accuracy: 0.9959183673469387\n",
      "loss: 0.03884769231081009, accuracy: 0.9918367346938776\n",
      "loss: 0.041939932852983475, accuracy: 0.9918367346938776\n",
      "loss: 0.045695625245571136, accuracy: 0.9959183673469387\n",
      "loss: 0.04123951867222786, accuracy: 0.9877551020408163\n",
      "loss: 0.04927767440676689, accuracy: 0.9836734693877551\n",
      "loss: 0.03878939896821976, accuracy: 0.9959183673469387\n",
      "loss: 0.0451391264796257, accuracy: 0.9877551020408163\n",
      "loss: 0.03946680575609207, accuracy: 0.9959183673469387\n",
      "loss: 0.04072404280304909, accuracy: 0.9918367346938776\n",
      "loss: 0.041047465056180954, accuracy: 0.9918367346938776\n",
      "loss: 0.04173613339662552, accuracy: 0.9877551020408163\n",
      "loss: 0.04066510125994682, accuracy: 1.0\n",
      "loss: 0.03929470106959343, accuracy: 0.9959183673469387\n",
      "loss: 0.04061172157526016, accuracy: 0.9918367346938776\n",
      "loss: 0.039354100823402405, accuracy: 0.9959183673469387\n",
      "loss: 0.040672022849321365, accuracy: 0.9877551020408163\n",
      "loss: 0.04494556039571762, accuracy: 0.9918367346938776\n",
      "loss: 0.039049360901117325, accuracy: 0.9959183673469387\n",
      "loss: 0.04138152673840523, accuracy: 0.9877551020408163\n",
      "loss: 0.04230746626853943, accuracy: 0.9959183673469387\n",
      "loss: 0.04109107330441475, accuracy: 0.9877551020408163\n",
      "loss: 0.03939380124211311, accuracy: 0.9918367346938776\n",
      "loss: 0.04124675318598747, accuracy: 0.9918367346938776\n",
      "loss: 0.03775534778833389, accuracy: 0.9918367346938776\n",
      "loss: 0.04030837491154671, accuracy: 0.9959183673469387\n",
      "loss: 0.040156517177820206, accuracy: 0.9877551020408163\n",
      "loss: 0.03810868039727211, accuracy: 0.9959183673469387\n",
      "loss: 0.0398864671587944, accuracy: 0.9959183673469387\n",
      "loss: 0.04123754799365997, accuracy: 0.9918367346938776\n",
      "loss: 0.03854859247803688, accuracy: 0.9918367346938776\n",
      "loss: 0.040474552661180496, accuracy: 0.9918367346938776\n",
      "loss: 0.044094767421483994, accuracy: 0.9918367346938776\n",
      "loss: 0.04343133419752121, accuracy: 0.9959183673469387\n",
      "loss: 0.03809903934597969, accuracy: 0.9918367346938776\n",
      "loss: 0.03877953067421913, accuracy: 0.9918367346938776\n",
      "loss: 0.041909508407115936, accuracy: 0.9877551020408163\n",
      "loss: 0.04649335518479347, accuracy: 0.9836734693877551\n",
      "loss: 0.036988720297813416, accuracy: 0.9959183673469387\n",
      "loss: 0.04127654805779457, accuracy: 0.9959183673469387\n",
      "loss: 0.038207557052373886, accuracy: 0.9877551020408163\n",
      "loss: 0.03809285908937454, accuracy: 0.9918367346938776\n",
      "loss: 0.040878694504499435, accuracy: 0.9959183673469387\n",
      "loss: 0.039038561284542084, accuracy: 0.9959183673469387\n",
      "loss: 0.042390886694192886, accuracy: 0.9959183673469387\n",
      "loss: 0.038882240653038025, accuracy: 0.9959183673469387\n",
      "loss: 0.03746131807565689, accuracy: 0.9918367346938776\n",
      "loss: 0.040276817977428436, accuracy: 0.9918367346938776\n",
      "loss: 0.03953365609049797, accuracy: 0.9918367346938776\n",
      "loss: 0.03751344233751297, accuracy: 0.9959183673469387\n",
      "loss: 0.03635920584201813, accuracy: 1.0\n",
      "loss: 0.037093594670295715, accuracy: 0.9959183673469387\n",
      "loss: 0.041023653000593185, accuracy: 0.9959183673469387\n",
      "loss: 0.03581919148564339, accuracy: 1.0\n",
      "loss: 0.03885271027684212, accuracy: 0.9918367346938776\n",
      "loss: 0.039736486971378326, accuracy: 0.9877551020408163\n",
      "loss: 0.040708765387535095, accuracy: 0.9918367346938776\n",
      "loss: 0.034980833530426025, accuracy: 0.9918367346938776\n",
      "loss: 0.03786829486489296, accuracy: 1.0\n",
      "loss: 0.0365445613861084, accuracy: 0.9959183673469387\n",
      "loss: 0.03530928120017052, accuracy: 1.0\n",
      "loss: 0.040053803473711014, accuracy: 0.9877551020408163\n",
      "loss: 0.03661557659506798, accuracy: 0.9959183673469387\n",
      "loss: 0.04401349648833275, accuracy: 0.9836734693877551\n",
      "loss: 0.035852331668138504, accuracy: 1.0\n",
      "loss: 0.03858589008450508, accuracy: 0.9877551020408163\n",
      "loss: 0.0378459095954895, accuracy: 0.9959183673469387\n",
      "loss: 0.03760146349668503, accuracy: 0.9918367346938776\n",
      "loss: 0.036320991814136505, accuracy: 0.9877551020408163\n",
      "loss: 0.03840220719575882, accuracy: 0.9877551020408163\n",
      "loss: 0.03680390864610672, accuracy: 0.9959183673469387\n",
      "loss: 0.03553786128759384, accuracy: 0.9918367346938776\n",
      "loss: 0.035215266048908234, accuracy: 0.9959183673469387\n",
      "loss: 0.037065066397190094, accuracy: 0.9918367346938776\n",
      "loss: 0.039421163499355316, accuracy: 0.9918367346938776\n",
      "loss: 0.04005827382206917, accuracy: 0.9959183673469387\n",
      "loss: 0.03574841842055321, accuracy: 0.9918367346938776\n",
      "loss: 0.03460175171494484, accuracy: 1.0\n",
      "loss: 0.0366261750459671, accuracy: 0.9918367346938776\n",
      "loss: 0.0388791598379612, accuracy: 0.9959183673469387\n",
      "loss: 0.034901950508356094, accuracy: 1.0\n",
      "loss: 0.03580296039581299, accuracy: 0.9959183673469387\n",
      "loss: 0.03751758486032486, accuracy: 0.9918367346938776\n",
      "loss: 0.037973787635564804, accuracy: 0.9918367346938776\n",
      "loss: 0.03725528344511986, accuracy: 0.9959183673469387\n",
      "loss: 0.036353614181280136, accuracy: 0.9918367346938776\n",
      "loss: 0.038076721131801605, accuracy: 0.9918367346938776\n",
      "loss: 0.03845417499542236, accuracy: 0.9918367346938776\n",
      "loss: 0.040688034147024155, accuracy: 0.9877551020408163\n",
      "loss: 0.03617221117019653, accuracy: 0.9918367346938776\n",
      "loss: 0.03592965751886368, accuracy: 0.9959183673469387\n",
      "loss: 0.03684329241514206, accuracy: 0.9959183673469387\n",
      "loss: 0.036196548491716385, accuracy: 0.9918367346938776\n",
      "loss: 0.038749657571315765, accuracy: 0.9918367346938776\n",
      "loss: 0.037926193326711655, accuracy: 0.9959183673469387\n",
      "loss: 0.035584159195423126, accuracy: 0.9918367346938776\n",
      "loss: 0.03940785303711891, accuracy: 0.9918367346938776\n",
      "loss: 0.036716192960739136, accuracy: 0.9918367346938776\n",
      "loss: 0.036526184529066086, accuracy: 0.9959183673469387\n",
      "loss: 0.03657224774360657, accuracy: 0.9918367346938776\n",
      "loss: 0.03417118266224861, accuracy: 1.0\n",
      "loss: 0.03881850838661194, accuracy: 0.9836734693877551\n",
      "loss: 0.036271288990974426, accuracy: 0.9918367346938776\n",
      "loss: 0.03778424486517906, accuracy: 0.9959183673469387\n",
      "loss: 0.035960644483566284, accuracy: 0.9959183673469387\n",
      "loss: 0.03570633381605148, accuracy: 0.9918367346938776\n",
      "loss: 0.034691713750362396, accuracy: 0.9918367346938776\n",
      "loss: 0.036341384053230286, accuracy: 1.0\n",
      "loss: 0.03550925850868225, accuracy: 1.0\n",
      "loss: 0.036998674273490906, accuracy: 0.9918367346938776\n",
      "loss: 0.03706510365009308, accuracy: 0.9959183673469387\n",
      "loss: 0.03449902683496475, accuracy: 1.0\n",
      "loss: 0.036800019443035126, accuracy: 0.9959183673469387\n",
      "loss: 0.038424693048000336, accuracy: 0.9836734693877551\n",
      "loss: 0.03798650950193405, accuracy: 0.9918367346938776\n",
      "loss: 0.03753796964883804, accuracy: 0.9918367346938776\n",
      "loss: 0.0335695743560791, accuracy: 0.9959183673469387\n",
      "loss: 0.03795928508043289, accuracy: 0.9959183673469387\n",
      "loss: 0.03526262938976288, accuracy: 0.9918367346938776\n",
      "loss: 0.038515765219926834, accuracy: 0.9918367346938776\n",
      "loss: 0.039865653961896896, accuracy: 0.9877551020408163\n",
      "loss: 0.03567654639482498, accuracy: 0.9959183673469387\n",
      "loss: 0.03456805646419525, accuracy: 0.9959183673469387\n",
      "loss: 0.03831452876329422, accuracy: 1.0\n",
      "loss: 0.0374692901968956, accuracy: 0.9959183673469387\n",
      "loss: 0.03654691204428673, accuracy: 0.9918367346938776\n",
      "loss: 0.03802572190761566, accuracy: 0.9918367346938776\n",
      "loss: 0.03279387205839157, accuracy: 1.0\n",
      "loss: 0.032568447291851044, accuracy: 1.0\n",
      "loss: 0.03505715727806091, accuracy: 0.9918367346938776\n",
      "loss: 0.03510362282395363, accuracy: 1.0\n",
      "loss: 0.034491971135139465, accuracy: 0.9959183673469387\n",
      "loss: 0.03843512758612633, accuracy: 0.9918367346938776\n",
      "loss: 0.03301054984331131, accuracy: 1.0\n",
      "loss: 0.03552401438355446, accuracy: 1.0\n",
      "loss: 0.03980671241879463, accuracy: 0.9877551020408163\n",
      "loss: 0.03509698063135147, accuracy: 0.9918367346938776\n",
      "loss: 0.034326307475566864, accuracy: 1.0\n",
      "loss: 0.034013066440820694, accuracy: 1.0\n",
      "loss: 0.033479828387498856, accuracy: 0.9959183673469387\n",
      "loss: 0.03396904468536377, accuracy: 0.9959183673469387\n",
      "loss: 0.032549090683460236, accuracy: 0.9959183673469387\n",
      "loss: 0.032221224159002304, accuracy: 0.9959183673469387\n",
      "loss: 0.032728005200624466, accuracy: 1.0\n",
      "loss: 0.03600998595356941, accuracy: 0.9959183673469387\n",
      "loss: 0.03429426625370979, accuracy: 0.9959183673469387\n",
      "loss: 0.03227370232343674, accuracy: 1.0\n",
      "loss: 0.03902623802423477, accuracy: 0.9918367346938776\n",
      "loss: 0.033151187002658844, accuracy: 1.0\n",
      "loss: 0.033831510692834854, accuracy: 1.0\n",
      "loss: 0.033743083477020264, accuracy: 1.0\n",
      "loss: 0.03466057777404785, accuracy: 0.9918367346938776\n",
      "loss: 0.033652566373348236, accuracy: 1.0\n",
      "loss: 0.03338266909122467, accuracy: 1.0\n",
      "loss: 0.035229142755270004, accuracy: 0.9918367346938776\n",
      "loss: 0.034814316779375076, accuracy: 0.9959183673469387\n",
      "loss: 0.03381149470806122, accuracy: 0.9959183673469387\n",
      "loss: 0.03372960165143013, accuracy: 0.9959183673469387\n",
      "loss: 0.03140106052160263, accuracy: 0.9959183673469387\n",
      "loss: 0.03477306663990021, accuracy: 1.0\n",
      "loss: 0.03169398382306099, accuracy: 0.9959183673469387\n",
      "loss: 0.03414137661457062, accuracy: 0.9959183673469387\n",
      "loss: 0.031968772411346436, accuracy: 1.0\n",
      "loss: 0.03424924239516258, accuracy: 0.9959183673469387\n",
      "loss: 0.03424140438437462, accuracy: 0.9918367346938776\n",
      "loss: 0.030979635193943977, accuracy: 0.9959183673469387\n",
      "loss: 0.035290420055389404, accuracy: 0.9918367346938776\n",
      "loss: 0.035480208694934845, accuracy: 1.0\n",
      "loss: 0.03166564553976059, accuracy: 0.9959183673469387\n",
      "loss: 0.03522169589996338, accuracy: 0.9959183673469387\n",
      "loss: 0.03448198363184929, accuracy: 0.9959183673469387\n",
      "loss: 0.03178063780069351, accuracy: 1.0\n",
      "loss: 0.03196060657501221, accuracy: 1.0\n",
      "loss: 0.03227967396378517, accuracy: 0.9918367346938776\n",
      "loss: 0.032984327524900436, accuracy: 0.9959183673469387\n",
      "loss: 0.03279634937644005, accuracy: 1.0\n",
      "loss: 0.03327871114015579, accuracy: 1.0\n",
      "loss: 0.03167416527867317, accuracy: 1.0\n",
      "loss: 0.03204548731446266, accuracy: 0.9959183673469387\n",
      "loss: 0.03132982552051544, accuracy: 0.9959183673469387\n",
      "loss: 0.03134855255484581, accuracy: 1.0\n",
      "loss: 0.031368959695100784, accuracy: 0.9959183673469387\n",
      "loss: 0.03248004987835884, accuracy: 1.0\n",
      "loss: 0.033109478652477264, accuracy: 0.9959183673469387\n",
      "loss: 0.03364311903715134, accuracy: 1.0\n",
      "loss: 0.03174111619591713, accuracy: 1.0\n",
      "loss: 0.03550560399889946, accuracy: 0.9918367346938776\n",
      "loss: 0.030876023694872856, accuracy: 1.0\n",
      "loss: 0.03434561938047409, accuracy: 0.9918367346938776\n",
      "loss: 0.03174300864338875, accuracy: 1.0\n",
      "loss: 0.03132311999797821, accuracy: 0.9959183673469387\n",
      "loss: 0.032526031136512756, accuracy: 1.0\n",
      "loss: 0.03358818590641022, accuracy: 0.9918367346938776\n",
      "loss: 0.0325804278254509, accuracy: 0.9959183673469387\n",
      "loss: 0.033713482320308685, accuracy: 0.9959183673469387\n",
      "loss: 0.0316641703248024, accuracy: 1.0\n",
      "loss: 0.03193461522459984, accuracy: 1.0\n",
      "loss: 0.03633708134293556, accuracy: 0.9918367346938776\n",
      "loss: 0.032625604420900345, accuracy: 1.0\n",
      "loss: 0.03303201496601105, accuracy: 0.9959183673469387\n",
      "loss: 0.03588186204433441, accuracy: 1.0\n",
      "loss: 0.031980086117982864, accuracy: 0.9959183673469387\n",
      "loss: 0.031599875539541245, accuracy: 1.0\n",
      "loss: 0.03035333938896656, accuracy: 1.0\n",
      "loss: 0.032631512731313705, accuracy: 1.0\n",
      "loss: 0.03195888176560402, accuracy: 1.0\n",
      "loss: 0.03196733444929123, accuracy: 1.0\n",
      "loss: 0.03147698566317558, accuracy: 0.9959183673469387\n",
      "loss: 0.03943604230880737, accuracy: 0.9877551020408163\n",
      "loss: 0.03159959241747856, accuracy: 1.0\n",
      "loss: 0.030057769268751144, accuracy: 1.0\n",
      "loss: 0.02964094467461109, accuracy: 0.9959183673469387\n",
      "loss: 0.031005222350358963, accuracy: 0.9959183673469387\n",
      "loss: 0.0316094309091568, accuracy: 0.9959183673469387\n",
      "loss: 0.0343078076839447, accuracy: 0.9959183673469387\n",
      "loss: 0.03085251711308956, accuracy: 0.9959183673469387\n",
      "loss: 0.03101383149623871, accuracy: 1.0\n",
      "loss: 0.031397707760334015, accuracy: 1.0\n",
      "loss: 0.031739894300699234, accuracy: 0.9959183673469387\n",
      "loss: 0.03037724830210209, accuracy: 0.9959183673469387\n",
      "loss: 0.03196185454726219, accuracy: 1.0\n",
      "loss: 0.02894693799316883, accuracy: 1.0\n",
      "loss: 0.030095601454377174, accuracy: 0.9959183673469387\n",
      "loss: 0.030571522191166878, accuracy: 0.9959183673469387\n",
      "loss: 0.03174076974391937, accuracy: 0.9959183673469387\n",
      "loss: 0.029905622825026512, accuracy: 0.9959183673469387\n",
      "loss: 0.03314979374408722, accuracy: 0.9959183673469387\n",
      "loss: 0.030940819531679153, accuracy: 0.9959183673469387\n",
      "loss: 0.031690631061792374, accuracy: 0.9959183673469387\n",
      "loss: 0.05659680813550949, accuracy: 0.9836734693877551\n",
      "loss: 0.19000601768493652, accuracy: 0.9306122448979591\n",
      "loss: 0.07221286743879318, accuracy: 0.9591836734693877\n",
      "loss: 0.03604481741786003, accuracy: 0.9959183673469387\n",
      "loss: 0.03504890948534012, accuracy: 1.0\n",
      "loss: 0.03341873735189438, accuracy: 0.9959183673469387\n",
      "loss: 0.03267015516757965, accuracy: 0.9959183673469387\n",
      "loss: 0.0323459766805172, accuracy: 0.9959183673469387\n",
      "loss: 0.03470522165298462, accuracy: 0.9877551020408163\n",
      "loss: 0.03094806708395481, accuracy: 1.0\n",
      "loss: 0.03145090490579605, accuracy: 1.0\n",
      "loss: 0.030864974483847618, accuracy: 0.9959183673469387\n",
      "loss: 0.031041059643030167, accuracy: 1.0\n",
      "loss: 0.029828868806362152, accuracy: 1.0\n",
      "loss: 0.03254619240760803, accuracy: 0.9959183673469387\n",
      "loss: 0.030869007110595703, accuracy: 0.9959183673469387\n",
      "loss: 0.03148027881979942, accuracy: 1.0\n",
      "loss: 0.02926570177078247, accuracy: 1.0\n",
      "loss: 0.030107814818620682, accuracy: 1.0\n",
      "loss: 0.029310010373592377, accuracy: 1.0\n",
      "loss: 0.03328701853752136, accuracy: 0.9959183673469387\n",
      "loss: 0.029788702726364136, accuracy: 0.9959183673469387\n",
      "loss: 0.029168948531150818, accuracy: 1.0\n",
      "loss: 0.030788462609052658, accuracy: 0.9959183673469387\n",
      "loss: 0.030674397945404053, accuracy: 0.9959183673469387\n",
      "loss: 0.02863689325749874, accuracy: 1.0\n",
      "loss: 0.030220257118344307, accuracy: 0.9959183673469387\n",
      "loss: 0.029346777126193047, accuracy: 0.9918367346938776\n",
      "loss: 0.031595610082149506, accuracy: 1.0\n",
      "loss: 0.030351489782333374, accuracy: 1.0\n",
      "loss: 0.02910800464451313, accuracy: 1.0\n",
      "loss: 0.031096201390028, accuracy: 0.9959183673469387\n",
      "loss: 0.029077569022774696, accuracy: 1.0\n",
      "loss: 0.02803206630051136, accuracy: 1.0\n",
      "loss: 0.028671128675341606, accuracy: 1.0\n",
      "loss: 0.031042616814374924, accuracy: 1.0\n",
      "loss: 0.028048712760210037, accuracy: 1.0\n",
      "loss: 0.029259173199534416, accuracy: 1.0\n",
      "loss: 0.028219427913427353, accuracy: 1.0\n",
      "loss: 0.031070714816451073, accuracy: 0.9959183673469387\n",
      "loss: 0.030773751437664032, accuracy: 1.0\n",
      "loss: 0.029659148305654526, accuracy: 1.0\n",
      "loss: 0.029945509508252144, accuracy: 0.9959183673469387\n",
      "loss: 0.029540210962295532, accuracy: 1.0\n",
      "loss: 0.02690420299768448, accuracy: 1.0\n",
      "loss: 0.028575150296092033, accuracy: 1.0\n",
      "loss: 0.030626565217971802, accuracy: 1.0\n",
      "loss: 0.029796447604894638, accuracy: 1.0\n",
      "loss: 0.029065841808915138, accuracy: 1.0\n",
      "loss: 0.028705209493637085, accuracy: 1.0\n",
      "loss: 0.028386471793055534, accuracy: 1.0\n",
      "loss: 0.02908417023718357, accuracy: 1.0\n",
      "loss: 0.029248623177409172, accuracy: 0.9959183673469387\n",
      "loss: 0.03054814040660858, accuracy: 1.0\n",
      "loss: 0.028958968818187714, accuracy: 1.0\n",
      "loss: 0.029622666537761688, accuracy: 1.0\n",
      "loss: 0.02773893065750599, accuracy: 1.0\n",
      "loss: 0.029009388759732246, accuracy: 1.0\n",
      "loss: 0.028087086975574493, accuracy: 1.0\n",
      "loss: 0.028238173574209213, accuracy: 1.0\n",
      "loss: 0.029045123606920242, accuracy: 1.0\n",
      "loss: 0.02741783671081066, accuracy: 1.0\n",
      "loss: 0.02525792084634304, accuracy: 1.0\n",
      "loss: 0.027515597641468048, accuracy: 1.0\n",
      "loss: 0.029569562524557114, accuracy: 1.0\n",
      "loss: 0.029518362134695053, accuracy: 1.0\n",
      "loss: 0.02727968618273735, accuracy: 1.0\n",
      "loss: 0.0284856129437685, accuracy: 1.0\n",
      "loss: 0.026932476088404655, accuracy: 1.0\n",
      "loss: 0.02783832512795925, accuracy: 1.0\n",
      "loss: 0.02938139997422695, accuracy: 1.0\n",
      "loss: 0.027780907228589058, accuracy: 0.9959183673469387\n",
      "loss: 0.032368309795856476, accuracy: 1.0\n",
      "loss: 0.027664460241794586, accuracy: 1.0\n",
      "loss: 0.02706836722791195, accuracy: 1.0\n",
      "loss: 0.032940663397312164, accuracy: 1.0\n",
      "loss: 0.02849067747592926, accuracy: 1.0\n",
      "loss: 0.029067734256386757, accuracy: 1.0\n",
      "loss: 0.029858309775590897, accuracy: 1.0\n",
      "loss: 0.026189450174570084, accuracy: 1.0\n",
      "loss: 0.030368700623512268, accuracy: 1.0\n",
      "loss: 0.02721245400607586, accuracy: 1.0\n",
      "loss: 0.028567342087626457, accuracy: 1.0\n",
      "loss: 0.02845718339085579, accuracy: 1.0\n",
      "loss: 0.02734328620135784, accuracy: 1.0\n",
      "loss: 0.026860715821385384, accuracy: 1.0\n",
      "loss: 0.027447275817394257, accuracy: 1.0\n",
      "loss: 0.028889579698443413, accuracy: 1.0\n",
      "loss: 0.027623724192380905, accuracy: 0.9959183673469387\n",
      "loss: 0.02837388403713703, accuracy: 1.0\n",
      "loss: 0.028081906959414482, accuracy: 1.0\n",
      "loss: 0.02797720953822136, accuracy: 1.0\n",
      "loss: 0.028394725173711777, accuracy: 0.9959183673469387\n",
      "loss: 0.02892257459461689, accuracy: 1.0\n",
      "loss: 0.026732927188277245, accuracy: 1.0\n",
      "loss: 0.030588587746024132, accuracy: 0.9918367346938776\n",
      "loss: 0.031552232801914215, accuracy: 1.0\n",
      "loss: 0.02794218249619007, accuracy: 1.0\n",
      "loss: 0.027847351506352425, accuracy: 1.0\n",
      "loss: 0.026046112179756165, accuracy: 1.0\n",
      "loss: 0.02651260606944561, accuracy: 0.9959183673469387\n",
      "loss: 0.02594275027513504, accuracy: 1.0\n",
      "loss: 0.0292875524610281, accuracy: 1.0\n",
      "loss: 0.02644163742661476, accuracy: 1.0\n",
      "loss: 0.027556119486689568, accuracy: 1.0\n",
      "loss: 0.027306051924824715, accuracy: 1.0\n",
      "loss: 0.02587990276515484, accuracy: 1.0\n",
      "loss: 0.027230121195316315, accuracy: 1.0\n",
      "loss: 0.02698500081896782, accuracy: 1.0\n",
      "loss: 0.026150329038500786, accuracy: 1.0\n",
      "loss: 0.0276317298412323, accuracy: 1.0\n",
      "loss: 0.026655402034521103, accuracy: 1.0\n",
      "loss: 0.027214929461479187, accuracy: 1.0\n",
      "loss: 0.026733268052339554, accuracy: 1.0\n",
      "loss: 0.026501502841711044, accuracy: 1.0\n",
      "loss: 0.026502083986997604, accuracy: 1.0\n",
      "loss: 0.027350742369890213, accuracy: 1.0\n",
      "loss: 0.02596973069012165, accuracy: 1.0\n",
      "loss: 0.029227416962385178, accuracy: 0.9959183673469387\n",
      "loss: 0.02709187939763069, accuracy: 1.0\n",
      "loss: 0.0267154723405838, accuracy: 1.0\n",
      "loss: 0.027465054765343666, accuracy: 1.0\n",
      "loss: 0.02874162420630455, accuracy: 0.9959183673469387\n",
      "loss: 0.02538207359611988, accuracy: 1.0\n",
      "loss: 0.02572251856327057, accuracy: 1.0\n",
      "loss: 0.025541802868247032, accuracy: 0.9959183673469387\n",
      "loss: 0.028596708551049232, accuracy: 1.0\n",
      "loss: 0.030320772901177406, accuracy: 1.0\n",
      "loss: 0.025953302159905434, accuracy: 1.0\n",
      "loss: 0.026582060381770134, accuracy: 1.0\n",
      "loss: 0.025966482236981392, accuracy: 1.0\n",
      "loss: 0.025014793500304222, accuracy: 1.0\n",
      "loss: 0.027370188385248184, accuracy: 1.0\n",
      "loss: 0.02551707811653614, accuracy: 1.0\n",
      "loss: 0.02640732377767563, accuracy: 1.0\n",
      "loss: 0.026254555210471153, accuracy: 1.0\n",
      "loss: 0.026030385866761208, accuracy: 1.0\n",
      "loss: 0.025045964866876602, accuracy: 1.0\n",
      "loss: 0.02493898756802082, accuracy: 1.0\n",
      "loss: 0.02445962466299534, accuracy: 1.0\n",
      "loss: 0.02555808052420616, accuracy: 1.0\n",
      "loss: 0.025371836498379707, accuracy: 1.0\n",
      "loss: 0.024975938722491264, accuracy: 1.0\n",
      "loss: 0.029398856684565544, accuracy: 0.9918367346938776\n",
      "loss: 0.025847172364592552, accuracy: 1.0\n",
      "loss: 0.028502071276307106, accuracy: 1.0\n",
      "loss: 0.02656201273202896, accuracy: 1.0\n",
      "loss: 0.025917058810591698, accuracy: 1.0\n",
      "loss: 0.02982894703745842, accuracy: 1.0\n",
      "loss: 0.029361005872488022, accuracy: 1.0\n",
      "loss: 0.025759372860193253, accuracy: 1.0\n",
      "loss: 0.02560354582965374, accuracy: 1.0\n",
      "loss: 0.02648158185184002, accuracy: 0.9959183673469387\n",
      "loss: 0.025145046412944794, accuracy: 1.0\n",
      "loss: 0.025254033505916595, accuracy: 1.0\n",
      "loss: 0.025573061779141426, accuracy: 1.0\n",
      "loss: 0.024809688329696655, accuracy: 1.0\n",
      "loss: 0.024197831749916077, accuracy: 1.0\n",
      "loss: 0.02688860520720482, accuracy: 1.0\n",
      "loss: 0.02664162963628769, accuracy: 1.0\n",
      "loss: 0.025269318372011185, accuracy: 1.0\n",
      "loss: 0.02578340284526348, accuracy: 1.0\n",
      "loss: 0.024570729583501816, accuracy: 1.0\n",
      "loss: 0.02546730637550354, accuracy: 1.0\n",
      "loss: 0.024401677772402763, accuracy: 1.0\n",
      "loss: 0.024335680529475212, accuracy: 1.0\n",
      "loss: 0.025370119139552116, accuracy: 1.0\n",
      "loss: 0.02402552030980587, accuracy: 1.0\n",
      "loss: 0.02639847621321678, accuracy: 0.9918367346938776\n",
      "loss: 0.023235397413372993, accuracy: 1.0\n",
      "loss: 0.02540566958487034, accuracy: 1.0\n",
      "loss: 0.024833189323544502, accuracy: 1.0\n",
      "loss: 0.02474087104201317, accuracy: 1.0\n",
      "loss: 0.024515215307474136, accuracy: 1.0\n",
      "loss: 0.025172386318445206, accuracy: 1.0\n",
      "loss: 0.02462136186659336, accuracy: 1.0\n",
      "loss: 0.02425016090273857, accuracy: 1.0\n",
      "loss: 0.026928957551717758, accuracy: 1.0\n",
      "loss: 0.02801310271024704, accuracy: 1.0\n",
      "loss: 0.024653322994709015, accuracy: 1.0\n",
      "loss: 0.023845046758651733, accuracy: 1.0\n",
      "loss: 0.025747405365109444, accuracy: 1.0\n",
      "loss: 0.025050126016139984, accuracy: 1.0\n",
      "loss: 0.024972645565867424, accuracy: 1.0\n",
      "loss: 0.028033684939146042, accuracy: 1.0\n",
      "loss: 0.024054408073425293, accuracy: 1.0\n",
      "loss: 0.02473367191851139, accuracy: 1.0\n",
      "loss: 0.025455957278609276, accuracy: 1.0\n",
      "loss: 0.025613093748688698, accuracy: 1.0\n",
      "loss: 0.024874601513147354, accuracy: 1.0\n",
      "loss: 0.027340471744537354, accuracy: 1.0\n",
      "loss: 0.02696196362376213, accuracy: 1.0\n",
      "loss: 0.024187862873077393, accuracy: 1.0\n",
      "loss: 0.024547245353460312, accuracy: 1.0\n",
      "loss: 0.023470044136047363, accuracy: 1.0\n",
      "loss: 0.02358955517411232, accuracy: 1.0\n",
      "loss: 0.02501576952636242, accuracy: 1.0\n",
      "loss: 0.02464713342487812, accuracy: 1.0\n",
      "loss: 0.024941187351942062, accuracy: 1.0\n",
      "loss: 0.0234843697398901, accuracy: 1.0\n",
      "loss: 0.023679491132497787, accuracy: 1.0\n",
      "loss: 0.02488074079155922, accuracy: 1.0\n",
      "loss: 0.024742530658841133, accuracy: 1.0\n",
      "loss: 0.02686925418674946, accuracy: 1.0\n",
      "loss: 0.02461310848593712, accuracy: 1.0\n",
      "loss: 0.025028906762599945, accuracy: 1.0\n",
      "loss: 0.024981558322906494, accuracy: 1.0\n",
      "loss: 0.0244558397680521, accuracy: 1.0\n",
      "loss: 0.02573058381676674, accuracy: 1.0\n",
      "loss: 0.027424504980444908, accuracy: 1.0\n",
      "loss: 0.024869205430150032, accuracy: 1.0\n",
      "loss: 0.02314305491745472, accuracy: 1.0\n",
      "loss: 0.02379583939909935, accuracy: 1.0\n",
      "loss: 0.02512103132903576, accuracy: 0.9959183673469387\n",
      "loss: 0.024797221645712852, accuracy: 1.0\n",
      "loss: 0.024791192263364792, accuracy: 1.0\n",
      "loss: 0.024423666298389435, accuracy: 1.0\n",
      "loss: 0.02738436684012413, accuracy: 1.0\n",
      "loss: 0.023512423038482666, accuracy: 1.0\n",
      "loss: 0.02356940694153309, accuracy: 1.0\n",
      "loss: 0.025106051936745644, accuracy: 1.0\n",
      "loss: 0.023405807092785835, accuracy: 1.0\n",
      "loss: 0.026124102994799614, accuracy: 1.0\n",
      "loss: 0.023323962464928627, accuracy: 1.0\n",
      "loss: 0.024608949199318886, accuracy: 1.0\n",
      "loss: 0.022989612072706223, accuracy: 1.0\n",
      "loss: 0.02544984593987465, accuracy: 1.0\n",
      "loss: 0.02305544726550579, accuracy: 1.0\n",
      "loss: 0.024546930566430092, accuracy: 1.0\n",
      "loss: 0.023618096485733986, accuracy: 0.9959183673469387\n",
      "loss: 0.02680251933634281, accuracy: 1.0\n",
      "loss: 0.022801747545599937, accuracy: 1.0\n",
      "loss: 0.024437885731458664, accuracy: 1.0\n",
      "loss: 0.022963901981711388, accuracy: 1.0\n",
      "loss: 0.022708384320139885, accuracy: 1.0\n",
      "loss: 0.023068740963935852, accuracy: 1.0\n",
      "loss: 0.023613635450601578, accuracy: 1.0\n",
      "loss: 0.02498757652938366, accuracy: 1.0\n",
      "loss: 0.02299470640718937, accuracy: 1.0\n",
      "loss: 0.024647090584039688, accuracy: 1.0\n",
      "loss: 0.024339502677321434, accuracy: 1.0\n",
      "loss: 0.02320762164890766, accuracy: 1.0\n",
      "loss: 0.022651225328445435, accuracy: 1.0\n",
      "loss: 0.02462773583829403, accuracy: 1.0\n",
      "loss: 0.022269511595368385, accuracy: 1.0\n",
      "loss: 0.022864168509840965, accuracy: 1.0\n",
      "loss: 0.023810580372810364, accuracy: 1.0\n",
      "loss: 0.023437319323420525, accuracy: 1.0\n",
      "loss: 0.022304648533463478, accuracy: 1.0\n",
      "loss: 0.02270112745463848, accuracy: 1.0\n",
      "loss: 0.023541158065199852, accuracy: 1.0\n",
      "loss: 0.023107105866074562, accuracy: 1.0\n",
      "loss: 0.02250867523252964, accuracy: 1.0\n",
      "loss: 0.02277863398194313, accuracy: 1.0\n",
      "loss: 0.02223150059580803, accuracy: 1.0\n",
      "loss: 0.022156836465001106, accuracy: 1.0\n",
      "loss: 0.021845374256372452, accuracy: 1.0\n",
      "loss: 0.025807546451687813, accuracy: 0.9959183673469387\n",
      "loss: 0.023283785209059715, accuracy: 1.0\n",
      "loss: 0.022225074470043182, accuracy: 1.0\n",
      "loss: 0.023048395290970802, accuracy: 1.0\n",
      "loss: 0.023353559896349907, accuracy: 1.0\n",
      "loss: 0.02204989828169346, accuracy: 1.0\n",
      "loss: 0.02192983403801918, accuracy: 1.0\n",
      "loss: 0.021534152328968048, accuracy: 1.0\n",
      "loss: 0.02405627816915512, accuracy: 1.0\n",
      "loss: 0.023041918873786926, accuracy: 1.0\n",
      "loss: 0.023483458906412125, accuracy: 1.0\n",
      "loss: 0.024015652015805244, accuracy: 1.0\n",
      "loss: 0.023586001247167587, accuracy: 1.0\n",
      "loss: 0.02286621369421482, accuracy: 1.0\n",
      "loss: 0.02187916822731495, accuracy: 1.0\n",
      "loss: 0.02240167185664177, accuracy: 1.0\n",
      "loss: 0.023878013715147972, accuracy: 0.9959183673469387\n",
      "loss: 0.023497026413679123, accuracy: 1.0\n",
      "loss: 0.02298213541507721, accuracy: 1.0\n",
      "loss: 0.022392626851797104, accuracy: 1.0\n",
      "loss: 0.021618803963065147, accuracy: 1.0\n",
      "loss: 0.022956756874918938, accuracy: 1.0\n",
      "loss: 0.02384515479207039, accuracy: 1.0\n",
      "loss: 0.022177519276738167, accuracy: 1.0\n",
      "loss: 0.022328365594148636, accuracy: 1.0\n",
      "loss: 0.021955981850624084, accuracy: 1.0\n",
      "loss: 0.021645452827215195, accuracy: 1.0\n",
      "loss: 0.023114729672670364, accuracy: 1.0\n",
      "loss: 0.02146376483142376, accuracy: 1.0\n",
      "loss: 0.023588281124830246, accuracy: 1.0\n",
      "loss: 0.021290259435772896, accuracy: 1.0\n",
      "loss: 0.023302549496293068, accuracy: 1.0\n",
      "loss: 0.024207934737205505, accuracy: 1.0\n",
      "loss: 0.02187603898346424, accuracy: 1.0\n",
      "loss: 0.021890204399824142, accuracy: 1.0\n",
      "loss: 0.02239301986992359, accuracy: 1.0\n",
      "loss: 0.0212561022490263, accuracy: 1.0\n",
      "loss: 0.02280815690755844, accuracy: 1.0\n",
      "loss: 0.022036386653780937, accuracy: 1.0\n",
      "loss: 0.0220321174710989, accuracy: 1.0\n",
      "loss: 0.02364342473447323, accuracy: 1.0\n",
      "loss: 0.02128664404153824, accuracy: 1.0\n",
      "loss: 0.02255840040743351, accuracy: 1.0\n",
      "loss: 0.022157972678542137, accuracy: 1.0\n",
      "loss: 0.021845292299985886, accuracy: 1.0\n",
      "loss: 0.02284301444888115, accuracy: 1.0\n",
      "loss: 0.022471869364380836, accuracy: 1.0\n",
      "loss: 0.021962115541100502, accuracy: 1.0\n",
      "loss: 0.023688899353146553, accuracy: 1.0\n",
      "loss: 0.022135987877845764, accuracy: 1.0\n",
      "loss: 0.021226711571216583, accuracy: 1.0\n",
      "loss: 0.022729365155100822, accuracy: 1.0\n",
      "loss: 0.0211009718477726, accuracy: 1.0\n",
      "loss: 0.021016621962189674, accuracy: 1.0\n",
      "loss: 0.021342942491173744, accuracy: 1.0\n",
      "loss: 0.021509038284420967, accuracy: 1.0\n",
      "loss: 0.022754531353712082, accuracy: 1.0\n",
      "loss: 0.022714732214808464, accuracy: 1.0\n",
      "loss: 0.02185370959341526, accuracy: 1.0\n",
      "loss: 0.021040594205260277, accuracy: 1.0\n",
      "loss: 0.02128422260284424, accuracy: 1.0\n",
      "loss: 0.023149307817220688, accuracy: 1.0\n",
      "loss: 0.02320670895278454, accuracy: 1.0\n",
      "loss: 0.02081766538321972, accuracy: 1.0\n",
      "loss: 0.022737396880984306, accuracy: 1.0\n",
      "loss: 0.021355358883738518, accuracy: 1.0\n",
      "loss: 0.020496338605880737, accuracy: 1.0\n",
      "loss: 0.021472830325365067, accuracy: 1.0\n",
      "loss: 0.020466875284910202, accuracy: 1.0\n",
      "loss: 0.02285003662109375, accuracy: 1.0\n",
      "loss: 0.023496219888329506, accuracy: 1.0\n",
      "loss: 0.022252162918448448, accuracy: 1.0\n",
      "loss: 0.021153265610337257, accuracy: 1.0\n",
      "loss: 0.022100431844592094, accuracy: 1.0\n",
      "loss: 0.020498380064964294, accuracy: 1.0\n",
      "loss: 0.02105647884309292, accuracy: 1.0\n",
      "loss: 0.022386277094483376, accuracy: 1.0\n",
      "loss: 0.020492492243647575, accuracy: 1.0\n",
      "loss: 0.02173808217048645, accuracy: 1.0\n",
      "loss: 0.020867155864834785, accuracy: 1.0\n",
      "loss: 0.020325006917119026, accuracy: 1.0\n",
      "loss: 0.021129349246621132, accuracy: 1.0\n",
      "loss: 0.02050589583814144, accuracy: 1.0\n",
      "loss: 0.020220808684825897, accuracy: 1.0\n",
      "loss: 0.02172977291047573, accuracy: 1.0\n",
      "loss: 0.020862853154540062, accuracy: 1.0\n",
      "loss: 0.02029496058821678, accuracy: 1.0\n",
      "loss: 0.020959997549653053, accuracy: 1.0\n",
      "loss: 0.021309323608875275, accuracy: 1.0\n",
      "loss: 0.02114037796854973, accuracy: 1.0\n",
      "loss: 0.020099274814128876, accuracy: 1.0\n",
      "loss: 0.021047022193670273, accuracy: 1.0\n",
      "loss: 0.021549386903643608, accuracy: 1.0\n",
      "loss: 0.020016539841890335, accuracy: 1.0\n",
      "loss: 0.021691739559173584, accuracy: 1.0\n",
      "loss: 0.020839504897594452, accuracy: 1.0\n",
      "loss: 0.020507030189037323, accuracy: 1.0\n",
      "loss: 0.019752923399209976, accuracy: 1.0\n",
      "loss: 0.020113442093133926, accuracy: 1.0\n",
      "loss: 0.024278933182358742, accuracy: 1.0\n",
      "loss: 0.021066194400191307, accuracy: 1.0\n",
      "loss: 0.022155970335006714, accuracy: 1.0\n",
      "loss: 0.0205368809401989, accuracy: 1.0\n",
      "loss: 0.020016204565763474, accuracy: 1.0\n",
      "loss: 0.020091617479920387, accuracy: 1.0\n",
      "loss: 0.019976511597633362, accuracy: 1.0\n",
      "loss: 0.020985079929232597, accuracy: 1.0\n",
      "loss: 0.01948421820998192, accuracy: 1.0\n",
      "loss: 0.021894831210374832, accuracy: 1.0\n",
      "loss: 0.019976112991571426, accuracy: 1.0\n",
      "loss: 0.020513219758868217, accuracy: 1.0\n",
      "loss: 0.021132221445441246, accuracy: 1.0\n",
      "loss: 0.021981138736009598, accuracy: 1.0\n",
      "loss: 0.01956591010093689, accuracy: 1.0\n",
      "loss: 0.021031683310866356, accuracy: 1.0\n",
      "loss: 0.01982499286532402, accuracy: 1.0\n",
      "loss: 0.019927551969885826, accuracy: 1.0\n",
      "loss: 0.022419678047299385, accuracy: 1.0\n",
      "loss: 0.02046835608780384, accuracy: 1.0\n",
      "loss: 0.02021925151348114, accuracy: 1.0\n",
      "loss: 0.02079703100025654, accuracy: 1.0\n",
      "loss: 0.01978396624326706, accuracy: 1.0\n",
      "loss: 0.020773358643054962, accuracy: 1.0\n",
      "loss: 0.019660230726003647, accuracy: 1.0\n",
      "loss: 0.02020213007926941, accuracy: 1.0\n",
      "loss: 0.019133619964122772, accuracy: 1.0\n",
      "loss: 0.019654659554362297, accuracy: 1.0\n",
      "loss: 0.020378755405545235, accuracy: 1.0\n",
      "loss: 0.02031972073018551, accuracy: 1.0\n",
      "loss: 0.020195597782731056, accuracy: 1.0\n",
      "loss: 0.020125960931181908, accuracy: 1.0\n",
      "loss: 0.019509771838784218, accuracy: 1.0\n",
      "loss: 0.020224006846547127, accuracy: 1.0\n",
      "loss: 0.019550902768969536, accuracy: 1.0\n",
      "loss: 0.01950160041451454, accuracy: 1.0\n",
      "loss: 0.0212335716933012, accuracy: 1.0\n",
      "loss: 0.01993381977081299, accuracy: 1.0\n",
      "loss: 0.018981250002980232, accuracy: 1.0\n",
      "loss: 0.020039210096001625, accuracy: 1.0\n",
      "loss: 0.019489070400595665, accuracy: 1.0\n",
      "loss: 0.020258750766515732, accuracy: 1.0\n",
      "loss: 0.02004992961883545, accuracy: 1.0\n",
      "loss: 0.021171685308218002, accuracy: 1.0\n",
      "loss: 0.019482651725411415, accuracy: 1.0\n",
      "loss: 0.020046338438987732, accuracy: 1.0\n",
      "loss: 0.019301757216453552, accuracy: 1.0\n",
      "loss: 0.01945412904024124, accuracy: 1.0\n",
      "loss: 0.01918967440724373, accuracy: 1.0\n",
      "loss: 0.018798118457198143, accuracy: 1.0\n",
      "loss: 0.020962942391633987, accuracy: 1.0\n",
      "loss: 0.019410304725170135, accuracy: 1.0\n",
      "loss: 0.019480107352137566, accuracy: 1.0\n",
      "loss: 0.019323131069540977, accuracy: 1.0\n",
      "loss: 0.020077284425497055, accuracy: 1.0\n",
      "loss: 0.019949065521359444, accuracy: 1.0\n",
      "loss: 0.019375288859009743, accuracy: 1.0\n",
      "loss: 0.018582643941044807, accuracy: 1.0\n",
      "loss: 0.02168617583811283, accuracy: 1.0\n",
      "loss: 0.020695002749562263, accuracy: 1.0\n",
      "loss: 0.020510422065854073, accuracy: 1.0\n",
      "loss: 0.020451078191399574, accuracy: 1.0\n",
      "loss: 0.019680960103869438, accuracy: 1.0\n",
      "loss: 0.019057532772421837, accuracy: 1.0\n",
      "loss: 0.02037062682211399, accuracy: 1.0\n",
      "loss: 0.019286099821329117, accuracy: 1.0\n",
      "loss: 0.018331604078412056, accuracy: 1.0\n",
      "loss: 0.018804969266057014, accuracy: 1.0\n",
      "loss: 0.02009928785264492, accuracy: 1.0\n",
      "loss: 0.019328812137246132, accuracy: 1.0\n",
      "loss: 0.01931336149573326, accuracy: 1.0\n",
      "loss: 0.01980755291879177, accuracy: 1.0\n",
      "loss: 0.019145671278238297, accuracy: 1.0\n",
      "loss: 0.01912572793662548, accuracy: 1.0\n",
      "loss: 0.018850885331630707, accuracy: 1.0\n",
      "loss: 0.018655357882380486, accuracy: 1.0\n",
      "loss: 0.019092844799160957, accuracy: 1.0\n",
      "loss: 0.018932832404971123, accuracy: 1.0\n",
      "loss: 0.01906859315931797, accuracy: 1.0\n",
      "loss: 0.01845882460474968, accuracy: 1.0\n",
      "loss: 0.019400183111429214, accuracy: 1.0\n",
      "loss: 0.021447716280817986, accuracy: 1.0\n",
      "loss: 0.018554028123617172, accuracy: 1.0\n",
      "loss: 0.018877767026424408, accuracy: 1.0\n",
      "loss: 0.01847808063030243, accuracy: 1.0\n",
      "loss: 0.01896127499639988, accuracy: 1.0\n",
      "loss: 0.02259637787938118, accuracy: 1.0\n",
      "loss: 0.019298262894153595, accuracy: 1.0\n",
      "loss: 0.01862015761435032, accuracy: 1.0\n",
      "loss: 0.020431652665138245, accuracy: 1.0\n",
      "loss: 0.018885144963860512, accuracy: 1.0\n",
      "loss: 0.018605845049023628, accuracy: 1.0\n",
      "loss: 0.019357604905962944, accuracy: 1.0\n",
      "loss: 0.02013218030333519, accuracy: 1.0\n",
      "loss: 0.01920110359787941, accuracy: 1.0\n",
      "loss: 0.018945569172501564, accuracy: 1.0\n",
      "loss: 0.018346218392252922, accuracy: 1.0\n",
      "loss: 0.019674096256494522, accuracy: 1.0\n",
      "loss: 0.01898527704179287, accuracy: 1.0\n",
      "loss: 0.019571244716644287, accuracy: 1.0\n",
      "loss: 0.019591331481933594, accuracy: 1.0\n",
      "loss: 0.019343193620443344, accuracy: 1.0\n",
      "loss: 0.018345601856708527, accuracy: 1.0\n",
      "loss: 0.018463293090462685, accuracy: 1.0\n",
      "loss: 0.01869865134358406, accuracy: 1.0\n",
      "loss: 0.02134610339999199, accuracy: 1.0\n",
      "loss: 0.019402863457798958, accuracy: 1.0\n",
      "loss: 0.018407203257083893, accuracy: 1.0\n",
      "loss: 0.01848912052810192, accuracy: 1.0\n",
      "loss: 0.01983419619500637, accuracy: 1.0\n",
      "loss: 0.019130023196339607, accuracy: 1.0\n",
      "loss: 0.01874609850347042, accuracy: 1.0\n",
      "loss: 0.018639041110873222, accuracy: 1.0\n",
      "loss: 0.01784253865480423, accuracy: 1.0\n",
      "loss: 0.01857907511293888, accuracy: 1.0\n",
      "loss: 0.01981363631784916, accuracy: 1.0\n",
      "loss: 0.019974306225776672, accuracy: 1.0\n",
      "loss: 0.02431018464267254, accuracy: 1.0\n",
      "loss: 0.01966668851673603, accuracy: 1.0\n",
      "loss: 0.019976628944277763, accuracy: 1.0\n",
      "loss: 0.020051581785082817, accuracy: 1.0\n",
      "loss: 0.018585441634058952, accuracy: 1.0\n",
      "loss: 0.018041102215647697, accuracy: 1.0\n",
      "loss: 0.01891002617776394, accuracy: 1.0\n",
      "loss: 0.018313592299818993, accuracy: 1.0\n",
      "loss: 0.01812661439180374, accuracy: 1.0\n",
      "loss: 0.021581927314400673, accuracy: 1.0\n",
      "loss: 0.01801680028438568, accuracy: 1.0\n",
      "loss: 0.018490776419639587, accuracy: 1.0\n",
      "loss: 0.02043537236750126, accuracy: 1.0\n",
      "loss: 0.01772186905145645, accuracy: 1.0\n",
      "loss: 0.020044924691319466, accuracy: 1.0\n",
      "loss: 0.019496504217386246, accuracy: 1.0\n",
      "loss: 0.017854655161499977, accuracy: 1.0\n",
      "loss: 0.017780648544430733, accuracy: 1.0\n",
      "loss: 0.018213212490081787, accuracy: 1.0\n",
      "loss: 0.01790797710418701, accuracy: 1.0\n",
      "loss: 0.01832382008433342, accuracy: 1.0\n",
      "loss: 0.017895126715302467, accuracy: 1.0\n",
      "loss: 0.01715051755309105, accuracy: 1.0\n",
      "loss: 0.02017877995967865, accuracy: 1.0\n",
      "loss: 0.01824430376291275, accuracy: 1.0\n",
      "loss: 0.021167783066630363, accuracy: 1.0\n",
      "loss: 0.018835095688700676, accuracy: 1.0\n",
      "loss: 0.017849279567599297, accuracy: 1.0\n",
      "loss: 0.01955067366361618, accuracy: 1.0\n",
      "loss: 0.018318237736821175, accuracy: 1.0\n",
      "loss: 0.01770527847111225, accuracy: 1.0\n",
      "loss: 0.018040960654616356, accuracy: 1.0\n",
      "loss: 0.018148111179471016, accuracy: 1.0\n",
      "loss: 0.017648082226514816, accuracy: 1.0\n",
      "loss: 0.01783541589975357, accuracy: 1.0\n",
      "loss: 0.018941882997751236, accuracy: 1.0\n",
      "loss: 0.019231775775551796, accuracy: 1.0\n",
      "loss: 0.0190120842307806, accuracy: 1.0\n",
      "loss: 0.01750859245657921, accuracy: 1.0\n",
      "loss: 0.017916817218065262, accuracy: 1.0\n",
      "loss: 0.019913481548428535, accuracy: 1.0\n",
      "loss: 0.017756890505552292, accuracy: 1.0\n",
      "loss: 0.01761583611369133, accuracy: 1.0\n",
      "loss: 0.017133299261331558, accuracy: 1.0\n",
      "loss: 0.017907852306962013, accuracy: 1.0\n",
      "loss: 0.01753285713493824, accuracy: 1.0\n",
      "loss: 0.018526773899793625, accuracy: 1.0\n",
      "loss: 0.016895541921257973, accuracy: 1.0\n",
      "loss: 0.01770898327231407, accuracy: 1.0\n",
      "loss: 0.01742897555232048, accuracy: 1.0\n",
      "loss: 0.017520621418952942, accuracy: 1.0\n",
      "loss: 0.0181737057864666, accuracy: 1.0\n",
      "loss: 0.016683589667081833, accuracy: 1.0\n",
      "loss: 0.016945088282227516, accuracy: 1.0\n",
      "loss: 0.017514631152153015, accuracy: 1.0\n",
      "loss: 0.017375117167830467, accuracy: 1.0\n",
      "loss: 0.018224451690912247, accuracy: 1.0\n",
      "loss: 0.0177854523062706, accuracy: 1.0\n",
      "loss: 0.01784529536962509, accuracy: 1.0\n",
      "loss: 0.017782414332032204, accuracy: 1.0\n",
      "loss: 0.01681545190513134, accuracy: 1.0\n",
      "loss: 0.017710145562887192, accuracy: 1.0\n",
      "loss: 0.017982445657253265, accuracy: 1.0\n",
      "loss: 0.01696450263261795, accuracy: 1.0\n",
      "loss: 0.01724979281425476, accuracy: 1.0\n",
      "loss: 0.01796845719218254, accuracy: 1.0\n",
      "loss: 0.017695724964141846, accuracy: 1.0\n",
      "loss: 0.017126455903053284, accuracy: 1.0\n",
      "loss: 0.018467240035533905, accuracy: 1.0\n",
      "loss: 0.017762091010808945, accuracy: 1.0\n",
      "loss: 0.017216777428984642, accuracy: 1.0\n",
      "loss: 0.016881734132766724, accuracy: 1.0\n",
      "loss: 0.018546387553215027, accuracy: 1.0\n",
      "loss: 0.01720731519162655, accuracy: 1.0\n",
      "loss: 0.016828466206789017, accuracy: 1.0\n",
      "loss: 0.017039669677615166, accuracy: 1.0\n",
      "loss: 0.017877811565995216, accuracy: 1.0\n",
      "loss: 0.01823442243039608, accuracy: 1.0\n",
      "loss: 0.017320213839411736, accuracy: 1.0\n",
      "loss: 0.016949035227298737, accuracy: 1.0\n",
      "loss: 0.017031028866767883, accuracy: 1.0\n",
      "loss: 0.016912203282117844, accuracy: 1.0\n",
      "loss: 0.017219537869095802, accuracy: 1.0\n",
      "loss: 0.017880702391266823, accuracy: 1.0\n",
      "loss: 0.01765710487961769, accuracy: 1.0\n",
      "loss: 0.0168036799877882, accuracy: 1.0\n",
      "loss: 0.016529696062207222, accuracy: 1.0\n",
      "loss: 0.017655115574598312, accuracy: 1.0\n",
      "loss: 0.01691725291311741, accuracy: 1.0\n",
      "loss: 0.01698710024356842, accuracy: 1.0\n",
      "loss: 0.017108049243688583, accuracy: 1.0\n",
      "loss: 0.01768576167523861, accuracy: 1.0\n",
      "loss: 0.01664579287171364, accuracy: 1.0\n",
      "loss: 0.017366118729114532, accuracy: 1.0\n",
      "loss: 0.016776448115706444, accuracy: 1.0\n",
      "loss: 0.018585551530122757, accuracy: 1.0\n",
      "loss: 0.016634661704301834, accuracy: 1.0\n",
      "loss: 0.01838565431535244, accuracy: 1.0\n",
      "loss: 0.016653409227728844, accuracy: 1.0\n",
      "loss: 0.016727203503251076, accuracy: 1.0\n",
      "loss: 0.017787449061870575, accuracy: 1.0\n",
      "loss: 0.017048370093107224, accuracy: 1.0\n",
      "loss: 0.01792926900088787, accuracy: 1.0\n",
      "loss: 0.016385182738304138, accuracy: 1.0\n",
      "loss: 0.017077859491109848, accuracy: 1.0\n",
      "loss: 0.016829747706651688, accuracy: 1.0\n",
      "loss: 0.016111766919493675, accuracy: 1.0\n",
      "loss: 0.01742575876414776, accuracy: 1.0\n",
      "loss: 0.017181579023599625, accuracy: 1.0\n",
      "loss: 0.01695665717124939, accuracy: 1.0\n",
      "loss: 0.016782691702246666, accuracy: 1.0\n",
      "loss: 0.01720328815281391, accuracy: 1.0\n",
      "loss: 0.016956601291894913, accuracy: 1.0\n",
      "loss: 0.01655977964401245, accuracy: 1.0\n",
      "loss: 0.017140474170446396, accuracy: 1.0\n",
      "loss: 0.016648754477500916, accuracy: 1.0\n",
      "loss: 0.017631718888878822, accuracy: 1.0\n",
      "loss: 0.01724229007959366, accuracy: 1.0\n",
      "loss: 0.01696905866265297, accuracy: 1.0\n",
      "loss: 0.017238346859812737, accuracy: 1.0\n",
      "loss: 0.018898125737905502, accuracy: 1.0\n",
      "loss: 0.01644277386367321, accuracy: 1.0\n",
      "loss: 0.01828726939857006, accuracy: 1.0\n",
      "loss: 0.01835416816174984, accuracy: 1.0\n",
      "loss: 0.016881415620446205, accuracy: 1.0\n",
      "loss: 0.01647840067744255, accuracy: 1.0\n",
      "loss: 0.016876904293894768, accuracy: 1.0\n",
      "loss: 0.01660490408539772, accuracy: 1.0\n",
      "loss: 0.01588105969130993, accuracy: 1.0\n",
      "loss: 0.01718856208026409, accuracy: 1.0\n",
      "loss: 0.01638003997504711, accuracy: 1.0\n",
      "loss: 0.017757946625351906, accuracy: 1.0\n",
      "loss: 0.016326911747455597, accuracy: 1.0\n",
      "loss: 0.016851088032126427, accuracy: 1.0\n",
      "loss: 0.015820443630218506, accuracy: 1.0\n",
      "loss: 0.016144776716828346, accuracy: 1.0\n",
      "loss: 0.01690300554037094, accuracy: 1.0\n",
      "loss: 0.016911543905735016, accuracy: 1.0\n",
      "loss: 0.016191918402910233, accuracy: 1.0\n",
      "loss: 0.016814256086945534, accuracy: 1.0\n",
      "loss: 0.017367256805300713, accuracy: 1.0\n",
      "loss: 0.01662350445985794, accuracy: 1.0\n",
      "loss: 0.017134476453065872, accuracy: 1.0\n",
      "loss: 0.019380535930395126, accuracy: 1.0\n",
      "loss: 0.016279395669698715, accuracy: 1.0\n",
      "loss: 0.01574810780584812, accuracy: 1.0\n",
      "loss: 0.01618991047143936, accuracy: 1.0\n",
      "loss: 0.016412897035479546, accuracy: 1.0\n",
      "loss: 0.016570160165429115, accuracy: 1.0\n",
      "loss: 0.01713615469634533, accuracy: 1.0\n",
      "loss: 0.016292108222842216, accuracy: 1.0\n",
      "loss: 0.016046682372689247, accuracy: 1.0\n",
      "loss: 0.016090352088212967, accuracy: 1.0\n",
      "loss: 0.015758568421006203, accuracy: 1.0\n",
      "loss: 0.01715223677456379, accuracy: 1.0\n",
      "loss: 0.015987180173397064, accuracy: 1.0\n",
      "loss: 0.016083508729934692, accuracy: 1.0\n",
      "loss: 0.01593155413866043, accuracy: 1.0\n",
      "loss: 0.016224637627601624, accuracy: 1.0\n",
      "loss: 0.017464233562350273, accuracy: 1.0\n",
      "loss: 0.015952961519360542, accuracy: 1.0\n",
      "loss: 0.01597626693546772, accuracy: 1.0\n",
      "loss: 0.016136014834046364, accuracy: 1.0\n",
      "loss: 0.015941187739372253, accuracy: 1.0\n",
      "loss: 0.01563432440161705, accuracy: 1.0\n",
      "loss: 0.016151167452335358, accuracy: 1.0\n",
      "loss: 0.01572265475988388, accuracy: 1.0\n",
      "loss: 0.01578674092888832, accuracy: 1.0\n",
      "loss: 0.015980754047632217, accuracy: 1.0\n",
      "loss: 0.01652791164815426, accuracy: 1.0\n",
      "loss: 0.01681497134268284, accuracy: 1.0\n",
      "loss: 0.015726270154118538, accuracy: 1.0\n",
      "loss: 0.018974844366312027, accuracy: 1.0\n",
      "loss: 0.04149111360311508, accuracy: 0.9877551020408163\n",
      "loss: 0.015955032780766487, accuracy: 1.0\n",
      "loss: 0.017092376947402954, accuracy: 1.0\n",
      "loss: 0.016347864642739296, accuracy: 1.0\n",
      "loss: 0.016429360955953598, accuracy: 1.0\n",
      "loss: 0.01745811104774475, accuracy: 1.0\n",
      "loss: 0.015685781836509705, accuracy: 1.0\n",
      "loss: 0.016297398135066032, accuracy: 1.0\n",
      "loss: 0.01622285135090351, accuracy: 1.0\n",
      "loss: 0.016056109219789505, accuracy: 1.0\n",
      "loss: 0.016300981864333153, accuracy: 1.0\n",
      "loss: 0.015998149290680885, accuracy: 1.0\n",
      "loss: 0.016360893845558167, accuracy: 1.0\n",
      "loss: 0.016182726249098778, accuracy: 1.0\n",
      "loss: 0.015304573811590672, accuracy: 1.0\n",
      "loss: 0.01723334565758705, accuracy: 1.0\n",
      "loss: 0.01548527367413044, accuracy: 1.0\n",
      "loss: 0.01590382494032383, accuracy: 1.0\n",
      "loss: 0.015835773199796677, accuracy: 1.0\n",
      "loss: 0.01578054390847683, accuracy: 1.0\n",
      "loss: 0.015590696595609188, accuracy: 1.0\n",
      "loss: 0.01585468277335167, accuracy: 1.0\n",
      "loss: 0.014991026371717453, accuracy: 1.0\n",
      "loss: 0.015654757618904114, accuracy: 1.0\n",
      "loss: 0.015395929105579853, accuracy: 1.0\n",
      "loss: 0.01572570390999317, accuracy: 1.0\n",
      "loss: 0.01547370944172144, accuracy: 1.0\n",
      "loss: 0.015414265915751457, accuracy: 1.0\n",
      "loss: 0.015283397398889065, accuracy: 1.0\n",
      "loss: 0.015776386484503746, accuracy: 1.0\n",
      "loss: 0.01532338559627533, accuracy: 1.0\n",
      "loss: 0.01584089919924736, accuracy: 1.0\n",
      "loss: 0.015253787860274315, accuracy: 1.0\n",
      "loss: 0.01570463553071022, accuracy: 1.0\n",
      "loss: 0.016304561868309975, accuracy: 1.0\n",
      "loss: 0.01574273779988289, accuracy: 1.0\n",
      "loss: 0.015412874519824982, accuracy: 1.0\n",
      "loss: 0.01511948462575674, accuracy: 1.0\n",
      "loss: 0.015804169699549675, accuracy: 1.0\n",
      "loss: 0.01641569472849369, accuracy: 1.0\n",
      "loss: 0.015029081143438816, accuracy: 1.0\n",
      "loss: 0.015227428637444973, accuracy: 1.0\n",
      "loss: 0.015124914236366749, accuracy: 1.0\n",
      "loss: 0.016417337581515312, accuracy: 1.0\n",
      "loss: 0.01595599763095379, accuracy: 1.0\n",
      "loss: 0.016205374151468277, accuracy: 1.0\n",
      "loss: 0.01526410598307848, accuracy: 1.0\n",
      "loss: 0.015206517651677132, accuracy: 1.0\n",
      "loss: 0.0159935150295496, accuracy: 1.0\n",
      "loss: 0.016124654561281204, accuracy: 1.0\n",
      "loss: 0.015192247927188873, accuracy: 1.0\n",
      "loss: 0.015794100239872932, accuracy: 1.0\n",
      "loss: 0.01551488135010004, accuracy: 1.0\n",
      "loss: 0.01574479229748249, accuracy: 1.0\n",
      "loss: 0.016017548739910126, accuracy: 1.0\n",
      "loss: 0.014912771061062813, accuracy: 1.0\n",
      "loss: 0.016245581209659576, accuracy: 1.0\n",
      "loss: 0.015431003645062447, accuracy: 1.0\n",
      "loss: 0.015578905120491982, accuracy: 1.0\n",
      "loss: 0.015116245485842228, accuracy: 1.0\n",
      "loss: 0.014809063635766506, accuracy: 1.0\n",
      "loss: 0.014606949873268604, accuracy: 1.0\n",
      "loss: 0.015338129363954067, accuracy: 1.0\n",
      "loss: 0.015157111920416355, accuracy: 1.0\n",
      "loss: 0.015731170773506165, accuracy: 1.0\n",
      "loss: 0.01505003310739994, accuracy: 1.0\n",
      "loss: 0.014903728850185871, accuracy: 1.0\n",
      "loss: 0.015184200368821621, accuracy: 1.0\n",
      "loss: 0.014971764758229256, accuracy: 1.0\n",
      "loss: 0.015596824698150158, accuracy: 1.0\n",
      "loss: 0.015202593058347702, accuracy: 1.0\n",
      "loss: 0.01528091635555029, accuracy: 1.0\n",
      "loss: 0.014892811886966228, accuracy: 1.0\n",
      "loss: 0.01601642370223999, accuracy: 1.0\n",
      "loss: 0.015209870412945747, accuracy: 1.0\n",
      "loss: 0.014893397688865662, accuracy: 1.0\n",
      "loss: 0.015157817862927914, accuracy: 1.0\n",
      "loss: 0.015010981820523739, accuracy: 1.0\n",
      "loss: 0.015087648294866085, accuracy: 1.0\n",
      "loss: 0.015478995628654957, accuracy: 1.0\n",
      "loss: 0.01520735863596201, accuracy: 1.0\n",
      "loss: 0.015051105059683323, accuracy: 1.0\n",
      "loss: 0.015620906837284565, accuracy: 1.0\n",
      "loss: 0.01552148163318634, accuracy: 1.0\n",
      "loss: 0.015521051362156868, accuracy: 1.0\n",
      "loss: 0.014784355647861958, accuracy: 1.0\n",
      "loss: 0.0157339945435524, accuracy: 1.0\n",
      "loss: 0.014680832624435425, accuracy: 1.0\n",
      "loss: 0.014952429570257664, accuracy: 1.0\n",
      "loss: 0.014718319289386272, accuracy: 1.0\n",
      "loss: 0.016807734966278076, accuracy: 1.0\n",
      "loss: 0.015224069356918335, accuracy: 1.0\n",
      "loss: 0.015332668088376522, accuracy: 1.0\n",
      "loss: 0.015749681740999222, accuracy: 1.0\n",
      "loss: 0.014795294962823391, accuracy: 1.0\n",
      "loss: 0.015139784663915634, accuracy: 1.0\n",
      "loss: 0.01540494617074728, accuracy: 1.0\n",
      "loss: 0.014780977740883827, accuracy: 1.0\n",
      "loss: 0.014867324382066727, accuracy: 1.0\n",
      "loss: 0.0161347184330225, accuracy: 1.0\n",
      "loss: 0.01532625500112772, accuracy: 1.0\n",
      "loss: 0.015074055641889572, accuracy: 1.0\n",
      "loss: 0.014766257256269455, accuracy: 1.0\n",
      "loss: 0.0148693872615695, accuracy: 1.0\n",
      "loss: 0.016868578270077705, accuracy: 1.0\n",
      "loss: 0.01526376511901617, accuracy: 1.0\n",
      "loss: 0.014910993166267872, accuracy: 1.0\n",
      "loss: 0.0145863713696599, accuracy: 1.0\n",
      "loss: 0.014971417374908924, accuracy: 1.0\n",
      "loss: 0.01487806811928749, accuracy: 1.0\n",
      "loss: 0.014778601936995983, accuracy: 1.0\n",
      "loss: 0.014573322609066963, accuracy: 1.0\n",
      "loss: 0.014767540618777275, accuracy: 1.0\n",
      "loss: 0.014536258764564991, accuracy: 1.0\n",
      "loss: 0.015231392346322536, accuracy: 1.0\n",
      "loss: 0.015594352968037128, accuracy: 1.0\n",
      "loss: 0.014138014987111092, accuracy: 1.0\n",
      "loss: 0.014570323750376701, accuracy: 1.0\n",
      "loss: 0.014883402734994888, accuracy: 1.0\n",
      "loss: 0.014639440923929214, accuracy: 1.0\n",
      "loss: 0.014611026272177696, accuracy: 1.0\n",
      "loss: 0.015422655269503593, accuracy: 1.0\n",
      "loss: 0.014695892110466957, accuracy: 1.0\n",
      "loss: 0.014392433688044548, accuracy: 1.0\n",
      "loss: 0.015166383236646652, accuracy: 1.0\n",
      "loss: 0.015186870470643044, accuracy: 1.0\n",
      "loss: 0.01414493191987276, accuracy: 1.0\n",
      "loss: 0.014732391573488712, accuracy: 1.0\n",
      "loss: 0.014221365563571453, accuracy: 1.0\n",
      "loss: 0.014414685778319836, accuracy: 1.0\n",
      "loss: 0.01504434086382389, accuracy: 1.0\n",
      "loss: 0.014615196734666824, accuracy: 1.0\n",
      "loss: 0.014166092500090599, accuracy: 1.0\n",
      "loss: 0.014626438729465008, accuracy: 1.0\n",
      "loss: 0.014278816059231758, accuracy: 1.0\n",
      "loss: 0.014308166690170765, accuracy: 1.0\n",
      "loss: 0.014318610541522503, accuracy: 1.0\n",
      "loss: 0.014566189609467983, accuracy: 1.0\n",
      "loss: 0.014974556863307953, accuracy: 1.0\n",
      "loss: 0.015244763344526291, accuracy: 1.0\n",
      "loss: 0.014704995788633823, accuracy: 1.0\n",
      "loss: 0.014542540535330772, accuracy: 1.0\n",
      "loss: 0.014043278060853481, accuracy: 1.0\n",
      "loss: 0.014288164675235748, accuracy: 1.0\n",
      "loss: 0.01418125070631504, accuracy: 1.0\n",
      "loss: 0.013909793458878994, accuracy: 1.0\n",
      "loss: 0.01419595256447792, accuracy: 1.0\n",
      "loss: 0.015312911942601204, accuracy: 1.0\n",
      "loss: 0.01502588577568531, accuracy: 1.0\n",
      "loss: 0.014591322280466557, accuracy: 1.0\n",
      "loss: 0.01440800167620182, accuracy: 1.0\n",
      "loss: 0.015654774382710457, accuracy: 1.0\n",
      "loss: 0.014394696801900864, accuracy: 1.0\n",
      "loss: 0.014330513775348663, accuracy: 1.0\n",
      "loss: 0.014128649607300758, accuracy: 1.0\n",
      "loss: 0.014568505808711052, accuracy: 1.0\n",
      "loss: 0.014047144912183285, accuracy: 1.0\n",
      "loss: 0.013928180560469627, accuracy: 1.0\n",
      "loss: 0.014969570562243462, accuracy: 1.0\n",
      "loss: 0.014768624678254128, accuracy: 1.0\n",
      "loss: 0.014065139926970005, accuracy: 1.0\n",
      "loss: 0.014115266501903534, accuracy: 1.0\n",
      "loss: 0.015757737681269646, accuracy: 1.0\n",
      "loss: 0.01788134127855301, accuracy: 1.0\n",
      "loss: 0.01472928561270237, accuracy: 1.0\n",
      "loss: 0.014496937394142151, accuracy: 1.0\n",
      "loss: 0.014138213358819485, accuracy: 1.0\n",
      "loss: 0.014530190266668797, accuracy: 1.0\n",
      "loss: 0.013598195277154446, accuracy: 1.0\n",
      "loss: 0.014210803434252739, accuracy: 1.0\n",
      "loss: 0.013668626546859741, accuracy: 1.0\n",
      "loss: 0.014784439466893673, accuracy: 1.0\n",
      "loss: 0.013655506074428558, accuracy: 1.0\n",
      "loss: 0.014545775949954987, accuracy: 1.0\n",
      "loss: 0.014297865331172943, accuracy: 1.0\n",
      "loss: 0.013433135114610195, accuracy: 1.0\n",
      "loss: 0.014244075864553452, accuracy: 1.0\n",
      "loss: 0.014104286208748817, accuracy: 1.0\n",
      "loss: 0.014171043410897255, accuracy: 1.0\n",
      "loss: 0.013574712909758091, accuracy: 1.0\n",
      "loss: 0.014575404115021229, accuracy: 1.0\n",
      "loss: 0.013965358957648277, accuracy: 1.0\n",
      "loss: 0.013967083767056465, accuracy: 1.0\n",
      "loss: 0.01378131564706564, accuracy: 1.0\n",
      "loss: 0.01444354746490717, accuracy: 1.0\n",
      "loss: 0.01401575282216072, accuracy: 1.0\n",
      "loss: 0.01370277814567089, accuracy: 1.0\n",
      "loss: 0.01380923856049776, accuracy: 1.0\n",
      "loss: 0.014300946146249771, accuracy: 1.0\n",
      "loss: 0.01418396644294262, accuracy: 1.0\n",
      "loss: 0.015603294596076012, accuracy: 1.0\n",
      "loss: 0.013841284438967705, accuracy: 1.0\n",
      "loss: 0.013446376658976078, accuracy: 1.0\n",
      "loss: 0.014453367330133915, accuracy: 1.0\n",
      "loss: 0.014206310734152794, accuracy: 1.0\n",
      "loss: 0.014185233972966671, accuracy: 1.0\n",
      "loss: 0.013378521427512169, accuracy: 1.0\n",
      "loss: 0.013449653051793575, accuracy: 1.0\n",
      "loss: 0.014170079492032528, accuracy: 1.0\n",
      "loss: 0.01418836135417223, accuracy: 1.0\n",
      "loss: 0.013574528507888317, accuracy: 1.0\n",
      "loss: 0.013613457791507244, accuracy: 1.0\n",
      "loss: 0.013375064358115196, accuracy: 1.0\n",
      "loss: 0.014021502807736397, accuracy: 1.0\n",
      "loss: 0.014038847759366035, accuracy: 1.0\n",
      "loss: 0.013788369484245777, accuracy: 1.0\n",
      "loss: 0.013918498530983925, accuracy: 1.0\n",
      "loss: 0.013428155332803726, accuracy: 1.0\n",
      "loss: 0.014112581498920918, accuracy: 1.0\n",
      "loss: 0.013781679794192314, accuracy: 1.0\n",
      "loss: 0.01354073267430067, accuracy: 1.0\n",
      "loss: 0.013591052033007145, accuracy: 1.0\n",
      "loss: 0.013505428098142147, accuracy: 1.0\n",
      "loss: 0.013353057205677032, accuracy: 1.0\n",
      "loss: 0.013687223196029663, accuracy: 1.0\n",
      "loss: 0.01349035743623972, accuracy: 1.0\n",
      "loss: 0.013697663322091103, accuracy: 1.0\n",
      "loss: 0.013575337827205658, accuracy: 1.0\n",
      "loss: 0.014331704005599022, accuracy: 1.0\n",
      "loss: 0.013398915529251099, accuracy: 1.0\n",
      "loss: 0.013618997298181057, accuracy: 1.0\n",
      "loss: 0.014151404611766338, accuracy: 1.0\n",
      "loss: 0.013861054554581642, accuracy: 1.0\n",
      "loss: 0.014065450988709927, accuracy: 1.0\n",
      "loss: 0.014601542614400387, accuracy: 1.0\n",
      "loss: 0.014590183272957802, accuracy: 1.0\n",
      "loss: 0.01463328767567873, accuracy: 1.0\n",
      "loss: 0.013221604749560356, accuracy: 1.0\n",
      "loss: 0.013653692789375782, accuracy: 1.0\n",
      "loss: 0.013314465060830116, accuracy: 1.0\n",
      "loss: 0.013355806469917297, accuracy: 1.0\n",
      "loss: 0.013979374431073666, accuracy: 1.0\n",
      "loss: 0.013239789754152298, accuracy: 1.0\n",
      "loss: 0.013628958724439144, accuracy: 1.0\n",
      "loss: 0.01397106796503067, accuracy: 1.0\n",
      "loss: 0.014966920018196106, accuracy: 1.0\n",
      "loss: 0.014703321270644665, accuracy: 1.0\n",
      "loss: 0.013379864394664764, accuracy: 1.0\n",
      "loss: 0.014244191348552704, accuracy: 1.0\n",
      "loss: 0.01342333760112524, accuracy: 1.0\n",
      "loss: 0.013441947288811207, accuracy: 1.0\n",
      "loss: 0.013800514861941338, accuracy: 1.0\n",
      "loss: 0.01304568164050579, accuracy: 1.0\n",
      "loss: 0.013263913802802563, accuracy: 1.0\n",
      "loss: 0.013348028995096684, accuracy: 1.0\n",
      "loss: 0.013456929475069046, accuracy: 1.0\n",
      "loss: 0.013749725185334682, accuracy: 1.0\n",
      "loss: 0.013575524091720581, accuracy: 1.0\n",
      "loss: 0.013527029193937778, accuracy: 1.0\n",
      "loss: 0.013495453633368015, accuracy: 1.0\n",
      "loss: 0.013306966051459312, accuracy: 1.0\n",
      "loss: 0.013047503307461739, accuracy: 1.0\n",
      "loss: 0.013773925602436066, accuracy: 1.0\n",
      "loss: 0.013425098732113838, accuracy: 1.0\n",
      "loss: 0.013589542359113693, accuracy: 1.0\n",
      "loss: 0.013399863615632057, accuracy: 1.0\n",
      "loss: 0.013951684348285198, accuracy: 1.0\n",
      "loss: 0.012987012043595314, accuracy: 1.0\n",
      "loss: 0.013519493862986565, accuracy: 1.0\n",
      "loss: 0.013063452206552029, accuracy: 1.0\n",
      "loss: 0.013295775279402733, accuracy: 1.0\n",
      "loss: 0.013437197543680668, accuracy: 1.0\n",
      "loss: 0.013217789120972157, accuracy: 1.0\n",
      "loss: 0.012742502614855766, accuracy: 1.0\n",
      "loss: 0.013128961436450481, accuracy: 1.0\n",
      "loss: 0.013154596090316772, accuracy: 1.0\n",
      "loss: 0.0132319126278162, accuracy: 1.0\n",
      "loss: 0.012889029458165169, accuracy: 1.0\n",
      "loss: 0.013112979009747505, accuracy: 1.0\n",
      "loss: 0.013709710910916328, accuracy: 1.0\n",
      "loss: 0.013134601525962353, accuracy: 1.0\n",
      "loss: 0.013711382634937763, accuracy: 1.0\n",
      "loss: 0.015468211844563484, accuracy: 1.0\n",
      "loss: 0.012959606014192104, accuracy: 1.0\n",
      "loss: 0.013537717051804066, accuracy: 1.0\n",
      "loss: 0.013204237446188927, accuracy: 1.0\n",
      "loss: 0.013182243332266808, accuracy: 1.0\n",
      "loss: 0.012847055681049824, accuracy: 1.0\n",
      "loss: 0.012930051423609257, accuracy: 1.0\n",
      "loss: 0.013120857998728752, accuracy: 1.0\n",
      "loss: 0.013050351291894913, accuracy: 1.0\n",
      "loss: 0.01325614657253027, accuracy: 1.0\n",
      "loss: 0.012891651131212711, accuracy: 1.0\n",
      "loss: 0.013264601118862629, accuracy: 1.0\n",
      "loss: 0.012805241160094738, accuracy: 1.0\n",
      "loss: 0.012659793719649315, accuracy: 1.0\n",
      "loss: 0.013098777271807194, accuracy: 1.0\n",
      "loss: 0.013335522264242172, accuracy: 1.0\n",
      "loss: 0.013311768881976604, accuracy: 1.0\n",
      "loss: 0.013091552071273327, accuracy: 1.0\n",
      "loss: 0.012229284271597862, accuracy: 1.0\n",
      "loss: 0.013302872888743877, accuracy: 1.0\n",
      "loss: 0.012614468112587929, accuracy: 1.0\n",
      "loss: 0.013016775250434875, accuracy: 1.0\n",
      "loss: 0.013073540292680264, accuracy: 1.0\n",
      "loss: 0.012917779386043549, accuracy: 1.0\n",
      "loss: 0.012942786328494549, accuracy: 1.0\n",
      "loss: 0.013413838110864162, accuracy: 1.0\n",
      "loss: 0.012900732457637787, accuracy: 1.0\n",
      "loss: 0.012976779602468014, accuracy: 1.0\n",
      "loss: 0.012692494317889214, accuracy: 1.0\n",
      "loss: 0.01384248211979866, accuracy: 1.0\n",
      "loss: 0.013194135390222073, accuracy: 1.0\n",
      "loss: 0.0132924634963274, accuracy: 1.0\n",
      "loss: 0.013198604807257652, accuracy: 1.0\n",
      "loss: 0.012771820649504662, accuracy: 1.0\n",
      "loss: 0.012552819214761257, accuracy: 1.0\n",
      "loss: 0.012845744378864765, accuracy: 1.0\n",
      "loss: 0.012650584802031517, accuracy: 1.0\n",
      "loss: 0.01231823768466711, accuracy: 1.0\n",
      "loss: 0.012705122120678425, accuracy: 1.0\n",
      "loss: 0.012553645297884941, accuracy: 1.0\n",
      "loss: 0.012613851577043533, accuracy: 1.0\n",
      "loss: 0.012998053804039955, accuracy: 1.0\n",
      "loss: 0.012799064628779888, accuracy: 1.0\n",
      "loss: 0.013200926594436169, accuracy: 1.0\n",
      "loss: 0.012650623917579651, accuracy: 1.0\n",
      "loss: 0.012589369900524616, accuracy: 1.0\n",
      "loss: 0.01281453762203455, accuracy: 1.0\n",
      "loss: 0.012756372801959515, accuracy: 1.0\n",
      "loss: 0.012358550913631916, accuracy: 1.0\n",
      "loss: 0.013228426687419415, accuracy: 1.0\n",
      "loss: 0.014474630355834961, accuracy: 1.0\n",
      "loss: 0.01305484026670456, accuracy: 1.0\n",
      "loss: 0.01352126244455576, accuracy: 1.0\n",
      "loss: 0.012634643353521824, accuracy: 1.0\n",
      "loss: 0.012439841404557228, accuracy: 1.0\n",
      "loss: 0.012604533694684505, accuracy: 1.0\n",
      "loss: 0.012752649374306202, accuracy: 1.0\n",
      "loss: 0.012676257640123367, accuracy: 1.0\n",
      "loss: 0.012547043152153492, accuracy: 1.0\n",
      "loss: 0.013036931864917278, accuracy: 1.0\n",
      "loss: 0.012026890181005001, accuracy: 1.0\n",
      "loss: 0.01296512596309185, accuracy: 1.0\n",
      "loss: 0.012309418059885502, accuracy: 1.0\n",
      "loss: 0.012865685857832432, accuracy: 1.0\n",
      "loss: 0.01240998413413763, accuracy: 1.0\n",
      "loss: 0.012251781299710274, accuracy: 1.0\n",
      "loss: 0.012655816972255707, accuracy: 1.0\n",
      "loss: 0.012728654779493809, accuracy: 1.0\n",
      "loss: 0.013129458762705326, accuracy: 1.0\n",
      "loss: 0.012246133759617805, accuracy: 1.0\n",
      "loss: 0.012287464924156666, accuracy: 1.0\n",
      "loss: 0.01255345344543457, accuracy: 1.0\n",
      "loss: 0.012327705509960651, accuracy: 1.0\n",
      "loss: 0.012275594286620617, accuracy: 1.0\n",
      "loss: 0.012206079438328743, accuracy: 1.0\n",
      "loss: 0.01283988542854786, accuracy: 1.0\n",
      "loss: 0.012338296510279179, accuracy: 1.0\n",
      "loss: 0.01228883396834135, accuracy: 1.0\n",
      "loss: 0.012706717476248741, accuracy: 1.0\n",
      "loss: 0.01243180688470602, accuracy: 1.0\n",
      "loss: 0.012302796356379986, accuracy: 1.0\n",
      "loss: 0.012324056588113308, accuracy: 1.0\n",
      "loss: 0.013216393999755383, accuracy: 1.0\n",
      "loss: 0.012316333130002022, accuracy: 1.0\n",
      "loss: 0.01208871603012085, accuracy: 1.0\n",
      "loss: 0.012791866436600685, accuracy: 1.0\n",
      "loss: 0.011806226335465908, accuracy: 1.0\n",
      "loss: 0.012666775844991207, accuracy: 1.0\n",
      "loss: 0.012236409820616245, accuracy: 1.0\n",
      "loss: 0.012335733510553837, accuracy: 1.0\n",
      "loss: 0.012327522970736027, accuracy: 1.0\n",
      "loss: 0.012095029465854168, accuracy: 1.0\n",
      "loss: 0.01255080010741949, accuracy: 1.0\n",
      "loss: 0.011969290673732758, accuracy: 1.0\n",
      "loss: 0.012879446148872375, accuracy: 1.0\n",
      "loss: 0.01265514362603426, accuracy: 1.0\n",
      "loss: 0.012573648244142532, accuracy: 1.0\n",
      "loss: 0.012619775719940662, accuracy: 1.0\n",
      "loss: 0.012360858730971813, accuracy: 1.0\n",
      "loss: 0.012086974456906319, accuracy: 1.0\n",
      "loss: 0.012798039242625237, accuracy: 1.0\n",
      "loss: 0.012799805030226707, accuracy: 1.0\n",
      "loss: 0.012172128073871136, accuracy: 1.0\n",
      "loss: 0.012851550243794918, accuracy: 1.0\n",
      "loss: 0.01189087238162756, accuracy: 1.0\n",
      "loss: 0.013019820675253868, accuracy: 1.0\n",
      "loss: 0.012026532553136349, accuracy: 1.0\n",
      "loss: 0.011995143257081509, accuracy: 1.0\n",
      "loss: 0.012150107882916927, accuracy: 1.0\n",
      "loss: 0.012088497169315815, accuracy: 1.0\n",
      "loss: 0.012133126147091389, accuracy: 1.0\n",
      "loss: 0.011980457231402397, accuracy: 1.0\n",
      "loss: 0.012097356840968132, accuracy: 1.0\n",
      "loss: 0.012061475776135921, accuracy: 1.0\n",
      "loss: 0.01197111327201128, accuracy: 1.0\n",
      "loss: 0.012119289487600327, accuracy: 1.0\n",
      "loss: 0.011707540601491928, accuracy: 1.0\n",
      "loss: 0.012385564856231213, accuracy: 1.0\n",
      "loss: 0.011726267635822296, accuracy: 1.0\n",
      "loss: 0.012232786044478416, accuracy: 1.0\n",
      "loss: 0.01227541547268629, accuracy: 1.0\n",
      "loss: 0.012144135311245918, accuracy: 1.0\n",
      "loss: 0.012333185411989689, accuracy: 1.0\n",
      "loss: 0.012467125430703163, accuracy: 1.0\n",
      "loss: 0.012516031973063946, accuracy: 1.0\n",
      "loss: 0.012108304537832737, accuracy: 1.0\n",
      "loss: 0.011592606082558632, accuracy: 1.0\n",
      "loss: 0.012282006442546844, accuracy: 1.0\n",
      "loss: 0.012429832480847836, accuracy: 1.0\n",
      "loss: 0.012252719141542912, accuracy: 1.0\n",
      "loss: 0.012564962729811668, accuracy: 1.0\n",
      "loss: 0.013163471594452858, accuracy: 1.0\n",
      "loss: 0.012358699925243855, accuracy: 1.0\n",
      "loss: 0.01163641456514597, accuracy: 1.0\n",
      "loss: 0.012480986304581165, accuracy: 1.0\n",
      "loss: 0.011777426116168499, accuracy: 1.0\n",
      "loss: 0.011919303797185421, accuracy: 1.0\n",
      "loss: 0.011959023773670197, accuracy: 1.0\n",
      "loss: 0.012274171225726604, accuracy: 1.0\n",
      "loss: 0.011513556353747845, accuracy: 1.0\n",
      "loss: 0.01213829591870308, accuracy: 1.0\n",
      "loss: 0.011815852485597134, accuracy: 1.0\n",
      "loss: 0.012842580676078796, accuracy: 1.0\n",
      "loss: 0.012154689989984035, accuracy: 1.0\n",
      "loss: 0.01214305404573679, accuracy: 1.0\n",
      "loss: 0.012468584813177586, accuracy: 1.0\n",
      "loss: 0.012059567496180534, accuracy: 1.0\n",
      "loss: 0.013503978960216045, accuracy: 1.0\n",
      "loss: 0.012032200582325459, accuracy: 1.0\n",
      "loss: 0.012026204727590084, accuracy: 1.0\n",
      "loss: 0.013075063936412334, accuracy: 1.0\n",
      "loss: 0.012067724950611591, accuracy: 1.0\n",
      "loss: 0.011859368532896042, accuracy: 1.0\n",
      "loss: 0.012023313902318478, accuracy: 1.0\n",
      "loss: 0.012149450369179249, accuracy: 1.0\n",
      "loss: 0.0126784797757864, accuracy: 1.0\n",
      "loss: 0.011836347170174122, accuracy: 1.0\n",
      "loss: 0.012787708081305027, accuracy: 1.0\n",
      "loss: 0.011648139916360378, accuracy: 1.0\n",
      "loss: 0.01211568620055914, accuracy: 1.0\n",
      "loss: 0.011915675364434719, accuracy: 1.0\n",
      "loss: 0.012456025928258896, accuracy: 1.0\n",
      "loss: 0.012077240273356438, accuracy: 1.0\n",
      "loss: 0.011660175397992134, accuracy: 1.0\n",
      "loss: 0.012377327308058739, accuracy: 1.0\n",
      "loss: 0.011770921759307384, accuracy: 1.0\n",
      "loss: 0.011587378568947315, accuracy: 1.0\n",
      "loss: 0.0117314662784338, accuracy: 1.0\n",
      "loss: 0.011760198511183262, accuracy: 1.0\n",
      "loss: 0.012010014615952969, accuracy: 1.0\n",
      "loss: 0.011820622719824314, accuracy: 1.0\n",
      "loss: 0.01213244441896677, accuracy: 1.0\n",
      "loss: 0.013257581740617752, accuracy: 1.0\n",
      "loss: 0.01170163694769144, accuracy: 1.0\n",
      "loss: 0.011655359528958797, accuracy: 1.0\n",
      "loss: 0.01233851257711649, accuracy: 1.0\n",
      "loss: 0.011876808479428291, accuracy: 1.0\n",
      "loss: 0.011366167105734348, accuracy: 1.0\n",
      "loss: 0.012295146472752094, accuracy: 1.0\n",
      "loss: 0.011750645935535431, accuracy: 1.0\n",
      "loss: 0.01182348933070898, accuracy: 1.0\n",
      "loss: 0.01177467405796051, accuracy: 1.0\n",
      "loss: 0.011501613073050976, accuracy: 1.0\n",
      "loss: 0.011453947983682156, accuracy: 1.0\n",
      "loss: 0.011908811517059803, accuracy: 1.0\n",
      "loss: 0.011803235858678818, accuracy: 1.0\n",
      "loss: 0.011852367781102657, accuracy: 1.0\n",
      "loss: 0.01170868705958128, accuracy: 1.0\n",
      "loss: 0.011417309753596783, accuracy: 1.0\n",
      "loss: 0.011781255714595318, accuracy: 1.0\n",
      "loss: 0.01165832206606865, accuracy: 1.0\n",
      "loss: 0.012430407106876373, accuracy: 1.0\n",
      "loss: 0.011837066151201725, accuracy: 1.0\n",
      "loss: 0.012082449160516262, accuracy: 1.0\n",
      "loss: 0.012150549329817295, accuracy: 1.0\n",
      "loss: 0.011309760622680187, accuracy: 1.0\n",
      "loss: 0.011607532389461994, accuracy: 1.0\n",
      "loss: 0.011750217527151108, accuracy: 1.0\n",
      "loss: 0.012038775719702244, accuracy: 1.0\n",
      "loss: 0.01134423352777958, accuracy: 1.0\n",
      "loss: 0.011438910849392414, accuracy: 1.0\n",
      "loss: 0.01150735653936863, accuracy: 1.0\n",
      "loss: 0.011268130503594875, accuracy: 1.0\n",
      "loss: 0.01174083910882473, accuracy: 1.0\n",
      "loss: 0.011273233219981194, accuracy: 1.0\n",
      "loss: 0.011973259970545769, accuracy: 1.0\n",
      "loss: 0.01133343018591404, accuracy: 1.0\n",
      "loss: 0.012399133294820786, accuracy: 1.0\n",
      "loss: 0.012041063979268074, accuracy: 1.0\n",
      "loss: 0.011574028991162777, accuracy: 1.0\n",
      "loss: 0.011253624223172665, accuracy: 1.0\n",
      "loss: 0.01135548111051321, accuracy: 1.0\n",
      "loss: 0.011403234675526619, accuracy: 1.0\n",
      "loss: 0.011786854825913906, accuracy: 1.0\n",
      "loss: 0.01172039844095707, accuracy: 1.0\n",
      "loss: 0.01142865139991045, accuracy: 1.0\n",
      "loss: 0.011550056748092175, accuracy: 1.0\n",
      "loss: 0.011562556959688663, accuracy: 1.0\n",
      "loss: 0.011289803311228752, accuracy: 1.0\n",
      "loss: 0.011475308798253536, accuracy: 1.0\n",
      "loss: 0.011643286794424057, accuracy: 1.0\n",
      "loss: 0.01134574692696333, accuracy: 1.0\n",
      "loss: 0.011395563371479511, accuracy: 1.0\n",
      "loss: 0.011126309633255005, accuracy: 1.0\n",
      "loss: 0.011096296831965446, accuracy: 1.0\n",
      "loss: 0.012516665272414684, accuracy: 1.0\n",
      "loss: 0.011648612096905708, accuracy: 1.0\n",
      "loss: 0.011631841771304607, accuracy: 1.0\n",
      "loss: 0.011733739636838436, accuracy: 1.0\n",
      "loss: 0.011269310489296913, accuracy: 1.0\n",
      "loss: 0.011102855205535889, accuracy: 1.0\n",
      "loss: 0.010934969410300255, accuracy: 1.0\n",
      "loss: 0.01143841352313757, accuracy: 1.0\n",
      "loss: 0.01145343016833067, accuracy: 1.0\n",
      "loss: 0.011454524472355843, accuracy: 1.0\n",
      "loss: 0.011266632005572319, accuracy: 1.0\n",
      "loss: 0.011501189321279526, accuracy: 1.0\n",
      "loss: 0.011041661724448204, accuracy: 1.0\n",
      "loss: 0.011762646026909351, accuracy: 1.0\n",
      "loss: 0.011407860554754734, accuracy: 1.0\n",
      "loss: 0.011161237955093384, accuracy: 1.0\n",
      "loss: 0.010993974283337593, accuracy: 1.0\n",
      "loss: 0.011208031326532364, accuracy: 1.0\n",
      "loss: 0.011382535099983215, accuracy: 1.0\n",
      "loss: 0.011226215399801731, accuracy: 1.0\n",
      "loss: 0.011493225581943989, accuracy: 1.0\n",
      "loss: 0.011399617418646812, accuracy: 1.0\n",
      "loss: 0.011586910113692284, accuracy: 1.0\n",
      "loss: 0.010893033817410469, accuracy: 1.0\n",
      "loss: 0.011654455214738846, accuracy: 1.0\n",
      "loss: 0.011392666958272457, accuracy: 1.0\n",
      "loss: 0.011724571697413921, accuracy: 1.0\n",
      "loss: 0.01102000568062067, accuracy: 1.0\n",
      "loss: 0.011305905878543854, accuracy: 1.0\n",
      "loss: 0.012153823859989643, accuracy: 1.0\n",
      "loss: 0.011488527059555054, accuracy: 1.0\n",
      "loss: 0.01146604586392641, accuracy: 1.0\n",
      "loss: 0.011413047090172768, accuracy: 1.0\n",
      "loss: 0.011237047612667084, accuracy: 1.0\n",
      "loss: 0.010881147347390652, accuracy: 1.0\n",
      "loss: 0.0113619240000844, accuracy: 1.0\n",
      "loss: 0.011001432314515114, accuracy: 1.0\n",
      "loss: 0.01140730269253254, accuracy: 1.0\n",
      "loss: 0.010839763097465038, accuracy: 1.0\n",
      "loss: 0.01130498293787241, accuracy: 1.0\n",
      "loss: 0.011430302634835243, accuracy: 1.0\n",
      "loss: 0.011802968569099903, accuracy: 1.0\n",
      "loss: 0.011106934398412704, accuracy: 1.0\n",
      "loss: 0.010884396731853485, accuracy: 1.0\n",
      "loss: 0.011226003058254719, accuracy: 1.0\n",
      "loss: 0.011258799582719803, accuracy: 1.0\n",
      "loss: 0.010764251463115215, accuracy: 1.0\n",
      "loss: 0.01085484866052866, accuracy: 1.0\n",
      "loss: 0.011271720752120018, accuracy: 1.0\n",
      "loss: 0.010969665832817554, accuracy: 1.0\n",
      "loss: 0.010837644338607788, accuracy: 1.0\n",
      "loss: 0.011009017936885357, accuracy: 1.0\n",
      "loss: 0.011054164730012417, accuracy: 1.0\n",
      "loss: 0.011265666224062443, accuracy: 1.0\n",
      "loss: 0.011138228699564934, accuracy: 1.0\n",
      "loss: 0.011242137290537357, accuracy: 1.0\n",
      "loss: 0.010792425833642483, accuracy: 1.0\n",
      "loss: 0.01057124137878418, accuracy: 1.0\n",
      "loss: 0.011182082816958427, accuracy: 1.0\n",
      "loss: 0.011082536540925503, accuracy: 1.0\n",
      "loss: 0.011061972007155418, accuracy: 1.0\n",
      "loss: 0.011006232351064682, accuracy: 1.0\n",
      "loss: 0.010937950573861599, accuracy: 1.0\n",
      "loss: 0.0113207483664155, accuracy: 1.0\n",
      "loss: 0.010895353741943836, accuracy: 1.0\n",
      "loss: 0.010666622780263424, accuracy: 1.0\n",
      "loss: 0.01116766408085823, accuracy: 1.0\n",
      "loss: 0.011287063360214233, accuracy: 1.0\n",
      "loss: 0.011428790166974068, accuracy: 1.0\n",
      "loss: 0.010795426554977894, accuracy: 1.0\n",
      "loss: 0.010556626133620739, accuracy: 1.0\n",
      "loss: 0.011499617248773575, accuracy: 1.0\n",
      "loss: 0.010829088278114796, accuracy: 1.0\n",
      "loss: 0.010954227298498154, accuracy: 1.0\n",
      "loss: 0.010953586548566818, accuracy: 1.0\n",
      "loss: 0.010989169590175152, accuracy: 1.0\n",
      "loss: 0.011323210783302784, accuracy: 1.0\n",
      "loss: 0.011071031913161278, accuracy: 1.0\n",
      "loss: 0.011138550005853176, accuracy: 1.0\n",
      "loss: 0.011412935331463814, accuracy: 1.0\n",
      "loss: 0.011266006156802177, accuracy: 1.0\n",
      "loss: 0.010718167759478092, accuracy: 1.0\n",
      "loss: 0.011012308299541473, accuracy: 1.0\n",
      "loss: 0.010566463693976402, accuracy: 1.0\n",
      "loss: 0.010817447677254677, accuracy: 1.0\n",
      "loss: 0.011110380291938782, accuracy: 1.0\n",
      "loss: 0.010674943216145039, accuracy: 1.0\n",
      "loss: 0.010603314265608788, accuracy: 1.0\n",
      "loss: 0.01105155237019062, accuracy: 1.0\n",
      "loss: 0.010553675703704357, accuracy: 1.0\n",
      "loss: 0.011389059014618397, accuracy: 1.0\n",
      "loss: 0.01077205128967762, accuracy: 1.0\n",
      "loss: 0.011141845025122166, accuracy: 1.0\n",
      "loss: 0.010587823577225208, accuracy: 1.0\n",
      "loss: 0.010472448542714119, accuracy: 1.0\n",
      "loss: 0.011559417471289635, accuracy: 1.0\n",
      "loss: 0.010561536066234112, accuracy: 1.0\n",
      "loss: 0.010720081627368927, accuracy: 1.0\n",
      "loss: 0.010802878998219967, accuracy: 1.0\n",
      "loss: 0.010624606162309647, accuracy: 1.0\n",
      "loss: 0.010554052889347076, accuracy: 1.0\n",
      "loss: 0.010269243270158768, accuracy: 1.0\n",
      "loss: 0.011308232322335243, accuracy: 1.0\n",
      "loss: 0.010826743207871914, accuracy: 1.0\n",
      "loss: 0.010473063215613365, accuracy: 1.0\n",
      "loss: 0.010606086812913418, accuracy: 1.0\n",
      "loss: 0.010938652791082859, accuracy: 1.0\n",
      "loss: 0.010615595616400242, accuracy: 1.0\n",
      "loss: 0.010544848628342152, accuracy: 1.0\n",
      "loss: 0.010367644019424915, accuracy: 1.0\n",
      "loss: 0.010247667320072651, accuracy: 1.0\n",
      "loss: 0.010623394511640072, accuracy: 1.0\n",
      "loss: 0.010325761511921883, accuracy: 1.0\n",
      "loss: 0.01179581694304943, accuracy: 1.0\n",
      "loss: 0.010600761510431767, accuracy: 1.0\n",
      "loss: 0.01087532564997673, accuracy: 1.0\n",
      "loss: 0.010439309291541576, accuracy: 1.0\n",
      "loss: 0.011205563321709633, accuracy: 1.0\n",
      "loss: 0.010290123522281647, accuracy: 1.0\n",
      "loss: 0.010775163769721985, accuracy: 1.0\n",
      "loss: 0.010795973241329193, accuracy: 1.0\n",
      "loss: 0.010459153912961483, accuracy: 1.0\n",
      "loss: 0.010790891945362091, accuracy: 1.0\n",
      "loss: 0.010649009607732296, accuracy: 1.0\n",
      "loss: 0.010246101766824722, accuracy: 1.0\n",
      "loss: 0.01053513865917921, accuracy: 1.0\n",
      "loss: 0.010549530386924744, accuracy: 1.0\n",
      "loss: 0.01078740879893303, accuracy: 1.0\n",
      "loss: 0.01044220756739378, accuracy: 1.0\n",
      "loss: 0.011593964882194996, accuracy: 1.0\n",
      "loss: 0.011045146733522415, accuracy: 1.0\n",
      "loss: 0.010394293814897537, accuracy: 1.0\n",
      "loss: 0.010992408730089664, accuracy: 1.0\n",
      "loss: 0.010813655331730843, accuracy: 1.0\n",
      "loss: 0.010176698677241802, accuracy: 1.0\n",
      "loss: 0.010605774819850922, accuracy: 1.0\n",
      "loss: 0.01072506234049797, accuracy: 1.0\n",
      "loss: 0.010786722414195538, accuracy: 1.0\n",
      "loss: 0.010333150625228882, accuracy: 1.0\n",
      "loss: 0.01042341347783804, accuracy: 1.0\n",
      "loss: 0.01094688381999731, accuracy: 1.0\n",
      "loss: 0.01125960610806942, accuracy: 1.0\n",
      "loss: 0.010277336463332176, accuracy: 1.0\n",
      "loss: 0.010289843194186687, accuracy: 1.0\n",
      "loss: 0.010651901364326477, accuracy: 1.0\n",
      "loss: 0.01042371615767479, accuracy: 1.0\n",
      "loss: 0.01071871630847454, accuracy: 1.0\n",
      "loss: 0.010561615228652954, accuracy: 1.0\n",
      "loss: 0.01005828008055687, accuracy: 1.0\n",
      "loss: 0.011004210449755192, accuracy: 1.0\n",
      "loss: 0.01049942895770073, accuracy: 1.0\n",
      "loss: 0.010341143235564232, accuracy: 1.0\n",
      "loss: 0.010410337708890438, accuracy: 1.0\n",
      "loss: 0.010266443714499474, accuracy: 1.0\n",
      "loss: 0.010256944224238396, accuracy: 1.0\n",
      "loss: 0.010438143275678158, accuracy: 1.0\n",
      "loss: 0.010738457553088665, accuracy: 1.0\n",
      "loss: 0.010261527262628078, accuracy: 1.0\n",
      "loss: 0.010223522782325745, accuracy: 1.0\n",
      "loss: 0.01049262098968029, accuracy: 1.0\n",
      "loss: 0.0108367670327425, accuracy: 1.0\n",
      "loss: 0.01019267737865448, accuracy: 1.0\n",
      "loss: 0.010346864350140095, accuracy: 1.0\n",
      "loss: 0.010264240205287933, accuracy: 1.0\n",
      "loss: 0.010317755863070488, accuracy: 1.0\n",
      "loss: 0.010573298670351505, accuracy: 1.0\n",
      "loss: 0.011079597286880016, accuracy: 1.0\n",
      "loss: 0.010195386596024036, accuracy: 1.0\n",
      "loss: 0.010028793476521969, accuracy: 1.0\n",
      "loss: 0.010610600002110004, accuracy: 1.0\n",
      "loss: 0.010178976692259312, accuracy: 1.0\n",
      "loss: 0.010721007362008095, accuracy: 1.0\n",
      "loss: 0.010404297150671482, accuracy: 1.0\n",
      "loss: 0.011337550356984138, accuracy: 1.0\n",
      "loss: 0.010242442600429058, accuracy: 1.0\n",
      "loss: 0.010551572777330875, accuracy: 1.0\n",
      "loss: 0.01066187396645546, accuracy: 1.0\n",
      "loss: 0.010390514507889748, accuracy: 1.0\n",
      "loss: 0.010653904639184475, accuracy: 1.0\n",
      "loss: 0.01061942707747221, accuracy: 1.0\n",
      "loss: 0.010580424219369888, accuracy: 1.0\n",
      "loss: 0.009946449659764767, accuracy: 1.0\n",
      "loss: 0.01054823026061058, accuracy: 1.0\n",
      "loss: 0.010301326401531696, accuracy: 1.0\n",
      "loss: 0.010293878614902496, accuracy: 1.0\n",
      "loss: 0.01001291535794735, accuracy: 1.0\n",
      "loss: 0.010242392309010029, accuracy: 1.0\n",
      "loss: 0.01012522354722023, accuracy: 1.0\n",
      "loss: 0.010069022886455059, accuracy: 1.0\n",
      "loss: 0.010618266649544239, accuracy: 1.0\n",
      "loss: 0.010415646247565746, accuracy: 1.0\n",
      "loss: 0.009992274455726147, accuracy: 1.0\n",
      "loss: 0.010706170462071896, accuracy: 1.0\n",
      "loss: 0.01007623877376318, accuracy: 1.0\n",
      "loss: 0.010080849751830101, accuracy: 1.0\n",
      "loss: 0.010342788882553577, accuracy: 1.0\n",
      "loss: 0.010268962010741234, accuracy: 1.0\n",
      "loss: 0.010317771695554256, accuracy: 1.0\n",
      "loss: 0.010257348418235779, accuracy: 1.0\n",
      "loss: 0.010295921005308628, accuracy: 1.0\n",
      "loss: 0.009997732937335968, accuracy: 1.0\n",
      "loss: 0.010006389580667019, accuracy: 1.0\n",
      "loss: 0.010156557895243168, accuracy: 1.0\n",
      "loss: 0.010103825479745865, accuracy: 1.0\n",
      "loss: 0.010207493789494038, accuracy: 1.0\n",
      "loss: 0.009774533100426197, accuracy: 1.0\n",
      "loss: 0.010242193005979061, accuracy: 1.0\n",
      "loss: 0.010251767933368683, accuracy: 1.0\n",
      "loss: 0.009905022569000721, accuracy: 1.0\n",
      "loss: 0.010374092496931553, accuracy: 1.0\n",
      "loss: 0.010104348883032799, accuracy: 1.0\n",
      "loss: 0.010369492694735527, accuracy: 1.0\n",
      "loss: 0.010429882444441319, accuracy: 1.0\n",
      "loss: 0.010111493989825249, accuracy: 1.0\n",
      "loss: 0.010324237868189812, accuracy: 1.0\n",
      "loss: 0.010143522173166275, accuracy: 1.0\n",
      "loss: 0.010453745722770691, accuracy: 1.0\n",
      "loss: 0.0105354655534029, accuracy: 1.0\n",
      "loss: 0.010148610919713974, accuracy: 1.0\n",
      "loss: 0.010201937519013882, accuracy: 1.0\n",
      "loss: 0.010426797904074192, accuracy: 1.0\n",
      "loss: 0.010091757401823997, accuracy: 1.0\n",
      "loss: 0.010860994458198547, accuracy: 1.0\n",
      "loss: 0.010166036896407604, accuracy: 1.0\n",
      "loss: 0.010125206783413887, accuracy: 1.0\n",
      "loss: 0.009815762750804424, accuracy: 1.0\n",
      "loss: 0.010166455060243607, accuracy: 1.0\n",
      "loss: 0.010680203326046467, accuracy: 1.0\n",
      "loss: 0.0106932008638978, accuracy: 1.0\n",
      "loss: 0.009654849767684937, accuracy: 1.0\n",
      "loss: 0.010650929063558578, accuracy: 1.0\n",
      "loss: 0.010140154510736465, accuracy: 1.0\n",
      "loss: 0.010891841724514961, accuracy: 1.0\n",
      "loss: 0.010026264004409313, accuracy: 1.0\n",
      "loss: 0.009778155013918877, accuracy: 1.0\n",
      "loss: 0.00971264485269785, accuracy: 1.0\n",
      "loss: 0.009975407272577286, accuracy: 1.0\n",
      "loss: 0.01022313628345728, accuracy: 1.0\n",
      "loss: 0.009789708070456982, accuracy: 1.0\n",
      "loss: 0.0099274180829525, accuracy: 1.0\n",
      "loss: 0.009923086501657963, accuracy: 1.0\n",
      "loss: 0.010158130899071693, accuracy: 1.0\n",
      "loss: 0.01007376704365015, accuracy: 1.0\n",
      "loss: 0.009827573783695698, accuracy: 1.0\n",
      "loss: 0.009922719560563564, accuracy: 1.0\n",
      "loss: 0.009733528830111027, accuracy: 1.0\n",
      "loss: 0.010131173767149448, accuracy: 1.0\n",
      "loss: 0.010599460452795029, accuracy: 1.0\n",
      "loss: 0.010136948898434639, accuracy: 1.0\n",
      "loss: 0.010573336854577065, accuracy: 1.0\n",
      "loss: 0.00998702459037304, accuracy: 1.0\n",
      "loss: 0.010014322586357594, accuracy: 1.0\n",
      "loss: 0.009606770239770412, accuracy: 1.0\n",
      "loss: 0.010341244749724865, accuracy: 1.0\n",
      "loss: 0.009723066352307796, accuracy: 1.0\n",
      "loss: 0.009780246764421463, accuracy: 1.0\n",
      "loss: 0.010112909600138664, accuracy: 1.0\n",
      "loss: 0.010392745025455952, accuracy: 1.0\n",
      "loss: 0.01025058701634407, accuracy: 1.0\n",
      "loss: 0.009708344005048275, accuracy: 1.0\n",
      "loss: 0.01003789622336626, accuracy: 1.0\n",
      "loss: 0.009913234040141106, accuracy: 1.0\n",
      "loss: 0.009790370240807533, accuracy: 1.0\n",
      "loss: 0.009977557696402073, accuracy: 1.0\n",
      "loss: 0.009789649397134781, accuracy: 1.0\n",
      "loss: 0.009924064390361309, accuracy: 1.0\n",
      "loss: 0.010107400827109814, accuracy: 1.0\n",
      "loss: 0.009685943834483624, accuracy: 1.0\n",
      "loss: 0.010042999871075153, accuracy: 1.0\n",
      "loss: 0.010325921699404716, accuracy: 1.0\n",
      "loss: 0.009690193459391594, accuracy: 1.0\n",
      "loss: 0.009858367964625359, accuracy: 1.0\n",
      "loss: 0.009909591637551785, accuracy: 1.0\n",
      "loss: 0.009988806210458279, accuracy: 1.0\n",
      "loss: 0.010095912031829357, accuracy: 1.0\n",
      "loss: 0.01044091023504734, accuracy: 1.0\n",
      "loss: 0.009850725531578064, accuracy: 1.0\n",
      "loss: 0.009590836241841316, accuracy: 1.0\n",
      "loss: 0.009333131834864616, accuracy: 1.0\n",
      "loss: 0.010296591557562351, accuracy: 1.0\n",
      "loss: 0.00959242694079876, accuracy: 1.0\n",
      "loss: 0.009896751493215561, accuracy: 1.0\n",
      "loss: 0.009780330583453178, accuracy: 1.0\n",
      "loss: 0.009834597818553448, accuracy: 1.0\n",
      "loss: 0.00981852225959301, accuracy: 1.0\n",
      "loss: 0.009748470969498158, accuracy: 1.0\n",
      "loss: 0.009782779030501842, accuracy: 1.0\n",
      "loss: 0.009747158735990524, accuracy: 1.0\n",
      "loss: 0.009606924839317799, accuracy: 1.0\n",
      "loss: 0.009535416960716248, accuracy: 1.0\n",
      "loss: 0.01001032255589962, accuracy: 1.0\n",
      "loss: 0.009631923399865627, accuracy: 1.0\n",
      "loss: 0.009596745483577251, accuracy: 1.0\n",
      "loss: 0.00955367274582386, accuracy: 1.0\n",
      "loss: 0.009714048355817795, accuracy: 1.0\n",
      "loss: 0.00948566384613514, accuracy: 1.0\n",
      "loss: 0.00983837153762579, accuracy: 1.0\n",
      "loss: 0.00958227925002575, accuracy: 1.0\n",
      "loss: 0.009692134335637093, accuracy: 1.0\n",
      "loss: 0.00935329683125019, accuracy: 1.0\n",
      "loss: 0.01023949310183525, accuracy: 1.0\n",
      "loss: 0.009759611450135708, accuracy: 1.0\n",
      "loss: 0.010083375498652458, accuracy: 1.0\n",
      "loss: 0.009736137464642525, accuracy: 1.0\n",
      "loss: 0.009445792995393276, accuracy: 1.0\n",
      "loss: 0.009472714737057686, accuracy: 1.0\n",
      "loss: 0.009717727079987526, accuracy: 1.0\n",
      "loss: 0.009416639804840088, accuracy: 1.0\n",
      "loss: 0.00991139467805624, accuracy: 1.0\n",
      "loss: 0.009929142892360687, accuracy: 1.0\n",
      "loss: 0.009945554658770561, accuracy: 1.0\n",
      "loss: 0.009713923558592796, accuracy: 1.0\n",
      "loss: 0.009405916556715965, accuracy: 1.0\n",
      "loss: 0.009382897987961769, accuracy: 1.0\n",
      "loss: 0.009373809210956097, accuracy: 1.0\n",
      "loss: 0.009497947990894318, accuracy: 1.0\n",
      "loss: 0.00974125787615776, accuracy: 1.0\n",
      "loss: 0.009633059613406658, accuracy: 1.0\n",
      "loss: 0.009347033686935902, accuracy: 1.0\n",
      "loss: 0.00960979238152504, accuracy: 1.0\n",
      "loss: 0.010022877715528011, accuracy: 1.0\n",
      "loss: 0.009506612084805965, accuracy: 1.0\n",
      "loss: 0.009319081902503967, accuracy: 1.0\n",
      "loss: 0.009998816065490246, accuracy: 1.0\n",
      "loss: 0.00941822025924921, accuracy: 1.0\n",
      "loss: 0.009406797587871552, accuracy: 1.0\n",
      "loss: 0.009390534833073616, accuracy: 1.0\n",
      "loss: 0.009429513476788998, accuracy: 1.0\n",
      "loss: 0.00952567346394062, accuracy: 1.0\n",
      "loss: 0.009539545513689518, accuracy: 1.0\n",
      "loss: 0.00943609420210123, accuracy: 1.0\n",
      "loss: 0.009227466769516468, accuracy: 1.0\n",
      "loss: 0.00937578734010458, accuracy: 1.0\n",
      "loss: 0.009704092517495155, accuracy: 1.0\n",
      "loss: 0.009674947708845139, accuracy: 1.0\n",
      "loss: 0.009430142119526863, accuracy: 1.0\n",
      "loss: 0.009783072397112846, accuracy: 1.0\n",
      "loss: 0.009273638017475605, accuracy: 1.0\n",
      "loss: 0.00968503300100565, accuracy: 1.0\n",
      "loss: 0.009824391454458237, accuracy: 1.0\n",
      "loss: 0.00931208673864603, accuracy: 1.0\n",
      "loss: 0.009512689895927906, accuracy: 1.0\n",
      "loss: 0.009302066639065742, accuracy: 1.0\n",
      "loss: 0.009428763762116432, accuracy: 1.0\n",
      "loss: 0.009378138929605484, accuracy: 1.0\n",
      "loss: 0.00936022400856018, accuracy: 1.0\n",
      "loss: 0.009234418161213398, accuracy: 1.0\n",
      "loss: 0.009343095123767853, accuracy: 1.0\n",
      "loss: 0.00951564684510231, accuracy: 1.0\n",
      "loss: 0.009432048536837101, accuracy: 1.0\n",
      "loss: 0.009304107166826725, accuracy: 1.0\n",
      "loss: 0.009916963055729866, accuracy: 1.0\n",
      "loss: 0.009464538656175137, accuracy: 1.0\n",
      "loss: 0.009530414827167988, accuracy: 1.0\n",
      "loss: 0.009609109722077847, accuracy: 1.0\n",
      "loss: 0.009652221575379372, accuracy: 1.0\n",
      "loss: 0.009358396753668785, accuracy: 1.0\n",
      "loss: 0.008981885388493538, accuracy: 1.0\n",
      "loss: 0.00940701924264431, accuracy: 1.0\n",
      "loss: 0.010439523495733738, accuracy: 1.0\n",
      "loss: 0.010810324922204018, accuracy: 1.0\n",
      "loss: 0.00956906471401453, accuracy: 1.0\n",
      "loss: 0.009899880737066269, accuracy: 1.0\n",
      "loss: 0.009554044343531132, accuracy: 1.0\n",
      "loss: 0.009402048774063587, accuracy: 1.0\n",
      "loss: 0.009206661023199558, accuracy: 1.0\n",
      "loss: 0.009434796869754791, accuracy: 1.0\n",
      "loss: 0.009282347746193409, accuracy: 1.0\n",
      "loss: 0.00932975672185421, accuracy: 1.0\n",
      "loss: 0.009504269808530807, accuracy: 1.0\n",
      "loss: 0.009399504400789738, accuracy: 1.0\n",
      "loss: 0.009998259134590626, accuracy: 1.0\n",
      "loss: 0.009165802039206028, accuracy: 1.0\n",
      "loss: 0.009124386124312878, accuracy: 1.0\n",
      "loss: 0.009598903357982635, accuracy: 1.0\n",
      "loss: 0.009450092911720276, accuracy: 1.0\n",
      "loss: 0.00938870757818222, accuracy: 1.0\n",
      "loss: 0.009480435401201248, accuracy: 1.0\n",
      "loss: 0.009330082684755325, accuracy: 1.0\n",
      "loss: 0.009184157475829124, accuracy: 1.0\n",
      "loss: 0.009202494286000729, accuracy: 1.0\n",
      "loss: 0.009001706726849079, accuracy: 1.0\n",
      "loss: 0.009187852963805199, accuracy: 1.0\n",
      "loss: 0.009267223067581654, accuracy: 1.0\n",
      "loss: 0.009385550394654274, accuracy: 1.0\n",
      "loss: 0.009295327588915825, accuracy: 1.0\n",
      "loss: 0.009004168212413788, accuracy: 1.0\n",
      "loss: 0.00964699499309063, accuracy: 1.0\n",
      "loss: 0.009536863304674625, accuracy: 1.0\n",
      "loss: 0.009564368985593319, accuracy: 1.0\n",
      "loss: 0.009225145913660526, accuracy: 1.0\n",
      "loss: 0.009195017628371716, accuracy: 1.0\n",
      "loss: 0.009078886359930038, accuracy: 1.0\n",
      "loss: 0.009110231883823872, accuracy: 1.0\n",
      "loss: 0.00930090993642807, accuracy: 1.0\n",
      "loss: 0.009533354081213474, accuracy: 1.0\n",
      "loss: 0.00930801685899496, accuracy: 1.0\n",
      "loss: 0.009171927347779274, accuracy: 1.0\n",
      "loss: 0.009152432903647423, accuracy: 1.0\n",
      "loss: 0.009124699048697948, accuracy: 1.0\n",
      "loss: 0.009040942415595055, accuracy: 1.0\n",
      "loss: 0.00942076276987791, accuracy: 1.0\n",
      "loss: 0.009416932240128517, accuracy: 1.0\n",
      "loss: 0.00914017204195261, accuracy: 1.0\n",
      "loss: 0.008905565366148949, accuracy: 1.0\n",
      "loss: 0.009535951539874077, accuracy: 1.0\n",
      "loss: 0.00894962903112173, accuracy: 1.0\n",
      "loss: 0.01015076320618391, accuracy: 1.0\n",
      "loss: 0.009158818982541561, accuracy: 1.0\n",
      "loss: 0.009121136739850044, accuracy: 1.0\n",
      "loss: 0.009023995138704777, accuracy: 1.0\n",
      "loss: 0.009639449417591095, accuracy: 1.0\n",
      "loss: 0.009049312211573124, accuracy: 1.0\n",
      "loss: 0.009126552380621433, accuracy: 1.0\n",
      "loss: 0.00934571959078312, accuracy: 1.0\n",
      "loss: 0.009162181057035923, accuracy: 1.0\n",
      "loss: 0.009645254351198673, accuracy: 1.0\n",
      "loss: 0.009103020653128624, accuracy: 1.0\n",
      "loss: 0.008844644762575626, accuracy: 1.0\n",
      "loss: 0.009363231249153614, accuracy: 1.0\n",
      "loss: 0.009125812910497189, accuracy: 1.0\n",
      "loss: 0.009545065462589264, accuracy: 1.0\n",
      "loss: 0.00932740606367588, accuracy: 1.0\n",
      "loss: 0.009365497156977654, accuracy: 1.0\n",
      "loss: 0.009503385052084923, accuracy: 1.0\n",
      "loss: 0.008941096253693104, accuracy: 1.0\n",
      "loss: 0.008743943646550179, accuracy: 1.0\n",
      "loss: 0.008918124251067638, accuracy: 1.0\n",
      "loss: 0.008916904218494892, accuracy: 1.0\n",
      "loss: 0.008875260129570961, accuracy: 1.0\n",
      "loss: 0.009159833192825317, accuracy: 1.0\n",
      "loss: 0.009047343395650387, accuracy: 1.0\n",
      "loss: 0.009564121253788471, accuracy: 1.0\n",
      "loss: 0.008933315984904766, accuracy: 1.0\n",
      "loss: 0.009092356078326702, accuracy: 1.0\n",
      "loss: 0.008848585188388824, accuracy: 1.0\n",
      "loss: 0.008894991129636765, accuracy: 1.0\n",
      "loss: 0.009220453910529613, accuracy: 1.0\n",
      "loss: 0.008880828507244587, accuracy: 1.0\n",
      "loss: 0.009024528786540031, accuracy: 1.0\n",
      "loss: 0.008940919302403927, accuracy: 1.0\n",
      "loss: 0.008896999061107635, accuracy: 1.0\n",
      "loss: 0.009470421820878983, accuracy: 1.0\n",
      "loss: 0.009207576513290405, accuracy: 1.0\n",
      "loss: 0.00873134471476078, accuracy: 1.0\n",
      "loss: 0.008867848664522171, accuracy: 1.0\n",
      "loss: 0.008798036724328995, accuracy: 1.0\n",
      "loss: 0.009325036779046059, accuracy: 1.0\n",
      "loss: 0.008775850757956505, accuracy: 1.0\n",
      "loss: 0.009625659324228764, accuracy: 1.0\n",
      "loss: 0.008897082880139351, accuracy: 1.0\n",
      "loss: 0.009190612472593784, accuracy: 1.0\n",
      "loss: 0.008774432353675365, accuracy: 1.0\n",
      "loss: 0.00891153048723936, accuracy: 1.0\n",
      "loss: 0.009050147607922554, accuracy: 1.0\n",
      "loss: 0.009044677950441837, accuracy: 1.0\n",
      "loss: 0.008936304599046707, accuracy: 1.0\n",
      "loss: 0.009129059500992298, accuracy: 1.0\n",
      "loss: 0.008937527425587177, accuracy: 1.0\n",
      "loss: 0.009101632982492447, accuracy: 1.0\n",
      "loss: 0.008901317603886127, accuracy: 1.0\n",
      "loss: 0.008944638073444366, accuracy: 1.0\n",
      "loss: 0.009150823578238487, accuracy: 1.0\n",
      "loss: 0.009369189850986004, accuracy: 1.0\n",
      "loss: 0.008695919997990131, accuracy: 1.0\n",
      "loss: 0.009105505421757698, accuracy: 1.0\n",
      "loss: 0.00900089181959629, accuracy: 1.0\n",
      "loss: 0.008843894116580486, accuracy: 1.0\n",
      "loss: 0.008983364328742027, accuracy: 1.0\n",
      "loss: 0.008931618183851242, accuracy: 1.0\n",
      "loss: 0.008703474886715412, accuracy: 1.0\n",
      "loss: 0.008919350802898407, accuracy: 1.0\n",
      "loss: 0.008803946897387505, accuracy: 1.0\n",
      "loss: 0.008882650174200535, accuracy: 1.0\n",
      "loss: 0.008846665732562542, accuracy: 1.0\n",
      "loss: 0.008869440294802189, accuracy: 1.0\n",
      "loss: 0.008730697445571423, accuracy: 1.0\n",
      "loss: 0.008808588609099388, accuracy: 1.0\n",
      "loss: 0.008688971400260925, accuracy: 1.0\n",
      "loss: 0.00883809756487608, accuracy: 1.0\n",
      "loss: 0.008735387586057186, accuracy: 1.0\n",
      "loss: 0.00905758049339056, accuracy: 1.0\n",
      "loss: 0.010072184726595879, accuracy: 1.0\n",
      "loss: 0.008954539895057678, accuracy: 1.0\n",
      "loss: 0.00902883242815733, accuracy: 1.0\n",
      "loss: 0.00872240774333477, accuracy: 1.0\n",
      "loss: 0.00902123749256134, accuracy: 1.0\n",
      "loss: 0.008748902939260006, accuracy: 1.0\n",
      "loss: 0.008909717202186584, accuracy: 1.0\n",
      "loss: 0.008595792576670647, accuracy: 1.0\n",
      "loss: 0.008885561488568783, accuracy: 1.0\n",
      "loss: 0.008773203007876873, accuracy: 1.0\n",
      "loss: 0.008946838788688183, accuracy: 1.0\n",
      "loss: 0.008653122000396252, accuracy: 1.0\n",
      "loss: 0.008843133226037025, accuracy: 1.0\n",
      "loss: 0.008978120982646942, accuracy: 1.0\n",
      "loss: 0.0094535481184721, accuracy: 1.0\n",
      "loss: 0.008753791451454163, accuracy: 1.0\n",
      "loss: 0.008967544883489609, accuracy: 1.0\n",
      "loss: 0.008570224978029728, accuracy: 1.0\n",
      "loss: 0.008765874430537224, accuracy: 1.0\n",
      "loss: 0.008905896916985512, accuracy: 1.0\n",
      "loss: 0.008843880146741867, accuracy: 1.0\n",
      "loss: 0.008721239864826202, accuracy: 1.0\n",
      "loss: 0.00840265303850174, accuracy: 1.0\n",
      "loss: 0.008767904713749886, accuracy: 1.0\n",
      "loss: 0.008615558966994286, accuracy: 1.0\n",
      "loss: 0.00903543271124363, accuracy: 1.0\n",
      "loss: 0.008641588501632214, accuracy: 1.0\n",
      "loss: 0.008707068860530853, accuracy: 1.0\n",
      "loss: 0.008753196336328983, accuracy: 1.0\n",
      "loss: 0.008905083872377872, accuracy: 1.0\n",
      "loss: 0.008859099820256233, accuracy: 1.0\n",
      "loss: 0.008495477959513664, accuracy: 1.0\n",
      "loss: 0.008691400289535522, accuracy: 1.0\n",
      "loss: 0.008639038540422916, accuracy: 1.0\n",
      "loss: 0.008737755008041859, accuracy: 1.0\n",
      "loss: 0.008318145759403706, accuracy: 1.0\n",
      "loss: 0.008706723339855671, accuracy: 1.0\n",
      "loss: 0.008775103837251663, accuracy: 1.0\n",
      "loss: 0.008564604446291924, accuracy: 1.0\n",
      "loss: 0.008962049148976803, accuracy: 1.0\n",
      "loss: 0.009652073495090008, accuracy: 1.0\n",
      "loss: 0.008614171296358109, accuracy: 1.0\n",
      "loss: 0.008479803800582886, accuracy: 1.0\n",
      "loss: 0.008518265560269356, accuracy: 1.0\n",
      "loss: 0.008961150422692299, accuracy: 1.0\n",
      "loss: 0.00853540375828743, accuracy: 1.0\n",
      "loss: 0.008617901243269444, accuracy: 1.0\n",
      "loss: 0.00851497147232294, accuracy: 1.0\n",
      "loss: 0.008532293140888214, accuracy: 1.0\n",
      "loss: 0.008632086217403412, accuracy: 1.0\n",
      "loss: 0.008851450867950916, accuracy: 1.0\n",
      "loss: 0.009005030617117882, accuracy: 1.0\n",
      "loss: 0.008581828325986862, accuracy: 1.0\n",
      "loss: 0.008785699494183064, accuracy: 1.0\n",
      "loss: 0.0083576375618577, accuracy: 1.0\n",
      "loss: 0.008596579544246197, accuracy: 1.0\n",
      "loss: 0.00887624453753233, accuracy: 1.0\n",
      "loss: 0.008481130003929138, accuracy: 1.0\n",
      "loss: 0.008844560943543911, accuracy: 1.0\n",
      "loss: 0.008776333183050156, accuracy: 1.0\n",
      "loss: 0.008976503275334835, accuracy: 1.0\n",
      "loss: 0.008550324477255344, accuracy: 1.0\n",
      "loss: 0.00861729122698307, accuracy: 1.0\n",
      "loss: 0.008833849802613258, accuracy: 1.0\n",
      "loss: 0.008503492921590805, accuracy: 1.0\n",
      "loss: 0.008770467713475227, accuracy: 1.0\n",
      "loss: 0.008572548627853394, accuracy: 1.0\n",
      "loss: 0.008675397373735905, accuracy: 1.0\n",
      "loss: 0.008574925363063812, accuracy: 1.0\n",
      "loss: 0.008661183528602123, accuracy: 1.0\n",
      "loss: 0.008656649850308895, accuracy: 1.0\n",
      "loss: 0.008449976332485676, accuracy: 1.0\n",
      "loss: 0.008460210636258125, accuracy: 1.0\n",
      "loss: 0.008578183129429817, accuracy: 1.0\n",
      "loss: 0.008521078154444695, accuracy: 1.0\n",
      "loss: 0.008521397598087788, accuracy: 1.0\n",
      "loss: 0.00844980962574482, accuracy: 1.0\n",
      "loss: 0.008428414352238178, accuracy: 1.0\n",
      "loss: 0.008556400425732136, accuracy: 1.0\n",
      "loss: 0.008515367284417152, accuracy: 1.0\n",
      "loss: 0.008360090665519238, accuracy: 1.0\n",
      "loss: 0.008356979116797447, accuracy: 1.0\n",
      "loss: 0.00851435773074627, accuracy: 1.0\n",
      "loss: 0.00885757151991129, accuracy: 1.0\n",
      "loss: 0.008420877158641815, accuracy: 1.0\n",
      "loss: 0.008470815606415272, accuracy: 1.0\n",
      "loss: 0.008703685365617275, accuracy: 1.0\n",
      "loss: 0.00834938045591116, accuracy: 1.0\n",
      "loss: 0.008311554789543152, accuracy: 1.0\n",
      "loss: 0.008402775041759014, accuracy: 1.0\n",
      "loss: 0.008497143164277077, accuracy: 1.0\n",
      "loss: 0.008356411941349506, accuracy: 1.0\n",
      "loss: 0.00870341993868351, accuracy: 1.0\n",
      "loss: 0.008821495808660984, accuracy: 1.0\n",
      "loss: 0.008438892662525177, accuracy: 1.0\n",
      "loss: 0.008344822563230991, accuracy: 1.0\n",
      "loss: 0.008626122027635574, accuracy: 1.0\n",
      "loss: 0.008197516202926636, accuracy: 1.0\n",
      "loss: 0.00901980884373188, accuracy: 1.0\n",
      "loss: 0.008619248867034912, accuracy: 1.0\n",
      "loss: 0.008241980336606503, accuracy: 1.0\n",
      "loss: 0.008459399454295635, accuracy: 1.0\n",
      "loss: 0.008310145698487759, accuracy: 1.0\n",
      "loss: 0.008675667457282543, accuracy: 1.0\n",
      "loss: 0.008476155810058117, accuracy: 1.0\n",
      "loss: 0.0083395941182971, accuracy: 1.0\n",
      "loss: 0.008747528307139874, accuracy: 1.0\n",
      "loss: 0.008885083720088005, accuracy: 1.0\n",
      "loss: 0.008239452727138996, accuracy: 1.0\n",
      "loss: 0.008293506689369678, accuracy: 1.0\n",
      "loss: 0.008370610885322094, accuracy: 1.0\n",
      "loss: 0.008213553577661514, accuracy: 1.0\n",
      "loss: 0.008250276558101177, accuracy: 1.0\n",
      "loss: 0.008484036661684513, accuracy: 1.0\n",
      "loss: 0.008326306007802486, accuracy: 1.0\n",
      "loss: 0.008418734185397625, accuracy: 1.0\n",
      "loss: 0.008350837044417858, accuracy: 1.0\n",
      "loss: 0.008436856791377068, accuracy: 1.0\n",
      "loss: 0.008312476798892021, accuracy: 1.0\n",
      "loss: 0.008187847211956978, accuracy: 1.0\n",
      "loss: 0.00817921757698059, accuracy: 1.0\n",
      "loss: 0.008146892301738262, accuracy: 1.0\n",
      "loss: 0.008536082692444324, accuracy: 1.0\n",
      "loss: 0.008345017209649086, accuracy: 1.0\n",
      "loss: 0.008417115546762943, accuracy: 1.0\n",
      "loss: 0.007977836765348911, accuracy: 1.0\n",
      "loss: 0.008394662290811539, accuracy: 1.0\n",
      "loss: 0.00858297012746334, accuracy: 1.0\n",
      "loss: 0.00867402832955122, accuracy: 1.0\n",
      "loss: 0.00831600558012724, accuracy: 1.0\n",
      "loss: 0.008161541074514389, accuracy: 1.0\n",
      "loss: 0.008250170387327671, accuracy: 1.0\n",
      "loss: 0.0084694130346179, accuracy: 1.0\n",
      "loss: 0.008525959216058254, accuracy: 1.0\n",
      "loss: 0.008284348994493484, accuracy: 1.0\n",
      "loss: 0.008146355859935284, accuracy: 1.0\n",
      "loss: 0.008354270830750465, accuracy: 1.0\n",
      "loss: 0.008245952427387238, accuracy: 1.0\n",
      "loss: 0.008141874335706234, accuracy: 1.0\n",
      "loss: 0.00921344943344593, accuracy: 1.0\n",
      "loss: 0.008225001394748688, accuracy: 1.0\n",
      "loss: 0.0083342045545578, accuracy: 1.0\n",
      "loss: 0.008652891032397747, accuracy: 1.0\n",
      "loss: 0.008343363180756569, accuracy: 1.0\n",
      "loss: 0.008106064051389694, accuracy: 1.0\n",
      "loss: 0.008329713717103004, accuracy: 1.0\n",
      "loss: 0.009009496308863163, accuracy: 1.0\n",
      "loss: 0.008229533210396767, accuracy: 1.0\n",
      "loss: 0.00854509137570858, accuracy: 1.0\n",
      "loss: 0.008424443192780018, accuracy: 1.0\n",
      "loss: 0.008206994272768497, accuracy: 1.0\n",
      "loss: 0.008436838164925575, accuracy: 1.0\n",
      "loss: 0.008160405792295933, accuracy: 1.0\n",
      "loss: 0.008315427228808403, accuracy: 1.0\n",
      "loss: 0.008137762546539307, accuracy: 1.0\n",
      "loss: 0.008124089799821377, accuracy: 1.0\n",
      "loss: 0.008392805233597755, accuracy: 1.0\n",
      "loss: 0.00819068681448698, accuracy: 1.0\n",
      "loss: 0.008150321431457996, accuracy: 1.0\n",
      "loss: 0.008230765350162983, accuracy: 1.0\n",
      "loss: 0.008219026029109955, accuracy: 1.0\n",
      "loss: 0.008309843949973583, accuracy: 1.0\n",
      "loss: 0.008172205649316311, accuracy: 1.0\n",
      "loss: 0.008368182927370071, accuracy: 1.0\n",
      "loss: 0.008671815507113934, accuracy: 1.0\n",
      "loss: 0.00784994661808014, accuracy: 1.0\n",
      "loss: 0.008523311465978622, accuracy: 1.0\n",
      "loss: 0.008114861324429512, accuracy: 1.0\n",
      "loss: 0.008301600813865662, accuracy: 1.0\n",
      "loss: 0.00805130135267973, accuracy: 1.0\n",
      "loss: 0.008599271066486835, accuracy: 1.0\n",
      "loss: 0.008013675920665264, accuracy: 1.0\n",
      "loss: 0.008331205695867538, accuracy: 1.0\n",
      "loss: 0.008036687038838863, accuracy: 1.0\n",
      "loss: 0.008051710203289986, accuracy: 1.0\n",
      "loss: 0.00829915702342987, accuracy: 1.0\n",
      "loss: 0.008489754982292652, accuracy: 1.0\n",
      "loss: 0.007935554720461369, accuracy: 1.0\n",
      "loss: 0.008076725527644157, accuracy: 1.0\n",
      "loss: 0.00795238371938467, accuracy: 1.0\n",
      "loss: 0.008138538338243961, accuracy: 1.0\n",
      "loss: 0.008007068186998367, accuracy: 1.0\n",
      "loss: 0.008036639541387558, accuracy: 1.0\n",
      "loss: 0.008147732354700565, accuracy: 1.0\n",
      "loss: 0.008533403277397156, accuracy: 1.0\n",
      "loss: 0.008005806244909763, accuracy: 1.0\n",
      "loss: 0.008039725944399834, accuracy: 1.0\n",
      "loss: 0.0081289391964674, accuracy: 1.0\n",
      "loss: 0.008424893021583557, accuracy: 1.0\n",
      "loss: 0.008224871009588242, accuracy: 1.0\n",
      "loss: 0.00825450848788023, accuracy: 1.0\n",
      "loss: 0.00808798149228096, accuracy: 1.0\n",
      "loss: 0.008192351087927818, accuracy: 1.0\n",
      "loss: 0.008107159286737442, accuracy: 1.0\n",
      "loss: 0.008061270229518414, accuracy: 1.0\n",
      "loss: 0.008163735270500183, accuracy: 1.0\n",
      "loss: 0.00806702021509409, accuracy: 1.0\n",
      "loss: 0.007918504066765308, accuracy: 1.0\n",
      "loss: 0.00796576403081417, accuracy: 1.0\n",
      "loss: 0.007863201200962067, accuracy: 1.0\n",
      "loss: 0.008224943652749062, accuracy: 1.0\n",
      "loss: 0.007946896366775036, accuracy: 1.0\n",
      "loss: 0.008064930327236652, accuracy: 1.0\n",
      "loss: 0.00838027149438858, accuracy: 1.0\n",
      "loss: 0.008360727690160275, accuracy: 1.0\n",
      "loss: 0.008178449235856533, accuracy: 1.0\n",
      "loss: 0.008343321271240711, accuracy: 1.0\n",
      "loss: 0.008184118196368217, accuracy: 1.0\n",
      "loss: 0.008098773658275604, accuracy: 1.0\n",
      "loss: 0.008381767198443413, accuracy: 1.0\n",
      "loss: 0.008144227787852287, accuracy: 1.0\n",
      "loss: 0.008232100866734982, accuracy: 1.0\n",
      "loss: 0.008374522440135479, accuracy: 1.0\n",
      "loss: 0.0077647012658417225, accuracy: 1.0\n",
      "loss: 0.007985598407685757, accuracy: 1.0\n",
      "loss: 0.00818326510488987, accuracy: 1.0\n",
      "loss: 0.007894201204180717, accuracy: 1.0\n",
      "loss: 0.008304154500365257, accuracy: 1.0\n",
      "loss: 0.007918954826891422, accuracy: 1.0\n",
      "loss: 0.008306233212351799, accuracy: 1.0\n",
      "loss: 0.008476229384541512, accuracy: 1.0\n",
      "loss: 0.007938902825117111, accuracy: 1.0\n",
      "loss: 0.008028394542634487, accuracy: 1.0\n",
      "loss: 0.007980418391525745, accuracy: 1.0\n",
      "loss: 0.008231287822127342, accuracy: 1.0\n",
      "loss: 0.008101587183773518, accuracy: 1.0\n",
      "loss: 0.007797211408615112, accuracy: 1.0\n",
      "loss: 0.00806997437030077, accuracy: 1.0\n",
      "loss: 0.007778218016028404, accuracy: 1.0\n",
      "loss: 0.00807818304747343, accuracy: 1.0\n",
      "loss: 0.007803682237863541, accuracy: 1.0\n",
      "loss: 0.008227784186601639, accuracy: 1.0\n",
      "loss: 0.007833899930119514, accuracy: 1.0\n",
      "loss: 0.007925663143396378, accuracy: 1.0\n",
      "loss: 0.007780118845403194, accuracy: 1.0\n",
      "loss: 0.008184152655303478, accuracy: 1.0\n",
      "loss: 0.007981657050549984, accuracy: 1.0\n",
      "loss: 0.007720184978097677, accuracy: 1.0\n",
      "loss: 0.008081486448645592, accuracy: 1.0\n",
      "loss: 0.008026435039937496, accuracy: 1.0\n",
      "loss: 0.007916001603007317, accuracy: 1.0\n",
      "loss: 0.007893281057476997, accuracy: 1.0\n",
      "loss: 0.008033070713281631, accuracy: 1.0\n",
      "loss: 0.007861948572099209, accuracy: 1.0\n",
      "loss: 0.008168911561369896, accuracy: 1.0\n",
      "loss: 0.007942447438836098, accuracy: 1.0\n",
      "loss: 0.007892698980867863, accuracy: 1.0\n",
      "loss: 0.008108453825116158, accuracy: 1.0\n",
      "loss: 0.008327661082148552, accuracy: 1.0\n",
      "loss: 0.007781894877552986, accuracy: 1.0\n",
      "loss: 0.008088541217148304, accuracy: 1.0\n",
      "loss: 0.007920563220977783, accuracy: 1.0\n",
      "loss: 0.00785900093615055, accuracy: 1.0\n",
      "loss: 0.007900468073785305, accuracy: 1.0\n",
      "loss: 0.007709043100476265, accuracy: 1.0\n",
      "loss: 0.007892400026321411, accuracy: 1.0\n",
      "loss: 0.007949636317789555, accuracy: 1.0\n",
      "loss: 0.00810919888317585, accuracy: 1.0\n",
      "loss: 0.007742841262370348, accuracy: 1.0\n",
      "loss: 0.007616072427481413, accuracy: 1.0\n",
      "loss: 0.00810752809047699, accuracy: 1.0\n",
      "loss: 0.007864819839596748, accuracy: 1.0\n",
      "loss: 0.008224542252719402, accuracy: 1.0\n",
      "loss: 0.00810703169554472, accuracy: 1.0\n",
      "loss: 0.008099992759525776, accuracy: 1.0\n",
      "loss: 0.0080428346991539, accuracy: 1.0\n",
      "loss: 0.007799672428518534, accuracy: 1.0\n",
      "loss: 0.007614354137331247, accuracy: 1.0\n",
      "loss: 0.007814041338860989, accuracy: 1.0\n",
      "loss: 0.00779698183760047, accuracy: 1.0\n",
      "loss: 0.007583769038319588, accuracy: 1.0\n",
      "loss: 0.007733441423624754, accuracy: 1.0\n",
      "loss: 0.007922045886516571, accuracy: 1.0\n",
      "loss: 0.008088205941021442, accuracy: 1.0\n",
      "loss: 0.007823551073670387, accuracy: 1.0\n",
      "loss: 0.007799182552844286, accuracy: 1.0\n",
      "loss: 0.007696190848946571, accuracy: 1.0\n",
      "loss: 0.007633158937096596, accuracy: 1.0\n",
      "loss: 0.008081723004579544, accuracy: 1.0\n",
      "loss: 0.008060887455940247, accuracy: 1.0\n",
      "loss: 0.007779017090797424, accuracy: 1.0\n",
      "loss: 0.007782800123095512, accuracy: 1.0\n",
      "loss: 0.007918830029666424, accuracy: 1.0\n",
      "loss: 0.007841733284294605, accuracy: 1.0\n",
      "loss: 0.00816369242966175, accuracy: 1.0\n",
      "loss: 0.007764068432152271, accuracy: 1.0\n",
      "loss: 0.007942487485706806, accuracy: 1.0\n",
      "loss: 0.007948803715407848, accuracy: 1.0\n",
      "loss: 0.007879231125116348, accuracy: 1.0\n",
      "loss: 0.007701622322201729, accuracy: 1.0\n",
      "loss: 0.007802731357514858, accuracy: 1.0\n",
      "loss: 0.007830723188817501, accuracy: 1.0\n",
      "loss: 0.007677646819502115, accuracy: 1.0\n",
      "loss: 0.007623914163559675, accuracy: 1.0\n",
      "loss: 0.007993845269083977, accuracy: 1.0\n",
      "loss: 0.0075544374994933605, accuracy: 1.0\n",
      "loss: 0.007771944161504507, accuracy: 1.0\n",
      "loss: 0.00754921929910779, accuracy: 1.0\n",
      "loss: 0.0076412116177380085, accuracy: 1.0\n",
      "loss: 0.007698763161897659, accuracy: 1.0\n",
      "loss: 0.007815120741724968, accuracy: 1.0\n",
      "loss: 0.0078105139546096325, accuracy: 1.0\n",
      "loss: 0.007679978851228952, accuracy: 1.0\n",
      "loss: 0.007695660926401615, accuracy: 1.0\n",
      "loss: 0.007635794579982758, accuracy: 1.0\n",
      "loss: 0.007636419031769037, accuracy: 1.0\n",
      "loss: 0.007457059342414141, accuracy: 1.0\n",
      "loss: 0.007529870141297579, accuracy: 1.0\n",
      "loss: 0.00791675690561533, accuracy: 1.0\n",
      "loss: 0.007804738357663155, accuracy: 1.0\n",
      "loss: 0.0075451708398759365, accuracy: 1.0\n",
      "loss: 0.007958411239087582, accuracy: 1.0\n",
      "loss: 0.007520962040871382, accuracy: 1.0\n",
      "loss: 0.007556546479463577, accuracy: 1.0\n",
      "loss: 0.007820205762982368, accuracy: 1.0\n",
      "loss: 0.007979667745530605, accuracy: 1.0\n",
      "loss: 0.007461792789399624, accuracy: 1.0\n",
      "loss: 0.007561550009995699, accuracy: 1.0\n",
      "loss: 0.007736051455140114, accuracy: 1.0\n",
      "loss: 0.007582871243357658, accuracy: 1.0\n",
      "loss: 0.007545511703938246, accuracy: 1.0\n",
      "loss: 0.007991551421582699, accuracy: 1.0\n",
      "loss: 0.007574310526251793, accuracy: 1.0\n",
      "loss: 0.007684855721890926, accuracy: 1.0\n",
      "loss: 0.007738426793366671, accuracy: 1.0\n",
      "loss: 0.0077158864587545395, accuracy: 1.0\n",
      "loss: 0.007924506440758705, accuracy: 1.0\n",
      "loss: 0.007693661376833916, accuracy: 1.0\n",
      "loss: 0.007552724331617355, accuracy: 1.0\n",
      "loss: 0.007818738929927349, accuracy: 1.0\n",
      "loss: 0.007569741923362017, accuracy: 1.0\n",
      "loss: 0.007554484996944666, accuracy: 1.0\n",
      "loss: 0.007479476742446423, accuracy: 1.0\n",
      "loss: 0.007667127996683121, accuracy: 1.0\n",
      "loss: 0.0078098406083881855, accuracy: 1.0\n",
      "loss: 0.007677478715777397, accuracy: 1.0\n",
      "loss: 0.007797365542501211, accuracy: 1.0\n",
      "loss: 0.007480604574084282, accuracy: 1.0\n",
      "loss: 0.007407025899738073, accuracy: 1.0\n",
      "loss: 0.007872586138546467, accuracy: 1.0\n",
      "loss: 0.007543994579464197, accuracy: 1.0\n",
      "loss: 0.007569196633994579, accuracy: 1.0\n",
      "loss: 0.007854059338569641, accuracy: 1.0\n",
      "loss: 0.007321692071855068, accuracy: 1.0\n",
      "loss: 0.0074184732511639595, accuracy: 1.0\n",
      "loss: 0.007561998907476664, accuracy: 1.0\n",
      "loss: 0.007356979418545961, accuracy: 1.0\n",
      "loss: 0.007644387427717447, accuracy: 1.0\n",
      "loss: 0.00767035037279129, accuracy: 1.0\n",
      "loss: 0.008150668814778328, accuracy: 1.0\n",
      "loss: 0.007487485185265541, accuracy: 1.0\n",
      "loss: 0.007513039745390415, accuracy: 1.0\n",
      "loss: 0.007375752087682486, accuracy: 1.0\n",
      "loss: 0.007562953047454357, accuracy: 1.0\n",
      "loss: 0.007715897634625435, accuracy: 1.0\n",
      "loss: 0.007600150071084499, accuracy: 1.0\n",
      "loss: 0.007596106268465519, accuracy: 1.0\n",
      "loss: 0.0075035011395812035, accuracy: 1.0\n",
      "loss: 0.007462992332875729, accuracy: 1.0\n",
      "loss: 0.007425898220390081, accuracy: 1.0\n",
      "loss: 0.007371910382062197, accuracy: 1.0\n",
      "loss: 0.007654257584363222, accuracy: 1.0\n",
      "loss: 0.007538095116615295, accuracy: 1.0\n",
      "loss: 0.007454521022737026, accuracy: 1.0\n",
      "loss: 0.007436639163643122, accuracy: 1.0\n",
      "loss: 0.007681922987103462, accuracy: 1.0\n",
      "loss: 0.007627575658261776, accuracy: 1.0\n",
      "loss: 0.0075043849647045135, accuracy: 1.0\n",
      "loss: 0.007308299653232098, accuracy: 1.0\n",
      "loss: 0.0076265716925263405, accuracy: 1.0\n",
      "loss: 0.007648190017789602, accuracy: 1.0\n",
      "loss: 0.007472260855138302, accuracy: 1.0\n",
      "loss: 0.007575541269034147, accuracy: 1.0\n",
      "loss: 0.007483920082449913, accuracy: 1.0\n",
      "loss: 0.007663306314498186, accuracy: 1.0\n",
      "loss: 0.00761145306751132, accuracy: 1.0\n",
      "loss: 0.00754776643589139, accuracy: 1.0\n",
      "loss: 0.007739982567727566, accuracy: 1.0\n",
      "loss: 0.007530748378485441, accuracy: 1.0\n",
      "loss: 0.007390373386442661, accuracy: 1.0\n",
      "loss: 0.007457666099071503, accuracy: 1.0\n",
      "loss: 0.007658079266548157, accuracy: 1.0\n",
      "loss: 0.007582133635878563, accuracy: 1.0\n",
      "loss: 0.007404263596981764, accuracy: 1.0\n",
      "loss: 0.00739069702103734, accuracy: 1.0\n",
      "loss: 0.007356401067227125, accuracy: 1.0\n",
      "loss: 0.007531192619353533, accuracy: 1.0\n",
      "loss: 0.007777838502079248, accuracy: 1.0\n",
      "loss: 0.007356190122663975, accuracy: 1.0\n",
      "loss: 0.007405674550682306, accuracy: 1.0\n",
      "loss: 0.007684255484491587, accuracy: 1.0\n",
      "loss: 0.007354417350143194, accuracy: 1.0\n",
      "loss: 0.007509916089475155, accuracy: 1.0\n",
      "loss: 0.007405238226056099, accuracy: 1.0\n",
      "loss: 0.007448780816048384, accuracy: 1.0\n",
      "loss: 0.007229654584079981, accuracy: 1.0\n",
      "loss: 0.007426478900015354, accuracy: 1.0\n",
      "loss: 0.00718052638694644, accuracy: 1.0\n",
      "loss: 0.00740219047293067, accuracy: 1.0\n",
      "loss: 0.007216522935777903, accuracy: 1.0\n",
      "loss: 0.007678424008190632, accuracy: 1.0\n",
      "loss: 0.007419845089316368, accuracy: 1.0\n",
      "loss: 0.007452591322362423, accuracy: 1.0\n",
      "loss: 0.007339008618146181, accuracy: 1.0\n",
      "loss: 0.008086676709353924, accuracy: 1.0\n",
      "loss: 0.0073725017718970776, accuracy: 1.0\n",
      "loss: 0.007848931476473808, accuracy: 1.0\n",
      "loss: 0.00733229098841548, accuracy: 1.0\n",
      "loss: 0.007582191377878189, accuracy: 1.0\n",
      "loss: 0.00751953199505806, accuracy: 1.0\n",
      "loss: 0.007700515910983086, accuracy: 1.0\n",
      "loss: 0.007289049215614796, accuracy: 1.0\n",
      "loss: 0.007635772693902254, accuracy: 1.0\n",
      "loss: 0.007371928542852402, accuracy: 1.0\n",
      "loss: 0.007362127769738436, accuracy: 1.0\n",
      "loss: 0.00749170919880271, accuracy: 1.0\n",
      "loss: 0.007392061408609152, accuracy: 1.0\n",
      "loss: 0.007347797974944115, accuracy: 1.0\n",
      "loss: 0.007313666399568319, accuracy: 1.0\n",
      "loss: 0.0073022907599806786, accuracy: 1.0\n",
      "loss: 0.007351342588663101, accuracy: 1.0\n",
      "loss: 0.007344231009483337, accuracy: 1.0\n",
      "loss: 0.007379430346190929, accuracy: 1.0\n",
      "loss: 0.007271159905940294, accuracy: 1.0\n",
      "loss: 0.007288292981684208, accuracy: 1.0\n",
      "loss: 0.007195327430963516, accuracy: 1.0\n",
      "loss: 0.0074211712926626205, accuracy: 1.0\n",
      "loss: 0.007370621431618929, accuracy: 1.0\n",
      "loss: 0.007300045806914568, accuracy: 1.0\n",
      "loss: 0.007261798717081547, accuracy: 1.0\n",
      "loss: 0.0072268228977918625, accuracy: 1.0\n",
      "loss: 0.007317301817238331, accuracy: 1.0\n",
      "loss: 0.007278170436620712, accuracy: 1.0\n",
      "loss: 0.007373695727437735, accuracy: 1.0\n",
      "loss: 0.007318419869989157, accuracy: 1.0\n",
      "loss: 0.0075202020816504955, accuracy: 1.0\n",
      "loss: 0.007372586522251368, accuracy: 1.0\n",
      "loss: 0.007328438106924295, accuracy: 1.0\n",
      "loss: 0.007241202052682638, accuracy: 1.0\n",
      "loss: 0.007189032156020403, accuracy: 1.0\n",
      "loss: 0.007453406695276499, accuracy: 1.0\n",
      "loss: 0.007061481475830078, accuracy: 1.0\n",
      "loss: 0.007326251361519098, accuracy: 1.0\n",
      "loss: 0.0073866210877895355, accuracy: 1.0\n",
      "loss: 0.007341525051742792, accuracy: 1.0\n",
      "loss: 0.007415790110826492, accuracy: 1.0\n",
      "loss: 0.007131279446184635, accuracy: 1.0\n",
      "loss: 0.007227321621030569, accuracy: 1.0\n",
      "loss: 0.0071923150680959225, accuracy: 1.0\n",
      "loss: 0.007181741297245026, accuracy: 1.0\n",
      "loss: 0.00714414706453681, accuracy: 1.0\n",
      "loss: 0.007625177502632141, accuracy: 1.0\n",
      "loss: 0.007320147007703781, accuracy: 1.0\n",
      "loss: 0.007415114436298609, accuracy: 1.0\n",
      "loss: 0.007413082756102085, accuracy: 1.0\n",
      "loss: 0.007220931351184845, accuracy: 1.0\n",
      "loss: 0.007166145369410515, accuracy: 1.0\n",
      "loss: 0.007249959744513035, accuracy: 1.0\n",
      "loss: 0.007037114351987839, accuracy: 1.0\n",
      "loss: 0.007195947226136923, accuracy: 1.0\n",
      "loss: 0.007380474824458361, accuracy: 1.0\n",
      "loss: 0.007351486943662167, accuracy: 1.0\n",
      "loss: 0.007401949726045132, accuracy: 1.0\n",
      "loss: 0.007114628329873085, accuracy: 1.0\n",
      "loss: 0.007156105246394873, accuracy: 1.0\n",
      "loss: 0.0071634938940405846, accuracy: 1.0\n",
      "loss: 0.007140664849430323, accuracy: 1.0\n",
      "loss: 0.007224204950034618, accuracy: 1.0\n",
      "loss: 0.00719298142939806, accuracy: 1.0\n",
      "loss: 0.007267829962074757, accuracy: 1.0\n",
      "loss: 0.007071791682392359, accuracy: 1.0\n",
      "loss: 0.007108786143362522, accuracy: 1.0\n",
      "loss: 0.007033687550574541, accuracy: 1.0\n",
      "loss: 0.0071397870779037476, accuracy: 1.0\n",
      "loss: 0.007039838470518589, accuracy: 1.0\n",
      "loss: 0.007044486701488495, accuracy: 1.0\n",
      "loss: 0.007068869657814503, accuracy: 1.0\n",
      "loss: 0.0071175917983055115, accuracy: 1.0\n",
      "loss: 0.007196044083684683, accuracy: 1.0\n",
      "loss: 0.007107338402420282, accuracy: 1.0\n",
      "loss: 0.007016904652118683, accuracy: 1.0\n",
      "loss: 0.0070846471935510635, accuracy: 1.0\n",
      "loss: 0.007048727013170719, accuracy: 1.0\n",
      "loss: 0.0071745398454368114, accuracy: 1.0\n",
      "loss: 0.006971159018576145, accuracy: 1.0\n",
      "loss: 0.007305702660232782, accuracy: 1.0\n",
      "loss: 0.007267284672707319, accuracy: 1.0\n",
      "loss: 0.0071015856228768826, accuracy: 1.0\n",
      "loss: 0.007126993034034967, accuracy: 1.0\n",
      "loss: 0.007024285849183798, accuracy: 1.0\n",
      "loss: 0.0071614570915699005, accuracy: 1.0\n",
      "loss: 0.00703090475872159, accuracy: 1.0\n",
      "loss: 0.007449315395206213, accuracy: 1.0\n",
      "loss: 0.007292918395251036, accuracy: 1.0\n",
      "loss: 0.007232153322547674, accuracy: 1.0\n",
      "loss: 0.007500700186938047, accuracy: 1.0\n",
      "loss: 0.007196350954473019, accuracy: 1.0\n",
      "loss: 0.007157041225582361, accuracy: 1.0\n",
      "loss: 0.007317351642996073, accuracy: 1.0\n",
      "loss: 0.007027330342680216, accuracy: 1.0\n",
      "loss: 0.006998627446591854, accuracy: 1.0\n",
      "loss: 0.007293203845620155, accuracy: 1.0\n",
      "loss: 0.007139538414776325, accuracy: 1.0\n",
      "loss: 0.007201571948826313, accuracy: 1.0\n",
      "loss: 0.007003368344157934, accuracy: 1.0\n",
      "loss: 0.0069124531000852585, accuracy: 1.0\n",
      "loss: 0.006980577949434519, accuracy: 1.0\n",
      "loss: 0.007104570511728525, accuracy: 1.0\n",
      "loss: 0.0070313322357833385, accuracy: 1.0\n",
      "loss: 0.007335419300943613, accuracy: 1.0\n",
      "loss: 0.0072478074580430984, accuracy: 1.0\n",
      "loss: 0.007097519934177399, accuracy: 1.0\n",
      "loss: 0.007162952795624733, accuracy: 1.0\n",
      "loss: 0.007063132710754871, accuracy: 1.0\n",
      "loss: 0.007026487495750189, accuracy: 1.0\n",
      "loss: 0.0069863381795585155, accuracy: 1.0\n",
      "loss: 0.006925838999450207, accuracy: 1.0\n",
      "loss: 0.007066271733492613, accuracy: 1.0\n",
      "loss: 0.007130062207579613, accuracy: 1.0\n",
      "loss: 0.007015755400061607, accuracy: 1.0\n",
      "loss: 0.007066220976412296, accuracy: 1.0\n",
      "loss: 0.0071020652540028095, accuracy: 1.0\n",
      "loss: 0.0071273487992584705, accuracy: 1.0\n",
      "loss: 0.00698218634352088, accuracy: 1.0\n",
      "loss: 0.0072465515695512295, accuracy: 1.0\n",
      "loss: 0.007030169945210218, accuracy: 1.0\n",
      "loss: 0.006958755198866129, accuracy: 1.0\n",
      "loss: 0.0071114422753453255, accuracy: 1.0\n",
      "loss: 0.007057117763906717, accuracy: 1.0\n",
      "loss: 0.007108382415026426, accuracy: 1.0\n",
      "loss: 0.007046821061521769, accuracy: 1.0\n",
      "loss: 0.007033315021544695, accuracy: 1.0\n",
      "loss: 0.006986740045249462, accuracy: 1.0\n",
      "loss: 0.0070485444739460945, accuracy: 1.0\n",
      "loss: 0.006914290599524975, accuracy: 1.0\n",
      "loss: 0.007204284425824881, accuracy: 1.0\n",
      "loss: 0.006984400562942028, accuracy: 1.0\n",
      "loss: 0.006948838941752911, accuracy: 1.0\n",
      "loss: 0.007124714553356171, accuracy: 1.0\n",
      "loss: 0.007124888710677624, accuracy: 1.0\n",
      "loss: 0.007012863643467426, accuracy: 1.0\n",
      "loss: 0.0070037152618169785, accuracy: 1.0\n",
      "loss: 0.006914167199283838, accuracy: 1.0\n",
      "loss: 0.006956224329769611, accuracy: 1.0\n",
      "loss: 0.007068893872201443, accuracy: 1.0\n",
      "loss: 0.006979736033827066, accuracy: 1.0\n",
      "loss: 0.006965707987546921, accuracy: 1.0\n",
      "loss: 0.006797270383685827, accuracy: 1.0\n",
      "loss: 0.006968643516302109, accuracy: 1.0\n",
      "loss: 0.0068877083249390125, accuracy: 1.0\n",
      "loss: 0.0068734148517251015, accuracy: 1.0\n",
      "loss: 0.0071300240233540535, accuracy: 1.0\n",
      "loss: 0.007180532440543175, accuracy: 1.0\n",
      "loss: 0.007130538113415241, accuracy: 1.0\n",
      "loss: 0.0069328900426626205, accuracy: 1.0\n",
      "loss: 0.00713362405076623, accuracy: 1.0\n",
      "loss: 0.007400676608085632, accuracy: 1.0\n",
      "loss: 0.006948300637304783, accuracy: 1.0\n",
      "loss: 0.007102669216692448, accuracy: 1.0\n",
      "loss: 0.006967111025005579, accuracy: 1.0\n",
      "loss: 0.006875475402921438, accuracy: 1.0\n",
      "loss: 0.00709674833342433, accuracy: 1.0\n",
      "loss: 0.0070602004416286945, accuracy: 1.0\n",
      "loss: 0.007013884373009205, accuracy: 1.0\n",
      "loss: 0.007129381410777569, accuracy: 1.0\n",
      "loss: 0.0070858546532690525, accuracy: 1.0\n",
      "loss: 0.007097791880369186, accuracy: 1.0\n",
      "loss: 0.006982550024986267, accuracy: 1.0\n",
      "loss: 0.006762876641005278, accuracy: 1.0\n",
      "loss: 0.007107075769454241, accuracy: 1.0\n",
      "loss: 0.007006468717008829, accuracy: 1.0\n",
      "loss: 0.006891707424074411, accuracy: 1.0\n",
      "loss: 0.007089841645210981, accuracy: 1.0\n",
      "loss: 0.006775848101824522, accuracy: 1.0\n",
      "loss: 0.006963156629353762, accuracy: 1.0\n",
      "loss: 0.006831970997154713, accuracy: 1.0\n",
      "loss: 0.0070389690808951855, accuracy: 1.0\n",
      "loss: 0.006902841851115227, accuracy: 1.0\n",
      "loss: 0.007121052127331495, accuracy: 1.0\n",
      "loss: 0.006819811649620533, accuracy: 1.0\n",
      "loss: 0.006920801941305399, accuracy: 1.0\n",
      "loss: 0.006809551268815994, accuracy: 1.0\n",
      "loss: 0.0068036471493542194, accuracy: 1.0\n",
      "loss: 0.007212638854980469, accuracy: 1.0\n",
      "loss: 0.007002090569585562, accuracy: 1.0\n",
      "loss: 0.006829200312495232, accuracy: 1.0\n",
      "loss: 0.006759300362318754, accuracy: 1.0\n",
      "loss: 0.006910216528922319, accuracy: 1.0\n",
      "loss: 0.0067239864729344845, accuracy: 1.0\n",
      "loss: 0.007112949620932341, accuracy: 1.0\n",
      "loss: 0.006965799257159233, accuracy: 1.0\n",
      "loss: 0.006859931163489819, accuracy: 1.0\n",
      "loss: 0.006910515949130058, accuracy: 1.0\n",
      "loss: 0.006796946749091148, accuracy: 1.0\n",
      "loss: 0.0068229567259550095, accuracy: 1.0\n",
      "loss: 0.006759606767445803, accuracy: 1.0\n",
      "loss: 0.006832892540842295, accuracy: 1.0\n",
      "loss: 0.006817988120019436, accuracy: 1.0\n",
      "loss: 0.006673690862953663, accuracy: 1.0\n",
      "loss: 0.00667532766237855, accuracy: 1.0\n",
      "loss: 0.006894491612911224, accuracy: 1.0\n",
      "loss: 0.006925457157194614, accuracy: 1.0\n",
      "loss: 0.006812915671616793, accuracy: 1.0\n",
      "loss: 0.007130875252187252, accuracy: 1.0\n",
      "loss: 0.006826767232269049, accuracy: 1.0\n",
      "loss: 0.006746327504515648, accuracy: 1.0\n",
      "loss: 0.006971629336476326, accuracy: 1.0\n",
      "loss: 0.006795082241296768, accuracy: 1.0\n",
      "loss: 0.006843595299869776, accuracy: 1.0\n",
      "loss: 0.00671522319316864, accuracy: 1.0\n",
      "loss: 0.006728971842676401, accuracy: 1.0\n",
      "loss: 0.006838906090706587, accuracy: 1.0\n",
      "loss: 0.0068672397173941135, accuracy: 1.0\n",
      "loss: 0.006923436187207699, accuracy: 1.0\n",
      "loss: 0.006693074479699135, accuracy: 1.0\n",
      "loss: 0.006764859426766634, accuracy: 1.0\n",
      "loss: 0.0067367288284003735, accuracy: 1.0\n",
      "loss: 0.006878983229398727, accuracy: 1.0\n",
      "loss: 0.0067710429430007935, accuracy: 1.0\n",
      "loss: 0.006928403861820698, accuracy: 1.0\n",
      "loss: 0.006713909562677145, accuracy: 1.0\n",
      "loss: 0.006770731415599585, accuracy: 1.0\n",
      "loss: 0.006944017484784126, accuracy: 1.0\n",
      "loss: 0.006884225178509951, accuracy: 1.0\n",
      "loss: 0.006606859154999256, accuracy: 1.0\n",
      "loss: 0.006717617157846689, accuracy: 1.0\n",
      "loss: 0.007121340371668339, accuracy: 1.0\n",
      "loss: 0.0066747465170919895, accuracy: 1.0\n",
      "loss: 0.00678931875154376, accuracy: 1.0\n",
      "loss: 0.006617606617510319, accuracy: 1.0\n",
      "loss: 0.0066999224945902824, accuracy: 1.0\n",
      "loss: 0.006772602442651987, accuracy: 1.0\n",
      "loss: 0.006703539751470089, accuracy: 1.0\n",
      "loss: 0.006662944797426462, accuracy: 1.0\n",
      "loss: 0.006865659262984991, accuracy: 1.0\n",
      "loss: 0.006828669458627701, accuracy: 1.0\n",
      "loss: 0.006695532705634832, accuracy: 1.0\n",
      "loss: 0.006775520741939545, accuracy: 1.0\n",
      "loss: 0.006878430023789406, accuracy: 1.0\n",
      "loss: 0.0067480867728590965, accuracy: 1.0\n",
      "loss: 0.006826055236160755, accuracy: 1.0\n",
      "loss: 0.006715575233101845, accuracy: 1.0\n",
      "loss: 0.006884682923555374, accuracy: 1.0\n",
      "loss: 0.0068688285537064075, accuracy: 1.0\n",
      "loss: 0.0068763368763029575, accuracy: 1.0\n",
      "loss: 0.006723486818373203, accuracy: 1.0\n",
      "loss: 0.0067282686941325665, accuracy: 1.0\n",
      "loss: 0.006763271521776915, accuracy: 1.0\n",
      "loss: 0.006628630682826042, accuracy: 1.0\n",
      "loss: 0.006627581547945738, accuracy: 1.0\n",
      "loss: 0.006621761247515678, accuracy: 1.0\n",
      "loss: 0.006716633681207895, accuracy: 1.0\n",
      "loss: 0.006722439080476761, accuracy: 1.0\n",
      "loss: 0.0068014562129974365, accuracy: 1.0\n",
      "loss: 0.006676628720015287, accuracy: 1.0\n",
      "loss: 0.006652443669736385, accuracy: 1.0\n",
      "loss: 0.007181080058217049, accuracy: 1.0\n",
      "loss: 0.006880775094032288, accuracy: 1.0\n",
      "loss: 0.006701866630464792, accuracy: 1.0\n",
      "loss: 0.006806505378335714, accuracy: 1.0\n",
      "loss: 0.006757206749171019, accuracy: 1.0\n",
      "loss: 0.0065807923674583435, accuracy: 1.0\n",
      "loss: 0.0065193441696465015, accuracy: 1.0\n",
      "loss: 0.0066684698686003685, accuracy: 1.0\n",
      "loss: 0.006703530438244343, accuracy: 1.0\n",
      "loss: 0.006639354396611452, accuracy: 1.0\n",
      "loss: 0.006864671129733324, accuracy: 1.0\n",
      "loss: 0.0065645016729831696, accuracy: 1.0\n",
      "loss: 0.006540983449667692, accuracy: 1.0\n",
      "loss: 0.006528249476104975, accuracy: 1.0\n",
      "loss: 0.006575315725058317, accuracy: 1.0\n",
      "loss: 0.006564700044691563, accuracy: 1.0\n",
      "loss: 0.006876565515995026, accuracy: 1.0\n",
      "loss: 0.006769808940589428, accuracy: 1.0\n",
      "loss: 0.0065879072062671185, accuracy: 1.0\n",
      "loss: 0.007063973695039749, accuracy: 1.0\n",
      "loss: 0.006552737206220627, accuracy: 1.0\n",
      "loss: 0.00650160014629364, accuracy: 1.0\n",
      "loss: 0.006700270343571901, accuracy: 1.0\n",
      "loss: 0.006484066601842642, accuracy: 1.0\n",
      "loss: 0.006639542058110237, accuracy: 1.0\n",
      "loss: 0.006659800186753273, accuracy: 1.0\n",
      "loss: 0.0067162299528717995, accuracy: 1.0\n",
      "loss: 0.006510183215141296, accuracy: 1.0\n",
      "loss: 0.006561104208230972, accuracy: 1.0\n",
      "loss: 0.0066105760633945465, accuracy: 1.0\n",
      "loss: 0.006567486561834812, accuracy: 1.0\n",
      "loss: 0.00665388023480773, accuracy: 1.0\n",
      "loss: 0.00662064366042614, accuracy: 1.0\n",
      "loss: 0.006471564527601004, accuracy: 1.0\n",
      "loss: 0.006995111703872681, accuracy: 1.0\n",
      "loss: 0.00673731230199337, accuracy: 1.0\n",
      "loss: 0.006495111621916294, accuracy: 1.0\n",
      "loss: 0.006583083420991898, accuracy: 1.0\n",
      "loss: 0.006567435339093208, accuracy: 1.0\n",
      "loss: 0.006648072972893715, accuracy: 1.0\n",
      "loss: 0.00673342403024435, accuracy: 1.0\n",
      "loss: 0.0065283821895718575, accuracy: 1.0\n",
      "loss: 0.006488610990345478, accuracy: 1.0\n",
      "loss: 0.006586078554391861, accuracy: 1.0\n",
      "loss: 0.006528051104396582, accuracy: 1.0\n",
      "loss: 0.00654279300943017, accuracy: 1.0\n",
      "loss: 0.0066021159291267395, accuracy: 1.0\n",
      "loss: 0.0065799555741250515, accuracy: 1.0\n",
      "loss: 0.006585093215107918, accuracy: 1.0\n",
      "loss: 0.006890346296131611, accuracy: 1.0\n",
      "loss: 0.0064582061022520065, accuracy: 1.0\n",
      "loss: 0.006572657264769077, accuracy: 1.0\n",
      "loss: 0.006581687368452549, accuracy: 1.0\n",
      "loss: 0.006837880704551935, accuracy: 1.0\n",
      "loss: 0.006618802901357412, accuracy: 1.0\n",
      "loss: 0.006579472217708826, accuracy: 1.0\n",
      "loss: 0.006366515997797251, accuracy: 1.0\n",
      "loss: 0.00664557795971632, accuracy: 1.0\n",
      "loss: 0.006565471179783344, accuracy: 1.0\n",
      "loss: 0.0064862133003771305, accuracy: 1.0\n",
      "loss: 0.0066387224942445755, accuracy: 1.0\n",
      "loss: 0.006499497685581446, accuracy: 1.0\n",
      "loss: 0.0064439368434250355, accuracy: 1.0\n",
      "loss: 0.006640289910137653, accuracy: 1.0\n",
      "loss: 0.0064544109627604485, accuracy: 1.0\n",
      "loss: 0.006351766176521778, accuracy: 1.0\n",
      "loss: 0.006743818521499634, accuracy: 1.0\n",
      "loss: 0.006570287514477968, accuracy: 1.0\n",
      "loss: 0.006357983220368624, accuracy: 1.0\n",
      "loss: 0.006382920779287815, accuracy: 1.0\n",
      "loss: 0.006866283714771271, accuracy: 1.0\n",
      "loss: 0.006525027099996805, accuracy: 1.0\n",
      "loss: 0.006505098193883896, accuracy: 1.0\n",
      "loss: 0.006487717851996422, accuracy: 1.0\n",
      "loss: 0.006403897888958454, accuracy: 1.0\n",
      "loss: 0.006509265396744013, accuracy: 1.0\n",
      "loss: 0.006363648921251297, accuracy: 1.0\n",
      "loss: 0.006356458179652691, accuracy: 1.0\n",
      "loss: 0.0065417056903243065, accuracy: 1.0\n",
      "loss: 0.006488058250397444, accuracy: 1.0\n",
      "loss: 0.006591152399778366, accuracy: 1.0\n",
      "loss: 0.006430147681385279, accuracy: 1.0\n",
      "loss: 0.00635941606014967, accuracy: 1.0\n",
      "loss: 0.006783393677324057, accuracy: 1.0\n",
      "loss: 0.006433537229895592, accuracy: 1.0\n",
      "loss: 0.006706688553094864, accuracy: 1.0\n",
      "loss: 0.006441851612180471, accuracy: 1.0\n",
      "loss: 0.00658273883163929, accuracy: 1.0\n",
      "loss: 0.00661495653912425, accuracy: 1.0\n",
      "loss: 0.00646847952157259, accuracy: 1.0\n",
      "loss: 0.006831304170191288, accuracy: 1.0\n",
      "loss: 0.006538327783346176, accuracy: 1.0\n",
      "loss: 0.006600829306989908, accuracy: 1.0\n",
      "loss: 0.006485134828835726, accuracy: 1.0\n",
      "loss: 0.00649982551112771, accuracy: 1.0\n",
      "loss: 0.0064009190537035465, accuracy: 1.0\n",
      "loss: 0.006346153095364571, accuracy: 1.0\n",
      "loss: 0.006519020535051823, accuracy: 1.0\n",
      "loss: 0.006440170109272003, accuracy: 1.0\n",
      "loss: 0.006481586489826441, accuracy: 1.0\n",
      "loss: 0.006516207940876484, accuracy: 1.0\n",
      "loss: 0.006501514930278063, accuracy: 1.0\n",
      "loss: 0.006442483980208635, accuracy: 1.0\n",
      "loss: 0.006567829754203558, accuracy: 1.0\n",
      "loss: 0.006389291025698185, accuracy: 1.0\n",
      "loss: 0.00666057551279664, accuracy: 1.0\n",
      "loss: 0.006346229929476976, accuracy: 1.0\n",
      "loss: 0.007029451429843903, accuracy: 1.0\n",
      "loss: 0.00647165160626173, accuracy: 1.0\n",
      "loss: 0.006367094814777374, accuracy: 1.0\n",
      "loss: 0.00626760208979249, accuracy: 1.0\n",
      "loss: 0.006425019819289446, accuracy: 1.0\n",
      "loss: 0.006467978935688734, accuracy: 1.0\n",
      "loss: 0.006445011124014854, accuracy: 1.0\n",
      "loss: 0.006353279110044241, accuracy: 1.0\n",
      "loss: 0.006340415216982365, accuracy: 1.0\n",
      "loss: 0.0067147184163331985, accuracy: 1.0\n",
      "loss: 0.0063055711798369884, accuracy: 1.0\n",
      "loss: 0.006480909883975983, accuracy: 1.0\n",
      "loss: 0.006526240613311529, accuracy: 1.0\n",
      "loss: 0.006563371513038874, accuracy: 1.0\n",
      "loss: 0.006337920669466257, accuracy: 1.0\n",
      "loss: 0.006388292647898197, accuracy: 1.0\n",
      "loss: 0.006496235262602568, accuracy: 1.0\n",
      "loss: 0.006437984760850668, accuracy: 1.0\n",
      "loss: 0.006373507436364889, accuracy: 1.0\n",
      "loss: 0.006329210475087166, accuracy: 1.0\n",
      "loss: 0.006319690495729446, accuracy: 1.0\n",
      "loss: 0.006366509944200516, accuracy: 1.0\n",
      "loss: 0.0063186888583004475, accuracy: 1.0\n",
      "loss: 0.0062875524163246155, accuracy: 1.0\n",
      "loss: 0.006368248723447323, accuracy: 1.0\n",
      "loss: 0.006221620831638575, accuracy: 1.0\n",
      "loss: 0.006437367293983698, accuracy: 1.0\n",
      "loss: 0.00651243282482028, accuracy: 1.0\n",
      "loss: 0.006406381726264954, accuracy: 1.0\n",
      "loss: 0.006352797616273165, accuracy: 1.0\n",
      "loss: 0.0066478559747338295, accuracy: 1.0\n",
      "loss: 0.006531941704452038, accuracy: 1.0\n",
      "loss: 0.006391784641891718, accuracy: 1.0\n",
      "loss: 0.006413549650460482, accuracy: 1.0\n",
      "loss: 0.006330134812742472, accuracy: 1.0\n",
      "loss: 0.006396314594894648, accuracy: 1.0\n",
      "loss: 0.006355597171932459, accuracy: 1.0\n",
      "loss: 0.00656090909615159, accuracy: 1.0\n",
      "loss: 0.006309120450168848, accuracy: 1.0\n",
      "loss: 0.0061572762206196785, accuracy: 1.0\n",
      "loss: 0.006335902959108353, accuracy: 1.0\n",
      "loss: 0.006400243379175663, accuracy: 1.0\n",
      "loss: 0.006295070983469486, accuracy: 1.0\n",
      "loss: 0.006338232196867466, accuracy: 1.0\n",
      "loss: 0.006533730309456587, accuracy: 1.0\n",
      "loss: 0.006535260938107967, accuracy: 1.0\n",
      "loss: 0.006343244109302759, accuracy: 1.0\n",
      "loss: 0.006479986943304539, accuracy: 1.0\n",
      "loss: 0.0063807289116084576, accuracy: 1.0\n",
      "loss: 0.0062199984677135944, accuracy: 1.0\n",
      "loss: 0.006294317077845335, accuracy: 1.0\n",
      "loss: 0.006393355783075094, accuracy: 1.0\n",
      "loss: 0.006296859122812748, accuracy: 1.0\n",
      "loss: 0.006433749105781317, accuracy: 1.0\n",
      "loss: 0.006478229071944952, accuracy: 1.0\n",
      "loss: 0.0062173581682145596, accuracy: 1.0\n",
      "loss: 0.006365220993757248, accuracy: 1.0\n",
      "loss: 0.006415215320885181, accuracy: 1.0\n",
      "loss: 0.006263900548219681, accuracy: 1.0\n",
      "loss: 0.006357031408697367, accuracy: 1.0\n",
      "loss: 0.006291555240750313, accuracy: 1.0\n",
      "loss: 0.00624946178868413, accuracy: 1.0\n",
      "loss: 0.00621536560356617, accuracy: 1.0\n",
      "loss: 0.0064405156299471855, accuracy: 1.0\n",
      "loss: 0.006231395062059164, accuracy: 1.0\n",
      "loss: 0.006232437677681446, accuracy: 1.0\n",
      "loss: 0.006285286042839289, accuracy: 1.0\n",
      "loss: 0.006213452201336622, accuracy: 1.0\n",
      "loss: 0.006260120775550604, accuracy: 1.0\n",
      "loss: 0.006239546462893486, accuracy: 1.0\n",
      "loss: 0.006137488875538111, accuracy: 1.0\n",
      "loss: 0.006631297525018454, accuracy: 1.0\n",
      "loss: 0.006512985564768314, accuracy: 1.0\n",
      "loss: 0.0066933054476976395, accuracy: 1.0\n",
      "loss: 0.006307181902229786, accuracy: 1.0\n",
      "loss: 0.006309217773377895, accuracy: 1.0\n",
      "loss: 0.006274270359426737, accuracy: 1.0\n",
      "loss: 0.006291628815233707, accuracy: 1.0\n",
      "loss: 0.006353355944156647, accuracy: 1.0\n",
      "loss: 0.006298789754509926, accuracy: 1.0\n",
      "loss: 0.006385147105902433, accuracy: 1.0\n",
      "loss: 0.0062064058147370815, accuracy: 1.0\n",
      "loss: 0.006096319295465946, accuracy: 1.0\n",
      "loss: 0.0063453116454184055, accuracy: 1.0\n",
      "loss: 0.006214290391653776, accuracy: 1.0\n",
      "loss: 0.0062282755970954895, accuracy: 1.0\n",
      "loss: 0.006178205832839012, accuracy: 1.0\n",
      "loss: 0.006337186321616173, accuracy: 1.0\n",
      "loss: 0.006214753724634647, accuracy: 1.0\n",
      "loss: 0.0061803944408893585, accuracy: 1.0\n",
      "loss: 0.006133967079222202, accuracy: 1.0\n",
      "loss: 0.006248580291867256, accuracy: 1.0\n",
      "loss: 0.006049417890608311, accuracy: 1.0\n",
      "loss: 0.006241276860237122, accuracy: 1.0\n",
      "loss: 0.006139260716736317, accuracy: 1.0\n",
      "loss: 0.006262198090553284, accuracy: 1.0\n",
      "loss: 0.006338934879750013, accuracy: 1.0\n",
      "loss: 0.006368542090058327, accuracy: 1.0\n",
      "loss: 0.006224175915122032, accuracy: 1.0\n",
      "loss: 0.006352028343826532, accuracy: 1.0\n",
      "loss: 0.006196393631398678, accuracy: 1.0\n",
      "loss: 0.006250945385545492, accuracy: 1.0\n",
      "loss: 0.00623483257368207, accuracy: 1.0\n",
      "loss: 0.0061318459920585155, accuracy: 1.0\n",
      "loss: 0.006190157495439053, accuracy: 1.0\n",
      "loss: 0.006288830656558275, accuracy: 1.0\n",
      "loss: 0.006101308856159449, accuracy: 1.0\n",
      "loss: 0.006154630798846483, accuracy: 1.0\n",
      "loss: 0.00608737301081419, accuracy: 1.0\n",
      "loss: 0.0061386642046272755, accuracy: 1.0\n",
      "loss: 0.0062072849832475185, accuracy: 1.0\n",
      "loss: 0.006287130061537027, accuracy: 1.0\n",
      "loss: 0.006221188232302666, accuracy: 1.0\n",
      "loss: 0.0062218280509114265, accuracy: 1.0\n",
      "loss: 0.006080309860408306, accuracy: 1.0\n",
      "loss: 0.006098541896790266, accuracy: 1.0\n",
      "loss: 0.006173449102789164, accuracy: 1.0\n",
      "loss: 0.0061759790405631065, accuracy: 1.0\n",
      "loss: 0.006260266527533531, accuracy: 1.0\n",
      "loss: 0.006126468535512686, accuracy: 1.0\n",
      "loss: 0.006090058013796806, accuracy: 1.0\n",
      "loss: 0.006179936230182648, accuracy: 1.0\n",
      "loss: 0.006207372061908245, accuracy: 1.0\n",
      "loss: 0.006217750255018473, accuracy: 1.0\n",
      "loss: 0.006047893315553665, accuracy: 1.0\n",
      "loss: 0.006211588624864817, accuracy: 1.0\n",
      "loss: 0.00621413579210639, accuracy: 1.0\n",
      "loss: 0.006123735569417477, accuracy: 1.0\n",
      "loss: 0.005948547273874283, accuracy: 1.0\n",
      "loss: 0.006098446901887655, accuracy: 1.0\n",
      "loss: 0.006203014403581619, accuracy: 1.0\n",
      "loss: 0.006120906677097082, accuracy: 1.0\n",
      "loss: 0.006075992248952389, accuracy: 1.0\n",
      "loss: 0.006224203854799271, accuracy: 1.0\n",
      "loss: 0.006249959114938974, accuracy: 1.0\n",
      "loss: 0.006125515326857567, accuracy: 1.0\n",
      "loss: 0.006224836688488722, accuracy: 1.0\n",
      "loss: 0.006147973705083132, accuracy: 1.0\n",
      "loss: 0.0064310054294764996, accuracy: 1.0\n",
      "loss: 0.006174161098897457, accuracy: 1.0\n",
      "loss: 0.006348493043333292, accuracy: 1.0\n",
      "loss: 0.006103367544710636, accuracy: 1.0\n",
      "loss: 0.006311396602541208, accuracy: 1.0\n",
      "loss: 0.006030609365552664, accuracy: 1.0\n",
      "loss: 0.0062047382816672325, accuracy: 1.0\n",
      "loss: 0.006005752366036177, accuracy: 1.0\n",
      "loss: 0.00628385366871953, accuracy: 1.0\n",
      "loss: 0.006266649346798658, accuracy: 1.0\n",
      "loss: 0.006073137279599905, accuracy: 1.0\n",
      "loss: 0.006129490211606026, accuracy: 1.0\n",
      "loss: 0.006074297707527876, accuracy: 1.0\n",
      "loss: 0.00615728972479701, accuracy: 1.0\n",
      "loss: 0.006054685451090336, accuracy: 1.0\n",
      "loss: 0.006264647003263235, accuracy: 1.0\n",
      "loss: 0.006138454657047987, accuracy: 1.0\n",
      "loss: 0.0063154795207083225, accuracy: 1.0\n",
      "loss: 0.006131559144705534, accuracy: 1.0\n",
      "loss: 0.006123102270066738, accuracy: 1.0\n",
      "loss: 0.006123438011854887, accuracy: 1.0\n",
      "loss: 0.006089108996093273, accuracy: 1.0\n",
      "loss: 0.0059414515271782875, accuracy: 1.0\n",
      "loss: 0.006093216594308615, accuracy: 1.0\n",
      "loss: 0.006025143899023533, accuracy: 1.0\n",
      "loss: 0.0059661404229700565, accuracy: 1.0\n",
      "loss: 0.006164181511849165, accuracy: 1.0\n",
      "loss: 0.006029114592820406, accuracy: 1.0\n",
      "loss: 0.005997553467750549, accuracy: 1.0\n",
      "loss: 0.006008052732795477, accuracy: 1.0\n",
      "loss: 0.0059993453323841095, accuracy: 1.0\n",
      "loss: 0.0060877082869410515, accuracy: 1.0\n",
      "loss: 0.0059622544795274734, accuracy: 1.0\n",
      "loss: 0.006051582284271717, accuracy: 1.0\n",
      "loss: 0.006013352423906326, accuracy: 1.0\n",
      "loss: 0.006140646990388632, accuracy: 1.0\n",
      "loss: 0.006187913008034229, accuracy: 1.0\n",
      "loss: 0.006043490953743458, accuracy: 1.0\n",
      "loss: 0.0059752329252660275, accuracy: 1.0\n",
      "loss: 0.0059165945276618, accuracy: 1.0\n",
      "loss: 0.005995902232825756, accuracy: 1.0\n",
      "loss: 0.005998912733048201, accuracy: 1.0\n",
      "loss: 0.00609305826947093, accuracy: 1.0\n",
      "loss: 0.0060619693249464035, accuracy: 1.0\n",
      "loss: 0.006239173002541065, accuracy: 1.0\n",
      "loss: 0.006088266149163246, accuracy: 1.0\n",
      "loss: 0.006296179722994566, accuracy: 1.0\n",
      "loss: 0.006050063297152519, accuracy: 1.0\n",
      "loss: 0.0061522480100393295, accuracy: 1.0\n",
      "loss: 0.006120850332081318, accuracy: 1.0\n",
      "loss: 0.0060777259059250355, accuracy: 1.0\n",
      "loss: 0.006054101511836052, accuracy: 1.0\n",
      "loss: 0.0059478143230080605, accuracy: 1.0\n",
      "loss: 0.005992052145302296, accuracy: 1.0\n",
      "loss: 0.005929576698690653, accuracy: 1.0\n",
      "loss: 0.0059999399818480015, accuracy: 1.0\n",
      "loss: 0.006163693033158779, accuracy: 1.0\n",
      "loss: 0.006004899740219116, accuracy: 1.0\n",
      "loss: 0.00593187939375639, accuracy: 1.0\n",
      "loss: 0.005956767592579126, accuracy: 1.0\n",
      "loss: 0.006152555346488953, accuracy: 1.0\n",
      "loss: 0.005925391335040331, accuracy: 1.0\n",
      "loss: 0.0060695502907037735, accuracy: 1.0\n",
      "loss: 0.005876803770661354, accuracy: 1.0\n",
      "loss: 0.0059474920853972435, accuracy: 1.0\n",
      "loss: 0.006013338919728994, accuracy: 1.0\n",
      "loss: 0.006017320789396763, accuracy: 1.0\n",
      "loss: 0.005990096367895603, accuracy: 1.0\n",
      "loss: 0.006031383294612169, accuracy: 1.0\n",
      "loss: 0.005966112017631531, accuracy: 1.0\n",
      "loss: 0.00603560172021389, accuracy: 1.0\n",
      "loss: 0.006097317673265934, accuracy: 1.0\n",
      "loss: 0.005978142376989126, accuracy: 1.0\n",
      "loss: 0.005878293886780739, accuracy: 1.0\n",
      "loss: 0.005979671608656645, accuracy: 1.0\n",
      "loss: 0.006119479425251484, accuracy: 1.0\n",
      "loss: 0.00627120491117239, accuracy: 1.0\n",
      "loss: 0.006088937632739544, accuracy: 1.0\n",
      "loss: 0.0058637321926653385, accuracy: 1.0\n",
      "loss: 0.0059342896565794945, accuracy: 1.0\n",
      "loss: 0.006052740383893251, accuracy: 1.0\n",
      "loss: 0.006015995051711798, accuracy: 1.0\n",
      "loss: 0.005941077601164579, accuracy: 1.0\n",
      "loss: 0.005896796938031912, accuracy: 1.0\n",
      "loss: 0.006150289438664913, accuracy: 1.0\n",
      "loss: 0.006350461393594742, accuracy: 1.0\n",
      "loss: 0.006139089353382587, accuracy: 1.0\n",
      "loss: 0.005984124261885881, accuracy: 1.0\n",
      "loss: 0.005766029004007578, accuracy: 1.0\n",
      "loss: 0.005892957095056772, accuracy: 1.0\n",
      "loss: 0.005910343024879694, accuracy: 1.0\n",
      "loss: 0.006078015081584454, accuracy: 1.0\n",
      "loss: 0.005928279832005501, accuracy: 1.0\n",
      "loss: 0.005777979735285044, accuracy: 1.0\n",
      "loss: 0.005936864763498306, accuracy: 1.0\n",
      "loss: 0.005854252725839615, accuracy: 1.0\n",
      "loss: 0.0059446613304317, accuracy: 1.0\n",
      "loss: 0.005925411824136972, accuracy: 1.0\n",
      "loss: 0.005897809751331806, accuracy: 1.0\n",
      "loss: 0.005927054211497307, accuracy: 1.0\n",
      "loss: 0.006106221582740545, accuracy: 1.0\n",
      "loss: 0.005900771357119083, accuracy: 1.0\n",
      "loss: 0.006158528383821249, accuracy: 1.0\n",
      "loss: 0.005841418169438839, accuracy: 1.0\n",
      "loss: 0.005930859129875898, accuracy: 1.0\n",
      "loss: 0.005926112178713083, accuracy: 1.0\n",
      "loss: 0.005912096705287695, accuracy: 1.0\n",
      "loss: 0.005852565634995699, accuracy: 1.0\n",
      "loss: 0.005879138596355915, accuracy: 1.0\n",
      "loss: 0.005859323777258396, accuracy: 1.0\n",
      "loss: 0.006000715773552656, accuracy: 1.0\n",
      "loss: 0.005808730609714985, accuracy: 1.0\n",
      "loss: 0.005969892255961895, accuracy: 1.0\n",
      "loss: 0.005885282531380653, accuracy: 1.0\n",
      "loss: 0.0058225165121257305, accuracy: 1.0\n",
      "loss: 0.006084344815462828, accuracy: 1.0\n",
      "loss: 0.005842418409883976, accuracy: 1.0\n",
      "loss: 0.005980060435831547, accuracy: 1.0\n",
      "loss: 0.005815924145281315, accuracy: 1.0\n",
      "loss: 0.005823102779686451, accuracy: 1.0\n",
      "loss: 0.005982200149446726, accuracy: 1.0\n",
      "loss: 0.005787813104689121, accuracy: 1.0\n",
      "loss: 0.00592803442850709, accuracy: 1.0\n",
      "loss: 0.005879498552531004, accuracy: 1.0\n",
      "loss: 0.005869000684469938, accuracy: 1.0\n",
      "loss: 0.005878848489373922, accuracy: 1.0\n",
      "loss: 0.0059744627214968204, accuracy: 1.0\n",
      "loss: 0.0058267186395823956, accuracy: 1.0\n",
      "loss: 0.005720794666558504, accuracy: 1.0\n",
      "loss: 0.005787125322967768, accuracy: 1.0\n",
      "loss: 0.005886169616132975, accuracy: 1.0\n",
      "loss: 0.005833298899233341, accuracy: 1.0\n",
      "loss: 0.005870057735592127, accuracy: 1.0\n",
      "loss: 0.005987797398120165, accuracy: 1.0\n",
      "loss: 0.005788469221442938, accuracy: 1.0\n",
      "loss: 0.005856681149452925, accuracy: 1.0\n",
      "loss: 0.005884837359189987, accuracy: 1.0\n",
      "loss: 0.005746121518313885, accuracy: 1.0\n",
      "loss: 0.005892198532819748, accuracy: 1.0\n",
      "loss: 0.0058623626828193665, accuracy: 1.0\n",
      "loss: 0.0058370293118059635, accuracy: 1.0\n",
      "loss: 0.005928258877247572, accuracy: 1.0\n",
      "loss: 0.005843541119247675, accuracy: 1.0\n",
      "loss: 0.005903580226004124, accuracy: 1.0\n",
      "loss: 0.005713221617043018, accuracy: 1.0\n",
      "loss: 0.005826018750667572, accuracy: 1.0\n",
      "loss: 0.005812849383801222, accuracy: 1.0\n",
      "loss: 0.005862926598638296, accuracy: 1.0\n",
      "loss: 0.005851349327713251, accuracy: 1.0\n",
      "loss: 0.005775672849267721, accuracy: 1.0\n",
      "loss: 0.00573248416185379, accuracy: 1.0\n",
      "loss: 0.005781015381217003, accuracy: 1.0\n",
      "loss: 0.005914722103625536, accuracy: 1.0\n",
      "loss: 0.005767438095062971, accuracy: 1.0\n",
      "loss: 0.00585244782269001, accuracy: 1.0\n",
      "loss: 0.00563149293884635, accuracy: 1.0\n",
      "loss: 0.005767638795077801, accuracy: 1.0\n",
      "loss: 0.005840316880494356, accuracy: 1.0\n",
      "loss: 0.005785004235804081, accuracy: 1.0\n",
      "loss: 0.005770594347268343, accuracy: 1.0\n",
      "loss: 0.005767035763710737, accuracy: 1.0\n",
      "loss: 0.005915573798120022, accuracy: 1.0\n",
      "loss: 0.005746935959905386, accuracy: 1.0\n",
      "loss: 0.005853582173585892, accuracy: 1.0\n",
      "loss: 0.005912150721997023, accuracy: 1.0\n",
      "loss: 0.005720182787626982, accuracy: 1.0\n",
      "loss: 0.005757140461355448, accuracy: 1.0\n",
      "loss: 0.005786627531051636, accuracy: 1.0\n",
      "loss: 0.00581737607717514, accuracy: 1.0\n",
      "loss: 0.0056664892472326756, accuracy: 1.0\n",
      "loss: 0.005698323715478182, accuracy: 1.0\n",
      "loss: 0.005734886974096298, accuracy: 1.0\n",
      "loss: 0.005762804765254259, accuracy: 1.0\n",
      "loss: 0.0056562102399766445, accuracy: 1.0\n",
      "loss: 0.005803681910037994, accuracy: 1.0\n",
      "loss: 0.005721407942473888, accuracy: 1.0\n",
      "loss: 0.0061045074835419655, accuracy: 1.0\n",
      "loss: 0.005884688347578049, accuracy: 1.0\n",
      "loss: 0.0057890284806489944, accuracy: 1.0\n",
      "loss: 0.005766941700130701, accuracy: 1.0\n",
      "loss: 0.005776110105216503, accuracy: 1.0\n",
      "loss: 0.005777471698820591, accuracy: 1.0\n",
      "loss: 0.0058932253159582615, accuracy: 1.0\n",
      "loss: 0.005744941998273134, accuracy: 1.0\n",
      "loss: 0.005729921627789736, accuracy: 1.0\n",
      "loss: 0.005787661764770746, accuracy: 1.0\n",
      "loss: 0.00578171294182539, accuracy: 1.0\n",
      "loss: 0.005701580084860325, accuracy: 1.0\n",
      "loss: 0.005605684593319893, accuracy: 1.0\n",
      "loss: 0.0058288006111979485, accuracy: 1.0\n",
      "loss: 0.005780997220426798, accuracy: 1.0\n",
      "loss: 0.005701507441699505, accuracy: 1.0\n",
      "loss: 0.005673147737979889, accuracy: 1.0\n",
      "loss: 0.005827296059578657, accuracy: 1.0\n",
      "loss: 0.005759957246482372, accuracy: 1.0\n",
      "loss: 0.005810803733766079, accuracy: 1.0\n",
      "loss: 0.005690900608897209, accuracy: 1.0\n",
      "loss: 0.00588482990860939, accuracy: 1.0\n",
      "loss: 0.005891891196370125, accuracy: 1.0\n",
      "loss: 0.005999990738928318, accuracy: 1.0\n",
      "loss: 0.0056747919879853725, accuracy: 1.0\n",
      "loss: 0.005627880804240704, accuracy: 1.0\n",
      "loss: 0.005613833200186491, accuracy: 1.0\n",
      "loss: 0.005643378011882305, accuracy: 1.0\n",
      "loss: 0.005770545452833176, accuracy: 1.0\n",
      "loss: 0.0057679396122694016, accuracy: 1.0\n",
      "loss: 0.005686025135219097, accuracy: 1.0\n",
      "loss: 0.00577966496348381, accuracy: 1.0\n",
      "loss: 0.005766470450907946, accuracy: 1.0\n",
      "loss: 0.005829667206853628, accuracy: 1.0\n",
      "loss: 0.005546848755329847, accuracy: 1.0\n",
      "loss: 0.005757431499660015, accuracy: 1.0\n",
      "loss: 0.005581473000347614, accuracy: 1.0\n",
      "loss: 0.005824741907417774, accuracy: 1.0\n",
      "loss: 0.00574549799785018, accuracy: 1.0\n",
      "loss: 0.0057006459683179855, accuracy: 1.0\n",
      "loss: 0.00559492502361536, accuracy: 1.0\n",
      "loss: 0.005723597016185522, accuracy: 1.0\n",
      "loss: 0.005731666926294565, accuracy: 1.0\n",
      "loss: 0.005672617815434933, accuracy: 1.0\n",
      "loss: 0.005700749345123768, accuracy: 1.0\n",
      "loss: 0.0057105389423668385, accuracy: 1.0\n",
      "loss: 0.005689411889761686, accuracy: 1.0\n",
      "loss: 0.005663246847689152, accuracy: 1.0\n",
      "loss: 0.005689258221536875, accuracy: 1.0\n",
      "loss: 0.005810640286654234, accuracy: 1.0\n",
      "loss: 0.0056710815988481045, accuracy: 1.0\n",
      "loss: 0.005677490495145321, accuracy: 1.0\n",
      "loss: 0.005946628749370575, accuracy: 1.0\n",
      "loss: 0.005635283887386322, accuracy: 1.0\n",
      "loss: 0.005639447830617428, accuracy: 1.0\n",
      "loss: 0.005781199317425489, accuracy: 1.0\n",
      "loss: 0.005816940683871508, accuracy: 1.0\n",
      "loss: 0.005574527662247419, accuracy: 1.0\n",
      "loss: 0.0056146290153265, accuracy: 1.0\n",
      "loss: 0.005642448551952839, accuracy: 1.0\n",
      "loss: 0.005603939760476351, accuracy: 1.0\n",
      "loss: 0.00596621772274375, accuracy: 1.0\n",
      "loss: 0.005496527999639511, accuracy: 1.0\n",
      "loss: 0.005691256374120712, accuracy: 1.0\n",
      "loss: 0.0055764345452189445, accuracy: 1.0\n",
      "loss: 0.005709497723728418, accuracy: 1.0\n",
      "loss: 0.0056618996895849705, accuracy: 1.0\n",
      "loss: 0.005809043534100056, accuracy: 1.0\n",
      "loss: 0.0055192625150084496, accuracy: 1.0\n",
      "loss: 0.005572439171373844, accuracy: 1.0\n",
      "loss: 0.005634062457829714, accuracy: 1.0\n",
      "loss: 0.0056383563205599785, accuracy: 1.0\n",
      "loss: 0.00563345942646265, accuracy: 1.0\n",
      "loss: 0.005629817023873329, accuracy: 1.0\n",
      "loss: 0.005654335953295231, accuracy: 1.0\n",
      "loss: 0.005631790496408939, accuracy: 1.0\n",
      "loss: 0.005742976441979408, accuracy: 1.0\n",
      "loss: 0.0056223189458251, accuracy: 1.0\n",
      "loss: 0.005856468342244625, accuracy: 1.0\n",
      "loss: 0.005675170570611954, accuracy: 1.0\n",
      "loss: 0.005749982316046953, accuracy: 1.0\n",
      "loss: 0.005856644362211227, accuracy: 1.0\n",
      "loss: 0.00569977518171072, accuracy: 1.0\n",
      "loss: 0.005714925471693277, accuracy: 1.0\n",
      "loss: 0.005601805169135332, accuracy: 1.0\n",
      "loss: 0.005728439427912235, accuracy: 1.0\n",
      "loss: 0.005662097595632076, accuracy: 1.0\n",
      "loss: 0.005628116894513369, accuracy: 1.0\n",
      "loss: 0.005607518367469311, accuracy: 1.0\n",
      "loss: 0.005565805826336145, accuracy: 1.0\n",
      "loss: 0.005689751822501421, accuracy: 1.0\n",
      "loss: 0.005546972621232271, accuracy: 1.0\n",
      "loss: 0.005549377761781216, accuracy: 1.0\n",
      "loss: 0.005812954157590866, accuracy: 1.0\n",
      "loss: 0.0055826264433562756, accuracy: 1.0\n",
      "loss: 0.005474427714943886, accuracy: 1.0\n",
      "loss: 0.005780165083706379, accuracy: 1.0\n",
      "loss: 0.005640780553221703, accuracy: 1.0\n",
      "loss: 0.005464436952024698, accuracy: 1.0\n",
      "loss: 0.005590714514255524, accuracy: 1.0\n",
      "loss: 0.005536260548979044, accuracy: 1.0\n",
      "loss: 0.0057622455060482025, accuracy: 1.0\n",
      "loss: 0.005544471554458141, accuracy: 1.0\n",
      "loss: 0.005607496481388807, accuracy: 1.0\n",
      "loss: 0.005676791071891785, accuracy: 1.0\n",
      "loss: 0.005625044461339712, accuracy: 1.0\n",
      "loss: 0.005477664992213249, accuracy: 1.0\n",
      "loss: 0.0055596716701984406, accuracy: 1.0\n",
      "loss: 0.005469302646815777, accuracy: 1.0\n",
      "loss: 0.005686444696038961, accuracy: 1.0\n",
      "loss: 0.0057488102465868, accuracy: 1.0\n",
      "loss: 0.005465934984385967, accuracy: 1.0\n",
      "loss: 0.0055536129511892796, accuracy: 1.0\n",
      "loss: 0.0056168860755860806, accuracy: 1.0\n",
      "loss: 0.005616589915007353, accuracy: 1.0\n",
      "loss: 0.005595868453383446, accuracy: 1.0\n",
      "loss: 0.005629058461636305, accuracy: 1.0\n",
      "loss: 0.005538628902286291, accuracy: 1.0\n",
      "loss: 0.005490652751177549, accuracy: 1.0\n",
      "loss: 0.00573705555871129, accuracy: 1.0\n",
      "loss: 0.005665501579642296, accuracy: 1.0\n",
      "loss: 0.0055380272679030895, accuracy: 1.0\n",
      "loss: 0.005637175869196653, accuracy: 1.0\n",
      "loss: 0.005528441164642572, accuracy: 1.0\n",
      "loss: 0.005521117709577084, accuracy: 1.0\n",
      "loss: 0.005426449701189995, accuracy: 1.0\n",
      "loss: 0.005529778078198433, accuracy: 1.0\n",
      "loss: 0.005458526778966188, accuracy: 1.0\n",
      "loss: 0.005470912437886, accuracy: 1.0\n",
      "loss: 0.005454798694700003, accuracy: 1.0\n",
      "loss: 0.005767520051449537, accuracy: 1.0\n",
      "loss: 0.0056815012358129025, accuracy: 1.0\n",
      "loss: 0.005598808638751507, accuracy: 1.0\n",
      "loss: 0.005472903605550528, accuracy: 1.0\n",
      "loss: 0.005555137060582638, accuracy: 1.0\n",
      "loss: 0.005668063648045063, accuracy: 1.0\n",
      "loss: 0.005452795419842005, accuracy: 1.0\n",
      "loss: 0.005567624699324369, accuracy: 1.0\n",
      "loss: 0.005527032073587179, accuracy: 1.0\n",
      "loss: 0.005565010942518711, accuracy: 1.0\n",
      "loss: 0.005436063278466463, accuracy: 1.0\n",
      "loss: 0.005651924759149551, accuracy: 1.0\n",
      "loss: 0.005418507382273674, accuracy: 1.0\n",
      "loss: 0.005570685490965843, accuracy: 1.0\n",
      "loss: 0.00544295459985733, accuracy: 1.0\n",
      "loss: 0.005514577031135559, accuracy: 1.0\n",
      "loss: 0.005461050197482109, accuracy: 1.0\n",
      "loss: 0.005380123853683472, accuracy: 1.0\n",
      "loss: 0.005595157388597727, accuracy: 1.0\n",
      "loss: 0.005541049875319004, accuracy: 1.0\n",
      "loss: 0.005497065372765064, accuracy: 1.0\n",
      "loss: 0.005600299686193466, accuracy: 1.0\n",
      "loss: 0.005479974672198296, accuracy: 1.0\n",
      "loss: 0.0055985227227211, accuracy: 1.0\n",
      "loss: 0.005721239373087883, accuracy: 1.0\n",
      "loss: 0.00545294675976038, accuracy: 1.0\n",
      "loss: 0.005488033872097731, accuracy: 1.0\n",
      "loss: 0.005551484879106283, accuracy: 1.0\n",
      "loss: 0.0054357582703232765, accuracy: 1.0\n",
      "loss: 0.005451537203043699, accuracy: 1.0\n",
      "loss: 0.00545228598639369, accuracy: 1.0\n",
      "loss: 0.005477988626807928, accuracy: 1.0\n",
      "loss: 0.0054429322481155396, accuracy: 1.0\n",
      "loss: 0.005434476770460606, accuracy: 1.0\n",
      "loss: 0.005404442083090544, accuracy: 1.0\n",
      "loss: 0.005615384317934513, accuracy: 1.0\n",
      "loss: 0.005610895808786154, accuracy: 1.0\n",
      "loss: 0.005582744255661964, accuracy: 1.0\n",
      "loss: 0.00549726840108633, accuracy: 1.0\n",
      "loss: 0.005596322473138571, accuracy: 1.0\n",
      "loss: 0.005323808640241623, accuracy: 1.0\n",
      "loss: 0.0054787746630609035, accuracy: 1.0\n",
      "loss: 0.005361774004995823, accuracy: 1.0\n",
      "loss: 0.00559690035879612, accuracy: 1.0\n",
      "loss: 0.005426255986094475, accuracy: 1.0\n",
      "loss: 0.005447420757263899, accuracy: 1.0\n",
      "loss: 0.005321044009178877, accuracy: 1.0\n",
      "loss: 0.0055614192970097065, accuracy: 1.0\n",
      "loss: 0.005446122493594885, accuracy: 1.0\n",
      "loss: 0.00561669422313571, accuracy: 1.0\n",
      "loss: 0.005483586341142654, accuracy: 1.0\n",
      "loss: 0.005477635655552149, accuracy: 1.0\n",
      "loss: 0.005357131361961365, accuracy: 1.0\n",
      "loss: 0.005421444308012724, accuracy: 1.0\n",
      "loss: 0.005642233416438103, accuracy: 1.0\n",
      "loss: 0.005691968370229006, accuracy: 1.0\n",
      "loss: 0.005475920159369707, accuracy: 1.0\n",
      "loss: 0.00545920804142952, accuracy: 1.0\n",
      "loss: 0.0053657242096960545, accuracy: 1.0\n",
      "loss: 0.00539611978456378, accuracy: 1.0\n",
      "loss: 0.005394692998379469, accuracy: 1.0\n",
      "loss: 0.005326887592673302, accuracy: 1.0\n",
      "loss: 0.005397883243858814, accuracy: 1.0\n",
      "loss: 0.0054006678983569145, accuracy: 1.0\n",
      "loss: 0.005373934283852577, accuracy: 1.0\n",
      "loss: 0.005370707716792822, accuracy: 1.0\n",
      "loss: 0.005375252570956945, accuracy: 1.0\n",
      "loss: 0.005431011784821749, accuracy: 1.0\n",
      "loss: 0.005484212655574083, accuracy: 1.0\n",
      "loss: 0.0053481413051486015, accuracy: 1.0\n",
      "loss: 0.0053908624686300755, accuracy: 1.0\n",
      "loss: 0.005449485965073109, accuracy: 1.0\n",
      "loss: 0.0054581621661782265, accuracy: 1.0\n",
      "loss: 0.00545164430513978, accuracy: 1.0\n",
      "loss: 0.005529291462153196, accuracy: 1.0\n",
      "loss: 0.005386172793805599, accuracy: 1.0\n",
      "loss: 0.005398270208388567, accuracy: 1.0\n",
      "loss: 0.005372541956603527, accuracy: 1.0\n",
      "loss: 0.00530225969851017, accuracy: 1.0\n",
      "loss: 0.005410168785601854, accuracy: 1.0\n",
      "loss: 0.005379891023039818, accuracy: 1.0\n",
      "loss: 0.005450556054711342, accuracy: 1.0\n",
      "loss: 0.005454992409795523, accuracy: 1.0\n",
      "loss: 0.00550446892157197, accuracy: 1.0\n",
      "loss: 0.005527344066649675, accuracy: 1.0\n",
      "loss: 0.005316184367984533, accuracy: 1.0\n",
      "loss: 0.0054373182356357574, accuracy: 1.0\n",
      "loss: 0.005507040303200483, accuracy: 1.0\n",
      "loss: 0.005631284322589636, accuracy: 1.0\n",
      "loss: 0.005328579805791378, accuracy: 1.0\n",
      "loss: 0.0054227313958108425, accuracy: 1.0\n",
      "loss: 0.005448168609291315, accuracy: 1.0\n",
      "loss: 0.005313135217875242, accuracy: 1.0\n",
      "loss: 0.005336410365998745, accuracy: 1.0\n",
      "loss: 0.005364207550883293, accuracy: 1.0\n",
      "loss: 0.005351017229259014, accuracy: 1.0\n",
      "loss: 0.005376041866838932, accuracy: 1.0\n",
      "loss: 0.005297723691910505, accuracy: 1.0\n",
      "loss: 0.005320751573890448, accuracy: 1.0\n",
      "loss: 0.005407229531556368, accuracy: 1.0\n",
      "loss: 0.005453812424093485, accuracy: 1.0\n",
      "loss: 0.005325937643647194, accuracy: 1.0\n",
      "loss: 0.005429302342236042, accuracy: 1.0\n",
      "loss: 0.005447546951472759, accuracy: 1.0\n",
      "loss: 0.005237176548689604, accuracy: 1.0\n",
      "loss: 0.005579256918281317, accuracy: 1.0\n",
      "loss: 0.005271909758448601, accuracy: 1.0\n",
      "loss: 0.005312785506248474, accuracy: 1.0\n",
      "loss: 0.005360049661248922, accuracy: 1.0\n",
      "loss: 0.00536156352609396, accuracy: 1.0\n",
      "loss: 0.005442141555249691, accuracy: 1.0\n",
      "loss: 0.005329993087798357, accuracy: 1.0\n",
      "loss: 0.005479217506945133, accuracy: 1.0\n",
      "loss: 0.005305157974362373, accuracy: 1.0\n",
      "loss: 0.005367790814489126, accuracy: 1.0\n",
      "loss: 0.005353884305804968, accuracy: 1.0\n",
      "loss: 0.005341341719031334, accuracy: 1.0\n",
      "loss: 0.005356447771191597, accuracy: 1.0\n",
      "loss: 0.005420792382210493, accuracy: 1.0\n",
      "loss: 0.005386983044445515, accuracy: 1.0\n",
      "loss: 0.0052935839630663395, accuracy: 1.0\n",
      "loss: 0.0053265332244336605, accuracy: 1.0\n",
      "loss: 0.005270539317280054, accuracy: 1.0\n",
      "loss: 0.005297429393976927, accuracy: 1.0\n",
      "loss: 0.0053614480420947075, accuracy: 1.0\n",
      "loss: 0.005274306517094374, accuracy: 1.0\n",
      "loss: 0.0053573716431856155, accuracy: 1.0\n",
      "loss: 0.005334507208317518, accuracy: 1.0\n",
      "loss: 0.005338358227163553, accuracy: 1.0\n",
      "loss: 0.005410600453615189, accuracy: 1.0\n",
      "loss: 0.005417246371507645, accuracy: 1.0\n",
      "loss: 0.005315461196005344, accuracy: 1.0\n",
      "loss: 0.0053443387150764465, accuracy: 1.0\n",
      "loss: 0.005266471765935421, accuracy: 1.0\n",
      "loss: 0.005402750801295042, accuracy: 1.0\n",
      "loss: 0.005324243102222681, accuracy: 1.0\n",
      "loss: 0.005348504520952702, accuracy: 1.0\n",
      "loss: 0.005309558007866144, accuracy: 1.0\n",
      "loss: 0.005325568374246359, accuracy: 1.0\n",
      "loss: 0.005346055142581463, accuracy: 1.0\n",
      "loss: 0.005333377048373222, accuracy: 1.0\n",
      "loss: 0.005269154440611601, accuracy: 1.0\n",
      "loss: 0.005247532390058041, accuracy: 1.0\n",
      "loss: 0.005289970897138119, accuracy: 1.0\n",
      "loss: 0.0052699800580739975, accuracy: 1.0\n",
      "loss: 0.005205947905778885, accuracy: 1.0\n",
      "loss: 0.005310647655278444, accuracy: 1.0\n",
      "loss: 0.005271469242870808, accuracy: 1.0\n",
      "loss: 0.005364283919334412, accuracy: 1.0\n",
      "loss: 0.0053168609738349915, accuracy: 1.0\n",
      "loss: 0.005223835818469524, accuracy: 1.0\n",
      "loss: 0.005500138271600008, accuracy: 1.0\n",
      "loss: 0.005298775155097246, accuracy: 1.0\n",
      "loss: 0.00525928009301424, accuracy: 1.0\n",
      "loss: 0.005251214373856783, accuracy: 1.0\n",
      "loss: 0.00553263071924448, accuracy: 1.0\n",
      "loss: 0.005223768763244152, accuracy: 1.0\n",
      "loss: 0.005222508683800697, accuracy: 1.0\n",
      "loss: 0.005339154973626137, accuracy: 1.0\n",
      "loss: 0.005362969357520342, accuracy: 1.0\n",
      "loss: 0.005159386899322271, accuracy: 1.0\n",
      "loss: 0.005265234503895044, accuracy: 1.0\n",
      "loss: 0.005334533751010895, accuracy: 1.0\n",
      "loss: 0.005297967232763767, accuracy: 1.0\n",
      "loss: 0.0051558176055550575, accuracy: 1.0\n",
      "loss: 0.005146676208823919, accuracy: 1.0\n",
      "loss: 0.005123668350279331, accuracy: 1.0\n",
      "loss: 0.005340449046343565, accuracy: 1.0\n",
      "loss: 0.005161627195775509, accuracy: 1.0\n",
      "loss: 0.00524070905521512, accuracy: 1.0\n",
      "loss: 0.005179341416805983, accuracy: 1.0\n",
      "loss: 0.005151951219886541, accuracy: 1.0\n",
      "loss: 0.00513301370665431, accuracy: 1.0\n",
      "loss: 0.0051781474612653255, accuracy: 1.0\n",
      "loss: 0.005263825412839651, accuracy: 1.0\n",
      "loss: 0.005130589473992586, accuracy: 1.0\n",
      "loss: 0.005167622119188309, accuracy: 1.0\n",
      "loss: 0.005217494443058968, accuracy: 1.0\n",
      "loss: 0.005222327075898647, accuracy: 1.0\n",
      "loss: 0.005234661512076855, accuracy: 1.0\n",
      "loss: 0.0051885745488107204, accuracy: 1.0\n",
      "loss: 0.005262767896056175, accuracy: 1.0\n",
      "loss: 0.005239948630332947, accuracy: 1.0\n",
      "loss: 0.0051732296124100685, accuracy: 1.0\n",
      "loss: 0.005266684573143721, accuracy: 1.0\n",
      "loss: 0.005224226042628288, accuracy: 1.0\n",
      "loss: 0.005185251124203205, accuracy: 1.0\n",
      "loss: 0.0053230407647788525, accuracy: 1.0\n",
      "loss: 0.005288927815854549, accuracy: 1.0\n",
      "loss: 0.005161150824278593, accuracy: 1.0\n",
      "loss: 0.0054204585030674934, accuracy: 1.0\n",
      "loss: 0.005134115926921368, accuracy: 1.0\n",
      "loss: 0.005247659515589476, accuracy: 1.0\n",
      "loss: 0.0050888219848275185, accuracy: 1.0\n",
      "loss: 0.005075982306152582, accuracy: 1.0\n",
      "loss: 0.005261749494820833, accuracy: 1.0\n",
      "loss: 0.00525951711460948, accuracy: 1.0\n",
      "loss: 0.005217742174863815, accuracy: 1.0\n",
      "loss: 0.005328891798853874, accuracy: 1.0\n",
      "loss: 0.005203461740165949, accuracy: 1.0\n",
      "loss: 0.005353291518986225, accuracy: 1.0\n",
      "loss: 0.005225216504186392, accuracy: 1.0\n",
      "loss: 0.005175698548555374, accuracy: 1.0\n",
      "loss: 0.005189946386963129, accuracy: 1.0\n",
      "loss: 0.005121059715747833, accuracy: 1.0\n",
      "loss: 0.005142015404999256, accuracy: 1.0\n",
      "loss: 0.005204383283853531, accuracy: 1.0\n",
      "loss: 0.005142807960510254, accuracy: 1.0\n",
      "loss: 0.005140666849911213, accuracy: 1.0\n",
      "loss: 0.00523605290800333, accuracy: 1.0\n",
      "loss: 0.005182170309126377, accuracy: 1.0\n",
      "loss: 0.00516752852126956, accuracy: 1.0\n",
      "loss: 0.005186933558434248, accuracy: 1.0\n",
      "loss: 0.005160151049494743, accuracy: 1.0\n",
      "loss: 0.005192094948142767, accuracy: 1.0\n",
      "loss: 0.005069521721452475, accuracy: 1.0\n",
      "loss: 0.005103296600282192, accuracy: 1.0\n",
      "loss: 0.005114997737109661, accuracy: 1.0\n",
      "loss: 0.005167081020772457, accuracy: 1.0\n",
      "loss: 0.0053215352818369865, accuracy: 1.0\n",
      "loss: 0.005125230643898249, accuracy: 1.0\n",
      "loss: 0.005196579732000828, accuracy: 1.0\n",
      "loss: 0.005097516346722841, accuracy: 1.0\n",
      "loss: 0.00542444409802556, accuracy: 1.0\n",
      "loss: 0.00513315387070179, accuracy: 1.0\n",
      "loss: 0.005151988938450813, accuracy: 1.0\n",
      "loss: 0.005282009486109018, accuracy: 1.0\n",
      "loss: 0.005160454660654068, accuracy: 1.0\n",
      "loss: 0.005135699640959501, accuracy: 1.0\n",
      "loss: 0.0051175858825445175, accuracy: 1.0\n",
      "loss: 0.005194377154111862, accuracy: 1.0\n",
      "loss: 0.005063317250460386, accuracy: 1.0\n",
      "loss: 0.005120938178151846, accuracy: 1.0\n",
      "loss: 0.005078319925814867, accuracy: 1.0\n",
      "loss: 0.005192958749830723, accuracy: 1.0\n",
      "loss: 0.005204041488468647, accuracy: 1.0\n",
      "loss: 0.005043243058025837, accuracy: 1.0\n",
      "loss: 0.00509775010868907, accuracy: 1.0\n",
      "loss: 0.005121995694935322, accuracy: 1.0\n",
      "loss: 0.005114737432450056, accuracy: 1.0\n",
      "loss: 0.005073850508779287, accuracy: 1.0\n",
      "loss: 0.005261455662548542, accuracy: 1.0\n",
      "loss: 0.005132121965289116, accuracy: 1.0\n",
      "loss: 0.005217886064201593, accuracy: 1.0\n",
      "loss: 0.005176737904548645, accuracy: 1.0\n",
      "loss: 0.005153961945325136, accuracy: 1.0\n",
      "loss: 0.005162850953638554, accuracy: 1.0\n",
      "loss: 0.0051678442396223545, accuracy: 1.0\n",
      "loss: 0.005074354354292154, accuracy: 1.0\n",
      "loss: 0.005119690205901861, accuracy: 1.0\n",
      "loss: 0.005136607680469751, accuracy: 1.0\n",
      "loss: 0.005266475025564432, accuracy: 1.0\n",
      "loss: 0.00507976533845067, accuracy: 1.0\n",
      "loss: 0.005277962423861027, accuracy: 1.0\n",
      "loss: 0.005113572813570499, accuracy: 1.0\n",
      "loss: 0.0051656984724104404, accuracy: 1.0\n",
      "loss: 0.005081340670585632, accuracy: 1.0\n",
      "loss: 0.00511378888040781, accuracy: 1.0\n",
      "loss: 0.004975240211933851, accuracy: 1.0\n",
      "loss: 0.005101424176245928, accuracy: 1.0\n",
      "loss: 0.005141591653227806, accuracy: 1.0\n",
      "loss: 0.005149032920598984, accuracy: 1.0\n",
      "loss: 0.00505567854270339, accuracy: 1.0\n",
      "loss: 0.005093068815767765, accuracy: 1.0\n",
      "loss: 0.005002896301448345, accuracy: 1.0\n",
      "loss: 0.005141395144164562, accuracy: 1.0\n",
      "loss: 0.004988974425941706, accuracy: 1.0\n",
      "loss: 0.005116745829582214, accuracy: 1.0\n",
      "loss: 0.005170387681573629, accuracy: 1.0\n",
      "loss: 0.005001094192266464, accuracy: 1.0\n",
      "loss: 0.00502469576895237, accuracy: 1.0\n",
      "loss: 0.0051124379970133305, accuracy: 1.0\n",
      "loss: 0.005050646141171455, accuracy: 1.0\n",
      "loss: 0.005150806158781052, accuracy: 1.0\n",
      "loss: 0.005039319861680269, accuracy: 1.0\n",
      "loss: 0.005144679918885231, accuracy: 1.0\n",
      "loss: 0.005103396251797676, accuracy: 1.0\n",
      "loss: 0.004983147606253624, accuracy: 1.0\n",
      "loss: 0.0050600795075297356, accuracy: 1.0\n",
      "loss: 0.005211600102484226, accuracy: 1.0\n",
      "loss: 0.005068731959909201, accuracy: 1.0\n",
      "loss: 0.005077969282865524, accuracy: 1.0\n",
      "loss: 0.004995347466319799, accuracy: 1.0\n",
      "loss: 0.0050306254997849464, accuracy: 1.0\n",
      "loss: 0.005028559826314449, accuracy: 1.0\n",
      "loss: 0.005021944176405668, accuracy: 1.0\n",
      "loss: 0.005071601364761591, accuracy: 1.0\n",
      "loss: 0.00504264747723937, accuracy: 1.0\n",
      "loss: 0.005143415182828903, accuracy: 1.0\n",
      "loss: 0.005085610784590244, accuracy: 1.0\n",
      "loss: 0.005095507018268108, accuracy: 1.0\n",
      "loss: 0.004985117819160223, accuracy: 1.0\n",
      "loss: 0.005070915445685387, accuracy: 1.0\n",
      "loss: 0.00508835818618536, accuracy: 1.0\n",
      "loss: 0.005055754445493221, accuracy: 1.0\n",
      "loss: 0.005109361372888088, accuracy: 1.0\n",
      "loss: 0.005029020830988884, accuracy: 1.0\n",
      "loss: 0.005052253138273954, accuracy: 1.0\n",
      "loss: 0.005183235742151737, accuracy: 1.0\n",
      "loss: 0.005029256455600262, accuracy: 1.0\n",
      "loss: 0.005015798378735781, accuracy: 1.0\n",
      "loss: 0.005180742125958204, accuracy: 1.0\n",
      "loss: 0.005077499430626631, accuracy: 1.0\n",
      "loss: 0.005155298393219709, accuracy: 1.0\n",
      "loss: 0.005135466810315847, accuracy: 1.0\n",
      "loss: 0.004977114032953978, accuracy: 1.0\n",
      "loss: 0.004973267205059528, accuracy: 1.0\n",
      "loss: 0.0050979540683329105, accuracy: 1.0\n",
      "loss: 0.004939869977533817, accuracy: 1.0\n",
      "loss: 0.0050144800916314125, accuracy: 1.0\n",
      "loss: 0.005138058681041002, accuracy: 1.0\n",
      "loss: 0.005068940110504627, accuracy: 1.0\n",
      "loss: 0.004999243654310703, accuracy: 1.0\n",
      "loss: 0.005033441353589296, accuracy: 1.0\n",
      "loss: 0.004985785111784935, accuracy: 1.0\n",
      "loss: 0.005165249574929476, accuracy: 1.0\n",
      "loss: 0.005105424672365189, accuracy: 1.0\n",
      "loss: 0.005179627798497677, accuracy: 1.0\n",
      "loss: 0.0050875782035291195, accuracy: 1.0\n",
      "loss: 0.004989405162632465, accuracy: 1.0\n",
      "loss: 0.0051338160410523415, accuracy: 1.0\n",
      "loss: 0.005039705894887447, accuracy: 1.0\n",
      "loss: 0.005029649939388037, accuracy: 1.0\n",
      "loss: 0.004948211368173361, accuracy: 1.0\n",
      "loss: 0.0049522919580340385, accuracy: 1.0\n",
      "loss: 0.005028402432799339, accuracy: 1.0\n",
      "loss: 0.004997619893401861, accuracy: 1.0\n",
      "loss: 0.005060738418251276, accuracy: 1.0\n",
      "loss: 0.005191377364099026, accuracy: 1.0\n",
      "loss: 0.004993186332285404, accuracy: 1.0\n",
      "loss: 0.005068027414381504, accuracy: 1.0\n",
      "loss: 0.004985177889466286, accuracy: 1.0\n",
      "loss: 0.00499123428016901, accuracy: 1.0\n",
      "loss: 0.005146553739905357, accuracy: 1.0\n",
      "loss: 0.005019329953938723, accuracy: 1.0\n",
      "loss: 0.005050352308899164, accuracy: 1.0\n",
      "loss: 0.004905938636511564, accuracy: 1.0\n",
      "loss: 0.005018301773816347, accuracy: 1.0\n",
      "loss: 0.005006951745599508, accuracy: 1.0\n",
      "loss: 0.004939266014844179, accuracy: 1.0\n",
      "loss: 0.004957691300660372, accuracy: 1.0\n",
      "loss: 0.005055274348706007, accuracy: 1.0\n",
      "loss: 0.004929299466311932, accuracy: 1.0\n",
      "loss: 0.005035168491303921, accuracy: 1.0\n",
      "loss: 0.005081855691969395, accuracy: 1.0\n",
      "loss: 0.004976226482540369, accuracy: 1.0\n",
      "loss: 0.0050225816667079926, accuracy: 1.0\n",
      "loss: 0.00494989100843668, accuracy: 1.0\n",
      "loss: 0.004942045081406832, accuracy: 1.0\n",
      "loss: 0.004945203196257353, accuracy: 1.0\n",
      "loss: 0.005010602064430714, accuracy: 1.0\n",
      "loss: 0.0050661819986999035, accuracy: 1.0\n",
      "loss: 0.005139350425451994, accuracy: 1.0\n",
      "loss: 0.004890622105449438, accuracy: 1.0\n",
      "loss: 0.0049048177897930145, accuracy: 1.0\n",
      "loss: 0.005027781706303358, accuracy: 1.0\n",
      "loss: 0.005004821345210075, accuracy: 1.0\n",
      "loss: 0.004927737172693014, accuracy: 1.0\n",
      "loss: 0.004988028667867184, accuracy: 1.0\n",
      "loss: 0.004902415908873081, accuracy: 1.0\n",
      "loss: 0.005012982524931431, accuracy: 1.0\n",
      "loss: 0.004953837022185326, accuracy: 1.0\n",
      "loss: 0.004954585339874029, accuracy: 1.0\n",
      "loss: 0.005135629326105118, accuracy: 1.0\n",
      "loss: 0.0049538277089595795, accuracy: 1.0\n",
      "loss: 0.004903714172542095, accuracy: 1.0\n",
      "loss: 0.005099481903016567, accuracy: 1.0\n",
      "loss: 0.004948858637362719, accuracy: 1.0\n",
      "loss: 0.0049427468329668045, accuracy: 1.0\n",
      "loss: 0.0049634152092039585, accuracy: 1.0\n",
      "loss: 0.004859775770455599, accuracy: 1.0\n",
      "loss: 0.005071177612990141, accuracy: 1.0\n",
      "loss: 0.00493891816586256, accuracy: 1.0\n",
      "loss: 0.004947409965097904, accuracy: 1.0\n",
      "loss: 0.004921220242977142, accuracy: 1.0\n",
      "loss: 0.004823850467801094, accuracy: 1.0\n",
      "loss: 0.004922824911773205, accuracy: 1.0\n",
      "loss: 0.005271511618047953, accuracy: 1.0\n",
      "loss: 0.0049300906248390675, accuracy: 1.0\n",
      "loss: 0.004913968965411186, accuracy: 1.0\n",
      "loss: 0.004799166694283485, accuracy: 1.0\n",
      "loss: 0.004937807098031044, accuracy: 1.0\n",
      "loss: 0.004810822196304798, accuracy: 1.0\n",
      "loss: 0.004938097670674324, accuracy: 1.0\n",
      "loss: 0.0048646023496985435, accuracy: 1.0\n",
      "loss: 0.00492763752117753, accuracy: 1.0\n",
      "loss: 0.004985916428267956, accuracy: 1.0\n",
      "loss: 0.004933178890496492, accuracy: 1.0\n",
      "loss: 0.004902554210275412, accuracy: 1.0\n",
      "loss: 0.0049346680752933025, accuracy: 1.0\n",
      "loss: 0.004888955038040876, accuracy: 1.0\n",
      "loss: 0.004956105723977089, accuracy: 1.0\n",
      "loss: 0.004823494702577591, accuracy: 1.0\n",
      "loss: 0.004921928513795137, accuracy: 1.0\n",
      "loss: 0.004972465336322784, accuracy: 1.0\n",
      "loss: 0.004850550554692745, accuracy: 1.0\n",
      "loss: 0.004898405633866787, accuracy: 1.0\n",
      "loss: 0.004936800803989172, accuracy: 1.0\n",
      "loss: 0.004933231510221958, accuracy: 1.0\n",
      "loss: 0.004911962430924177, accuracy: 1.0\n",
      "loss: 0.004837065003812313, accuracy: 1.0\n",
      "loss: 0.004825511947274208, accuracy: 1.0\n",
      "loss: 0.00494250375777483, accuracy: 1.0\n",
      "loss: 0.004978232551366091, accuracy: 1.0\n",
      "loss: 0.004899714607745409, accuracy: 1.0\n",
      "loss: 0.004929851740598679, accuracy: 1.0\n",
      "loss: 0.004919501021504402, accuracy: 1.0\n",
      "loss: 0.004865544848144054, accuracy: 1.0\n",
      "loss: 0.0048620495945215225, accuracy: 1.0\n",
      "loss: 0.005025054328143597, accuracy: 1.0\n",
      "loss: 0.004781119525432587, accuracy: 1.0\n",
      "loss: 0.0048819794319570065, accuracy: 1.0\n",
      "loss: 0.004979575984179974, accuracy: 1.0\n",
      "loss: 0.004820770118385553, accuracy: 1.0\n",
      "loss: 0.00482088141143322, accuracy: 1.0\n",
      "loss: 0.004874102305620909, accuracy: 1.0\n",
      "loss: 0.004886725451797247, accuracy: 1.0\n",
      "loss: 0.004914344288408756, accuracy: 1.0\n",
      "loss: 0.004938406869769096, accuracy: 1.0\n",
      "loss: 0.004937188699841499, accuracy: 1.0\n",
      "loss: 0.004831825848668814, accuracy: 1.0\n",
      "loss: 0.0048302472569048405, accuracy: 1.0\n",
      "loss: 0.005030129570513964, accuracy: 1.0\n",
      "loss: 0.004931528586894274, accuracy: 1.0\n",
      "loss: 0.004907309077680111, accuracy: 1.0\n",
      "loss: 0.004881724715232849, accuracy: 1.0\n",
      "loss: 0.004877924453467131, accuracy: 1.0\n",
      "loss: 0.00486805709078908, accuracy: 1.0\n",
      "loss: 0.004809082485735416, accuracy: 1.0\n",
      "loss: 0.004835553467273712, accuracy: 1.0\n",
      "loss: 0.0049466900527477264, accuracy: 1.0\n",
      "loss: 0.004900417290627956, accuracy: 1.0\n",
      "loss: 0.0048224483616650105, accuracy: 1.0\n",
      "loss: 0.005015830975025892, accuracy: 1.0\n",
      "loss: 0.004873138386756182, accuracy: 1.0\n",
      "loss: 0.0048716068267822266, accuracy: 1.0\n",
      "loss: 0.005017735995352268, accuracy: 1.0\n",
      "loss: 0.0049188705161213875, accuracy: 1.0\n",
      "loss: 0.004875701852142811, accuracy: 1.0\n",
      "loss: 0.0047665066085755825, accuracy: 1.0\n",
      "loss: 0.004857570864260197, accuracy: 1.0\n",
      "loss: 0.004835342988371849, accuracy: 1.0\n",
      "loss: 0.004847544711083174, accuracy: 1.0\n",
      "loss: 0.004761198535561562, accuracy: 1.0\n",
      "loss: 0.0047715939581394196, accuracy: 1.0\n",
      "loss: 0.004904057364910841, accuracy: 1.0\n",
      "loss: 0.005076047033071518, accuracy: 1.0\n",
      "loss: 0.004860622342675924, accuracy: 1.0\n",
      "loss: 0.00480906805023551, accuracy: 1.0\n",
      "loss: 0.004898598417639732, accuracy: 1.0\n",
      "loss: 0.004865993745625019, accuracy: 1.0\n",
      "loss: 0.0047772470861673355, accuracy: 1.0\n",
      "loss: 0.004783676937222481, accuracy: 1.0\n",
      "loss: 0.004828971344977617, accuracy: 1.0\n",
      "loss: 0.004765798803418875, accuracy: 1.0\n",
      "loss: 0.004818324465304613, accuracy: 1.0\n",
      "loss: 0.004827461205422878, accuracy: 1.0\n",
      "loss: 0.004946825560182333, accuracy: 1.0\n",
      "loss: 0.004915910307317972, accuracy: 1.0\n",
      "loss: 0.004992364905774593, accuracy: 1.0\n",
      "loss: 0.004922729451209307, accuracy: 1.0\n",
      "loss: 0.00478275865316391, accuracy: 1.0\n",
      "loss: 0.004770061932504177, accuracy: 1.0\n",
      "loss: 0.0048226043581962585, accuracy: 1.0\n",
      "loss: 0.004812867846339941, accuracy: 1.0\n",
      "loss: 0.004774065222591162, accuracy: 1.0\n",
      "loss: 0.004791181534528732, accuracy: 1.0\n",
      "loss: 0.004773384891450405, accuracy: 1.0\n",
      "loss: 0.004861746449023485, accuracy: 1.0\n",
      "loss: 0.004735284019261599, accuracy: 1.0\n",
      "loss: 0.0049185543321073055, accuracy: 1.0\n",
      "loss: 0.004917601123452187, accuracy: 1.0\n",
      "loss: 0.00494336849078536, accuracy: 1.0\n",
      "loss: 0.00479367608204484, accuracy: 1.0\n",
      "loss: 0.004751958884298801, accuracy: 1.0\n",
      "loss: 0.004833131097257137, accuracy: 1.0\n",
      "loss: 0.004774599801748991, accuracy: 1.0\n",
      "loss: 0.005028167273849249, accuracy: 1.0\n",
      "loss: 0.004823858849704266, accuracy: 1.0\n",
      "loss: 0.004809493664652109, accuracy: 1.0\n",
      "loss: 0.0048170797526836395, accuracy: 1.0\n",
      "loss: 0.004709985572844744, accuracy: 1.0\n",
      "loss: 0.004804408643394709, accuracy: 1.0\n",
      "loss: 0.0048132906667888165, accuracy: 1.0\n",
      "loss: 0.004722018726170063, accuracy: 1.0\n",
      "loss: 0.004791179206222296, accuracy: 1.0\n",
      "loss: 0.004780569113790989, accuracy: 1.0\n",
      "loss: 0.0047258296981453896, accuracy: 1.0\n",
      "loss: 0.004701271653175354, accuracy: 1.0\n",
      "loss: 0.004788966849446297, accuracy: 1.0\n",
      "loss: 0.004752724897116423, accuracy: 1.0\n",
      "loss: 0.004735805094242096, accuracy: 1.0\n",
      "loss: 0.0047278860583901405, accuracy: 1.0\n",
      "loss: 0.004659888334572315, accuracy: 1.0\n",
      "loss: 0.004762810189276934, accuracy: 1.0\n",
      "loss: 0.004874962382018566, accuracy: 1.0\n",
      "loss: 0.004754015244543552, accuracy: 1.0\n",
      "loss: 0.0047409492544829845, accuracy: 1.0\n",
      "loss: 0.004761137068271637, accuracy: 1.0\n",
      "loss: 0.004722407553344965, accuracy: 1.0\n",
      "loss: 0.004759183153510094, accuracy: 1.0\n",
      "loss: 0.004928096197545528, accuracy: 1.0\n",
      "loss: 0.004701161757111549, accuracy: 1.0\n",
      "loss: 0.004724021069705486, accuracy: 1.0\n",
      "loss: 0.004672941751778126, accuracy: 1.0\n",
      "loss: 0.004796333611011505, accuracy: 1.0\n",
      "loss: 0.004763208795338869, accuracy: 1.0\n",
      "loss: 0.004785832483321428, accuracy: 1.0\n",
      "loss: 0.004804558586329222, accuracy: 1.0\n",
      "loss: 0.004664416890591383, accuracy: 1.0\n",
      "loss: 0.004768718499690294, accuracy: 1.0\n",
      "loss: 0.004752170294523239, accuracy: 1.0\n",
      "loss: 0.004689169581979513, accuracy: 1.0\n",
      "loss: 0.004705814179033041, accuracy: 1.0\n",
      "loss: 0.0048819673247635365, accuracy: 1.0\n",
      "loss: 0.004751523025333881, accuracy: 1.0\n",
      "loss: 0.004751316737383604, accuracy: 1.0\n",
      "loss: 0.004812257830053568, accuracy: 1.0\n",
      "loss: 0.00473529240116477, accuracy: 1.0\n",
      "loss: 0.004808207508176565, accuracy: 1.0\n",
      "loss: 0.0046541751362383366, accuracy: 1.0\n",
      "loss: 0.004699646960943937, accuracy: 1.0\n",
      "loss: 0.004773150663822889, accuracy: 1.0\n",
      "loss: 0.004827301017940044, accuracy: 1.0\n",
      "loss: 0.004738792777061462, accuracy: 1.0\n",
      "loss: 0.004745963029563427, accuracy: 1.0\n",
      "loss: 0.0046957917511463165, accuracy: 1.0\n",
      "loss: 0.005091526545584202, accuracy: 1.0\n",
      "loss: 0.004785363562405109, accuracy: 1.0\n",
      "loss: 0.0048648156225681305, accuracy: 1.0\n",
      "loss: 0.0047166780568659306, accuracy: 1.0\n",
      "loss: 0.004666892811655998, accuracy: 1.0\n",
      "loss: 0.004742669872939587, accuracy: 1.0\n",
      "loss: 0.00468904385343194, accuracy: 1.0\n",
      "loss: 0.004671803675591946, accuracy: 1.0\n",
      "loss: 0.004714536480605602, accuracy: 1.0\n",
      "loss: 0.004663617350161076, accuracy: 1.0\n",
      "loss: 0.004590114112943411, accuracy: 1.0\n",
      "loss: 0.0047688717022538185, accuracy: 1.0\n",
      "loss: 0.0047927615232765675, accuracy: 1.0\n",
      "loss: 0.004680520389229059, accuracy: 1.0\n",
      "loss: 0.004827749915421009, accuracy: 1.0\n",
      "loss: 0.004631362855434418, accuracy: 1.0\n",
      "loss: 0.004837088752537966, accuracy: 1.0\n",
      "loss: 0.0048075588420033455, accuracy: 1.0\n",
      "loss: 0.004830957390367985, accuracy: 1.0\n",
      "loss: 0.00474889762699604, accuracy: 1.0\n",
      "loss: 0.004732287023216486, accuracy: 1.0\n",
      "loss: 0.0047254059463739395, accuracy: 1.0\n",
      "loss: 0.004757764283567667, accuracy: 1.0\n",
      "loss: 0.00477934256196022, accuracy: 1.0\n",
      "loss: 0.004743904806673527, accuracy: 1.0\n",
      "loss: 0.0047057862393558025, accuracy: 1.0\n",
      "loss: 0.004692661575973034, accuracy: 1.0\n",
      "loss: 0.004684318322688341, accuracy: 1.0\n",
      "loss: 0.00472218357026577, accuracy: 1.0\n",
      "loss: 0.004628039430826902, accuracy: 1.0\n",
      "loss: 0.004705148749053478, accuracy: 1.0\n",
      "loss: 0.004686039872467518, accuracy: 1.0\n",
      "loss: 0.004671325441449881, accuracy: 1.0\n",
      "loss: 0.004634827375411987, accuracy: 1.0\n",
      "loss: 0.0046624476090073586, accuracy: 1.0\n",
      "loss: 0.0046897223219275475, accuracy: 1.0\n",
      "loss: 0.004695652052760124, accuracy: 1.0\n",
      "loss: 0.0046581909991800785, accuracy: 1.0\n",
      "loss: 0.0048379371874034405, accuracy: 1.0\n",
      "loss: 0.004808120895177126, accuracy: 1.0\n",
      "loss: 0.004712083842605352, accuracy: 1.0\n",
      "loss: 0.00473292451351881, accuracy: 1.0\n",
      "loss: 0.0046776775270700455, accuracy: 1.0\n",
      "loss: 0.004718116018921137, accuracy: 1.0\n",
      "loss: 0.004706755746155977, accuracy: 1.0\n",
      "loss: 0.004602149128913879, accuracy: 1.0\n",
      "loss: 0.0047768037766218185, accuracy: 1.0\n",
      "loss: 0.004758099559694529, accuracy: 1.0\n",
      "loss: 0.004701034165918827, accuracy: 1.0\n",
      "loss: 0.004732487723231316, accuracy: 1.0\n",
      "loss: 0.004600547719746828, accuracy: 1.0\n",
      "loss: 0.004595760256052017, accuracy: 1.0\n",
      "loss: 0.004628635477274656, accuracy: 1.0\n",
      "loss: 0.004671652801334858, accuracy: 1.0\n",
      "loss: 0.004563807975500822, accuracy: 1.0\n",
      "loss: 0.004604533314704895, accuracy: 1.0\n",
      "loss: 0.00456153554841876, accuracy: 1.0\n",
      "loss: 0.0046441019512712955, accuracy: 1.0\n",
      "loss: 0.004961369559168816, accuracy: 1.0\n",
      "loss: 0.004612206481397152, accuracy: 1.0\n",
      "loss: 0.004654788412153721, accuracy: 1.0\n",
      "loss: 0.004623888060450554, accuracy: 1.0\n",
      "loss: 0.004746054764837027, accuracy: 1.0\n",
      "loss: 0.0047303298488259315, accuracy: 1.0\n",
      "loss: 0.004651534836739302, accuracy: 1.0\n",
      "loss: 0.004816541448235512, accuracy: 1.0\n",
      "loss: 0.0045860689133405685, accuracy: 1.0\n",
      "loss: 0.004729457199573517, accuracy: 1.0\n",
      "loss: 0.0046063982881605625, accuracy: 1.0\n",
      "loss: 0.0046056341379880905, accuracy: 1.0\n",
      "loss: 0.00463112024590373, accuracy: 1.0\n",
      "loss: 0.004606043919920921, accuracy: 1.0\n",
      "loss: 0.004622073378413916, accuracy: 1.0\n",
      "loss: 0.004659339319914579, accuracy: 1.0\n",
      "loss: 0.0047147236764431, accuracy: 1.0\n",
      "loss: 0.004686915781348944, accuracy: 1.0\n",
      "loss: 0.004530303180217743, accuracy: 1.0\n",
      "loss: 0.004655227065086365, accuracy: 1.0\n",
      "loss: 0.004704807419329882, accuracy: 1.0\n",
      "loss: 0.0045815338380634785, accuracy: 1.0\n",
      "loss: 0.004653483629226685, accuracy: 1.0\n",
      "loss: 0.004625651985406876, accuracy: 1.0\n",
      "loss: 0.0046425978653132915, accuracy: 1.0\n",
      "loss: 0.004629739094525576, accuracy: 1.0\n",
      "loss: 0.004665709100663662, accuracy: 1.0\n",
      "loss: 0.0045915162190794945, accuracy: 1.0\n",
      "loss: 0.004576130770146847, accuracy: 1.0\n",
      "loss: 0.004657769575715065, accuracy: 1.0\n",
      "loss: 0.004621537402272224, accuracy: 1.0\n",
      "loss: 0.004528425633907318, accuracy: 1.0\n",
      "loss: 0.004636403173208237, accuracy: 1.0\n",
      "loss: 0.004702579230070114, accuracy: 1.0\n",
      "loss: 0.0046740020625293255, accuracy: 1.0\n",
      "loss: 0.004688084591180086, accuracy: 1.0\n",
      "loss: 0.004732406232506037, accuracy: 1.0\n",
      "loss: 0.004643735941499472, accuracy: 1.0\n",
      "loss: 0.00461028516292572, accuracy: 1.0\n",
      "loss: 0.0045739891938865185, accuracy: 1.0\n",
      "loss: 0.004548592492938042, accuracy: 1.0\n",
      "loss: 0.004649432376027107, accuracy: 1.0\n",
      "loss: 0.004617603495717049, accuracy: 1.0\n",
      "loss: 0.004669840447604656, accuracy: 1.0\n",
      "loss: 0.004705184139311314, accuracy: 1.0\n",
      "loss: 0.0046196370385587215, accuracy: 1.0\n",
      "loss: 0.004523205105215311, accuracy: 1.0\n",
      "loss: 0.00460110604763031, accuracy: 1.0\n",
      "loss: 0.00452874880284071, accuracy: 1.0\n",
      "loss: 0.004625281319022179, accuracy: 1.0\n",
      "loss: 0.0046882242895662785, accuracy: 1.0\n",
      "loss: 0.004637779202312231, accuracy: 1.0\n",
      "loss: 0.004801363218575716, accuracy: 1.0\n",
      "loss: 0.0045696706511080265, accuracy: 1.0\n",
      "loss: 0.004589702934026718, accuracy: 1.0\n",
      "loss: 0.004699641838669777, accuracy: 1.0\n",
      "loss: 0.0046502272598445415, accuracy: 1.0\n",
      "loss: 0.004590147640556097, accuracy: 1.0\n",
      "loss: 0.004566191229969263, accuracy: 1.0\n",
      "loss: 0.004576985724270344, accuracy: 1.0\n",
      "loss: 0.004659783560782671, accuracy: 1.0\n",
      "loss: 0.004555784165859222, accuracy: 1.0\n",
      "loss: 0.004562829155474901, accuracy: 1.0\n",
      "loss: 0.00463690422475338, accuracy: 1.0\n",
      "loss: 0.004711399786174297, accuracy: 1.0\n",
      "loss: 0.004491046536713839, accuracy: 1.0\n",
      "loss: 0.004555389750748873, accuracy: 1.0\n",
      "loss: 0.004550665616989136, accuracy: 1.0\n",
      "loss: 0.004591716453433037, accuracy: 1.0\n",
      "loss: 0.004507949575781822, accuracy: 1.0\n",
      "loss: 0.004569782875478268, accuracy: 1.0\n",
      "loss: 0.00453305896371603, accuracy: 1.0\n",
      "loss: 0.004779578186571598, accuracy: 1.0\n",
      "loss: 0.004610831383615732, accuracy: 1.0\n",
      "loss: 0.004652561154216528, accuracy: 1.0\n",
      "loss: 0.004567661322653294, accuracy: 1.0\n",
      "loss: 0.0045395465567708015, accuracy: 1.0\n",
      "loss: 0.004531679674983025, accuracy: 1.0\n",
      "loss: 0.004502813331782818, accuracy: 1.0\n",
      "loss: 0.0044587585143744946, accuracy: 1.0\n",
      "loss: 0.004689352586865425, accuracy: 1.0\n",
      "loss: 0.004508226178586483, accuracy: 1.0\n",
      "loss: 0.0045076594687998295, accuracy: 1.0\n",
      "loss: 0.004563148133456707, accuracy: 1.0\n",
      "loss: 0.004575755912810564, accuracy: 1.0\n",
      "loss: 0.004586037714034319, accuracy: 1.0\n",
      "loss: 0.004524161573499441, accuracy: 1.0\n",
      "loss: 0.004474186804145575, accuracy: 1.0\n",
      "loss: 0.0045808544382452965, accuracy: 1.0\n",
      "loss: 0.0046074348501861095, accuracy: 1.0\n",
      "loss: 0.004603822249919176, accuracy: 1.0\n",
      "loss: 0.004575737752020359, accuracy: 1.0\n",
      "loss: 0.004595243372023106, accuracy: 1.0\n",
      "loss: 0.004537736997008324, accuracy: 1.0\n",
      "loss: 0.0044922009110450745, accuracy: 1.0\n",
      "loss: 0.004556771367788315, accuracy: 1.0\n",
      "loss: 0.004528322722762823, accuracy: 1.0\n",
      "loss: 0.00462142052128911, accuracy: 1.0\n",
      "loss: 0.004538866225630045, accuracy: 1.0\n",
      "loss: 0.004551391117274761, accuracy: 1.0\n",
      "loss: 0.004563893657177687, accuracy: 1.0\n",
      "loss: 0.004493804182857275, accuracy: 1.0\n",
      "loss: 0.004684720654040575, accuracy: 1.0\n",
      "loss: 0.0045043788850307465, accuracy: 1.0\n",
      "loss: 0.004516405984759331, accuracy: 1.0\n",
      "loss: 0.004527506418526173, accuracy: 1.0\n",
      "loss: 0.004556105472147465, accuracy: 1.0\n",
      "loss: 0.004447999410331249, accuracy: 1.0\n",
      "loss: 0.004459277726709843, accuracy: 1.0\n",
      "loss: 0.004478852730244398, accuracy: 1.0\n",
      "loss: 0.004517699126154184, accuracy: 1.0\n",
      "loss: 0.004613744560629129, accuracy: 1.0\n",
      "loss: 0.004473199602216482, accuracy: 1.0\n",
      "loss: 0.004477451089769602, accuracy: 1.0\n",
      "loss: 0.0045100930146873, accuracy: 1.0\n",
      "loss: 0.004447710234671831, accuracy: 1.0\n",
      "loss: 0.004496625158935785, accuracy: 1.0\n",
      "loss: 0.0044670552015304565, accuracy: 1.0\n",
      "loss: 0.0044611063785851, accuracy: 1.0\n",
      "loss: 0.0044950987212359905, accuracy: 1.0\n",
      "loss: 0.0044168285094201565, accuracy: 1.0\n",
      "loss: 0.0044923825189471245, accuracy: 1.0\n",
      "loss: 0.004452734254300594, accuracy: 1.0\n",
      "loss: 0.0046556140296161175, accuracy: 1.0\n",
      "loss: 0.004482986871153116, accuracy: 1.0\n",
      "loss: 0.0046081263571977615, accuracy: 1.0\n",
      "loss: 0.004606952425092459, accuracy: 1.0\n",
      "loss: 0.004529634024947882, accuracy: 1.0\n",
      "loss: 0.004428860265761614, accuracy: 1.0\n",
      "loss: 0.004576897248625755, accuracy: 1.0\n",
      "loss: 0.0045983144082129, accuracy: 1.0\n",
      "loss: 0.004500080831348896, accuracy: 1.0\n",
      "loss: 0.004600277170538902, accuracy: 1.0\n",
      "loss: 0.004496013280004263, accuracy: 1.0\n",
      "loss: 0.004433670546859503, accuracy: 1.0\n",
      "loss: 0.004478842485696077, accuracy: 1.0\n",
      "loss: 0.00449774693697691, accuracy: 1.0\n",
      "loss: 0.0044606891460716724, accuracy: 1.0\n",
      "loss: 0.004518348723649979, accuracy: 1.0\n",
      "loss: 0.004506802186369896, accuracy: 1.0\n",
      "loss: 0.0043869647197425365, accuracy: 1.0\n",
      "loss: 0.004503985401242971, accuracy: 1.0\n",
      "loss: 0.004523646552115679, accuracy: 1.0\n",
      "loss: 0.004531895276159048, accuracy: 1.0\n",
      "loss: 0.004620776977390051, accuracy: 1.0\n",
      "loss: 0.004619747865945101, accuracy: 1.0\n",
      "loss: 0.004473516251891851, accuracy: 1.0\n",
      "loss: 0.004515540320426226, accuracy: 1.0\n",
      "loss: 0.004409869201481342, accuracy: 1.0\n",
      "loss: 0.004468756262212992, accuracy: 1.0\n",
      "loss: 0.004462705925107002, accuracy: 1.0\n",
      "loss: 0.004541042260825634, accuracy: 1.0\n",
      "loss: 0.004533316940069199, accuracy: 1.0\n",
      "loss: 0.00448470376431942, accuracy: 1.0\n",
      "loss: 0.004487731959670782, accuracy: 1.0\n",
      "loss: 0.004412825219333172, accuracy: 1.0\n",
      "loss: 0.004514963831752539, accuracy: 1.0\n",
      "loss: 0.00463330140337348, accuracy: 1.0\n",
      "loss: 0.004472353495657444, accuracy: 1.0\n",
      "loss: 0.004581444431096315, accuracy: 1.0\n",
      "loss: 0.004444523248821497, accuracy: 1.0\n",
      "loss: 0.004470814019441605, accuracy: 1.0\n",
      "loss: 0.004513437394052744, accuracy: 1.0\n",
      "loss: 0.004532646853476763, accuracy: 1.0\n",
      "loss: 0.00445518409833312, accuracy: 1.0\n",
      "loss: 0.004443917889147997, accuracy: 1.0\n",
      "loss: 0.004524655640125275, accuracy: 1.0\n",
      "loss: 0.004434990230947733, accuracy: 1.0\n",
      "loss: 0.004442906007170677, accuracy: 1.0\n",
      "loss: 0.004392893984913826, accuracy: 1.0\n",
      "loss: 0.004479452036321163, accuracy: 1.0\n",
      "loss: 0.004398767836391926, accuracy: 1.0\n",
      "loss: 0.004446104168891907, accuracy: 1.0\n",
      "loss: 0.004416331183165312, accuracy: 1.0\n",
      "loss: 0.004399022087454796, accuracy: 1.0\n",
      "loss: 0.00443760771304369, accuracy: 1.0\n",
      "loss: 0.004489542916417122, accuracy: 1.0\n",
      "loss: 0.004511548206210136, accuracy: 1.0\n",
      "loss: 0.004560179077088833, accuracy: 1.0\n",
      "loss: 0.004458336159586906, accuracy: 1.0\n",
      "loss: 0.004496626555919647, accuracy: 1.0\n",
      "loss: 0.004392652772367001, accuracy: 1.0\n",
      "loss: 0.004416651092469692, accuracy: 1.0\n",
      "loss: 0.004436527844518423, accuracy: 1.0\n",
      "loss: 0.004385158885270357, accuracy: 1.0\n",
      "loss: 0.0044086528941988945, accuracy: 1.0\n",
      "loss: 0.004364450462162495, accuracy: 1.0\n",
      "loss: 0.004496409557759762, accuracy: 1.0\n",
      "loss: 0.004439290147274733, accuracy: 1.0\n",
      "loss: 0.0043822722509503365, accuracy: 1.0\n",
      "loss: 0.004502791911363602, accuracy: 1.0\n",
      "loss: 0.004414224531501532, accuracy: 1.0\n",
      "loss: 0.004377380944788456, accuracy: 1.0\n",
      "loss: 0.004382620565593243, accuracy: 1.0\n",
      "loss: 0.00446490989997983, accuracy: 1.0\n",
      "loss: 0.004374666605144739, accuracy: 1.0\n",
      "loss: 0.00438507366925478, accuracy: 1.0\n",
      "loss: 0.0044377087615430355, accuracy: 1.0\n",
      "loss: 0.004411341622471809, accuracy: 1.0\n",
      "loss: 0.004411476198583841, accuracy: 1.0\n",
      "loss: 0.004428713582456112, accuracy: 1.0\n",
      "loss: 0.004454073496162891, accuracy: 1.0\n",
      "loss: 0.0044002230279147625, accuracy: 1.0\n",
      "loss: 0.004492897540330887, accuracy: 1.0\n",
      "loss: 0.004477307666093111, accuracy: 1.0\n",
      "loss: 0.004422393627464771, accuracy: 1.0\n",
      "loss: 0.00443378696218133, accuracy: 1.0\n",
      "loss: 0.00436651986092329, accuracy: 1.0\n",
      "loss: 0.004541190341114998, accuracy: 1.0\n",
      "loss: 0.0043657501228153706, accuracy: 1.0\n",
      "loss: 0.004406244494020939, accuracy: 1.0\n",
      "loss: 0.004390589892864227, accuracy: 1.0\n",
      "loss: 0.004348777234554291, accuracy: 1.0\n",
      "loss: 0.004304687958210707, accuracy: 1.0\n",
      "loss: 0.004430791828781366, accuracy: 1.0\n",
      "loss: 0.004359610844403505, accuracy: 1.0\n",
      "loss: 0.004358879290521145, accuracy: 1.0\n",
      "loss: 0.004445946775376797, accuracy: 1.0\n",
      "loss: 0.004347478970885277, accuracy: 1.0\n",
      "loss: 0.004334300756454468, accuracy: 1.0\n",
      "loss: 0.004430312663316727, accuracy: 1.0\n",
      "loss: 0.00439287768676877, accuracy: 1.0\n",
      "loss: 0.004355445969849825, accuracy: 1.0\n",
      "loss: 0.0044340468011796474, accuracy: 1.0\n",
      "loss: 0.004378854297101498, accuracy: 1.0\n",
      "loss: 0.004368769936263561, accuracy: 1.0\n",
      "loss: 0.004463352262973785, accuracy: 1.0\n",
      "loss: 0.004519838374108076, accuracy: 1.0\n",
      "loss: 0.004393726121634245, accuracy: 1.0\n",
      "loss: 0.004414977040141821, accuracy: 1.0\n",
      "loss: 0.004487902857363224, accuracy: 1.0\n",
      "loss: 0.004347833339124918, accuracy: 1.0\n",
      "loss: 0.004372930154204369, accuracy: 1.0\n",
      "loss: 0.0043942490592598915, accuracy: 1.0\n",
      "loss: 0.004285487346351147, accuracy: 1.0\n",
      "loss: 0.0043810089118778706, accuracy: 1.0\n",
      "loss: 0.004311818163841963, accuracy: 1.0\n",
      "loss: 0.004412432666867971, accuracy: 1.0\n",
      "loss: 0.004357286263257265, accuracy: 1.0\n",
      "loss: 0.004323497414588928, accuracy: 1.0\n",
      "loss: 0.004341134335845709, accuracy: 1.0\n",
      "loss: 0.004342850763350725, accuracy: 1.0\n",
      "loss: 0.0043547688983380795, accuracy: 1.0\n",
      "loss: 0.004408086184412241, accuracy: 1.0\n",
      "loss: 0.004368520341813564, accuracy: 1.0\n",
      "loss: 0.0043733692727983, accuracy: 1.0\n",
      "loss: 0.00450938381254673, accuracy: 1.0\n",
      "loss: 0.004326890222728252, accuracy: 1.0\n",
      "loss: 0.00434105284512043, accuracy: 1.0\n",
      "loss: 0.004351157695055008, accuracy: 1.0\n",
      "loss: 0.004447048995643854, accuracy: 1.0\n",
      "loss: 0.004302585031837225, accuracy: 1.0\n",
      "loss: 0.004362933337688446, accuracy: 1.0\n",
      "loss: 0.004455499351024628, accuracy: 1.0\n",
      "loss: 0.004376884549856186, accuracy: 1.0\n",
      "loss: 0.004325396381318569, accuracy: 1.0\n",
      "loss: 0.004370162263512611, accuracy: 1.0\n",
      "loss: 0.004474173299968243, accuracy: 1.0\n",
      "loss: 0.004361788276582956, accuracy: 1.0\n",
      "loss: 0.004484191536903381, accuracy: 1.0\n",
      "loss: 0.004437107592821121, accuracy: 1.0\n",
      "loss: 0.004351153038442135, accuracy: 1.0\n",
      "loss: 0.004393278621137142, accuracy: 1.0\n",
      "loss: 0.0042675635777413845, accuracy: 1.0\n",
      "loss: 0.004398096352815628, accuracy: 1.0\n",
      "loss: 0.004325492307543755, accuracy: 1.0\n",
      "loss: 0.004343137145042419, accuracy: 1.0\n",
      "loss: 0.004278940614312887, accuracy: 1.0\n",
      "loss: 0.004310245625674725, accuracy: 1.0\n",
      "loss: 0.004324852954596281, accuracy: 1.0\n",
      "loss: 0.004310310818254948, accuracy: 1.0\n",
      "loss: 0.00436449283733964, accuracy: 1.0\n",
      "loss: 0.004360932391136885, accuracy: 1.0\n",
      "loss: 0.004322523716837168, accuracy: 1.0\n",
      "loss: 0.00431934604421258, accuracy: 1.0\n",
      "loss: 0.0043209935538470745, accuracy: 1.0\n",
      "loss: 0.004368429072201252, accuracy: 1.0\n",
      "loss: 0.004373741801828146, accuracy: 1.0\n",
      "loss: 0.004273706115782261, accuracy: 1.0\n",
      "loss: 0.004293499980121851, accuracy: 1.0\n",
      "loss: 0.00427274452522397, accuracy: 1.0\n",
      "loss: 0.0043529123067855835, accuracy: 1.0\n",
      "loss: 0.004368106834590435, accuracy: 1.0\n",
      "loss: 0.004343430045992136, accuracy: 1.0\n",
      "loss: 0.004319504834711552, accuracy: 1.0\n",
      "loss: 0.004395950119942427, accuracy: 1.0\n",
      "loss: 0.004275245126336813, accuracy: 1.0\n",
      "loss: 0.004484210163354874, accuracy: 1.0\n",
      "loss: 0.004325986374169588, accuracy: 1.0\n",
      "loss: 0.004491936415433884, accuracy: 1.0\n",
      "loss: 0.00429851608350873, accuracy: 1.0\n",
      "loss: 0.004340838640928268, accuracy: 1.0\n",
      "loss: 0.004342071712017059, accuracy: 1.0\n",
      "loss: 0.00435921223834157, accuracy: 1.0\n",
      "loss: 0.004349699709564447, accuracy: 1.0\n",
      "loss: 0.004466937854886055, accuracy: 1.0\n",
      "loss: 0.004326543305069208, accuracy: 1.0\n",
      "loss: 0.004245101939886808, accuracy: 1.0\n",
      "loss: 0.004394921939820051, accuracy: 1.0\n",
      "loss: 0.00436807656660676, accuracy: 1.0\n",
      "loss: 0.0043211220763623714, accuracy: 1.0\n",
      "loss: 0.00430886959657073, accuracy: 1.0\n",
      "loss: 0.004300151951611042, accuracy: 1.0\n",
      "loss: 0.004377206787467003, accuracy: 1.0\n",
      "loss: 0.004236789420247078, accuracy: 1.0\n",
      "loss: 0.00425491901114583, accuracy: 1.0\n",
      "loss: 0.004252932034432888, accuracy: 1.0\n",
      "loss: 0.004690033849328756, accuracy: 1.0\n",
      "loss: 0.0042497823014855385, accuracy: 1.0\n",
      "loss: 0.004265584051609039, accuracy: 1.0\n",
      "loss: 0.0043037766590714455, accuracy: 1.0\n",
      "loss: 0.004192017484456301, accuracy: 1.0\n",
      "loss: 0.004395585507154465, accuracy: 1.0\n",
      "loss: 0.004341067746281624, accuracy: 1.0\n",
      "loss: 0.004351094830781221, accuracy: 1.0\n",
      "loss: 0.004328115843236446, accuracy: 1.0\n",
      "loss: 0.004239825531840324, accuracy: 1.0\n",
      "loss: 0.004290681332349777, accuracy: 1.0\n",
      "loss: 0.00431613577529788, accuracy: 1.0\n",
      "loss: 0.004373545292764902, accuracy: 1.0\n",
      "loss: 0.004311430733650923, accuracy: 1.0\n",
      "loss: 0.0043748291209340096, accuracy: 1.0\n",
      "loss: 0.004302839282900095, accuracy: 1.0\n",
      "loss: 0.004244324751198292, accuracy: 1.0\n",
      "loss: 0.004234394524246454, accuracy: 1.0\n",
      "loss: 0.004260324407368898, accuracy: 1.0\n",
      "loss: 0.004286549054086208, accuracy: 1.0\n",
      "loss: 0.004246016964316368, accuracy: 1.0\n",
      "loss: 0.004225468263030052, accuracy: 1.0\n",
      "loss: 0.004214663524180651, accuracy: 1.0\n",
      "loss: 0.00420816196128726, accuracy: 1.0\n",
      "loss: 0.00422277208417654, accuracy: 1.0\n",
      "loss: 0.004318128805607557, accuracy: 1.0\n",
      "loss: 0.004215281456708908, accuracy: 1.0\n",
      "loss: 0.004224917385727167, accuracy: 1.0\n",
      "loss: 0.004241783171892166, accuracy: 1.0\n",
      "loss: 0.004260994493961334, accuracy: 1.0\n",
      "loss: 0.004277308005839586, accuracy: 1.0\n",
      "loss: 0.004314294084906578, accuracy: 1.0\n",
      "loss: 0.004312974866479635, accuracy: 1.0\n",
      "loss: 0.004266725387424231, accuracy: 1.0\n",
      "loss: 0.004164916928857565, accuracy: 1.0\n",
      "loss: 0.004374288953840733, accuracy: 1.0\n",
      "loss: 0.004204217344522476, accuracy: 1.0\n",
      "loss: 0.004271191544830799, accuracy: 1.0\n",
      "loss: 0.004238946363329887, accuracy: 1.0\n",
      "loss: 0.0042856852523982525, accuracy: 1.0\n",
      "loss: 0.0042514377273619175, accuracy: 1.0\n",
      "loss: 0.00422015693038702, accuracy: 1.0\n",
      "loss: 0.004187560174614191, accuracy: 1.0\n",
      "loss: 0.0042962306179106236, accuracy: 1.0\n",
      "loss: 0.0043294900096952915, accuracy: 1.0\n",
      "loss: 0.004251830279827118, accuracy: 1.0\n",
      "loss: 0.004245320800691843, accuracy: 1.0\n",
      "loss: 0.00427973223850131, accuracy: 1.0\n",
      "loss: 0.004209837876260281, accuracy: 1.0\n",
      "loss: 0.00418126629665494, accuracy: 1.0\n",
      "loss: 0.0042433044873178005, accuracy: 1.0\n",
      "loss: 0.004277429077774286, accuracy: 1.0\n",
      "loss: 0.004220244940370321, accuracy: 1.0\n",
      "loss: 0.004197800066322088, accuracy: 1.0\n",
      "loss: 0.004237562883645296, accuracy: 1.0\n",
      "loss: 0.00434089032933116, accuracy: 1.0\n",
      "loss: 0.0042625851929187775, accuracy: 1.0\n",
      "loss: 0.004230346065014601, accuracy: 1.0\n",
      "loss: 0.004174989648163319, accuracy: 1.0\n",
      "loss: 0.004251472651958466, accuracy: 1.0\n",
      "loss: 0.004158706869930029, accuracy: 1.0\n",
      "loss: 0.004206255078315735, accuracy: 1.0\n",
      "loss: 0.004269979894161224, accuracy: 1.0\n",
      "loss: 0.004181056283414364, accuracy: 1.0\n",
      "loss: 0.004224471282213926, accuracy: 1.0\n",
      "loss: 0.004299950320273638, accuracy: 1.0\n",
      "loss: 0.004153543151915073, accuracy: 1.0\n",
      "loss: 0.004195756278932095, accuracy: 1.0\n",
      "loss: 0.0043222238309681416, accuracy: 1.0\n",
      "loss: 0.004252108279615641, accuracy: 1.0\n",
      "loss: 0.004221664275974035, accuracy: 1.0\n",
      "loss: 0.004201444331556559, accuracy: 1.0\n",
      "loss: 0.004121467471122742, accuracy: 1.0\n",
      "loss: 0.004181475844234228, accuracy: 1.0\n",
      "loss: 0.00420684227719903, accuracy: 1.0\n",
      "loss: 0.0041611879132688046, accuracy: 1.0\n",
      "loss: 0.004288082476705313, accuracy: 1.0\n",
      "loss: 0.004171798937022686, accuracy: 1.0\n",
      "loss: 0.004241784568876028, accuracy: 1.0\n",
      "loss: 0.004252962302416563, accuracy: 1.0\n",
      "loss: 0.004251689650118351, accuracy: 1.0\n",
      "loss: 0.004171122331172228, accuracy: 1.0\n",
      "loss: 0.0042669824324548244, accuracy: 1.0\n",
      "loss: 0.004324109293520451, accuracy: 1.0\n",
      "loss: 0.004161603283137083, accuracy: 1.0\n",
      "loss: 0.004214131738990545, accuracy: 1.0\n",
      "loss: 0.004211498890072107, accuracy: 1.0\n",
      "loss: 0.004206361249089241, accuracy: 1.0\n",
      "loss: 0.004179042298346758, accuracy: 1.0\n",
      "loss: 0.00425248546525836, accuracy: 1.0\n",
      "loss: 0.004167765378952026, accuracy: 1.0\n",
      "loss: 0.004220273345708847, accuracy: 1.0\n",
      "loss: 0.00424045929685235, accuracy: 1.0\n",
      "loss: 0.004141668323427439, accuracy: 1.0\n",
      "loss: 0.004215676803141832, accuracy: 1.0\n",
      "loss: 0.004166979342699051, accuracy: 1.0\n",
      "loss: 0.004190594889223576, accuracy: 1.0\n",
      "loss: 0.004119680728763342, accuracy: 1.0\n",
      "loss: 0.004210595972836018, accuracy: 1.0\n",
      "loss: 0.004140595439821482, accuracy: 1.0\n",
      "loss: 0.004141791723668575, accuracy: 1.0\n",
      "loss: 0.004213005304336548, accuracy: 1.0\n",
      "loss: 0.004128551576286554, accuracy: 1.0\n",
      "loss: 0.004174909554421902, accuracy: 1.0\n",
      "loss: 0.004126003012061119, accuracy: 1.0\n",
      "loss: 0.004172259476035833, accuracy: 1.0\n",
      "loss: 0.0042743622325360775, accuracy: 1.0\n",
      "loss: 0.004203687887638807, accuracy: 1.0\n",
      "loss: 0.0042133028618991375, accuracy: 1.0\n",
      "loss: 0.004184957593679428, accuracy: 1.0\n",
      "loss: 0.004165272228419781, accuracy: 1.0\n",
      "loss: 0.0042024338617920876, accuracy: 1.0\n",
      "loss: 0.004228288307785988, accuracy: 1.0\n",
      "loss: 0.004156104288995266, accuracy: 1.0\n",
      "loss: 0.004101356957107782, accuracy: 1.0\n",
      "loss: 0.004140041768550873, accuracy: 1.0\n",
      "loss: 0.004226943012326956, accuracy: 1.0\n",
      "loss: 0.00422156136482954, accuracy: 1.0\n",
      "loss: 0.004161860328167677, accuracy: 1.0\n",
      "loss: 0.004242096096277237, accuracy: 1.0\n",
      "loss: 0.004120707046240568, accuracy: 1.0\n",
      "loss: 0.004274439997971058, accuracy: 1.0\n",
      "loss: 0.004147729370743036, accuracy: 1.0\n",
      "loss: 0.00409436970949173, accuracy: 1.0\n",
      "loss: 0.004118659067898989, accuracy: 1.0\n",
      "loss: 0.0041348435916006565, accuracy: 1.0\n",
      "loss: 0.004188027698546648, accuracy: 1.0\n",
      "loss: 0.00416905153542757, accuracy: 1.0\n",
      "loss: 0.004159627482295036, accuracy: 1.0\n",
      "loss: 0.004159645643085241, accuracy: 1.0\n",
      "loss: 0.004203298594802618, accuracy: 1.0\n",
      "loss: 0.004098018165677786, accuracy: 1.0\n",
      "loss: 0.004137326031923294, accuracy: 1.0\n",
      "loss: 0.004165449645370245, accuracy: 1.0\n",
      "loss: 0.004384174942970276, accuracy: 1.0\n",
      "loss: 0.004232441540807486, accuracy: 1.0\n",
      "loss: 0.004178444389253855, accuracy: 1.0\n",
      "loss: 0.004082567524164915, accuracy: 1.0\n",
      "loss: 0.00416572717949748, accuracy: 1.0\n",
      "loss: 0.0041909306310117245, accuracy: 1.0\n",
      "loss: 0.004158985801041126, accuracy: 1.0\n",
      "loss: 0.004091509152203798, accuracy: 1.0\n",
      "loss: 0.00410528713837266, accuracy: 1.0\n",
      "loss: 0.004082704894244671, accuracy: 1.0\n",
      "loss: 0.004056169185787439, accuracy: 1.0\n",
      "loss: 0.004179042298346758, accuracy: 1.0\n",
      "loss: 0.004299641586840153, accuracy: 1.0\n",
      "loss: 0.004167442210018635, accuracy: 1.0\n",
      "loss: 0.00407713558524847, accuracy: 1.0\n",
      "loss: 0.004118218552321196, accuracy: 1.0\n",
      "loss: 0.004054951947182417, accuracy: 1.0\n",
      "loss: 0.00412549776956439, accuracy: 1.0\n",
      "loss: 0.004273198079317808, accuracy: 1.0\n",
      "loss: 0.0041222963482141495, accuracy: 1.0\n",
      "loss: 0.004177316091954708, accuracy: 1.0\n",
      "loss: 0.004074892494827509, accuracy: 1.0\n",
      "loss: 0.004092010203748941, accuracy: 1.0\n",
      "loss: 0.004123859107494354, accuracy: 1.0\n",
      "loss: 0.004159703850746155, accuracy: 1.0\n",
      "loss: 0.004070774186402559, accuracy: 1.0\n",
      "loss: 0.00404023751616478, accuracy: 1.0\n",
      "loss: 0.004115890711545944, accuracy: 1.0\n",
      "loss: 0.00422033341601491, accuracy: 1.0\n",
      "loss: 0.0041663432493805885, accuracy: 1.0\n",
      "loss: 0.004127955995500088, accuracy: 1.0\n",
      "loss: 0.00417740736156702, accuracy: 1.0\n",
      "loss: 0.004110697656869888, accuracy: 1.0\n",
      "loss: 0.004098296165466309, accuracy: 1.0\n",
      "loss: 0.00410003587603569, accuracy: 1.0\n",
      "loss: 0.004132888745516539, accuracy: 1.0\n",
      "loss: 0.004209658596664667, accuracy: 1.0\n",
      "loss: 0.004046915099024773, accuracy: 1.0\n",
      "loss: 0.004139207303524017, accuracy: 1.0\n",
      "loss: 0.004060651641339064, accuracy: 1.0\n",
      "loss: 0.004105664324015379, accuracy: 1.0\n",
      "loss: 0.004146102350205183, accuracy: 1.0\n",
      "loss: 0.004197487607598305, accuracy: 1.0\n",
      "loss: 0.004126477520912886, accuracy: 1.0\n",
      "loss: 0.00409688102081418, accuracy: 1.0\n",
      "loss: 0.004115777090191841, accuracy: 1.0\n",
      "loss: 0.00410148361697793, accuracy: 1.0\n",
      "loss: 0.004138403106480837, accuracy: 1.0\n",
      "loss: 0.004111974500119686, accuracy: 1.0\n",
      "loss: 0.004078970290720463, accuracy: 1.0\n",
      "loss: 0.004087039735168219, accuracy: 1.0\n",
      "loss: 0.004137580282986164, accuracy: 1.0\n",
      "loss: 0.0040600914508104324, accuracy: 1.0\n",
      "loss: 0.004090394824743271, accuracy: 1.0\n",
      "loss: 0.004168636631220579, accuracy: 1.0\n",
      "loss: 0.0041462392546236515, accuracy: 1.0\n",
      "loss: 0.004145303275436163, accuracy: 1.0\n",
      "loss: 0.004064193461090326, accuracy: 1.0\n",
      "loss: 0.004082163330167532, accuracy: 1.0\n",
      "loss: 0.004062920808792114, accuracy: 1.0\n",
      "loss: 0.0041272761300206184, accuracy: 1.0\n",
      "loss: 0.0040740082040429115, accuracy: 1.0\n",
      "loss: 0.004101450089365244, accuracy: 1.0\n",
      "loss: 0.0041200220584869385, accuracy: 1.0\n",
      "loss: 0.004028548486530781, accuracy: 1.0\n",
      "loss: 0.004083301872014999, accuracy: 1.0\n",
      "loss: 0.00403063278645277, accuracy: 1.0\n",
      "loss: 0.004074594937264919, accuracy: 1.0\n",
      "loss: 0.004068695940077305, accuracy: 1.0\n",
      "loss: 0.004077551420778036, accuracy: 1.0\n",
      "loss: 0.004153580404818058, accuracy: 1.0\n",
      "loss: 0.004111606162041426, accuracy: 1.0\n",
      "loss: 0.004085500258952379, accuracy: 1.0\n",
      "loss: 0.004036155063658953, accuracy: 1.0\n",
      "loss: 0.004135925322771072, accuracy: 1.0\n",
      "loss: 0.004071435891091824, accuracy: 1.0\n",
      "loss: 0.004071989096701145, accuracy: 1.0\n",
      "loss: 0.003997455351054668, accuracy: 1.0\n",
      "loss: 0.004092174582183361, accuracy: 1.0\n",
      "loss: 0.0040204450488090515, accuracy: 1.0\n",
      "loss: 0.004013294819742441, accuracy: 1.0\n",
      "loss: 0.004121487960219383, accuracy: 1.0\n",
      "loss: 0.004028064198791981, accuracy: 1.0\n",
      "loss: 0.004129540175199509, accuracy: 1.0\n",
      "loss: 0.004036372061818838, accuracy: 1.0\n",
      "loss: 0.004068272653967142, accuracy: 1.0\n",
      "loss: 0.00409459276124835, accuracy: 1.0\n",
      "loss: 0.0040315198712050915, accuracy: 1.0\n",
      "loss: 0.00413165520876646, accuracy: 1.0\n",
      "loss: 0.004061498679220676, accuracy: 1.0\n",
      "loss: 0.004068915732204914, accuracy: 1.0\n",
      "loss: 0.0040404885075986385, accuracy: 1.0\n",
      "loss: 0.004086182918399572, accuracy: 1.0\n",
      "loss: 0.004043419845402241, accuracy: 1.0\n",
      "loss: 0.0040281699039042, accuracy: 1.0\n",
      "loss: 0.004034124314785004, accuracy: 1.0\n",
      "loss: 0.0041094087064266205, accuracy: 1.0\n",
      "loss: 0.0040574632585048676, accuracy: 1.0\n",
      "loss: 0.004019970539957285, accuracy: 1.0\n",
      "loss: 0.004143571015447378, accuracy: 1.0\n",
      "loss: 0.004116423893719912, accuracy: 1.0\n",
      "loss: 0.00413488456979394, accuracy: 1.0\n",
      "loss: 0.00422904035076499, accuracy: 1.0\n",
      "loss: 0.004199226852506399, accuracy: 1.0\n",
      "loss: 0.004018885083496571, accuracy: 1.0\n",
      "loss: 0.0042806388810276985, accuracy: 1.0\n",
      "loss: 0.004022035747766495, accuracy: 1.0\n",
      "loss: 0.0040953923016786575, accuracy: 1.0\n",
      "loss: 0.0040829177014529705, accuracy: 1.0\n",
      "loss: 0.004086129833012819, accuracy: 1.0\n",
      "loss: 0.004065335262566805, accuracy: 1.0\n",
      "loss: 0.0040599387139081955, accuracy: 1.0\n",
      "loss: 0.003996105864644051, accuracy: 1.0\n",
      "loss: 0.00395751791074872, accuracy: 1.0\n",
      "loss: 0.004051348194479942, accuracy: 1.0\n",
      "loss: 0.004027241375297308, accuracy: 1.0\n",
      "loss: 0.0040212273597717285, accuracy: 1.0\n",
      "loss: 0.004119283985346556, accuracy: 1.0\n",
      "loss: 0.004074776079505682, accuracy: 1.0\n",
      "loss: 0.004025710746645927, accuracy: 1.0\n",
      "loss: 0.0041426741518080235, accuracy: 1.0\n",
      "loss: 0.004039133433252573, accuracy: 1.0\n",
      "loss: 0.0039777616038918495, accuracy: 1.0\n",
      "loss: 0.004088574554771185, accuracy: 1.0\n",
      "loss: 0.003993787802755833, accuracy: 1.0\n",
      "loss: 0.003971520811319351, accuracy: 1.0\n",
      "loss: 0.003994183614850044, accuracy: 1.0\n",
      "loss: 0.004046271555125713, accuracy: 1.0\n",
      "loss: 0.004042827989906073, accuracy: 1.0\n",
      "loss: 0.004053003154695034, accuracy: 1.0\n",
      "loss: 0.0039521269500255585, accuracy: 1.0\n",
      "loss: 0.004006245639175177, accuracy: 1.0\n",
      "loss: 0.004036945290863514, accuracy: 1.0\n",
      "loss: 0.004091628361493349, accuracy: 1.0\n",
      "loss: 0.004162725992500782, accuracy: 1.0\n",
      "loss: 0.004035762511193752, accuracy: 1.0\n",
      "loss: 0.0040588718838989735, accuracy: 1.0\n",
      "loss: 0.003956364933401346, accuracy: 1.0\n",
      "loss: 0.003975627478212118, accuracy: 1.0\n",
      "loss: 0.003997512627393007, accuracy: 1.0\n",
      "loss: 0.003969473764300346, accuracy: 1.0\n",
      "loss: 0.003990735858678818, accuracy: 1.0\n",
      "loss: 0.003994286991655827, accuracy: 1.0\n",
      "loss: 0.004099131561815739, accuracy: 1.0\n",
      "loss: 0.0040161944925785065, accuracy: 1.0\n",
      "loss: 0.00399089464917779, accuracy: 1.0\n",
      "loss: 0.004077875055372715, accuracy: 1.0\n",
      "loss: 0.004033330827951431, accuracy: 1.0\n",
      "loss: 0.003955534193664789, accuracy: 1.0\n",
      "loss: 0.004006182309240103, accuracy: 1.0\n",
      "loss: 0.004011369775980711, accuracy: 1.0\n",
      "loss: 0.004083843436092138, accuracy: 1.0\n",
      "loss: 0.004020617809146643, accuracy: 1.0\n",
      "loss: 0.004027765244245529, accuracy: 1.0\n",
      "loss: 0.004004638642072678, accuracy: 1.0\n",
      "loss: 0.00397210568189621, accuracy: 1.0\n",
      "loss: 0.0040243868716061115, accuracy: 1.0\n",
      "loss: 0.003949906211346388, accuracy: 1.0\n",
      "loss: 0.004093741066753864, accuracy: 1.0\n",
      "loss: 0.004041380248963833, accuracy: 1.0\n",
      "loss: 0.0039802915416657925, accuracy: 1.0\n",
      "loss: 0.003997321240603924, accuracy: 1.0\n",
      "loss: 0.003932802937924862, accuracy: 1.0\n",
      "loss: 0.004007913637906313, accuracy: 1.0\n",
      "loss: 0.004001081455498934, accuracy: 1.0\n",
      "loss: 0.003993301186710596, accuracy: 1.0\n",
      "loss: 0.003990620374679565, accuracy: 1.0\n",
      "loss: 0.00401756726205349, accuracy: 1.0\n",
      "loss: 0.003963763359934092, accuracy: 1.0\n",
      "loss: 0.003995644394308329, accuracy: 1.0\n",
      "loss: 0.004045641515403986, accuracy: 1.0\n",
      "loss: 0.0039994982071220875, accuracy: 1.0\n",
      "loss: 0.003958492074161768, accuracy: 1.0\n",
      "loss: 0.003916665446013212, accuracy: 1.0\n",
      "loss: 0.003979537636041641, accuracy: 1.0\n",
      "loss: 0.004002568777650595, accuracy: 1.0\n",
      "loss: 0.003954691346734762, accuracy: 1.0\n",
      "loss: 0.004012786317616701, accuracy: 1.0\n",
      "loss: 0.004000909626483917, accuracy: 1.0\n",
      "loss: 0.003980414010584354, accuracy: 1.0\n",
      "loss: 0.004023783840239048, accuracy: 1.0\n",
      "loss: 0.003969998098909855, accuracy: 1.0\n",
      "loss: 0.004010792821645737, accuracy: 1.0\n",
      "loss: 0.004003558773547411, accuracy: 1.0\n",
      "loss: 0.0039630769751966, accuracy: 1.0\n",
      "loss: 0.00398028502240777, accuracy: 1.0\n",
      "loss: 0.003990962170064449, accuracy: 1.0\n",
      "loss: 0.004003081936389208, accuracy: 1.0\n",
      "loss: 0.003946654964238405, accuracy: 1.0\n",
      "loss: 0.004000108223408461, accuracy: 1.0\n",
      "loss: 0.003907958511263132, accuracy: 1.0\n",
      "loss: 0.0040098996832966805, accuracy: 1.0\n",
      "loss: 0.003925287630409002, accuracy: 1.0\n",
      "loss: 0.0039502340368926525, accuracy: 1.0\n",
      "loss: 0.0039306702092289925, accuracy: 1.0\n",
      "loss: 0.003953257575631142, accuracy: 1.0\n",
      "loss: 0.00405231723561883, accuracy: 1.0\n",
      "loss: 0.004040579777210951, accuracy: 1.0\n",
      "loss: 0.003923278301954269, accuracy: 1.0\n",
      "loss: 0.0039061326533555984, accuracy: 1.0\n",
      "loss: 0.004059822764247656, accuracy: 1.0\n",
      "loss: 0.003971415106207132, accuracy: 1.0\n",
      "loss: 0.003948602359741926, accuracy: 1.0\n",
      "loss: 0.003964239731431007, accuracy: 1.0\n",
      "loss: 0.003960371017456055, accuracy: 1.0\n",
      "loss: 0.0038920955266803503, accuracy: 1.0\n",
      "loss: 0.00390865933150053, accuracy: 1.0\n",
      "loss: 0.003952097147703171, accuracy: 1.0\n",
      "loss: 0.004007513634860516, accuracy: 1.0\n",
      "loss: 0.003921551164239645, accuracy: 1.0\n",
      "loss: 0.003927655518054962, accuracy: 1.0\n",
      "loss: 0.003956448286771774, accuracy: 1.0\n",
      "loss: 0.004004931077361107, accuracy: 1.0\n",
      "loss: 0.0039748274721205235, accuracy: 1.0\n",
      "loss: 0.003916195593774319, accuracy: 1.0\n",
      "loss: 0.0040219491347670555, accuracy: 1.0\n",
      "loss: 0.0038953376933932304, accuracy: 1.0\n",
      "loss: 0.003969280514866114, accuracy: 1.0\n",
      "loss: 0.004011667333543301, accuracy: 1.0\n",
      "loss: 0.004006220493465662, accuracy: 1.0\n",
      "loss: 0.0039008629973977804, accuracy: 1.0\n",
      "loss: 0.0039024939760565758, accuracy: 1.0\n",
      "loss: 0.003945923876017332, accuracy: 1.0\n",
      "loss: 0.003963088616728783, accuracy: 1.0\n",
      "loss: 0.003919852431863546, accuracy: 1.0\n",
      "loss: 0.003938073758035898, accuracy: 1.0\n",
      "loss: 0.0039987945929169655, accuracy: 1.0\n",
      "loss: 0.003968909848481417, accuracy: 1.0\n",
      "loss: 0.003960359841585159, accuracy: 1.0\n",
      "loss: 0.0038661484140902758, accuracy: 1.0\n",
      "loss: 0.003904837416484952, accuracy: 1.0\n",
      "loss: 0.003927011042833328, accuracy: 1.0\n",
      "loss: 0.0038876242469996214, accuracy: 1.0\n",
      "loss: 0.004024096764624119, accuracy: 1.0\n",
      "loss: 0.003948799800127745, accuracy: 1.0\n",
      "loss: 0.003907311707735062, accuracy: 1.0\n",
      "loss: 0.0038550233002752066, accuracy: 1.0\n",
      "loss: 0.003938939422369003, accuracy: 1.0\n",
      "loss: 0.003969061654061079, accuracy: 1.0\n",
      "loss: 0.003931709099560976, accuracy: 1.0\n",
      "loss: 0.003897224785760045, accuracy: 1.0\n",
      "loss: 0.0038519715890288353, accuracy: 1.0\n",
      "loss: 0.003921916242688894, accuracy: 1.0\n",
      "loss: 0.0038518505170941353, accuracy: 1.0\n",
      "loss: 0.003946247976273298, accuracy: 1.0\n",
      "loss: 0.0038980781100690365, accuracy: 1.0\n",
      "loss: 0.003913466352969408, accuracy: 1.0\n",
      "loss: 0.003883795812726021, accuracy: 1.0\n",
      "loss: 0.003943490795791149, accuracy: 1.0\n",
      "loss: 0.003946762997657061, accuracy: 1.0\n",
      "loss: 0.003873656503856182, accuracy: 1.0\n",
      "loss: 0.0038643935695290565, accuracy: 1.0\n",
      "loss: 0.003856339957565069, accuracy: 1.0\n",
      "loss: 0.0039148819632828236, accuracy: 1.0\n",
      "loss: 0.0038445808459073305, accuracy: 1.0\n",
      "loss: 0.0038935504853725433, accuracy: 1.0\n",
      "loss: 0.003871469059959054, accuracy: 1.0\n",
      "loss: 0.004058792255818844, accuracy: 1.0\n",
      "loss: 0.003931182902306318, accuracy: 1.0\n",
      "loss: 0.003931113518774509, accuracy: 1.0\n",
      "loss: 0.003904944285750389, accuracy: 1.0\n",
      "loss: 0.004002951551228762, accuracy: 1.0\n",
      "loss: 0.0038661397993564606, accuracy: 1.0\n",
      "loss: 0.003903396660462022, accuracy: 1.0\n",
      "loss: 0.0039028034079819918, accuracy: 1.0\n",
      "loss: 0.0038754702545702457, accuracy: 1.0\n",
      "loss: 0.004094056319445372, accuracy: 1.0\n",
      "loss: 0.0038703728932887316, accuracy: 1.0\n",
      "loss: 0.0038935246411710978, accuracy: 1.0\n",
      "loss: 0.003978301305323839, accuracy: 1.0\n",
      "loss: 0.003895443631336093, accuracy: 1.0\n",
      "loss: 0.003870737738907337, accuracy: 1.0\n",
      "loss: 0.003847521962597966, accuracy: 1.0\n",
      "loss: 0.003988960292190313, accuracy: 1.0\n",
      "loss: 0.003827931359410286, accuracy: 1.0\n",
      "loss: 0.003866867395117879, accuracy: 1.0\n",
      "loss: 0.0038656110409647226, accuracy: 1.0\n",
      "loss: 0.003898909315466881, accuracy: 1.0\n",
      "loss: 0.003955078311264515, accuracy: 1.0\n",
      "loss: 0.004010901320725679, accuracy: 1.0\n",
      "loss: 0.003886880585923791, accuracy: 1.0\n",
      "loss: 0.003881686832755804, accuracy: 1.0\n",
      "loss: 0.003989560529589653, accuracy: 1.0\n",
      "loss: 0.0037984030786901712, accuracy: 1.0\n",
      "loss: 0.0038002310320734978, accuracy: 1.0\n",
      "loss: 0.003857176285237074, accuracy: 1.0\n",
      "loss: 0.003825389314442873, accuracy: 1.0\n",
      "loss: 0.00384688307531178, accuracy: 1.0\n",
      "loss: 0.003848516847938299, accuracy: 1.0\n",
      "loss: 0.0038896745536476374, accuracy: 1.0\n",
      "loss: 0.003929188475012779, accuracy: 1.0\n",
      "loss: 0.003806120716035366, accuracy: 1.0\n",
      "loss: 0.0038047009147703648, accuracy: 1.0\n",
      "loss: 0.003888112958520651, accuracy: 1.0\n",
      "loss: 0.0038502139504998922, accuracy: 1.0\n",
      "loss: 0.003923722077161074, accuracy: 1.0\n",
      "loss: 0.003921933472156525, accuracy: 1.0\n",
      "loss: 0.00388153875246644, accuracy: 1.0\n",
      "loss: 0.0038614035584032536, accuracy: 1.0\n",
      "loss: 0.003791961120441556, accuracy: 1.0\n",
      "loss: 0.0038941451348364353, accuracy: 1.0\n",
      "loss: 0.0038191177882254124, accuracy: 1.0\n",
      "loss: 0.0038820074405521154, accuracy: 1.0\n",
      "loss: 0.0038373523857444525, accuracy: 1.0\n",
      "loss: 0.0038287853822112083, accuracy: 1.0\n",
      "loss: 0.0038661144208163023, accuracy: 1.0\n",
      "loss: 0.003823023522272706, accuracy: 1.0\n",
      "loss: 0.003855078248307109, accuracy: 1.0\n",
      "loss: 0.0038239234127104282, accuracy: 1.0\n",
      "loss: 0.0038454618770629168, accuracy: 1.0\n",
      "loss: 0.0038618354592472315, accuracy: 1.0\n",
      "loss: 0.0038348305970430374, accuracy: 1.0\n",
      "loss: 0.0038468078710138798, accuracy: 1.0\n",
      "loss: 0.0038537036161869764, accuracy: 1.0\n",
      "loss: 0.0038202046416699886, accuracy: 1.0\n",
      "loss: 0.0038412415888160467, accuracy: 1.0\n",
      "loss: 0.003811895614489913, accuracy: 1.0\n",
      "loss: 0.003906464669853449, accuracy: 1.0\n",
      "loss: 0.003942444454878569, accuracy: 1.0\n",
      "loss: 0.003853283589705825, accuracy: 1.0\n",
      "loss: 0.003847389714792371, accuracy: 1.0\n",
      "loss: 0.0038230593781918287, accuracy: 1.0\n",
      "loss: 0.003960511181503534, accuracy: 1.0\n",
      "loss: 0.0037720846012234688, accuracy: 1.0\n",
      "loss: 0.003823807230219245, accuracy: 1.0\n",
      "loss: 0.0037702955305576324, accuracy: 1.0\n",
      "loss: 0.003846429055556655, accuracy: 1.0\n",
      "loss: 0.0038408248219639063, accuracy: 1.0\n",
      "loss: 0.003817829070612788, accuracy: 1.0\n",
      "loss: 0.00384534546174109, accuracy: 1.0\n",
      "loss: 0.003838261356577277, accuracy: 1.0\n",
      "loss: 0.0037746280431747437, accuracy: 1.0\n",
      "loss: 0.003827212378382683, accuracy: 1.0\n",
      "loss: 0.0038299437146633863, accuracy: 1.0\n",
      "loss: 0.0037963143549859524, accuracy: 1.0\n",
      "loss: 0.0038191424682736397, accuracy: 1.0\n",
      "loss: 0.0038563557900488377, accuracy: 1.0\n",
      "loss: 0.0038473918102681637, accuracy: 1.0\n",
      "loss: 0.0038718548603355885, accuracy: 1.0\n",
      "loss: 0.003925625700503588, accuracy: 1.0\n",
      "loss: 0.0038426178507506847, accuracy: 1.0\n",
      "loss: 0.0038539564702659845, accuracy: 1.0\n",
      "loss: 0.003835442243143916, accuracy: 1.0\n",
      "loss: 0.0038428176194429398, accuracy: 1.0\n",
      "loss: 0.003839345881715417, accuracy: 1.0\n",
      "loss: 0.003796809818595648, accuracy: 1.0\n",
      "loss: 0.0038542060647159815, accuracy: 1.0\n",
      "loss: 0.003913674037903547, accuracy: 1.0\n",
      "loss: 0.003754601115360856, accuracy: 1.0\n",
      "loss: 0.003836844116449356, accuracy: 1.0\n",
      "loss: 0.003862078068777919, accuracy: 1.0\n",
      "loss: 0.0037936551962047815, accuracy: 1.0\n",
      "loss: 0.0038194588851183653, accuracy: 1.0\n",
      "loss: 0.0038943951949477196, accuracy: 1.0\n",
      "loss: 0.003832467133179307, accuracy: 1.0\n",
      "loss: 0.0038872649893164635, accuracy: 1.0\n",
      "loss: 0.0037818741984665394, accuracy: 1.0\n",
      "loss: 0.0038934482727199793, accuracy: 1.0\n",
      "loss: 0.003894548863172531, accuracy: 1.0\n",
      "loss: 0.0038326336070895195, accuracy: 1.0\n",
      "loss: 0.0037819959688931704, accuracy: 1.0\n",
      "loss: 0.003842125879600644, accuracy: 1.0\n",
      "loss: 0.003748967545107007, accuracy: 1.0\n",
      "loss: 0.0038269376382231712, accuracy: 1.0\n",
      "loss: 0.0038260510191321373, accuracy: 1.0\n",
      "loss: 0.0037918337620794773, accuracy: 1.0\n",
      "loss: 0.0038885956164449453, accuracy: 1.0\n",
      "loss: 0.003766888054087758, accuracy: 1.0\n",
      "loss: 0.003772169118747115, accuracy: 1.0\n",
      "loss: 0.0038326652720570564, accuracy: 1.0\n",
      "loss: 0.003792018862441182, accuracy: 1.0\n",
      "loss: 0.003792133182287216, accuracy: 1.0\n",
      "loss: 0.0037791833747178316, accuracy: 1.0\n",
      "loss: 0.0037534437142312527, accuracy: 1.0\n",
      "loss: 0.0038807864766567945, accuracy: 1.0\n",
      "loss: 0.0037855589762330055, accuracy: 1.0\n",
      "loss: 0.003766599576920271, accuracy: 1.0\n",
      "loss: 0.0037629452999681234, accuracy: 1.0\n",
      "loss: 0.0037577026523649693, accuracy: 1.0\n",
      "loss: 0.0037413588725030422, accuracy: 1.0\n",
      "loss: 0.0037648070137947798, accuracy: 1.0\n",
      "loss: 0.0038299874868243933, accuracy: 1.0\n",
      "loss: 0.0037640524096786976, accuracy: 1.0\n",
      "loss: 0.0037652812898159027, accuracy: 1.0\n",
      "loss: 0.0038153084460645914, accuracy: 1.0\n",
      "loss: 0.00376750691793859, accuracy: 1.0\n",
      "loss: 0.0037437332794070244, accuracy: 1.0\n",
      "loss: 0.0037678631488233805, accuracy: 1.0\n",
      "loss: 0.0037624225951731205, accuracy: 1.0\n",
      "loss: 0.003777899546548724, accuracy: 1.0\n",
      "loss: 0.0038320182356983423, accuracy: 1.0\n",
      "loss: 0.0037760755512863398, accuracy: 1.0\n",
      "loss: 0.003740085521712899, accuracy: 1.0\n",
      "loss: 0.003755741287022829, accuracy: 1.0\n",
      "loss: 0.003710811259225011, accuracy: 1.0\n",
      "loss: 0.003752676770091057, accuracy: 1.0\n",
      "loss: 0.0037799584679305553, accuracy: 1.0\n",
      "loss: 0.0037432292010635138, accuracy: 1.0\n",
      "loss: 0.003779021790251136, accuracy: 1.0\n",
      "loss: 0.0037605499383062124, accuracy: 1.0\n",
      "loss: 0.003835345385596156, accuracy: 1.0\n",
      "loss: 0.0038395076990127563, accuracy: 1.0\n",
      "loss: 0.0037496089935302734, accuracy: 1.0\n",
      "loss: 0.003843920771032572, accuracy: 1.0\n",
      "loss: 0.003730497322976589, accuracy: 1.0\n",
      "loss: 0.0038821964990347624, accuracy: 1.0\n",
      "loss: 0.003749082563444972, accuracy: 1.0\n",
      "loss: 0.0037287974264472723, accuracy: 1.0\n",
      "loss: 0.0038353470154106617, accuracy: 1.0\n",
      "loss: 0.0037494623102247715, accuracy: 1.0\n",
      "loss: 0.0038054799661040306, accuracy: 1.0\n",
      "loss: 0.0037922498304396868, accuracy: 1.0\n",
      "loss: 0.003701254492625594, accuracy: 1.0\n",
      "loss: 0.0037758296821266413, accuracy: 1.0\n",
      "loss: 0.0037392706144601107, accuracy: 1.0\n",
      "loss: 0.003748904215171933, accuracy: 1.0\n",
      "loss: 0.003775946795940399, accuracy: 1.0\n",
      "loss: 0.0038238607812672853, accuracy: 1.0\n",
      "loss: 0.0037849636282771826, accuracy: 1.0\n",
      "loss: 0.0037211920134723186, accuracy: 1.0\n",
      "loss: 0.003759428858757019, accuracy: 1.0\n",
      "loss: 0.0037412797100842, accuracy: 1.0\n",
      "loss: 0.003759475192055106, accuracy: 1.0\n",
      "loss: 0.0037496371660381556, accuracy: 1.0\n",
      "loss: 0.0037336142268031836, accuracy: 1.0\n",
      "loss: 0.0037933618295937777, accuracy: 1.0\n",
      "loss: 0.0037620447110384703, accuracy: 1.0\n",
      "loss: 0.0037103777285665274, accuracy: 1.0\n",
      "loss: 0.0037739365361630917, accuracy: 1.0\n",
      "loss: 0.003753582714125514, accuracy: 1.0\n",
      "loss: 0.003734432626515627, accuracy: 1.0\n",
      "loss: 0.003860858269035816, accuracy: 1.0\n",
      "loss: 0.003721856279298663, accuracy: 1.0\n",
      "loss: 0.0036773832980543375, accuracy: 1.0\n",
      "loss: 0.0037333713844418526, accuracy: 1.0\n",
      "loss: 0.0037079460453242064, accuracy: 1.0\n",
      "loss: 0.0037741507403552532, accuracy: 1.0\n",
      "loss: 0.0037711351178586483, accuracy: 1.0\n",
      "loss: 0.0038279409054666758, accuracy: 1.0\n",
      "loss: 0.003743199398741126, accuracy: 1.0\n",
      "loss: 0.003700018161907792, accuracy: 1.0\n",
      "loss: 0.0036996612325310707, accuracy: 1.0\n",
      "loss: 0.0037127903196960688, accuracy: 1.0\n",
      "loss: 0.0036509798374027014, accuracy: 1.0\n",
      "loss: 0.003679027082398534, accuracy: 1.0\n",
      "loss: 0.003720186883583665, accuracy: 1.0\n",
      "loss: 0.003812790848314762, accuracy: 1.0\n",
      "loss: 0.0037260078825056553, accuracy: 1.0\n",
      "loss: 0.003722607158124447, accuracy: 1.0\n",
      "loss: 0.003723802510648966, accuracy: 1.0\n",
      "loss: 0.0036839605309069157, accuracy: 1.0\n",
      "loss: 0.0037461144383996725, accuracy: 1.0\n",
      "loss: 0.00368681107647717, accuracy: 1.0\n",
      "loss: 0.0037525028456002474, accuracy: 1.0\n",
      "loss: 0.003708025673404336, accuracy: 1.0\n",
      "loss: 0.0037452506367117167, accuracy: 1.0\n",
      "loss: 0.0037585790269076824, accuracy: 1.0\n",
      "loss: 0.0036991843953728676, accuracy: 1.0\n",
      "loss: 0.0037813596427440643, accuracy: 1.0\n",
      "loss: 0.0037114147562533617, accuracy: 1.0\n",
      "loss: 0.0037216446362435818, accuracy: 1.0\n",
      "loss: 0.003683684393763542, accuracy: 1.0\n",
      "loss: 0.003694473532959819, accuracy: 1.0\n",
      "loss: 0.0037368331104516983, accuracy: 1.0\n",
      "loss: 0.0038063712418079376, accuracy: 1.0\n",
      "loss: 0.0036938327830284834, accuracy: 1.0\n",
      "loss: 0.003769748145714402, accuracy: 1.0\n",
      "loss: 0.0036827248986810446, accuracy: 1.0\n",
      "loss: 0.0037195098120719194, accuracy: 1.0\n",
      "loss: 0.0037269818130880594, accuracy: 1.0\n",
      "loss: 0.003677540458738804, accuracy: 1.0\n",
      "loss: 0.0037720114924013615, accuracy: 1.0\n",
      "loss: 0.0037486704532057047, accuracy: 1.0\n",
      "loss: 0.003734089434146881, accuracy: 1.0\n",
      "loss: 0.003695839550346136, accuracy: 1.0\n",
      "loss: 0.003693982493132353, accuracy: 1.0\n",
      "loss: 0.0036866082809865475, accuracy: 1.0\n",
      "loss: 0.0037349744234234095, accuracy: 1.0\n",
      "loss: 0.0037351232022047043, accuracy: 1.0\n",
      "loss: 0.003752888413146138, accuracy: 1.0\n",
      "loss: 0.003804704174399376, accuracy: 1.0\n",
      "loss: 0.0036987601779401302, accuracy: 1.0\n",
      "loss: 0.003787501249462366, accuracy: 1.0\n",
      "loss: 0.0036630374379456043, accuracy: 1.0\n",
      "loss: 0.0036793199833482504, accuracy: 1.0\n",
      "loss: 0.003724527545273304, accuracy: 1.0\n",
      "loss: 0.0036906360182911158, accuracy: 1.0\n",
      "loss: 0.0037167081609368324, accuracy: 1.0\n",
      "loss: 0.003805701620876789, accuracy: 1.0\n",
      "loss: 0.0036631824914366007, accuracy: 1.0\n",
      "loss: 0.003708931617438793, accuracy: 1.0\n",
      "loss: 0.003785377135500312, accuracy: 1.0\n",
      "loss: 0.003771181683987379, accuracy: 1.0\n",
      "loss: 0.0037196381017565727, accuracy: 1.0\n",
      "loss: 0.0037349234335124493, accuracy: 1.0\n",
      "loss: 0.0036921820137649775, accuracy: 1.0\n",
      "loss: 0.0036537598352879286, accuracy: 1.0\n",
      "loss: 0.0036972954403609037, accuracy: 1.0\n",
      "loss: 0.00366111914627254, accuracy: 1.0\n",
      "loss: 0.003644388634711504, accuracy: 1.0\n",
      "loss: 0.0036904867738485336, accuracy: 1.0\n",
      "loss: 0.003638650057837367, accuracy: 1.0\n",
      "loss: 0.0037232053000479937, accuracy: 1.0\n",
      "loss: 0.003711238270625472, accuracy: 1.0\n",
      "loss: 0.0037611473817378283, accuracy: 1.0\n",
      "loss: 0.003695447463542223, accuracy: 1.0\n",
      "loss: 0.003733138320967555, accuracy: 1.0\n",
      "loss: 0.0036444494035094976, accuracy: 1.0\n",
      "loss: 0.0036570709198713303, accuracy: 1.0\n",
      "loss: 0.0036669813562184572, accuracy: 1.0\n",
      "loss: 0.0036688551772385836, accuracy: 1.0\n",
      "loss: 0.0036678146570920944, accuracy: 1.0\n",
      "loss: 0.003659239038825035, accuracy: 1.0\n",
      "loss: 0.0036834697239100933, accuracy: 1.0\n",
      "loss: 0.0037358489353209734, accuracy: 1.0\n",
      "loss: 0.0036541768349707127, accuracy: 1.0\n",
      "loss: 0.003667795564979315, accuracy: 1.0\n",
      "loss: 0.003639776725322008, accuracy: 1.0\n",
      "loss: 0.0036737737245857716, accuracy: 1.0\n",
      "loss: 0.0036990491207689047, accuracy: 1.0\n",
      "loss: 0.0036634006537497044, accuracy: 1.0\n",
      "loss: 0.0036646693479269743, accuracy: 1.0\n",
      "loss: 0.0036044474691152573, accuracy: 1.0\n",
      "loss: 0.0036604497581720352, accuracy: 1.0\n",
      "loss: 0.0036654744762927294, accuracy: 1.0\n",
      "loss: 0.003728194162249565, accuracy: 1.0\n",
      "loss: 0.003693331265822053, accuracy: 1.0\n",
      "loss: 0.0036824848502874374, accuracy: 1.0\n",
      "loss: 0.003706323681399226, accuracy: 1.0\n",
      "loss: 0.003659400623291731, accuracy: 1.0\n",
      "loss: 0.0036797919310629368, accuracy: 1.0\n",
      "loss: 0.0036903093568980694, accuracy: 1.0\n",
      "loss: 0.0036663662176579237, accuracy: 1.0\n",
      "loss: 0.00366253312677145, accuracy: 1.0\n",
      "loss: 0.0036432703491300344, accuracy: 1.0\n",
      "loss: 0.0036434591747820377, accuracy: 1.0\n",
      "loss: 0.003677952103316784, accuracy: 1.0\n",
      "loss: 0.0036349096335470676, accuracy: 1.0\n",
      "loss: 0.003679570509120822, accuracy: 1.0\n",
      "loss: 0.0036908925976604223, accuracy: 1.0\n",
      "loss: 0.003668842138722539, accuracy: 1.0\n",
      "loss: 0.003659341251477599, accuracy: 1.0\n",
      "loss: 0.0036101634614169598, accuracy: 1.0\n",
      "loss: 0.003666702890768647, accuracy: 1.0\n",
      "loss: 0.0036521973088383675, accuracy: 1.0\n",
      "loss: 0.003614213317632675, accuracy: 1.0\n",
      "loss: 0.0036068193148821592, accuracy: 1.0\n",
      "loss: 0.003589262720197439, accuracy: 1.0\n",
      "loss: 0.0037787293549627066, accuracy: 1.0\n",
      "loss: 0.0036592816468328238, accuracy: 1.0\n",
      "loss: 0.0036709364503622055, accuracy: 1.0\n",
      "loss: 0.003694667248055339, accuracy: 1.0\n",
      "loss: 0.0036682751961052418, accuracy: 1.0\n",
      "loss: 0.00370419560931623, accuracy: 1.0\n",
      "loss: 0.003699399996548891, accuracy: 1.0\n",
      "loss: 0.0037093358114361763, accuracy: 1.0\n",
      "loss: 0.0036910485941916704, accuracy: 1.0\n",
      "loss: 0.00361698679625988, accuracy: 1.0\n",
      "loss: 0.0036866425070911646, accuracy: 1.0\n",
      "loss: 0.003607766702771187, accuracy: 1.0\n",
      "loss: 0.0037616300396621227, accuracy: 1.0\n",
      "loss: 0.0036467446479946375, accuracy: 1.0\n",
      "loss: 0.0036806734278798103, accuracy: 1.0\n",
      "loss: 0.003670457750558853, accuracy: 1.0\n",
      "loss: 0.003675371641293168, accuracy: 1.0\n",
      "loss: 0.0036502613220363855, accuracy: 1.0\n",
      "loss: 0.0036411061882972717, accuracy: 1.0\n",
      "the training time is: 82.73742485046387\n",
      "accuracy: 0.7433333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_out_1.train_evaluate(q3_out_1_iter, data_test_iter, q3_out_1.u1, q3_out_1.v1, q3_out_1.b1, q3_out_1.u2, q3_out_1.v2, q3_out_1.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6b3335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_out_1.decision_boundary(q3_out_1.u1, q3_out_1.v1, q3_out_1.b1, q3_out_1.u2, q3_out_1.v2, q3_out_1.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "324aa592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# min-max module\n",
    "def plot_decision_boundary(x, y_class, color = ['b', 'r']):\n",
    "    col = []\n",
    "    for i in range(0, len(y_class)):\n",
    "        col.append(color[y_class[i]])\n",
    "    plt.scatter(x[:, 0], x[:, 1], c = col)\n",
    "\n",
    "def max_min(in_0, in_1, out_0, out_1, test_iter): # 可以改成max或者不改\n",
    "    accuracy_rate = [0., 0.]\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_iter:\n",
    "            y_hat_in_0 = in_0.mlqp(x, in_0.u1_max, in_0.v1_max, in_0.b1_max, in_0.u2_max, in_0.v2_max, in_0.b2_max)\n",
    "            y_hat_in_0 = y_hat_in_0.argmax(axis=1)\n",
    "            \n",
    "            y_hat_in_1 = in_1.mlqp(x, in_1.u1_max, in_1.v1_max, in_1.b1_max, in_1.u2_max, in_1.v2_max, in_1.b2_max)\n",
    "            y_hat_in_1 = y_hat_in_1.argmax(axis=1)\n",
    "            \n",
    "            y_hat_out_0 = out_0.mlqp(x, out_0.u1_max, out_0.v1_max, out_0.b1_max, out_0.u2_max, out_0.v2_max, out_0.b2_max)\n",
    "            y_hat_out_0 = y_hat_out_0.argmax(axis=1)\n",
    "            \n",
    "            y_hat_out_1 = out_1.mlqp(x, out_1.u1_max, out_1.v1_max, out_1.b1_max, out_1.u2_max, out_1.v2_max, out_1.b2_max)\n",
    "            y_hat_out_1 = y_hat_out_1.argmax(axis=1)\n",
    "            \n",
    "            y_hat = torch.max(torch.min(y_hat_in_0, y_hat_in_1), torch.min(y_hat_out_0, y_hat_out_1))\n",
    "            \n",
    "            plot_decision_boundary(x, y_hat)\n",
    "            accuracy_rate[0] = accuracy_rate[0] + float((y_hat.type(y.dtype) == y).type(y.dtype).sum())\n",
    "            accuracy_rate[1] = accuracy_rate[1] + y.numel()\n",
    "        print('accuracy: {0}'.format(accuracy_rate[0] / accuracy_rate[1]))\n",
    "        \n",
    "        \n",
    "max_min(q3_in_0, q3_in_1, q3_out_0, q3_out_1, data_test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a8629a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def max_min_boundary(in_0, in_1, out_0, out_1):\n",
    "    x = torch.from_numpy(in_0.create_points(-6, 6, 121)).float()\n",
    "    with torch.no_grad():\n",
    "        y_hat_in_0 = in_0.mlqp(x, in_0.u1_max, in_0.v1_max, in_0.b1_max, in_0.u2_max, in_0.v2_max, in_0.b2_max)\n",
    "        y_hat_in_0 = y_hat_in_0.argmax(axis=1)\n",
    "            \n",
    "        y_hat_in_1 = in_1.mlqp(x, in_1.u1_max, in_1.v1_max, in_1.b1_max, in_1.u2_max, in_1.v2_max, in_1.b2_max)\n",
    "        y_hat_in_1 = y_hat_in_1.argmax(axis=1)\n",
    "            \n",
    "        y_hat_out_0 = out_0.mlqp(x, out_0.u1_max, out_0.v1_max, out_0.b1_max, out_0.u2_max, out_0.v2_max, out_0.b2_max)\n",
    "        y_hat_out_0 = y_hat_out_0.argmax(axis=1)\n",
    "            \n",
    "        y_hat_out_1 = out_1.mlqp(x, out_1.u1_max, out_1.v1_max, out_1.b1_max, out_1.u2_max, out_1.v2_max, out_1.b2_max)\n",
    "        y_hat_out_1 = y_hat_out_1.argmax(axis=1)\n",
    "            \n",
    "        y_hat = torch.max(torch.min(y_hat_in_0, y_hat_in_1), torch.min(y_hat_out_0, y_hat_out_1))\n",
    "        plot_decision_boundary(x, y_hat)\n",
    "        \n",
    "max_min_boundary(q3_in_0, q3_in_1, q3_out_0, q3_out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f42fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
